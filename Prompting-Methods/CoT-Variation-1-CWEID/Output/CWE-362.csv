,Unnamed: 0.1,Unnamed: 0,Access Gained,Attack Origin,Authentication Required,Availability,CVE ID,CVE Page,CWE ID,Complexity,Confidentiality,Integrity,Known Exploits,Publish Date,Score,Summary,Update Date,Vulnerability Classification,add_lines,codeLink,commit_id,commit_message,del_lines,file_name,files_changed,func_after,func_before,lang,lines_after,lines_before,parentID,patch,project,project_after,project_before,vul,vul_func_with_fix,idx,primevul_func_before_fix,primevul_func_after_fix,C1_Description_of_Functionality_In_Context,C2_Description_of_Functionality_Generic,C3_Explanation_of_Vulnerability_In_Context,C4_Explanation_of_Vulnerability_Generic,C5_Explanation_Vulnerability_Fixed_In_Context,C6_Explanation_Vulnerability_Fixed_Generic
0,177805,177805,,Local,Not required,Partial,CVE-2018-19489,https://www.cvedetails.com/cve/CVE-2018-19489/,CWE-362,Low,,,,2018-12-13,2.1,v9fs_wstat in hw/9pfs/9p.c in QEMU allows guest OS users to cause a denial of service (crash) because of a race condition during file renaming.,2019-05-31,DoS,1,https://git.qemu.org/?p=qemu.git;a=commit;h=1d20398694a3b67a388d955b7a945ba4aa90a8a8,1d20398694a3b67a388d955b7a945ba4aa90a8a8,,0,,,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
",C,"    V9fsState *s = pdu->s;
",,5298f4d67a911dd9cefa4c4185eed242074d64c2,"@@ -2855,6 +2855,7 @@ static void coroutine_fn v9fs_wstat(void *opaque)
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
+    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
@@ -2920,7 +2921,9 @@ static void coroutine_fn v9fs_wstat(void *opaque)
         }
     }
     if (v9stat.name.size != 0) {
+        v9fs_path_write_lock(s);
         err = v9fs_complete_rename(pdu, fidp, -1, &v9stat.name);
+        v9fs_path_unlock(s);
         if (err < 0) {
             goto out;
         }",qemu,https://git.qemu.org/?p=qemu.git;a=blob;f=hw/9pfs/9p.c;h=bdf7919abfc517ae90b41c66d965137f1c7ee510;hb=bdf7919abfc517ae90b41c66d965137f1c7ee510,https://git.qemu.org/?p=qemu.git;a=blob;f=hw/9pfs/9p.c;h=267a25533b775b7e9a95c7bbd44a17914842e3cf;hb=267a25533b775b7e9a95c7bbd44a17914842e3cf,1,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
//fix_flaw_line_below:
//    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
",177805,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","The provided code defines a coroutine function named `v9fs_wstat`, which is designed to handle a specific operation related to file status in a V9FS (a virtual file system protocol). It initializes a `V9fsStat` structure and attempts to unmarshal data from a `V9fsPDU` (Protocol Data Unit) using a specified offset and format, extracting a file identifier (`fid`), an unused value, and the file status information. If the unmarshalling process encounters an error, the function will proceed to a cleanup section labeled `out_nofid`, although the complete error handling and cleanup logic is not shown in the provided snippet.","The code defines a coroutine function that processes a file status update within a virtual file system protocol. It initializes a structure to hold file status information and attempts to extract relevant data from a protocol data unit, including a file identifier and an unused value, using a specified offset. The function is designed to handle potential errors during this data extraction process, with a mechanism in place to manage cleanup if the extraction fails. However, the complete error handling and subsequent actions are not fully detailed in the provided code snippet.","The vulnerability identified as CWE-362, which pertains to a race condition, arises from the potential for concurrent access to shared resources without proper synchronization. In the vulnerable code, the function retrieves a protocol data unit without ensuring that the associated state remains valid throughout its processing. The fix introduces a reference to the state associated with the protocol data unit, ensuring that the state is explicitly accessed and managed during the operation. This change helps prevent scenarios where the state could be modified by another thread or process while the function is executing, thereby mitigating the risk of inconsistent or unexpected behavior.","The vulnerability identified as CWE-362, which involves a race condition, occurs when multiple threads or processes access shared resources without proper synchronization, leading to unpredictable behavior. In the vulnerable code, there was a risk that the state associated with a protocol data unit could be altered by another thread while the function was executing, potentially resulting in inconsistent data being processed. The fix addresses this issue by explicitly referencing and managing the state during the operation, ensuring that the function operates on a stable and valid state throughout its execution, thereby reducing the risk of concurrent modification and enhancing overall reliability.","The vulnerability was fixed by introducing a reference to the state associated with the protocol data unit within the function. By adding the line that retrieves the state from the protocol data unit (`V9fsState *s = pdu->s;`), the code ensures that the function has a direct and stable reference to the relevant state throughout its execution. This change mitigates the risk of race conditions by making it clear which state is being accessed, thereby preventing potential modifications by other threads or processes while the function is processing the file status update, thus enhancing the overall safety and reliability of the operation.","The vulnerability was fixed by ensuring that the function explicitly references and manages the state associated with the protocol data unit during its execution. By incorporating a mechanism to retrieve and maintain a stable reference to this state, the code prevents potential race conditions that could arise from concurrent modifications by other threads or processes. This change enhances the reliability of the operation by ensuring that the function operates on a consistent and valid state throughout its execution, thereby reducing the risk of unpredictable behavior and improving overall safety in a multi-threaded environment."
1,177889,177889,,Remote,Not required,Partial,CVE-2015-3196,https://www.cvedetails.com/cve/CVE-2015-3196/,CWE-362,Medium,,,,2015-12-06,4.3,"ssl/s3_clnt.c in OpenSSL 1.0.0 before 1.0.0t, 1.0.1 before 1.0.1p, and 1.0.2 before 1.0.2d, when used for a multi-threaded client, writes the PSK identity hint to an incorrect data structure, which allows remote servers to cause a denial of service (race condition and double free) via a crafted ServerKeyExchange message.",2019-06-13,DoS,1,https://git.openssl.org/?p=openssl.git;a=commit;h=3c66a669dfc7b3792f7af0758ea26fe8502ce70c,3c66a669dfc7b3792f7af0758ea26fe8502ce70c,,1,,,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strdup((char *)p);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
",C,"        s->session->psk_identity = BUF_strndup((char *)p, i);
","        s->session->psk_identity = BUF_strdup((char *)p);
",f8f75a7e4a7746caba9cab843e14ed6a41dfad21,"@@ -2780,7 +2780,7 @@ int ssl3_get_client_key_exchange(SSL *s)
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
-        s->session->psk_identity = BUF_strdup((char *)p);
+        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
",openssl,https://git.openssl.org/?p=openssl.git;a=blob;f=ssl/s3_srvr.c;h=acd3b9e964a239b8b01f7aa84179c8e4f09a0e14;hb=3c66a669dfc7b3792f7af0758ea26fe8502ce70c,https://git.openssl.org/?p=openssl.git;a=blob;f=ssl/s3_srvr.c;h=8885694c02b34522d8acb980727c4a7841716993,1,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
//flaw_line_below:
        s->session->psk_identity = BUF_strdup((char *)p);
//fix_flaw_line_below:
//        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
",177889,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strdup((char *)p);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","The `ssl3_get_client_key_exchange` function is part of the SSL/TLS handshake process, responsible for handling the client's key exchange message. It begins by retrieving the message and determining the key exchange algorithm used (e.g., RSA, DH, ECDH, PSK). Depending on the algorithm, it performs specific operations such as decrypting a premaster secret using RSA, computing a shared secret with DH or ECDH, or handling PSK authentication. The function also includes error handling for various failure scenarios, ensuring that appropriate alerts are sent back to the client in case of issues, and ultimately generates the master secret for session encryption.","The provided code is part of the SSL/TLS handshake process, specifically responsible for processing the client's key exchange message. It identifies the key exchange method used and performs the necessary operations to establish a secure connection, such as decrypting a premaster secret or computing a shared secret based on the chosen algorithm. The code includes robust error handling to manage various failure scenarios, ensuring that appropriate alerts are sent to the client when issues arise. Ultimately, it generates a master secret that is essential for encrypting the session, facilitating secure communication between the client and server.","The vulnerability identified by CWE-362, which pertains to a race condition, arises from the handling of the decryption process in the original code. Specifically, the code does not adequately ensure that the decryption operation and subsequent checks on the decrypted data occur atomically, allowing an attacker to potentially exploit timing discrepancies. In the fixed code, the introduction of constant-time comparisons, such as `constant_time_eq_int_8` and `constant_time_eq_8`, mitigates this risk by ensuring that the execution time does not vary based on the values being compared, thus preventing timing attacks that could reveal information about the decryption's success or failure.","The vulnerability identified by CWE-362, related to race conditions, occurs when the timing of operations allows an attacker to infer information about the success or failure of cryptographic operations. In the vulnerable code, the decryption process and subsequent checks were not executed in a constant-time manner, making them susceptible to timing attacks. The fix involved implementing constant-time comparisons to ensure that the execution time remains consistent regardless of the values being processed. This change effectively prevents attackers from gaining insights into the decryption's outcome based on timing variations, thereby enhancing the security of the cryptographic operations.","The vulnerability was fixed by incorporating constant-time comparison functions, specifically `constant_time_eq_int_8` and `constant_time_eq_8`, to evaluate the results of decryption and version checks. In the original code, the checks for the decryption length and version correctness could reveal information based on execution time, potentially allowing an attacker to exploit timing discrepancies. By using these constant-time functions, the fixed code ensures that the time taken for comparisons does not vary with the input values, thereby preventing timing attacks and enhancing the overall security of the cryptographic operations.","The vulnerability was fixed by implementing constant-time comparison techniques to evaluate the results of cryptographic operations, specifically focusing on decryption success and version checks. In the original code, the timing of these checks could inadvertently leak information to an attacker, allowing them to infer whether a decryption had succeeded or failed based on how long the operations took. By ensuring that the execution time remains consistent regardless of the input values, the revised code effectively mitigates the risk of timing attacks, thereby enhancing the security of the cryptographic processes involved."
2,178347,178347,,Remote,Not required,Partial,CVE-2014-3509,https://www.cvedetails.com/cve/CVE-2014-3509/,CWE-362,Medium,Partial,Partial,,2014-08-13,6.8,"Race condition in the ssl_parse_serverhello_tlsext function in t1_lib.c in OpenSSL 1.0.0 before 1.0.0n and 1.0.1 before 1.0.1i, when multithreading and session resumption are used, allows remote SSL servers to cause a denial of service (memory overwrite and client application crash) or possibly have unspecified other impact by sending Elliptic Curve (EC) Supported Point Formats Extension data.",2017-11-14,DoS,10,https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,,7,,,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       if (!s->hit)
                                {
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                {
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
",C,"                       if (!s->hit)
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
","                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
",0042fb5fd1c9d257d713b15a1f45da05cf5c1c87,"@@ -2647,15 +2647,18 @@ static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
-                       s->session->tlsext_ecpointformatlist_length = 0;
-                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
-                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
+                       if (!s->hit)
                                {
-                               *al = TLS1_AD_INTERNAL_ERROR;
-                               return 0;
+                               s->session->tlsext_ecpointformatlist_length = 0;
+                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
+                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
+                                       {
+                                       *al = TLS1_AD_INTERNAL_ERROR;
+                                       return 0;
+                                       }
+                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
+                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
-                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
-                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;",openssl,https://git.openssl.org/gitweb/?p=openssl.git;a=blob;f=ssl/t1_lib.c;h=749d88d1a9d9a6faa535437c236104497f0d1a66;hb=fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,https://git.openssl.org/gitweb/?p=openssl.git;a=blob;f=ssl/t1_lib.c;h=4374d6aadd3d5b0804d13d338b27408d8102e8b5,1,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
//flaw_line_below:
                       s->session->tlsext_ecpointformatlist_length = 0;
//flaw_line_below:
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
//flaw_line_below:
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
//fix_flaw_line_below:
//                       if (!s->hit)
                                {
//flaw_line_below:
                               *al = TLS1_AD_INTERNAL_ERROR;
//flaw_line_below:
                               return 0;
//fix_flaw_line_below:
//                               s->session->tlsext_ecpointformatlist_length = 0;
//fix_flaw_line_below:
//                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
//fix_flaw_line_below:
//                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
//fix_flaw_line_below:
//                                       {
//fix_flaw_line_below:
//                                       *al = TLS1_AD_INTERNAL_ERROR;
//fix_flaw_line_below:
//                                       return 0;
//fix_flaw_line_below:
//                                       }
//fix_flaw_line_below:
//                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
//fix_flaw_line_below:
//                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
//flaw_line_below:
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
//flaw_line_below:
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
",178347,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                {
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       if (!s->hit)
                                {
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","The `ssl_scan_serverhello_tlsext` function processes the TLS ServerHello message extensions in a secure socket layer (SSL) context. It begins by initializing various parameters and checking for the presence of specific extensions, such as server name indication, application layer protocol negotiation, and session tickets. The function validates the received data, ensuring it adheres to expected formats and lengths, and handles memory allocation for extension data. It also manages flags related to features like heartbeat support and renegotiation. Finally, it updates the pointer to the data buffer and checks for the necessity of a renegotiation indication, returning appropriate error codes when necessary.","The provided code processes the extensions in a TLS ServerHello message during the SSL handshake. It initializes various parameters and checks for specific extensions, validating their formats and sizes. The code manages memory allocation for extension data and updates relevant flags based on the presence of features like server name indication and application layer protocol negotiation. It also ensures compliance with security requirements regarding renegotiation. Throughout the process, it handles potential errors by returning appropriate codes, ensuring that the handshake proceeds correctly while maintaining secure communication standards.","The vulnerability identified by CWE-362, which relates to a race condition, arises from the handling of certain TLS extensions without adequate synchronization, potentially allowing an attacker to exploit timing discrepancies. In the vulnerable code, the absence of checks for the renegotiation extension could lead to unsafe legacy renegotiation scenarios. The fixed code addresses this by ensuring that if the renegotiation extension is not seen, the handshake will fail unless specific options are enabled. This change prevents attackers from exploiting the timing of renegotiation requests, thereby enhancing the security of the SSL/TLS handshake process.","The vulnerability identified by CWE-362 involves a race condition that can occur during the handling of TLS extensions, particularly regarding renegotiation. In the vulnerable code, the absence of checks for the renegotiation extension could allow an attacker to exploit timing discrepancies, potentially leading to unsafe renegotiation scenarios. The fix introduces a mechanism to ensure that if the renegotiation extension is not present, the handshake will fail unless specific options are enabled. This change effectively mitigates the risk of exploitation by enforcing stricter checks, thereby enhancing the overall security of the handshake process.","The vulnerability was fixed by adding a check for the presence of the renegotiation extension within the handshake process. In the updated code, after processing the extensions, it verifies if the renegotiation flag was seen. If the renegotiation extension is not present and specific options are not enabled, the code sets an error code and returns a handshake failure. This is implemented through the condition that checks the `renegotiate_seen` variable alongside the options flags. By enforcing this check, the code prevents unsafe legacy renegotiation, thereby enhancing the security of the TLS handshake.","The vulnerability was addressed by implementing a validation mechanism that checks for the presence of a specific extension related to renegotiation during the handshake process. The updated code ensures that if this extension is not detected, the handshake will fail unless certain security options are explicitly enabled. This change introduces a safeguard against unsafe renegotiation scenarios by enforcing stricter conditions for proceeding with the handshake. As a result, the fix effectively mitigates the risk of exploitation by ensuring that the handshake process adheres to secure renegotiation practices."
3,178408,178408,,Local,Not required,,CVE-2011-4029,https://www.cvedetails.com/cve/CVE-2011-4029/,CWE-362,Medium,Partial,,,2012-07-03,1.9,"The LockServer function in os/utils.c in X.Org xserver before 1.11.2 allows local users to change the permissions of arbitrary files to 444, read those files, and possibly cause a denial of service (removed execution permission) via a symlink attack on a temporary lock file.",2012-07-17,DoS,1,https://cgit.freedesktop.org/xorg/xserver/commit/?id=b67581cf825940fdf52bf2e0af4330e695d724a4,b67581cf825940fdf52bf2e0af4330e695d724a4,,1,,,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) chmod(tmp, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
",C,"  (void) fchmod(lfd, 0444);
","  (void) chmod(tmp, 0444);
",6ba44b91e37622ef8c146d8f2ac92d708a18ed34,"@@ -297,7 +297,7 @@ LockServer(void)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
-  (void) chmod(tmp, 0444);
+  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*",xserver,https://cgit.freedesktop.org/xorg/xserver/tree/os/utils.c?id=b67581cf825940fdf52bf2e0af4330e695d724a4,https://cgit.freedesktop.org/xorg/xserver/tree/os/utils.c?id=6ba44b91e37622ef8c146d8f2ac92d708a18ed34,1,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
//flaw_line_below:
  (void) chmod(tmp, 0444);
//fix_flaw_line_below:
//  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
",178408,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) chmod(tmp, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","The `LockServer` function is designed to manage a locking mechanism for a server process, ensuring that only one instance runs for a given display. It first constructs a temporary lock file path based on predefined constants and the display number. The function attempts to create this temporary file, writing the current process ID (PID) to it, and then changes its permissions. It then tries to rename the temporary file to the final lock file name, checking for existing locks by reading the PID from the lock file and verifying if the process is still active. If the lock is stale or the server is already running, it handles these scenarios appropriately, ultimately ensuring that the server can safely start without conflicts.","The code implements a locking mechanism for a server process to prevent multiple instances from running simultaneously for a specific display. It constructs a temporary file path and attempts to create a file that contains the current process ID. After successfully creating this file, it tries to rename it to a permanent lock file. The code checks for existing locks by reading the process ID from the lock file and verifying if the associated process is still active. If a conflict is detected, it handles the situation by reporting an error or removing stale locks, ensuring safe server operation.","The vulnerability identified as CWE-362, which pertains to race conditions, arises from the potential for a time-of-check to time-of-use flaw in the original code. Specifically, after creating a temporary file to store the process ID, the code checks for an existing lock by reading the lock file and verifying the associated process's status. If another process creates a lock file between these operations, it could lead to multiple instances running simultaneously. The fixed code addresses this by ensuring that the temporary file is created with exclusive access and by checking the lock file's validity before proceeding, thus mitigating the risk of race conditions.","The vulnerability identified as CWE-362, related to race conditions, occurs when a process checks for a condition and then acts on that condition without proper synchronization, allowing another process to intervene in between. In the vulnerable code, after creating a temporary file to store the process ID, the code checks for an existing lock and the status of the associated process. If another instance creates a lock file during this interval, it could lead to multiple processes running simultaneously. The fix ensures exclusive access when creating the temporary file and validates the lock file before proceeding, effectively preventing race conditions.","The vulnerability was fixed by enhancing the locking mechanism to prevent race conditions. The code now uses exclusive file creation with the `O_EXCL` flag when opening the temporary file, ensuring that if another process attempts to create the same file, it will fail immediately. Additionally, the code checks the validity of the lock file by reading the process ID and verifying the associated process's status before proceeding with the lock. This is done through the `kill` function, which checks if the process is still active, thereby ensuring that only one instance can run at a time and preventing conflicts.","The vulnerability was fixed by implementing a more robust locking mechanism that prevents race conditions. The code now ensures that the temporary file is created with exclusive access, which prevents multiple processes from simultaneously creating the same lock file. Additionally, it includes checks to validate the existing lock file by confirming the status of the associated process before proceeding. This approach effectively mitigates the risk of multiple instances running concurrently, as it ensures that any stale or conflicting locks are properly handled, allowing only one active process to operate at a time."
4,178753,178753,,Local,Not required,Complete,CVE-2012-3511,https://www.cvedetails.com/cve/CVE-2012-3511/,CWE-362,High,Complete,Complete,,2012-10-03,6.2,Multiple race conditions in the madvise_remove function in mm/madvise.c in the Linux kernel before 3.4.5 allow local users to cause a denial of service (use-after-free and system crash) via vectors involving a (1) munmap or (2) close system call.,2013-10-23,DoS ,13,https://github.com/torvalds/linux/commit/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,"mm: Hold a file reference in madvise_remove

Otherwise the code races with munmap (causing a use-after-free
of the vma) or with close (causing a use-after-free of the struct
file).

The bug was introduced by commit 90ed52ebe481 (""[PATCH] holepunch: fix
mmap_sem i_mutex deadlock"")

Cc: Hugh Dickins <hugh@veritas.com>
Cc: Miklos Szeredi <mszeredi@suse.cz>
Cc: Badari Pulavarty <pbadari@us.ibm.com>
Cc: Nick Piggin <npiggin@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Andy Lutomirski <luto@amacapital.net>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",4,mm/madvise.c,"{""sha"": ""14d260fa0d17939a2279c244df91789cd30720e4"", ""filename"": ""mm/madvise.c"", ""status"": ""modified"", ""additions"": 14, ""deletions"": 4, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb/mm/madvise.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb/mm/madvise.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/madvise.c?ref=9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb"", ""patch"": ""@@ -15,6 +15,7 @@\n #include <linux/sched.h>\n #include <linux/ksm.h>\n #include <linux/fs.h>\n+#include <linux/file.h>\n \n /*\n  * Any behaviour which results in changes to the vma->vm_flags needs to\n@@ -204,14 +205,16 @@ static long madvise_remove(struct vm_area_struct *vma,\n {\n \tloff_t offset;\n \tint error;\n+\tstruct file *f;\n \n \t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n \n \tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n \t\treturn -EINVAL;\n \n-\tif (!vma->vm_file || !vma->vm_file->f_mapping\n-\t\t|| !vma->vm_file->f_mapping->host) {\n+\tf = vma->vm_file;\n+\n+\tif (!f || !f->f_mapping || !f->f_mapping->host) {\n \t\t\treturn -EINVAL;\n \t}\n \n@@ -221,11 +224,18 @@ static long madvise_remove(struct vm_area_struct *vma,\n \toffset = (loff_t)(start - vma->vm_start)\n \t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n \n-\t/* filesystem's fallocate may need to take i_mutex */\n+\t/*\n+\t * Filesystem's fallocate may need to take i_mutex.  We need to\n+\t * explicitly grab a reference because the vma (and hence the\n+\t * vma's reference to the file) can go away as soon as we drop\n+\t * mmap_sem.\n+\t */\n+\tget_file(f);\n \tup_read(&current->mm->mmap_sem);\n-\terror = do_fallocate(vma->vm_file,\n+\terror = do_fallocate(f,\n \t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n \t\t\t\toffset, end - start);\n+\tfput(f);\n \tdown_read(&current->mm->mmap_sem);\n \treturn error;\n }""}","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/* filesystem's fallocate may need to take i_mutex */
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(vma->vm_file,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
",C,"	struct file *f;
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
	error = do_fallocate(f,
	fput(f);
","	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
	/* filesystem's fallocate may need to take i_mutex */
	error = do_fallocate(vma->vm_file,
",,"@@ -15,6 +15,7 @@
 #include <linux/sched.h>
 #include <linux/ksm.h>
 #include <linux/fs.h>
+#include <linux/file.h>
 
 /*
  * Any behaviour which results in changes to the vma->vm_flags needs to
@@ -204,14 +205,16 @@ static long madvise_remove(struct vm_area_struct *vma,
 {
 	loff_t offset;
 	int error;
+	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
-	if (!vma->vm_file || !vma->vm_file->f_mapping
-		|| !vma->vm_file->f_mapping->host) {
+	f = vma->vm_file;
+
+	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
@@ -221,11 +224,18 @@ static long madvise_remove(struct vm_area_struct *vma,
 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
-	/* filesystem's fallocate may need to take i_mutex */
+	/*
+	 * Filesystem's fallocate may need to take i_mutex.  We need to
+	 * explicitly grab a reference because the vma (and hence the
+	 * vma's reference to the file) can go away as soon as we drop
+	 * mmap_sem.
+	 */
+	get_file(f);
 	up_read(&current->mm->mmap_sem);
-	error = do_fallocate(vma->vm_file,
+	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
+	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }",linux,9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,1b7fa4c27111757789b21bb78543317dad4cfd08,1,"static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
//fix_flaw_line_below:
//	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
//flaw_line_below:
	if (!vma->vm_file || !vma->vm_file->f_mapping
//flaw_line_below:
		|| !vma->vm_file->f_mapping->host) {
//fix_flaw_line_below:
//	f = vma->vm_file;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
//flaw_line_below:
	/* filesystem's fallocate may need to take i_mutex */
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * Filesystem's fallocate may need to take i_mutex.  We need to
//fix_flaw_line_below:
//	 * explicitly grab a reference because the vma (and hence the
//fix_flaw_line_below:
//	 * vma's reference to the file) can go away as soon as we drop
//fix_flaw_line_below:
//	 * mmap_sem.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	get_file(f);
 	up_read(&current->mm->mmap_sem);
//flaw_line_below:
	error = do_fallocate(vma->vm_file,
//fix_flaw_line_below:
//	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
//fix_flaw_line_below:
//	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
",178753,"static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/* filesystem's fallocate may need to take i_mutex */
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(vma->vm_file,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","The `madvise_remove` function is designed to handle memory management advice for a specific virtual memory area (VMA) in a Linux kernel context. It first checks if the VMA has certain flags set, such as being locked or using huge pages, returning an error if so. The function also verifies that the VMA is associated with a valid file and that it has the appropriate shared and writable permissions. If these conditions are met, it calculates the offset for the file operation and temporarily releases the memory map semaphore to perform a file allocation operation that removes a range of bytes (punching a hole) in the associated file, before reacquiring the semaphore and returning any error encountered.","The provided code defines a function that manages memory advice for a specific area in a system's virtual memory. It begins by checking for certain conditions related to memory flags and the validity of the associated file, returning errors if any checks fail. If the conditions are satisfied, it calculates an offset for a file operation. The function then temporarily releases a lock to perform an operation that removes a specified range of bytes from the file, effectively creating a gap in the file's data. Finally, it reacquires the lock and returns any errors encountered during the operation.","The vulnerability identified by CWE-362, which relates to a race condition, arises from the potential for a file reference to be invalidated after releasing a lock on the memory map. In the vulnerable code, the function directly accesses the file associated with the virtual memory area without ensuring that it remains valid after dropping the lock. The fixed code addresses this by explicitly obtaining a reference to the file using `get_file(f)` before releasing the lock, ensuring that the file remains valid during the subsequent operation. This prevents the race condition by maintaining a valid reference to the file throughout the critical section of the code.","The vulnerability identified by CWE-362, which pertains to a race condition, occurs when a resource, such as a file, can be invalidated after a lock is released, leading to potential access violations. In the vulnerable code, the function accesses the file associated with a memory area without ensuring its validity after dropping the lock, creating a window where the file could be modified or deallocated by another thread. The fix involves obtaining a reference to the file before releasing the lock, ensuring that the resource remains valid during critical operations, thus preventing the race condition and enhancing overall stability.","The vulnerability was fixed by introducing a mechanism to maintain a valid reference to the file associated with the virtual memory area. In the updated code, a reference to the file is obtained using `get_file(f)` before releasing the memory map semaphore with `up_read(&current->mm->mmap_sem)`. This ensures that the file remains valid during the critical operation of `do_fallocate`, which modifies the file. After the operation is complete, the reference is released with `fput(f)`, thereby preventing any race conditions that could arise from the file being invalidated while the lock was dropped.","The vulnerability was fixed by implementing a reference counting mechanism to ensure the validity of the resource being accessed. Before releasing the lock on the memory map, the code now explicitly obtains a reference to the associated file, which prevents it from being deallocated or modified by other threads during critical operations. This change ensures that the resource remains valid throughout the operation, thereby eliminating the risk of a race condition. After the operation is completed, the reference is properly released, maintaining the integrity and stability of the system while preventing potential access violations."
5,179043,179043,,Local,Not required,Complete,CVE-2011-2183,https://www.cvedetails.com/cve/CVE-2011-2183/,CWE-362,High,,,,2012-06-13,4.0,"Race condition in the scan_get_next_rmap_item function in mm/ksm.c in the Linux kernel before 2.6.39.3, when Kernel SamePage Merging (KSM) is enabled, allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via a crafted application.",2012-06-14,DoS ,6,https://github.com/torvalds/linux/commit/2b472611a32a72f4a118c069c2d62a1a3f087afd,2b472611a32a72f4a118c069c2d62a1a3f087afd,"ksm: fix NULL pointer dereference in scan_get_next_rmap_item()

Andrea Righi reported a case where an exiting task can race against
ksmd::scan_get_next_rmap_item (http://lkml.org/lkml/2011/6/1/742) easily
triggering a NULL pointer dereference in ksmd.

ksm_scan.mm_slot == &ksm_mm_head with only one registered mm

CPU 1 (__ksm_exit)		CPU 2 (scan_get_next_rmap_item)
 				list_empty() is false
lock				slot == &ksm_mm_head
list_del(slot->mm_list)
(list now empty)
unlock
				lock
				slot = list_entry(slot->mm_list.next)
				(list is empty, so slot is still ksm_mm_head)
				unlock
				slot->mm == NULL ... Oops

Close this race by revalidating that the new slot is not simply the list
head again.

Andrea's test case:

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/mman.h>

#define BUFSIZE getpagesize()

int main(int argc, char **argv)
{
	void *ptr;

	if (posix_memalign(&ptr, getpagesize(), BUFSIZE) < 0) {
		perror(""posix_memalign"");
		exit(1);
	}
	if (madvise(ptr, BUFSIZE, MADV_MERGEABLE) < 0) {
		perror(""madvise"");
		exit(1);
	}
	*(char *)NULL = 0;

	return 0;
}

Reported-by: Andrea Righi <andrea@betterlinux.com>
Tested-by: Andrea Righi <andrea@betterlinux.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
Signed-off-by: Hugh Dickins <hughd@google.com>
Signed-off-by: Chris Wright <chrisw@sous-sol.org>
Cc: <stable@kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,mm/ksm.c,"{""sha"": ""9a68b0cf0a1c4c8009ee25d2990530d7e2927132"", ""filename"": ""mm/ksm.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 0, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/2b472611a32a72f4a118c069c2d62a1a3f087afd/mm/ksm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/2b472611a32a72f4a118c069c2d62a1a3f087afd/mm/ksm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/ksm.c?ref=2b472611a32a72f4a118c069c2d62a1a3f087afd"", ""patch"": ""@@ -1302,6 +1302,12 @@ static struct rmap_item *scan_get_next_rmap_item(struct page **page)\n \t\tslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\n \t\tksm_scan.mm_slot = slot;\n \t\tspin_unlock(&ksm_mmlist_lock);\n+\t\t/*\n+\t\t * Although we tested list_empty() above, a racing __ksm_exit\n+\t\t * of the last mm on the list may have removed it since then.\n+\t\t */\n+\t\tif (slot == &ksm_mm_head)\n+\t\t\treturn NULL;\n next_mm:\n \t\tksm_scan.address = 0;\n \t\tksm_scan.rmap_list = &slot->rmap_list;""}","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
",C,"		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
",,,"@@ -1302,6 +1302,12 @@ static struct rmap_item *scan_get_next_rmap_item(struct page **page)
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
+		/*
+		 * Although we tested list_empty() above, a racing __ksm_exit
+		 * of the last mm on the list may have removed it since then.
+		 */
+		if (slot == &ksm_mm_head)
+			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;",linux,2b472611a32a72f4a118c069c2d62a1a3f087afd,c7cbb02222eccb82bfd42696b01abceddae663f2,1,"static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * Although we tested list_empty() above, a racing __ksm_exit
//fix_flaw_line_below:
//		 * of the last mm on the list may have removed it since then.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (slot == &ksm_mm_head)
//fix_flaw_line_below:
//			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
",179043,"static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","The `scan_get_next_rmap_item` function is designed to traverse and manage memory mapping structures in a Linux kernel context, specifically for Kernel Same-page Merging (KSM). It begins by checking if there are any memory management slots available for scanning. If the current slot is the head, it drains any lingering pages and prepares for the next slot. The function then iterates through virtual memory areas (VMAs) to find mergeable pages, checking their properties and flushing them as necessary. It retrieves the next reverse mapping item (rmap_item) for the identified pages, handling synchronization with locks and ensuring proper memory management, including cleanup of non-mergeable areas.","The provided code implements a function that scans through memory management structures to identify and manage pages eligible for merging in a memory optimization process. It begins by checking for available memory slots and, if necessary, prepares for the next slot by draining lingering pages. The function iterates through virtual memory areas to locate pages that can be merged, checking their properties and performing necessary operations to optimize memory usage. It retrieves relevant mapping information for these pages while ensuring proper synchronization and memory management, including the cleanup of areas that are no longer suitable for merging.","The vulnerability identified as CWE-362, which pertains to a race condition, arises from the potential for concurrent modifications to shared data structures without proper synchronization. In the vulnerable code, after checking if the list of memory management slots is empty, a subsequent operation could remove the last slot due to a racing condition with another thread executing the exit function. This could lead to dereferencing a null or invalid pointer when accessing the slot. The fix introduces an additional check after acquiring a lock to ensure that the slot is still valid, thereby preventing access to potentially invalid memory and ensuring thread safety.","The vulnerability identified as CWE-362 involves a race condition where multiple threads can concurrently modify shared data structures, leading to inconsistent states. In the vulnerable code, after checking for the presence of memory management slots, a subsequent operation could remove the last slot due to another thread's actions, resulting in the potential dereferencing of an invalid pointer. The fix addresses this issue by adding an additional validation step after acquiring a lock, ensuring that the slot remains valid before any further operations are performed, thus preventing access to invalid memory and enhancing thread safety.","The vulnerability was fixed by introducing an additional check after acquiring the lock on the memory management list. Specifically, after the initial check for an empty list, the code now verifies if the current slot is still valid by checking if it is equal to the head of the list. This is done right after the line that updates the slot variable. If the slot is found to be the head, the function returns early, preventing any further operations on an invalid slot. This ensures that the code does not attempt to dereference a potentially null pointer, thereby enhancing thread safety.","The vulnerability was addressed by adding a validation step after acquiring a lock on the shared data structure. This step checks whether the current slot remains valid, ensuring that it has not been removed by another thread during the execution. If the slot is found to be invalid, the function exits early, preventing any further operations that could lead to dereferencing a null or invalid pointer. This modification effectively synchronizes access to the shared resource, thereby mitigating the risk of race conditions and enhancing the overall thread safety of the code."
6,179198,179198,,Local,Not required,Partial,CVE-2013-3302,https://www.cvedetails.com/cve/CVE-2013-3302/,CWE-362,Medium,Partial,Partial,,2013-04-29,4.4,Race condition in the smb_send_rqst function in fs/cifs/transport.c in the Linux kernel before 3.7.2 allows local users to cause a denial of service (NULL pointer dereference and OOPS) or possibly have unspecified other impact via vectors involving a reconnection event.,2013-05-03,DoS ,3,https://github.com/torvalds/linux/commit/ea702b80e0bbb2448e201472127288beb82ca2fe,ea702b80e0bbb2448e201472127288beb82ca2fe,"cifs: move check for NULL socket into smb_send_rqst

Cai reported this oops:

[90701.616664] BUG: unable to handle kernel NULL pointer dereference at 0000000000000028
[90701.625438] IP: [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.632167] PGD fea319067 PUD 103fda4067 PMD 0
[90701.637255] Oops: 0000 [#1] SMP
[90701.640878] Modules linked in: des_generic md4 nls_utf8 cifs dns_resolver binfmt_misc tun sg igb iTCO_wdt iTCO_vendor_support lpc_ich pcspkr i2c_i801 i2c_core i7core_edac edac_core ioatdma dca mfd_core coretemp kvm_intel kvm crc32c_intel microcode sr_mod cdrom ata_generic sd_mod pata_acpi crc_t10dif ata_piix libata megaraid_sas dm_mirror dm_region_hash dm_log dm_mod
[90701.677655] CPU 10
[90701.679808] Pid: 9627, comm: ls Tainted: G        W    3.7.1+ #10 QCI QSSC-S4R/QSSC-S4R
[90701.688950] RIP: 0010:[<ffffffff814a343e>]  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.698383] RSP: 0018:ffff88177b431bb8  EFLAGS: 00010206
[90701.704309] RAX: ffff88177b431fd8 RBX: 00007ffffffff000 RCX: ffff88177b431bec
[90701.712271] RDX: 0000000000000003 RSI: 0000000000000006 RDI: 0000000000000000
[90701.720223] RBP: ffff88177b431bc8 R08: 0000000000000004 R09: 0000000000000000
[90701.728185] R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000001
[90701.736147] R13: ffff88184ef92000 R14: 0000000000000023 R15: ffff88177b431c88
[90701.744109] FS:  00007fd56a1a47c0(0000) GS:ffff88105fc40000(0000) knlGS:0000000000000000
[90701.753137] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
[90701.759550] CR2: 0000000000000028 CR3: 000000104f15f000 CR4: 00000000000007e0
[90701.767512] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
[90701.775465] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
[90701.783428] Process ls (pid: 9627, threadinfo ffff88177b430000, task ffff88185ca4cb60)
[90701.792261] Stack:
[90701.794505]  0000000000000023 ffff88177b431c50 ffff88177b431c38 ffffffffa014fcb1
[90701.802809]  ffff88184ef921bc 0000000000000000 00000001ffffffff ffff88184ef921c0
[90701.811123]  ffff88177b431c08 ffffffff815ca3d9 ffff88177b431c18 ffff880857758000
[90701.819433] Call Trace:
[90701.822183]  [<ffffffffa014fcb1>] smb_send_rqst+0x71/0x1f0 [cifs]
[90701.828991]  [<ffffffff815ca3d9>] ? schedule+0x29/0x70
[90701.834736]  [<ffffffffa014fe6d>] smb_sendv+0x3d/0x40 [cifs]
[90701.841062]  [<ffffffffa014fe96>] smb_send+0x26/0x30 [cifs]
[90701.847291]  [<ffffffffa015801f>] send_nt_cancel+0x6f/0xd0 [cifs]
[90701.854102]  [<ffffffffa015075e>] SendReceive+0x18e/0x360 [cifs]
[90701.860814]  [<ffffffffa0134a78>] CIFSFindFirst+0x1a8/0x3f0 [cifs]
[90701.867724]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs]
[90701.875601]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs]
[90701.883477]  [<ffffffffa01578e6>] cifs_query_dir_first+0x26/0x30 [cifs]
[90701.890869]  [<ffffffffa015480d>] initiate_cifs_search+0xed/0x250 [cifs]
[90701.898354]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.904486]  [<ffffffffa01554cb>] cifs_readdir+0x45b/0x8f0 [cifs]
[90701.911288]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.917410]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.923533]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.929657]  [<ffffffff81195848>] vfs_readdir+0xb8/0xe0
[90701.935490]  [<ffffffff81195b9f>] sys_getdents+0x8f/0x110
[90701.941521]  [<ffffffff815d3b99>] system_call_fastpath+0x16/0x1b
[90701.948222] Code: 66 90 55 65 48 8b 04 25 f0 c6 00 00 48 89 e5 53 48 83 ec 08 83 fe 01 48 8b 98 48 e0 ff ff 48 c7 80 48 e0 ff ff ff ff ff ff 74 22 <48> 8b 47 28 ff 50 68 65 48 8b 14 25 f0 c6 00 00 48 89 9a 48 e0
[90701.970313] RIP  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.977125]  RSP <ffff88177b431bb8>
[90701.981018] CR2: 0000000000000028
[90701.984809] ---[ end trace 24bd602971110a43 ]---

This is likely due to a race vs. a reconnection event.

The current code checks for a NULL socket in smb_send_kvec, but that's
too late. By the time that check is done, the socket will already have
been passed to kernel_setsockopt. Move the check into smb_send_rqst, so
that it's checked earlier.

In truth, this is a bit of a half-assed fix. The -ENOTSOCK error
return here looks like it could bubble back up to userspace. The locking
rules around the ssocket pointer are really unclear as well. There are
cases where the ssocket pointer is changed without holding the srv_mutex,
but I'm not clear whether there's a potential race here yet or not.

This code seems like it could benefit from some fundamental re-think of
how the socket handling should behave. Until then though, this patch
should at least fix the above oops in most cases.

Cc: <stable@vger.kernel.org> # 3.7+
Reported-and-Tested-by: CAI Qian <caiqian@redhat.com>
Signed-off-by: Jeff Layton <jlayton@redhat.com>
Signed-off-by: Steve French <smfrench@gmail.com>",0,fs/cifs/transport.c,"{""sha"": ""1a528680ec5a29e4b59485fba5df54568a1e3787"", ""filename"": ""fs/cifs/transport.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 3, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/ea702b80e0bbb2448e201472127288beb82ca2fe/fs/cifs/transport.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/ea702b80e0bbb2448e201472127288beb82ca2fe/fs/cifs/transport.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/cifs/transport.c?ref=ea702b80e0bbb2448e201472127288beb82ca2fe"", ""patch"": ""@@ -144,9 +144,6 @@ smb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n \n \t*sent = 0;\n \n-\tif (ssocket == NULL)\n-\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */\n-\n \tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n \tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n \tsmb_msg.msg_control = NULL;\n@@ -291,6 +288,9 @@ smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)\n \tstruct socket *ssocket = server->ssocket;\n \tint val = 1;\n \n+\tif (ssocket == NULL)\n+\t\treturn -ENOTSOCK;\n+\n \tcFYI(1, \""Sending smb: smb_len=%u\"", smb_buf_length);\n \tdump_smb(iov[0].iov_base, iov[0].iov_len);\n ""}","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
	if (ssocket == NULL)
		return -ENOTSOCK;

 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
",C,"	if (ssocket == NULL)
		return -ENOTSOCK;

",,,"@@ -144,9 +144,6 @@ smb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,
 
 	*sent = 0;
 
-	if (ssocket == NULL)
-		return -ENOTSOCK; /* BB eventually add reconnect code here */
-
 	smb_msg.msg_name = (struct sockaddr *) &server->dstaddr;
 	smb_msg.msg_namelen = sizeof(struct sockaddr);
 	smb_msg.msg_control = NULL;
@@ -291,6 +288,9 @@ smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
+	if (ssocket == NULL)
+		return -ENOTSOCK;
+
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 ",linux,ea702b80e0bbb2448e201472127288beb82ca2fe,ecccd1248d6e6986130ffcc3b0d003cb46a485c0,1,"smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
//fix_flaw_line_below:
//	if (ssocket == NULL)
//fix_flaw_line_below:
//		return -ENOTSOCK;
//fix_flaw_line_below:
//
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
",179198,"smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
	if (ssocket == NULL)
		return -ENOTSOCK;

 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","The `smb_send_rqst` function is responsible for sending an SMB (Server Message Block) request over a TCP socket to a specified server. It begins by preparing the socket for sending data using TCP_CORK to optimize transmission. The function then sends the initial SMB data using `smb_send_kvec`, followed by iterating through any additional pages of data, sending each one individually. After sending, it uncorks the socket to allow data transmission. If the total bytes sent do not match the expected length, it marks the server's TCP status for reconnection to handle potential partial data issues. Finally, it returns the result of the send operation.","The function facilitates the transmission of a request over a TCP connection to a server. It begins by preparing the socket for efficient data transfer and sends the initial data segment. Following this, it processes and sends additional data pages sequentially. After completing the data transmission, it ensures the socket is properly configured for subsequent communications. If the total amount of data sent does not match the expected size, it updates the server's status to indicate a need for reconnection, thereby preventing issues with incomplete data. Finally, it returns the outcome of the transmission attempt.","The vulnerability identified as CWE-362, which pertains to a race condition, arises when the code attempts to send data over a socket without first verifying that the socket is valid. In the vulnerable code, if the socket is null, the function could still proceed to send data, leading to undefined behavior or crashes. The fix introduces a check for the socket's validity at the beginning of the function, ensuring that if the socket is null, the function returns an error immediately. This prevents any operations on an invalid socket, thereby mitigating the risk of race conditions and enhancing overall stability.","The vulnerability identified as CWE-362, which involves a race condition, occurs when the code attempts to perform operations on a socket without first confirming its validity. In the original implementation, there was a risk of proceeding with data transmission even if the socket was null, potentially leading to crashes or undefined behavior. The fix addresses this issue by introducing a validation check at the start of the function, ensuring that any operations are only executed if the socket is valid. This change effectively prevents the execution of code that could result in race conditions, thereby enhancing the stability and reliability of the function.","The vulnerability was fixed by adding a validation check for the socket at the beginning of the function. Specifically, the code now checks if the socket is null using the condition `if (ssocket == NULL)`, and if this condition is met, it immediately returns an error code. This prevents any subsequent operations, such as sending data or setting socket options, from being executed on an invalid socket. By ensuring that the function only proceeds with a valid socket, the fix effectively mitigates the risk of race conditions and enhances the overall robustness of the data transmission process.","The vulnerability was addressed by implementing a validation check at the start of the function to ensure the socket is valid before any operations are performed. This check prevents the function from proceeding with data transmission if the socket is null, thereby avoiding potential crashes or undefined behavior. By returning an error immediately when the socket is found to be invalid, the fix ensures that subsequent operations are only executed under safe conditions. This change effectively eliminates the risk of race conditions, enhancing the stability and reliability of the overall data transmission process."
7,179380,179380,,Remote,Not required,Complete,CVE-2011-4348,https://www.cvedetails.com/cve/CVE-2011-4348/,CWE-362,Medium,,,,2013-06-08,7.1,"Race condition in the sctp_rcv function in net/sctp/input.c in the Linux kernel before 2.6.29 allows remote attackers to cause a denial of service (system hang) via SCTP packets.  NOTE: in some environments, this issue exists because of an incomplete fix for CVE-2011-2482.",2013-07-25,DoS ,13,https://github.com/torvalds/linux/commit/ae53b5bd77719fed58086c5be60ce4f22bffe1c6,ae53b5bd77719fed58086c5be60ce4f22bffe1c6,"sctp: Fix another socket race during accept/peeloff

There is a race between sctp_rcv() and sctp_accept() where we
have moved the association from the listening socket to the
accepted socket, but sctp_rcv() processing cached the old
socket and continues to use it.

The easy solution is to check for the socket mismatch once we've
grabed the socket lock.  If we hit a mis-match, that means
that were are currently holding the lock on the listening socket,
but the association is refrencing a newly accepted socket.  We need
to drop the lock on the old socket and grab the lock on the new one.

A more proper solution might be to create accepted sockets when
the new association is established, similar to TCP.  That would
eliminate the race for 1-to-1 style sockets, but it would still
existing for 1-to-many sockets where a user wished to peeloff an
association.  For now, we'll live with this easy solution as
it addresses the problem.

Reported-by: Michal Hocko <mhocko@suse.cz>
Reported-by: Karsten Keil <kkeil@suse.de>
Signed-off-by: Vlad Yasevich <vladislav.yasevich@hp.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",0,net/sctp/input.c,"{""sha"": ""2e4a8646dbc389dcb55fcce1cf6b3ad0f608af0d"", ""filename"": ""net/sctp/input.c"", ""status"": ""modified"", ""additions"": 13, ""deletions"": 0, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/ae53b5bd77719fed58086c5be60ce4f22bffe1c6/net/sctp/input.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/ae53b5bd77719fed58086c5be60ce4f22bffe1c6/net/sctp/input.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/sctp/input.c?ref=ae53b5bd77719fed58086c5be60ce4f22bffe1c6"", ""patch"": ""@@ -249,6 +249,19 @@ int sctp_rcv(struct sk_buff *skb)\n \t */\n \tsctp_bh_lock_sock(sk);\n \n+\tif (sk != rcvr->sk) {\n+\t\t/* Our cached sk is different from the rcvr->sk.  This is\n+\t\t * because migrate()/accept() may have moved the association\n+\t\t * to a new socket and released all the sockets.  So now we\n+\t\t * are holding a lock on the old socket while the user may\n+\t\t * be doing something with the new socket.  Switch our veiw\n+\t\t * of the current sk.\n+\t\t */\n+\t\tsctp_bh_unlock_sock(sk);\n+\t\tsk = rcvr->sk;\n+\t\tsctp_bh_lock_sock(sk);\n+\t}\n+\n \tif (sock_owned_by_user(sk)) {\n \t\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);\n \t\tsctp_add_backlog(sk, skb);""}","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
",C,"	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

",,,"@@ -249,6 +249,19 @@ int sctp_rcv(struct sk_buff *skb)
 	 */
 	sctp_bh_lock_sock(sk);
 
+	if (sk != rcvr->sk) {
+		/* Our cached sk is different from the rcvr->sk.  This is
+		 * because migrate()/accept() may have moved the association
+		 * to a new socket and released all the sockets.  So now we
+		 * are holding a lock on the old socket while the user may
+		 * be doing something with the new socket.  Switch our veiw
+		 * of the current sk.
+		 */
+		sctp_bh_unlock_sock(sk);
+		sk = rcvr->sk;
+		sctp_bh_lock_sock(sk);
+	}
+
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);",linux,ae53b5bd77719fed58086c5be60ce4f22bffe1c6,759af00ebef858015eb68876ac1f383bcb6a1774,1,"int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
//fix_flaw_line_below:
//	if (sk != rcvr->sk) {
//fix_flaw_line_below:
//		/* Our cached sk is different from the rcvr->sk.  This is
//fix_flaw_line_below:
//		 * because migrate()/accept() may have moved the association
//fix_flaw_line_below:
//		 * to a new socket and released all the sockets.  So now we
//fix_flaw_line_below:
//		 * are holding a lock on the old socket while the user may
//fix_flaw_line_below:
//		 * be doing something with the new socket.  Switch our veiw
//fix_flaw_line_below:
//		 * of the current sk.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		sctp_bh_unlock_sock(sk);
//fix_flaw_line_below:
//		sk = rcvr->sk;
//fix_flaw_line_below:
//		sctp_bh_lock_sock(sk);
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
",179380,"int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","The `sctp_rcv` function processes incoming SCTP (Stream Control Transmission Protocol) packets by first validating the packet type and performing checksum verification. It extracts the SCTP header and addresses, ensuring they are valid and unicast. The function then attempts to identify the associated SCTP association or endpoint. If the packet is unrecognized, it checks for ""out of the blue"" packets and applies necessary security policies. Valid packets are chunkified and queued for processing, while maintaining socket locks to manage concurrent access. Finally, it handles cleanup and statistics updates, discarding invalid packets as needed.","The code processes incoming packets for a specific transport protocol by validating their type and ensuring they meet checksum requirements. It extracts necessary headers and addresses, confirming they are valid and unicast. The function attempts to identify the relevant communication association or endpoint, handling cases where the packet is unrecognized. It applies security policies and manages concurrent access to resources. Valid packets are organized into manageable structures and queued for further processing, while invalid packets are discarded, and statistics are updated to reflect the handling of packets, including any discards or successful receptions.","The vulnerability identified as CWE-362, which pertains to race conditions, arises from the potential for concurrent access to shared resources without proper synchronization. In the vulnerable code, the socket structure could be accessed by multiple threads, leading to inconsistencies if one thread modifies the socket while another is processing it. The fix introduces a check to ensure that the socket being accessed is the current one, allowing the code to switch to the correct socket if it has changed due to operations like migration or acceptance. This prevents the risk of operating on stale or invalid socket references, thereby enhancing thread safety.","The vulnerability identified as CWE-362, related to race conditions, occurs when multiple threads access shared resources without adequate synchronization, leading to inconsistent states. In the vulnerable code, a thread could hold a lock on a socket while another thread modifies it, resulting in potential data corruption or unexpected behavior. The fix addresses this issue by introducing a mechanism to verify and switch to the correct socket if it has changed due to operations like migration or acceptance. This ensures that the thread always operates on the current and valid socket, thereby preventing conflicts and enhancing overall thread safety.","The vulnerability was fixed by adding a check to ensure that the socket being accessed is the current one, specifically by comparing the cached socket with the receiver's socket. If the cached socket differs from the receiver's socket, the code first unlocks the current socket and then updates the reference to the receiver's socket. This adjustment allows the function to operate on the correct socket, preventing potential race conditions that could arise from concurrent modifications. By ensuring that the thread always holds a lock on the appropriate socket, the fix enhances the safety and integrity of the socket operations.","The vulnerability was addressed by implementing a mechanism to verify the current socket being accessed during processing. When a thread detects that the cached socket differs from the active socket, it first releases the lock on the cached socket and then updates its reference to the active socket. This ensures that the thread operates on the correct socket, thereby preventing inconsistencies that could arise from concurrent modifications. By maintaining synchronization and ensuring that the thread always interacts with the appropriate socket, the fix effectively mitigates the risk of race conditions and enhances overall thread safety."
8,179565,179565,,Remote,Not required,Complete,CVE-2014-2706,https://www.cvedetails.com/cve/CVE-2014-2706/,CWE-362,Medium,,,,2014-04-14,7.1,"Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c.",2017-07-10,DoS ,15,https://github.com/torvalds/linux/commit/1d147bfa64293b2723c4fec50922168658e613ba,1d147bfa64293b2723c4fec50922168658e613ba,"mac80211: fix AP powersave TX vs. wakeup race

There is a race between the TX path and the STA wakeup: while
a station is sleeping, mac80211 buffers frames until it wakes
up, then the frames are transmitted. However, the RX and TX
path are concurrent, so the packet indicating wakeup can be
processed while a packet is being transmitted.

This can lead to a situation where the buffered frames list
is emptied on the one side, while a frame is being added on
the other side, as the station is still seen as sleeping in
the TX path.

As a result, the newly added frame will not be send anytime
soon. It might be sent much later (and out of order) when the
station goes to sleep and wakes up the next time.

Additionally, it can lead to the crash below.

Fix all this by synchronising both paths with a new lock.
Both path are not fastpath since they handle PS situations.

In a later patch we'll remove the extra skb queue locks to
reduce locking overhead.

BUG: unable to handle kernel
NULL pointer dereference at 000000b0
IP: [<ff6f1791>] ieee80211_report_used_skb+0x11/0x3e0 [mac80211]
*pde = 00000000
Oops: 0000 [#1] SMP DEBUG_PAGEALLOC
EIP: 0060:[<ff6f1791>] EFLAGS: 00210282 CPU: 1
EIP is at ieee80211_report_used_skb+0x11/0x3e0 [mac80211]
EAX: e5900da0 EBX: 00000000 ECX: 00000001 EDX: 00000000
ESI: e41d00c0 EDI: e5900da0 EBP: ebe458e4 ESP: ebe458b0
 DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
CR0: 8005003b CR2: 000000b0 CR3: 25a78000 CR4: 000407d0
DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
DR6: ffff0ff0 DR7: 00000400
Process iperf (pid: 3934, ti=ebe44000 task=e757c0b0 task.ti=ebe44000)
iwlwifi 0000:02:00.0: I iwl_pcie_enqueue_hcmd Sending command LQ_CMD (#4e), seq: 0x0903, 92 bytes at 3[3]:9
Stack:
 e403b32c ebe458c4 00200002 00200286 e403b338 ebe458cc c10960bb e5900da0
 ff76a6ec ebe458d8 00000000 e41d00c0 e5900da0 ebe458f0 ff6f1b75 e403b210
 ebe4598c ff723dc1 00000000 ff76a6ec e597c978 e403b758 00000002 00000002
Call Trace:
 [<ff6f1b75>] ieee80211_free_txskb+0x15/0x20 [mac80211]
 [<ff723dc1>] invoke_tx_handlers+0x1661/0x1780 [mac80211]
 [<ff7248a5>] ieee80211_tx+0x75/0x100 [mac80211]
 [<ff7249bf>] ieee80211_xmit+0x8f/0xc0 [mac80211]
 [<ff72550e>] ieee80211_subif_start_xmit+0x4fe/0xe20 [mac80211]
 [<c149ef70>] dev_hard_start_xmit+0x450/0x950
 [<c14b9aa9>] sch_direct_xmit+0xa9/0x250
 [<c14b9c9b>] __qdisc_run+0x4b/0x150
 [<c149f732>] dev_queue_xmit+0x2c2/0xca0

Cc: stable@vger.kernel.org
Reported-by: Yaara Rozenblum <yaara.rozenblum@intel.com>
Signed-off-by: Emmanuel Grumbach <emmanuel.grumbach@intel.com>
Reviewed-by: Stanislaw Gruszka <sgruszka@redhat.com>
[reword commit log, use a separate lock]
Signed-off-by: Johannes Berg <johannes.berg@intel.com>",0,net/mac80211/tx.c,"{""sha"": ""62a5f0889583437203a1a580b608829997c70ad1"", ""filename"": ""net/mac80211/sta_info.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 0, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/sta_info.c?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -330,6 +330,7 @@ struct sta_info *sta_info_alloc(struct ieee80211_sub_if_data *sdata,\n \trcu_read_unlock();\n \n \tspin_lock_init(&sta->lock);\n+\tspin_lock_init(&sta->ps_lock);\n \tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n \tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n \tmutex_init(&sta->ampdu_mlme.mtx);\n@@ -1109,6 +1110,8 @@ void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n \n \tskb_queue_head_init(&pending);\n \n+\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n+\tspin_lock(&sta->ps_lock);\n \t/* Send all buffered frames to the station */\n \tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n \t\tint count = skb_queue_len(&pending), tmp;\n@@ -1128,6 +1131,7 @@ void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n \t}\n \n \tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n+\tspin_unlock(&sta->ps_lock);\n \n \t/* This station just woke up and isn't aware of our SMPS state */\n \tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,""}<_**next**_>{""sha"": ""d3a6d8208f2f85f7db331238f41c0da2938f0f8d"", ""filename"": ""net/mac80211/sta_info.h"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 4, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/sta_info.h?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -267,6 +267,7 @@ struct ieee80211_tx_latency_stat {\n  * @drv_unblock_wk: used for driver PS unblocking\n  * @listen_interval: listen interval of this station, when we're acting as AP\n  * @_flags: STA flags, see &enum ieee80211_sta_info_flags, do not use directly\n+ * @ps_lock: used for powersave (when mac80211 is the AP) related locking\n  * @ps_tx_buf: buffers (per AC) of frames to transmit to this station\n  *\twhen it leaves power saving state or polls\n  * @tx_filtered: buffers (per AC) of frames we already tried to\n@@ -356,10 +357,8 @@ struct sta_info {\n \t/* use the accessors defined below */\n \tunsigned long _flags;\n \n-\t/*\n-\t * STA powersave frame queues, no more than the internal\n-\t * locking required.\n-\t */\n+\t/* STA powersave lock and frame queues */\n+\tspinlock_t ps_lock;\n \tstruct sk_buff_head ps_tx_buf[IEEE80211_NUM_ACS];\n \tstruct sk_buff_head tx_filtered[IEEE80211_NUM_ACS];\n \tunsigned long driver_buffered_tids;""}<_**next**_>{""sha"": ""4080c615636fabf3d430ecd898d4349ccd213464"", ""filename"": ""net/mac80211/tx.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 0, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/tx.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/tx.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/tx.c?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -478,6 +478,20 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n \t\t       sta->sta.addr, sta->sta.aid, ac);\n \t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n \t\t\tpurge_old_ps_buffers(tx->local);\n+\n+\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n+\t\tspin_lock(&sta->ps_lock);\n+\t\t/*\n+\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n+\t\t * been queued to pending queue. No reordering can happen, go\n+\t\t * ahead and Tx the packet.\n+\t\t */\n+\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n+\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n+\t\t\tspin_unlock(&sta->ps_lock);\n+\t\t\treturn TX_CONTINUE;\n+\t\t}\n+\n \t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n \t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n \t\t\tps_dbg(tx->sdata,\n@@ -492,6 +506,7 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n \t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n \t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n \t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n+\t\tspin_unlock(&sta->ps_lock);\n \n \t\tif (!timer_pending(&local->sta_cleanup))\n \t\t\tmod_timer(&local->sta_cleanup,""}","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);

		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
",C,"
		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

		spin_unlock(&sta->ps_lock);
",,,"@@ -478,6 +478,20 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
+
+		/* sync with ieee80211_sta_ps_deliver_wakeup */
+		spin_lock(&sta->ps_lock);
+		/*
+		 * STA woke up the meantime and all the frames on ps_tx_buf have
+		 * been queued to pending queue. No reordering can happen, go
+		 * ahead and Tx the packet.
+		 */
+		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
+		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
+			spin_unlock(&sta->ps_lock);
+			return TX_CONTINUE;
+		}
+
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
@@ -492,6 +506,7 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
+		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,",linux,1d147bfa64293b2723c4fec50922168658e613ba,50c11eb9982554e9f99b7bab322c517cbe5ce1a1,1,"ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/* sync with ieee80211_sta_ps_deliver_wakeup */
//fix_flaw_line_below:
//		spin_lock(&sta->ps_lock);
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * STA woke up the meantime and all the frames on ps_tx_buf have
//fix_flaw_line_below:
//		 * been queued to pending queue. No reordering can happen, go
//fix_flaw_line_below:
//		 * ahead and Tx the packet.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
//fix_flaw_line_below:
//		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
//fix_flaw_line_below:
//			spin_unlock(&sta->ps_lock);
//fix_flaw_line_below:
//			return TX_CONTINUE;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
//fix_flaw_line_below:
//		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
",179565,"ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);

		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","The function `ieee80211_tx_h_unicast_ps_buf` manages the transmission of unicast packets for stations in power-saving (PS) mode within a wireless network. It first checks if the station (STA) is valid and whether it is in PS mode, while also ensuring that the transmission control flags allow for buffering. If conditions are met, it manages the station's PS buffer by purging old packets if the buffer is full and queues the new packet. It updates transmission information, sets a timer for cleanup, and recalculates the Traffic Indication Map (TIM) to reflect buffered frames. If the station is in PS mode but polling, it proceeds to send the frame.","The code handles the transmission of unicast packets for devices in power-saving mode within a wireless network. It first verifies the validity of the device and checks if it is in power-saving mode while allowing for buffering. If conditions permit, it manages the device's buffer by removing the oldest packet if the buffer is full and adds the new packet to the queue. It updates transmission details, sets a timer for cleanup, and recalculates the traffic indication to reflect the buffered packets. If the device is in power-saving mode but actively polling, it proceeds to send the packet immediately.","The vulnerability identified as CWE-362, which pertains to race conditions, arises when multiple threads access shared data without proper synchronization, potentially leading to inconsistent states. In the vulnerable code, the lack of locking around the access to the power-saving buffer could allow one thread to modify the buffer while another is reading or writing to it, resulting in data corruption or unexpected behavior. The fix introduces a spin lock around critical sections, specifically before checking the power-saving flags and modifying the buffer, ensuring that only one thread can access the shared data at a time, thus preventing race conditions.","The vulnerability identified as CWE-362, related to race conditions, occurs when multiple threads access shared resources without proper synchronization, leading to potential data inconsistencies. In the original code, the absence of locking mechanisms allowed concurrent modifications to a shared buffer, which could result in corrupted data or unexpected behavior if one thread altered the buffer while another was processing it. The fix implemented a locking mechanism around critical sections to ensure that only one thread can access the shared resource at a time, thereby preventing simultaneous access and ensuring data integrity during operations involving the buffer.","The vulnerability was fixed by introducing a spin lock around critical sections of the code that access the power-saving buffer. Specifically, a lock was acquired using `spin_lock(&sta->ps_lock)` before checking the power-saving flags and modifying the buffer. This ensures that only one thread can access the buffer at a time, preventing race conditions. After the necessary operations, the lock is released with `spin_unlock(&sta->ps_lock)`. This change guarantees that the state of the buffer remains consistent and prevents data corruption when multiple threads attempt to read from or write to the buffer simultaneously.","The vulnerability was addressed by implementing a locking mechanism to synchronize access to shared resources. This involved acquiring a lock before any operations that could modify the shared buffer, ensuring that only one thread could access it at a time. By doing so, the code prevents concurrent modifications that could lead to data corruption or inconsistent states. After completing the necessary operations, the lock is released, allowing other threads to access the resource safely. This change effectively mitigates the risk of race conditions, ensuring data integrity during critical operations involving the shared buffer."
9,179567,179567,,Remote,Not required,Complete,CVE-2014-2672,https://www.cvedetails.com/cve/CVE-2014-2672/,CWE-362,Medium,,,,2014-04-01,7.1,Race condition in the ath_tx_aggr_sleep function in drivers/net/wireless/ath/ath9k/xmit.c in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via a large amount of network traffic that triggers certain list deletions.,2014-04-19,DoS ,5,https://github.com/torvalds/linux/commit/21f8aaee0c62708654988ce092838aa7df4d25d8,21f8aaee0c62708654988ce092838aa7df4d25d8,"ath9k: protect tid->sched check

We check tid->sched without a lock taken on ath_tx_aggr_sleep(). That
is race condition which can result of doing list_del(&tid->list) twice
(second time with poisoned list node) and cause crash like shown below:

[424271.637220] BUG: unable to handle kernel paging request at 00100104
[424271.637328] IP: [<f90fc072>] ath_tx_aggr_sleep+0x62/0xe0 [ath9k]
...
[424271.639953] Call Trace:
[424271.639998]  [<f90f6900>] ? ath9k_get_survey+0x110/0x110 [ath9k]
[424271.640083]  [<f90f6942>] ath9k_sta_notify+0x42/0x50 [ath9k]
[424271.640177]  [<f809cfef>] sta_ps_start+0x8f/0x1c0 [mac80211]
[424271.640258]  [<c10f730e>] ? free_compound_page+0x2e/0x40
[424271.640346]  [<f809e915>] ieee80211_rx_handlers+0x9d5/0x2340 [mac80211]
[424271.640437]  [<c112f048>] ? kmem_cache_free+0x1d8/0x1f0
[424271.640510]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640578]  [<c10fc23c>] ? put_page+0x2c/0x40
[424271.640640]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640706]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640787]  [<f809dde3>] ? ieee80211_rx_handlers_result+0x73/0x1d0 [mac80211]
[424271.640897]  [<f80a07a0>] ieee80211_prepare_and_rx_handle+0x520/0xad0 [mac80211]
[424271.641009]  [<f809e22d>] ? ieee80211_rx_handlers+0x2ed/0x2340 [mac80211]
[424271.641104]  [<c13846ce>] ? ip_output+0x7e/0xd0
[424271.641182]  [<f80a1057>] ieee80211_rx+0x307/0x7c0 [mac80211]
[424271.641266]  [<f90fa6ee>] ath_rx_tasklet+0x88e/0xf70 [ath9k]
[424271.641358]  [<f80a0f2c>] ? ieee80211_rx+0x1dc/0x7c0 [mac80211]
[424271.641445]  [<f90f82db>] ath9k_tasklet+0xcb/0x130 [ath9k]

Bug report:
https://bugzilla.kernel.org/show_bug.cgi?id=70551

Reported-and-tested-by: Max Sydorenko <maxim.stargazer@gmail.com>
Cc: stable@vger.kernel.org
Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
Signed-off-by: John W. Linville <linville@tuxdriver.com>",2,drivers/net/wireless/ath/ath9k/xmit.c,"{""sha"": ""4f4ce83f7ab4afd69fb4ed4ae937953711d3343a"", ""filename"": ""drivers/net/wireless/ath/ath9k/xmit.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 3, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/21f8aaee0c62708654988ce092838aa7df4d25d8/drivers/net/wireless/ath/ath9k/xmit.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/21f8aaee0c62708654988ce092838aa7df4d25d8/drivers/net/wireless/ath/ath9k/xmit.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/net/wireless/ath/ath9k/xmit.c?ref=21f8aaee0c62708654988ce092838aa7df4d25d8"", ""patch"": ""@@ -1444,14 +1444,16 @@ void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,\n \tfor (tidno = 0, tid = &an->tid[tidno];\n \t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n \n-\t\tif (!tid->sched)\n-\t\t\tcontinue;\n-\n \t\tac = tid->ac;\n \t\ttxq = ac->txq;\n \n \t\tath_txq_lock(sc, txq);\n \n+\t\tif (!tid->sched) {\n+\t\t\tath_txq_unlock(sc, txq);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tbuffered = ath_tid_has_buffered(tid);\n \n \t\ttid->sched = false;""}","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
		if (!tid->sched)
			continue;
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
",C,"		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

","		if (!tid->sched)
			continue;
",,"@@ -1444,14 +1444,16 @@ void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
-		if (!tid->sched)
-			continue;
-
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
+		if (!tid->sched) {
+			ath_txq_unlock(sc, txq);
+			continue;
+		}
+
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;",linux,21f8aaee0c62708654988ce092838aa7df4d25d8,1c97560f6d751a620978504a4a888c631192b71a,1,"void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
//flaw_line_below:
		if (!tid->sched)
//flaw_line_below:
			continue;
//flaw_line_below:

 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
//fix_flaw_line_below:
//		if (!tid->sched) {
//fix_flaw_line_below:
//			ath_txq_unlock(sc, txq);
//fix_flaw_line_below:
//			continue;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
",179567,"void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
		if (!tid->sched)
			continue;
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","The function `ath_tx_aggr_sleep` is designed to manage the transmission aggregation state of a specific station (`sta`) in a wireless network, particularly when the station is entering a sleep mode. It iterates through all possible Traffic Identifier (TID) values associated with the station, checking if each TID is scheduled for transmission. If a TID is found to be scheduled, it locks the corresponding transmission queue, marks the TID and its associated Access Category (AC) as unscheduled, and updates the buffered status of the station. This ensures that any buffered packets are appropriately managed before the station sleeps, preventing data loss.","The provided code manages the transmission aggregation state of a wireless station when it enters sleep mode. It iterates through all traffic identifiers associated with the station, checking if any are scheduled for transmission. For each scheduled identifier, it locks the corresponding transmission queue, marks the identifier and its associated category as unscheduled, and removes them from their respective lists. After unlocking the queue, it updates the station's buffered status to reflect whether any packets were waiting to be sent. This process ensures that buffered data is properly handled before the station goes to sleep, preventing potential data loss.","The vulnerability identified as CWE-362, which pertains to race conditions, was present in the original code due to the lack of proper locking mechanisms before checking the scheduling state of the transmission identifiers. In the vulnerable code, the function would attempt to access and modify the scheduling state of identifiers without first ensuring that the associated transmission queue was locked, potentially allowing concurrent modifications from other threads. The fixed code addresses this by locking the transmission queue before checking if the identifier is scheduled, ensuring that the state is consistent and preventing race conditions during the execution of the function.","The vulnerability identified as CWE-362, related to race conditions, arose from the original code's failure to properly synchronize access to shared resources. Specifically, the code allowed multiple threads to concurrently check and modify the scheduling state of transmission identifiers without adequate locking, leading to potential inconsistencies and unpredictable behavior. The fixed code resolves this issue by ensuring that the relevant resources are locked before any checks or modifications are made. This change guarantees that only one thread can access and modify the scheduling state at a time, thus preventing race conditions and ensuring data integrity during execution.","The vulnerability was fixed by introducing a locking mechanism around the critical sections of the code that access shared resources. In the revised code, the function now locks the transmission queue using `ath_txq_lock` before checking the scheduling state of the transmission identifiers. This ensures that the state of `tid->sched` is accessed in a thread-safe manner. Additionally, if the identifier is not scheduled, the code immediately unlocks the queue with `ath_txq_unlock` before continuing to the next iteration. This approach prevents concurrent modifications and ensures that the scheduling state remains consistent throughout the execution of the function.","The vulnerability was addressed by implementing a locking mechanism to ensure safe access to shared resources. The revised code now locks the relevant transmission queue before checking the scheduling state of the identifiers, which prevents multiple threads from concurrently modifying the same data. This change ensures that any checks or updates to the scheduling state are performed in a controlled manner, maintaining data integrity. Additionally, the code includes an early unlock and continue mechanism if an identifier is not scheduled, further enhancing thread safety and preventing race conditions during execution."
10,179745,179745,,Local,Not required,Complete,CVE-2015-7990,https://www.cvedetails.com/cve/CVE-2015-7990/,CWE-362,Medium,Partial,Partial,,2015-12-28,5.9,Race condition in the rds_sendmsg function in net/rds/sendmsg.c in the Linux kernel before 4.3.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2015-6937.,2018-10-16,DoS ,3,https://github.com/torvalds/linux/commit/8c7188b23474cca017b3ef354c4a58456f68303a,8c7188b23474cca017b3ef354c4a58456f68303a,"RDS: fix race condition when sending a message on unbound socket

Sasha's found a NULL pointer dereference in the RDS connection code when
sending a message to an apparently unbound socket.  The problem is caused
by the code checking if the socket is bound in rds_sendmsg(), which checks
the rs_bound_addr field without taking a lock on the socket.  This opens a
race where rs_bound_addr is temporarily set but where the transport is not
in rds_bind(), leading to a NULL pointer dereference when trying to
dereference 'trans' in __rds_conn_create().

Vegard wrote a reproducer for this issue, so kindly ask him to share if
you're interested.

I cannot reproduce the NULL pointer dereference using Vegard's reproducer
with this patch, whereas I could without.

Complete earlier incomplete fix to CVE-2015-6937:

  74e98eb08588 (""RDS: verify the underlying transport exists before creating a connection"")

Cc: David S. Miller <davem@davemloft.net>
Cc: stable@vger.kernel.org

Reviewed-by: Vegard Nossum <vegard.nossum@oracle.com>
Reviewed-by: Sasha Levin <sasha.levin@oracle.com>
Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Signed-off-by: Quentin Casasnovas <quentin.casasnovas@oracle.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/rds/send.c,"{""sha"": ""e3b118cae81d5e859e0244df1bf323aaa1798b8e"", ""filename"": ""net/rds/connection.c"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 6, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/connection.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/connection.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/connection.c?ref=8c7188b23474cca017b3ef354c4a58456f68303a"", ""patch"": ""@@ -186,12 +186,6 @@ static struct rds_connection *__rds_conn_create(struct net *net,\n \t\t}\n \t}\n \n-\tif (trans == NULL) {\n-\t\tkmem_cache_free(rds_conn_slab, conn);\n-\t\tconn = ERR_PTR(-ENODEV);\n-\t\tgoto out;\n-\t}\n-\n \tconn->c_trans = trans;\n \n \tret = trans->conn_alloc(conn, gfp);""}<_**next**_>{""sha"": ""c9cdb358ea885e3e356cc675b579f1313ed94ff9"", ""filename"": ""net/rds/send.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 1, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/send.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/send.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/send.c?ref=8c7188b23474cca017b3ef354c4a58456f68303a"", ""patch"": ""@@ -1013,11 +1013,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n \t\trelease_sock(sk);\n \t}\n \n-\t/* racing with another thread binding seems ok here */\n+\tlock_sock(sk);\n \tif (daddr == 0 || rs->rs_bound_addr == 0) {\n+\t\trelease_sock(sk);\n \t\tret = -ENOTCONN; /* XXX not a great errno */\n \t\tgoto out;\n \t}\n+\trelease_sock(sk);\n \n \tif (payload_len > rds_sk_sndbuf(rs)) {\n \t\tret = -EMSGSIZE;""}","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	/* racing with another thread binding seems ok here */
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
",C,"	lock_sock(sk);
		release_sock(sk);
	release_sock(sk);
","	/* racing with another thread binding seems ok here */
",,"@@ -1013,11 +1013,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 		release_sock(sk);
 	}
 
-	/* racing with another thread binding seems ok here */
+	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
+		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
+	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;",linux,8c7188b23474cca017b3ef354c4a58456f68303a,20f795666d3accbb75969730019aeb03f50ef0ec,1,"int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
//flaw_line_below:
	/* racing with another thread binding seems ok here */
//fix_flaw_line_below:
//	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
//fix_flaw_line_below:
//		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
//fix_flaw_line_below:
//	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
",179745,"int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	/* racing with another thread binding seems ok here */
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","The `rds_sendmsg` function is responsible for sending messages over a socket using the Reliable Datagram Sockets (RDS) protocol. It begins by validating the message parameters and extracting the destination address and port. The function checks for connection status and buffer size constraints before allocating a message structure to hold the data. It handles control messages, manages connection states, and ensures proper queuing of messages for transmission. If the send operation cannot proceed immediately, it may block or return an error based on the non-blocking flag. Finally, it cleans up resources and returns the result of the send operation.","The function facilitates the sending of messages over a specific socket using a specialized protocol. It begins by validating the message's parameters and extracting the destination information. The function checks the connection status and ensures that the message size does not exceed the allowed limits. It allocates necessary structures to hold the message data and manages control messages. If immediate sending is not possible, it may block or return an error based on the specified flags. Finally, it cleans up any allocated resources and returns the outcome of the send operation, indicating success or failure.","The vulnerability identified as CWE-362, which pertains to race conditions, arises from the improper handling of socket states during concurrent access. In the vulnerable code, the function checks the connection status and retrieves the destination address without adequately locking the socket, allowing multiple threads to potentially modify the socket state simultaneously. This could lead to inconsistent or invalid states being used for sending messages. The fixed code addresses this by ensuring that the socket is locked before checking the connection status and retrieving the address, thus preventing race conditions and ensuring thread safety.","The vulnerability identified as CWE-362, related to race conditions, occurs when multiple threads access shared resources without proper synchronization, leading to inconsistent states. In the vulnerable code, the function checks the connection status and retrieves necessary information without locking the shared resource, allowing concurrent modifications that could result in invalid data being used for message sending. The fix involves implementing appropriate locking mechanisms before accessing or modifying the shared resource, ensuring that only one thread can interact with it at a time, thus preventing race conditions and ensuring the integrity of the socket's state during operations.","The vulnerability was fixed by introducing proper locking mechanisms around critical sections of the code that access shared resources. Specifically, the function now uses `lock_sock` before checking the connection status and retrieving the destination address, ensuring that no other thread can modify the socket's state during this operation. Additionally, the code ensures that the socket is released with `release_sock` only after the necessary checks are completed. This change prevents race conditions by ensuring that the socket's state remains consistent and valid throughout the message-sending process, thereby enhancing thread safety.","The vulnerability was addressed by implementing proper synchronization mechanisms to control access to shared resources. The code now ensures that critical sections, where the connection status and necessary parameters are checked, are protected by acquiring a lock before any operations are performed. This prevents multiple threads from concurrently modifying the shared state, thereby eliminating the risk of inconsistent or invalid data being used. Additionally, the release of the lock is carefully managed to ensure that it occurs only after all necessary checks are completed, thus maintaining the integrity of the resource throughout the operation."
11,179750,179750,,Local,Not required,Complete,CVE-2015-7613,https://www.cvedetails.com/cve/CVE-2015-7613/,CWE-362,Medium,Complete,Complete,,2015-10-19,6.9,"Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",2016-12-07,+Priv ,7,https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf,b9a532277938798b53178d5a66af6e2915cb27cf,"Initialize msg/shm IPC objects before doing ipc_addid()

As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before
having initialized the IPC object state.  Yes, we initialize the IPC
object in a locked state, but with all the lockless RCU lookup work,
that IPC object lock no longer means that the state cannot be seen.

We already did this for the IPC semaphore code (see commit e8577d1f0329:
""ipc/sem.c: fully initialize sem_array before making it visible"") but we
clearly forgot about msg and shm.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Cc: Manfred Spraul <manfred@colorfullife.com>
Cc: Davidlohr Bueso <dbueso@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",6,ipc/msg.c,"{""sha"": ""1471db9a7e6112b3316ae887b50c6d8d1352f171"", ""filename"": ""ipc/msg.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 7, ""changes"": 14, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/msg.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \t\treturn retval;\n \t}\n \n-\t/* ipc_addid() locks msq upon success. */\n-\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n-\tif (id < 0) {\n-\t\tipc_rcu_putref(msq, msg_rcu_free);\n-\t\treturn id;\n-\t}\n-\n \tmsq->q_stime = msq->q_rtime = 0;\n \tmsq->q_ctime = get_seconds();\n \tmsq->q_cbytes = msq->q_qnum = 0;\n@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \tINIT_LIST_HEAD(&msq->q_receivers);\n \tINIT_LIST_HEAD(&msq->q_senders);\n \n+\t/* ipc_addid() locks msq upon success. */\n+\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n+\tif (id < 0) {\n+\t\tipc_rcu_putref(msq, msg_rcu_free);\n+\t\treturn id;\n+\t}\n+\n \tipc_unlock_object(&msq->q_perm);\n \trcu_read_unlock();\n ""}<_**next**_>{""sha"": ""41787276e14170af7de8261181721991fde528bf"", ""filename"": ""ipc/shm.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 6, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/shm.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -551,12 +551,6 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -565,6 +559,13 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*""}<_**next**_>{""sha"": ""0f401d94b7c657d5e7126fe78f149c94ffea8e24"", ""filename"": ""ipc/util.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/util.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \trcu_read_lock();\n \tspin_lock(&new->lock);\n \n+\tcurrent_euid_egid(&euid, &egid);\n+\tnew->cuid = new->uid = euid;\n+\tnew->gid = new->cgid = egid;\n+\n \tid = idr_alloc(&ids->ipcs_idr, new,\n \t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n \t\t       GFP_NOWAIT);\n@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \n \tids->in_use++;\n \n-\tcurrent_euid_egid(&euid, &egid);\n-\tnew->cuid = new->uid = euid;\n-\tnew->gid = new->cgid = egid;\n-\n \tif (next_id < 0) {\n \t\tnew->seq = ids->seq++;\n \t\tif (ids->seq > IPCID_SEQ_MAX)""}","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
",C,"	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

","	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
",,"@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)
 		return retval;
 	}
 
-	/* ipc_addid() locks msq upon success. */
-	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
-	if (id < 0) {
-		ipc_rcu_putref(msq, msg_rcu_free);
-		return id;
-	}
-
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
+	/* ipc_addid() locks msq upon success. */
+	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
+	if (id < 0) {
+		ipc_rcu_putref(msq, msg_rcu_free);
+		return id;
+	}
+
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 ",linux,b9a532277938798b53178d5a66af6e2915cb27cf,3225031fbeb1e32b269a82eccd815128267a4bfe,1,"static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
//flaw_line_below:
	/* ipc_addid() locks msq upon success. */
//flaw_line_below:
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
//flaw_line_below:
	if (id < 0) {
//flaw_line_below:
		ipc_rcu_putref(msq, msg_rcu_free);
//flaw_line_below:
		return id;
//flaw_line_below:
	}
//flaw_line_below:

 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
//fix_flaw_line_below:
//	/* ipc_addid() locks msq upon success. */
//fix_flaw_line_below:
//	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
//fix_flaw_line_below:
//	if (id < 0) {
//fix_flaw_line_below:
//		ipc_rcu_putref(msq, msg_rcu_free);
//fix_flaw_line_below:
//		return id;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
",179750,"static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","The `newque` function creates a new message queue within a specified IPC (Inter-Process Communication) namespace. It begins by allocating memory for a message queue structure and initializes its permissions and key based on the provided parameters. The function then calls a security check for the message queue allocation. If successful, it adds the message queue to the IPC ID management system, initializing various timestamps and counters related to the queue's state. Finally, it unlocks the IPC object and returns the unique identifier of the newly created message queue, or an error code if any step fails.","The code defines a function that creates a new message queue within a specified communication namespace. It begins by allocating memory for the message queue structure and setting its permissions and key based on input parameters. After performing a security check to ensure proper allocation, it adds the message queue to the system's identifier management. The function initializes various attributes related to the queue's state, such as timestamps and message counts. Finally, it unlocks the associated object and returns the unique identifier of the newly created message queue or an error code if any operation fails.","The vulnerability in the original code, identified as CWE-362 (Race Condition), arises from the timing of operations that manipulate the message queue structure. Specifically, the function allocates memory for the message queue and performs security checks before adding it to the identifier management system. If an error occurs after the allocation but before the addition, the message queue could be accessed by other processes, leading to potential data corruption or unauthorized access. The fixed code addresses this by ensuring that the message queue is only accessible after it has been successfully added to the identifier management, thus preventing race conditions.","The vulnerability identified as CWE-362 (Race Condition) in the original code stems from the improper handling of resource allocation and access control. Specifically, the code allows for the possibility that a resource could be accessed by other processes before it is fully initialized and registered within the system. This timing issue could lead to unauthorized access or manipulation of the resource, resulting in data corruption or security breaches. The fixed code resolves this vulnerability by ensuring that the resource is only accessible after it has been successfully registered, thereby preventing any race conditions during its lifecycle.","The vulnerability was fixed by restructuring the order of operations related to resource management. In the original code, the message queue was allocated and security checks were performed before it was added to the identifier management system. The revised code ensures that the message queue is only accessible after it has been successfully added using the `ipc_addid` function. This change prevents other processes from accessing the message queue until it is fully initialized and registered, thereby eliminating the race condition. Additionally, the locking mechanism is properly utilized to maintain synchronization during these critical operations.","The vulnerability was fixed by modifying the sequence of operations to ensure that the resource is only accessible after it has been fully initialized and registered within the system. The revised approach involves performing necessary checks and allocations before allowing any external access to the resource. By ensuring that the resource is securely registered before any potential access, the fix effectively prevents race conditions that could lead to unauthorized manipulation or data corruption. This change enhances the overall integrity and security of the resource management process, ensuring proper synchronization throughout its lifecycle."
12,179752,179752,,Local,Not required,Complete,CVE-2015-7613,https://www.cvedetails.com/cve/CVE-2015-7613/,CWE-362,Medium,Complete,Complete,,2015-10-19,6.9,"Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",2016-12-07,+Priv ,4,https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf,b9a532277938798b53178d5a66af6e2915cb27cf,"Initialize msg/shm IPC objects before doing ipc_addid()

As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before
having initialized the IPC object state.  Yes, we initialize the IPC
object in a locked state, but with all the lockless RCU lookup work,
that IPC object lock no longer means that the state cannot be seen.

We already did this for the IPC semaphore code (see commit e8577d1f0329:
""ipc/sem.c: fully initialize sem_array before making it visible"") but we
clearly forgot about msg and shm.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Cc: Manfred Spraul <manfred@colorfullife.com>
Cc: Davidlohr Bueso <dbueso@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",3,ipc/util.c,"{""sha"": ""1471db9a7e6112b3316ae887b50c6d8d1352f171"", ""filename"": ""ipc/msg.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 7, ""changes"": 14, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/msg.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \t\treturn retval;\n \t}\n \n-\t/* ipc_addid() locks msq upon success. */\n-\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n-\tif (id < 0) {\n-\t\tipc_rcu_putref(msq, msg_rcu_free);\n-\t\treturn id;\n-\t}\n-\n \tmsq->q_stime = msq->q_rtime = 0;\n \tmsq->q_ctime = get_seconds();\n \tmsq->q_cbytes = msq->q_qnum = 0;\n@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \tINIT_LIST_HEAD(&msq->q_receivers);\n \tINIT_LIST_HEAD(&msq->q_senders);\n \n+\t/* ipc_addid() locks msq upon success. */\n+\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n+\tif (id < 0) {\n+\t\tipc_rcu_putref(msq, msg_rcu_free);\n+\t\treturn id;\n+\t}\n+\n \tipc_unlock_object(&msq->q_perm);\n \trcu_read_unlock();\n ""}<_**next**_>{""sha"": ""41787276e14170af7de8261181721991fde528bf"", ""filename"": ""ipc/shm.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 6, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/shm.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -551,12 +551,6 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -565,6 +559,13 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*""}<_**next**_>{""sha"": ""0f401d94b7c657d5e7126fe78f149c94ffea8e24"", ""filename"": ""ipc/util.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/util.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \trcu_read_lock();\n \tspin_lock(&new->lock);\n \n+\tcurrent_euid_egid(&euid, &egid);\n+\tnew->cuid = new->uid = euid;\n+\tnew->gid = new->cgid = egid;\n+\n \tid = idr_alloc(&ids->ipcs_idr, new,\n \t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n \t\t       GFP_NOWAIT);\n@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \n \tids->in_use++;\n \n-\tcurrent_euid_egid(&euid, &egid);\n-\tnew->cuid = new->uid = euid;\n-\tnew->gid = new->cgid = egid;\n-\n \tif (next_id < 0) {\n \t\tnew->seq = ids->seq++;\n \t\tif (ids->seq > IPCID_SEQ_MAX)""}","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
",C,"	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

","	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
",,"@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
+	current_euid_egid(&euid, &egid);
+	new->cuid = new->uid = euid;
+	new->gid = new->cgid = egid;
+
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
 
 	ids->in_use++;
 
-	current_euid_egid(&euid, &egid);
-	new->cuid = new->uid = euid;
-	new->gid = new->cgid = egid;
-
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)",linux,b9a532277938798b53178d5a66af6e2915cb27cf,3225031fbeb1e32b269a82eccd815128267a4bfe,1,"int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
//fix_flaw_line_below:
//	current_euid_egid(&euid, &egid);
//fix_flaw_line_below:
//	new->cuid = new->uid = euid;
//fix_flaw_line_below:
//	new->gid = new->cgid = egid;
//fix_flaw_line_below:
//
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
//flaw_line_below:
	current_euid_egid(&euid, &egid);
//flaw_line_below:
	new->cuid = new->uid = euid;
//flaw_line_below:
	new->gid = new->cgid = egid;
//flaw_line_below:

 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
",179752,"int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","The `ipc_addid` function is responsible for adding a new IPC (Inter-Process Communication) identifier to a given set of IPC identifiers, ensuring that the system does not exceed predefined limits. It first checks if the current number of identifiers in use is less than the maximum allowed. If space is available, it initializes the lock for the new IPC permission structure, allocates a unique identifier using an IDR (ID Radix tree), and sets the creator's user and group IDs. The function also manages sequence numbers for the identifiers and returns the newly allocated ID or an error code if allocation fails.","The provided code defines a function that adds a new identifier to a collection of inter-process communication identifiers while ensuring that the total count does not exceed a specified limit. It begins by checking the current usage against the maximum allowed and initializes necessary locks for thread safety. If space is available, it allocates a unique identifier and sets the user and group information for the new entry. The function also manages sequence numbers to maintain uniqueness and returns the newly allocated identifier or an error code if the allocation fails, ensuring proper resource management throughout the process.","The vulnerability identified as CWE-362, which pertains to a race condition, arises from the improper ordering of operations in the original code. Specifically, the allocation of a new identifier occurs after the user and group IDs are set, which could allow concurrent threads to manipulate the shared data structure before the lock is acquired. In the fixed code, the setting of the user and group IDs is moved to occur after the lock is acquired, ensuring that the critical section is protected. This change prevents potential data corruption or unauthorized access by ensuring that shared resources are properly synchronized during the identifier allocation process.","The vulnerability identified as CWE-362, which involves a race condition, occurs when multiple threads can access and modify shared data simultaneously without proper synchronization. In the original code, critical operations, such as setting user and group identifiers, were performed before acquiring a lock, allowing potential interference from other threads. The fix involved rearranging the code to ensure that these operations occur only after the lock is acquired, thereby protecting the shared data from concurrent modifications. This change effectively mitigates the risk of data corruption and ensures that the integrity of the shared resource is maintained during identifier allocation.","The vulnerability was fixed by rearranging the order of operations in the code to ensure proper synchronization. In the original version, the user and group identifiers were set before acquiring the lock, which could lead to race conditions. In the revised code, the setting of `new->cuid`, `new->uid`, `new->gid`, and `new->cgid` now occurs after the `spin_lock(&new->lock)` call. This change ensures that the critical section is protected, preventing concurrent threads from accessing and modifying the shared data structure while it is being updated, thus maintaining data integrity during the identifier allocation process.","The vulnerability was fixed by restructuring the code to ensure that critical operations are performed within a properly synchronized context. Specifically, the assignment of user and group identifiers was moved to occur after acquiring the necessary lock, which prevents concurrent access to shared data. This adjustment ensures that the critical section is protected from interference by other threads, thereby eliminating the risk of race conditions. By enforcing this order of operations, the integrity of the shared resource is maintained, and the potential for data corruption or unauthorized modifications is significantly reduced."
13,179797,179797,,Local,Not required,Complete,CVE-2015-3339,https://www.cvedetails.com/cve/CVE-2015-3339/,CWE-362,High,Complete,Complete,,2015-05-27,6.2,"Race condition in the prepare_binprm function in fs/exec.c in the Linux kernel before 3.19.6 allows local users to gain privileges by executing a setuid program at a time instant when a chown to root is in progress, and the ownership is changed but the setuid bit is not yet stripped.",2016-12-30,+Priv ,1,https://github.com/torvalds/linux/commit/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,"fs: take i_mutex during prepare_binprm for set[ug]id executables

This prevents a race between chown() and execve(), where chowning a
setuid-user binary to root would momentarily make the binary setuid
root.

This patch was mostly written by Linus Torvalds.

Signed-off-by: Jann Horn <jann@thejh.net>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",25,fs/exec.c,"{""sha"": ""49a1c61433b73722683cad25eef1fb92045e265a"", ""filename"": ""fs/exec.c"", ""status"": ""modified"", ""additions"": 48, ""deletions"": 28, ""changes"": 76, ""blob_url"": ""https://github.com/torvalds/linux/blob/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543/fs/exec.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543/fs/exec.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/exec.c?ref=8b01fc86b9f425899f8a3a8fc1c47d73c2c20543"", ""patch"": ""@@ -1275,6 +1275,53 @@ static void check_unsafe_exec(struct linux_binprm *bprm)\n \tspin_unlock(&p->fs->lock);\n }\n \n+static void bprm_fill_uid(struct linux_binprm *bprm)\n+{\n+\tstruct inode *inode;\n+\tunsigned int mode;\n+\tkuid_t uid;\n+\tkgid_t gid;\n+\n+\t/* clear any previous set[ug]id data from a previous binary */\n+\tbprm->cred->euid = current_euid();\n+\tbprm->cred->egid = current_egid();\n+\n+\tif (bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID)\n+\t\treturn;\n+\n+\tif (task_no_new_privs(current))\n+\t\treturn;\n+\n+\tinode = file_inode(bprm->file);\n+\tmode = READ_ONCE(inode->i_mode);\n+\tif (!(mode & (S_ISUID|S_ISGID)))\n+\t\treturn;\n+\n+\t/* Be careful if suid/sgid is set */\n+\tmutex_lock(&inode->i_mutex);\n+\n+\t/* reload atomically mode/uid/gid now that lock held */\n+\tmode = inode->i_mode;\n+\tuid = inode->i_uid;\n+\tgid = inode->i_gid;\n+\tmutex_unlock(&inode->i_mutex);\n+\n+\t/* We ignore suid/sgid if there are no mappings for them in the ns */\n+\tif (!kuid_has_mapping(bprm->cred->user_ns, uid) ||\n+\t\t !kgid_has_mapping(bprm->cred->user_ns, gid))\n+\t\treturn;\n+\n+\tif (mode & S_ISUID) {\n+\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n+\t\tbprm->cred->euid = uid;\n+\t}\n+\n+\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n+\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n+\t\tbprm->cred->egid = gid;\n+\t}\n+}\n+\n /*\n  * Fill the binprm structure from the inode.\n  * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes\n@@ -1283,36 +1330,9 @@ static void check_unsafe_exec(struct linux_binprm *bprm)\n  */\n int prepare_binprm(struct linux_binprm *bprm)\n {\n-\tstruct inode *inode = file_inode(bprm->file);\n-\tumode_t mode = inode->i_mode;\n \tint retval;\n \n-\n-\t/* clear any previous set[ug]id data from a previous binary */\n-\tbprm->cred->euid = current_euid();\n-\tbprm->cred->egid = current_egid();\n-\n-\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n-\t    !task_no_new_privs(current) &&\n-\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n-\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n-\t\t/* Set-uid? */\n-\t\tif (mode & S_ISUID) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->euid = inode->i_uid;\n-\t\t}\n-\n-\t\t/* Set-gid? */\n-\t\t/*\n-\t\t * If setgid is set but no group execute bit then this\n-\t\t * is a candidate for mandatory locking, not a setgid\n-\t\t * executable.\n-\t\t */\n-\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->egid = inode->i_gid;\n-\t\t}\n-\t}\n+\tbprm_fill_uid(bprm);\n \n \t/* fill in binprm security blob */\n \tretval = security_bprm_set_creds(bprm);""}"," int prepare_binprm(struct linux_binprm *bprm)
 {
 	int retval;
 
	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
"," int prepare_binprm(struct linux_binprm *bprm)
 {
	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
 	int retval;
 
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
",C,"	bprm_fill_uid(bprm);
","	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
",,"@@ -1275,6 +1275,53 @@ static void check_unsafe_exec(struct linux_binprm *bprm)
 	spin_unlock(&p->fs->lock);
 }
 
+static void bprm_fill_uid(struct linux_binprm *bprm)
+{
+	struct inode *inode;
+	unsigned int mode;
+	kuid_t uid;
+	kgid_t gid;
+
+	/* clear any previous set[ug]id data from a previous binary */
+	bprm->cred->euid = current_euid();
+	bprm->cred->egid = current_egid();
+
+	if (bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID)
+		return;
+
+	if (task_no_new_privs(current))
+		return;
+
+	inode = file_inode(bprm->file);
+	mode = READ_ONCE(inode->i_mode);
+	if (!(mode & (S_ISUID|S_ISGID)))
+		return;
+
+	/* Be careful if suid/sgid is set */
+	mutex_lock(&inode->i_mutex);
+
+	/* reload atomically mode/uid/gid now that lock held */
+	mode = inode->i_mode;
+	uid = inode->i_uid;
+	gid = inode->i_gid;
+	mutex_unlock(&inode->i_mutex);
+
+	/* We ignore suid/sgid if there are no mappings for them in the ns */
+	if (!kuid_has_mapping(bprm->cred->user_ns, uid) ||
+		 !kgid_has_mapping(bprm->cred->user_ns, gid))
+		return;
+
+	if (mode & S_ISUID) {
+		bprm->per_clear |= PER_CLEAR_ON_SETID;
+		bprm->cred->euid = uid;
+	}
+
+	if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
+		bprm->per_clear |= PER_CLEAR_ON_SETID;
+		bprm->cred->egid = gid;
+	}
+}
+
 /*
  * Fill the binprm structure from the inode.
  * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes
@@ -1283,36 +1330,9 @@ static void check_unsafe_exec(struct linux_binprm *bprm)
  */
 int prepare_binprm(struct linux_binprm *bprm)
 {
-	struct inode *inode = file_inode(bprm->file);
-	umode_t mode = inode->i_mode;
 	int retval;
 
-
-	/* clear any previous set[ug]id data from a previous binary */
-	bprm->cred->euid = current_euid();
-	bprm->cred->egid = current_egid();
-
-	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
-	    !task_no_new_privs(current) &&
-	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
-	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
-		/* Set-uid? */
-		if (mode & S_ISUID) {
-			bprm->per_clear |= PER_CLEAR_ON_SETID;
-			bprm->cred->euid = inode->i_uid;
-		}
-
-		/* Set-gid? */
-		/*
-		 * If setgid is set but no group execute bit then this
-		 * is a candidate for mandatory locking, not a setgid
-		 * executable.
-		 */
-		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
-			bprm->per_clear |= PER_CLEAR_ON_SETID;
-			bprm->cred->egid = inode->i_gid;
-		}
-	}
+	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);",linux,8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,5224b9613b91d937c6948fe977023247afbcc04e,1," int prepare_binprm(struct linux_binprm *bprm)
 {
//flaw_line_below:
	struct inode *inode = file_inode(bprm->file);
//flaw_line_below:
	umode_t mode = inode->i_mode;
 	int retval;
 
//flaw_line_below:

//flaw_line_below:
	/* clear any previous set[ug]id data from a previous binary */
//flaw_line_below:
	bprm->cred->euid = current_euid();
//flaw_line_below:
	bprm->cred->egid = current_egid();
//flaw_line_below:

//flaw_line_below:
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
//flaw_line_below:
	    !task_no_new_privs(current) &&
//flaw_line_below:
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
//flaw_line_below:
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
//flaw_line_below:
		/* Set-uid? */
//flaw_line_below:
		if (mode & S_ISUID) {
//flaw_line_below:
			bprm->per_clear |= PER_CLEAR_ON_SETID;
//flaw_line_below:
			bprm->cred->euid = inode->i_uid;
//flaw_line_below:
		}
//flaw_line_below:

//flaw_line_below:
		/* Set-gid? */
//flaw_line_below:
		/*
//flaw_line_below:
		 * If setgid is set but no group execute bit then this
//flaw_line_below:
		 * is a candidate for mandatory locking, not a setgid
//flaw_line_below:
		 * executable.
//flaw_line_below:
		 */
//flaw_line_below:
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
//flaw_line_below:
			bprm->per_clear |= PER_CLEAR_ON_SETID;
//flaw_line_below:
			bprm->cred->egid = inode->i_gid;
//flaw_line_below:
		}
//flaw_line_below:
	}
//fix_flaw_line_below:
//	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
",179797," int prepare_binprm(struct linux_binprm *bprm)
 {
	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
 	int retval;
 
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
"," int prepare_binprm(struct linux_binprm *bprm)
 {
 	int retval;
 
	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
","The `prepare_binprm` function initializes a `linux_binprm` structure for executing a binary file in a Linux environment. It retrieves the inode of the file to check its permissions and sets the effective user and group IDs based on the file's attributes, particularly if the file has set-user-ID (SUID) or set-group-ID (SGID) permissions. The function also ensures that the current task is allowed to gain these privileges and prepares a security context for the binary. Finally, it clears the buffer associated with the binary and reads the initial data from the file into this buffer, returning any error encountered during the process.","The code initializes a structure that manages the execution of a binary file in a Linux system. It checks the file's permissions to determine if it has special attributes that allow it to run with elevated privileges. If applicable, it updates the effective user and group identifiers to match those specified in the file's metadata. The code also prepares a security context for the binary execution and clears any previous data in a buffer. Finally, it reads the initial portion of the binary file into this buffer, ensuring that the execution environment is correctly set up before proceeding.","The vulnerability identified by CWE-362, which relates to a race condition, arises from the original code's handling of user and group identifiers before the security context is fully established. In the vulnerable code, the effective user and group IDs were set directly based on the file's attributes, potentially allowing a malicious actor to exploit timing discrepancies between privilege checks and the actual execution context. The fixed code addresses this by using a dedicated function, `bprm_fill_uid`, to encapsulate the logic for setting these identifiers, ensuring that the security context is properly prepared before any privilege-related operations occur, thus mitigating the risk of race conditions.","The vulnerability identified by CWE-362, which pertains to race conditions, occurs when the code allows for a timing discrepancy between privilege checks and the actual setting of user and group identifiers. In the original implementation, the identifiers were set directly based on the file's attributes before the security context was fully established, creating an opportunity for exploitation. The fix involved restructuring the code to use a dedicated function that encapsulates the logic for setting these identifiers, ensuring that the security context is prepared first, thereby preventing potential attackers from manipulating the execution environment during this critical phase.","The vulnerability was fixed by introducing the `bprm_fill_uid` function, which centralizes the logic for setting the effective user and group identifiers. In the revised code, this function is called before the security context is established with `security_bprm_set_creds`. By ensuring that the identifiers are set within a controlled context, the risk of race conditions is mitigated, as the security context is fully prepared before any privilege-related operations take place. This change prevents potential exploitation during the critical phase of privilege assignment, enhancing the overall security of the binary execution process.","The vulnerability was fixed by restructuring the code to ensure that the setting of user and group identifiers occurs within a dedicated function that encapsulates this logic. This change ensures that the identifiers are established only after the security context is fully prepared, thereby preventing any timing discrepancies that could lead to race conditions. By controlling the sequence of operations, the fix effectively mitigates the risk of exploitation during the critical phase of privilege assignment, enhancing the security of the execution process and ensuring that the environment is properly validated before any sensitive actions are taken."
14,179863,179863,,Remote,Not required,Partial,CVE-2015-1791,https://www.cvedetails.com/cve/CVE-2015-1791/,CWE-362,Medium,Partial,Partial,,2015-06-12,6.8,"Race condition in the ssl3_get_new_session_ticket function in ssl/s3_clnt.c in OpenSSL before 0.9.8zg, 1.0.0 before 1.0.0s, 1.0.1 before 1.0.1n, and 1.0.2 before 1.0.2b, when used for a multi-threaded client, allows remote attackers to cause a denial of service (double free and application crash) or possibly have unspecified other impact by providing a NewSessionTicket during an attempt to reuse a ticket that had been obtained earlier.",2017-11-14,DoS ,32,https://github.com/openssl/openssl/commit/98ece4eebfb6cd45cc8d550c6ac0022965071afc,98ece4eebfb6cd45cc8d550c6ac0022965071afc,"Fix race condition in NewSessionTicket

If a NewSessionTicket is received by a multi-threaded client when
attempting to reuse a previous ticket then a race condition can occur
potentially leading to a double free of the ticket data.

CVE-2015-1791

This also fixes RT#3808 where a session ID is changed for a session already
in the client session cache. Since the session ID is the key to the cache
this breaks the cache access.

Parts of this patch were inspired by this Akamai change:
https://github.com/akamai/openssl/commit/c0bf69a791239ceec64509f9f19fcafb2461b0d3

Reviewed-by: Rich Salz <rsalz@openssl.org>",0,ssl/s3_clnt.c,"{""sha"": ""4e18b65605431b7ef0e7b0fec99604d9c77b404c"", ""filename"": ""include/openssl/ssl.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/include/openssl/ssl.h"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/include/openssl/ssl.h"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/include/openssl/ssl.h?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -2048,6 +2048,7 @@ void ERR_load_SSL_strings(void);\n # define SSL_F_SSL_READ                                   223\n # define SSL_F_SSL_SCAN_CLIENTHELLO_TLSEXT                320\n # define SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT                321\n+# define SSL_F_SSL_SESSION_DUP                            348\n # define SSL_F_SSL_SESSION_NEW                            189\n # define SSL_F_SSL_SESSION_PRINT_FP                       190\n # define SSL_F_SSL_SESSION_SET1_ID_CONTEXT                312""}<_**next**_>{""sha"": ""d6f53b0dea846bba4e7e93b37768faea1dfa879f"", ""filename"": ""ssl/s3_clnt.c"", ""status"": ""modified"", ""additions"": 32, ""deletions"": 0, ""changes"": 32, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/s3_clnt.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/s3_clnt.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/s3_clnt.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -2238,6 +2238,38 @@ int ssl3_get_new_session_ticket(SSL *s)\n     }\n \n     p = d = (unsigned char *)s->init_msg;\n+\n+    if (s->session->session_id_length > 0) {\n+        int i = s->session_ctx->session_cache_mode;\n+        SSL_SESSION *new_sess;\n+        /*\n+         * We reused an existing session, so we need to replace it with a new\n+         * one\n+         */\n+        if (i & SSL_SESS_CACHE_CLIENT) {\n+            /*\n+             * Remove the old session from the cache\n+             */\n+            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {\n+                if (s->session_ctx->remove_session_cb != NULL)\n+                    s->session_ctx->remove_session_cb(s->session_ctx,\n+                                                      s->session);\n+            } else {\n+                /* We carry on if this fails */\n+                SSL_CTX_remove_session(s->session_ctx, s->session);\n+            }\n+        }\n+\n+        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {\n+            al = SSL_AD_INTERNAL_ERROR;\n+            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);\n+            goto f_err;\n+        }\n+\n+        SSL_SESSION_free(s->session);\n+        s->session = new_sess;\n+    }\n+\n     n2l(p, s->session->tlsext_tick_lifetime_hint);\n     n2s(p, ticklen);\n     /* ticket_lifetime_hint + ticket_length + ticket */""}<_**next**_>{""sha"": ""4b4d89ce7a1e8c6a596224a60588f1a6b314b051"", ""filename"": ""ssl/ssl_err.c"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_err.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_err.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_err.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -274,6 +274,7 @@ static ERR_STRING_DATA SSL_str_functs[] = {\n      \""SSL_SCAN_CLIENTHELLO_TLSEXT\""},\n     {ERR_FUNC(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT),\n      \""SSL_SCAN_SERVERHELLO_TLSEXT\""},\n+    {ERR_FUNC(SSL_F_SSL_SESSION_DUP), \""ssl_session_dup\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_NEW), \""SSL_SESSION_new\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_PRINT_FP), \""SSL_SESSION_print_fp\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_SET1_ID_CONTEXT),""}<_**next**_>{""sha"": ""3252631e1c8981e39a828c59b98c7e63931e4a25"", ""filename"": ""ssl/ssl_locl.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_locl.h"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_locl.h"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_locl.h?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -1860,6 +1860,7 @@ __owur int ssl_set_peer_cert_type(SESS_CERT *c, int type);\n __owur int ssl_get_new_session(SSL *s, int session);\n __owur int ssl_get_prev_session(SSL *s, unsigned char *session, int len,\n                          const unsigned char *limit);\n+__owur SSL_SESSION *ssl_session_dup(SSL_SESSION *src, int ticket);\n __owur int ssl_cipher_id_cmp(const SSL_CIPHER *a, const SSL_CIPHER *b);\n DECLARE_OBJ_BSEARCH_GLOBAL_CMP_FN(SSL_CIPHER, SSL_CIPHER, ssl_cipher_id);\n __owur int ssl_cipher_ptr_id_cmp(const SSL_CIPHER *const *ap,""}<_**next**_>{""sha"": ""fd940541d5bc0fdb02c298da55170b40d8397eb4"", ""filename"": ""ssl/ssl_sess.c"", ""status"": ""modified"", ""additions"": 116, ""deletions"": 0, ""changes"": 116, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_sess.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_sess.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_sess.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -225,6 +225,122 @@ SSL_SESSION *SSL_SESSION_new(void)\n     return (ss);\n }\n \n+/*\n+ * Create a new SSL_SESSION and duplicate the contents of |src| into it. If\n+ * ticket == 0 then no ticket information is duplicated, otherwise it is.\n+ */\n+SSL_SESSION *ssl_session_dup(SSL_SESSION *src, int ticket)\n+{\n+    SSL_SESSION *dest;\n+\n+    dest = OPENSSL_malloc(sizeof(*src));\n+    if (dest == NULL) {\n+        goto err;\n+    }\n+    memcpy(dest, src, sizeof(*dest));\n+\n+#ifndef OPENSSL_NO_PSK\n+    if (src->psk_identity_hint) {\n+        dest->psk_identity_hint = BUF_strdup(src->psk_identity_hint);\n+        if (dest->psk_identity_hint == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->psk_identity_hint = NULL;\n+    }\n+    if (src->psk_identity) {\n+        dest->psk_identity = BUF_strdup(src->psk_identity);\n+        if (dest->psk_identity == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->psk_identity = NULL;\n+    }\n+#endif\n+\n+    if (src->sess_cert != NULL)\n+        CRYPTO_add(&src->sess_cert->references, 1, CRYPTO_LOCK_SSL_SESS_CERT);\n+\n+    if (src->peer != NULL)\n+        CRYPTO_add(&src->peer->references, 1, CRYPTO_LOCK_X509);\n+\n+    dest->references = 1;\n+\n+    if(src->ciphers != NULL) {\n+        dest->ciphers = sk_SSL_CIPHER_dup(src->ciphers);\n+        if (dest->ciphers == NULL)\n+            goto err;\n+    } else {\n+        dest->ciphers = NULL;\n+    }\n+\n+    if (!CRYPTO_dup_ex_data(CRYPTO_EX_INDEX_SSL_SESSION,\n+                                            &dest->ex_data, &src->ex_data)) {\n+        goto err;\n+    }\n+\n+    /* We deliberately don't copy the prev and next pointers */\n+    dest->prev = NULL;\n+    dest->next = NULL;\n+\n+#ifndef OPENSSL_NO_TLSEXT\n+    if (src->tlsext_hostname) {\n+        dest->tlsext_hostname = BUF_strdup(src->tlsext_hostname);\n+        if (dest->tlsext_hostname == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->tlsext_hostname = NULL;\n+    }\n+# ifndef OPENSSL_NO_EC\n+    if (src->tlsext_ecpointformatlist) {\n+        dest->tlsext_ecpointformatlist =\n+            BUF_memdup(src->tlsext_ecpointformatlist,\n+                       src->tlsext_ecpointformatlist_length);\n+        if (dest->tlsext_ecpointformatlist == NULL)\n+            goto err;\n+        dest->tlsext_ecpointformatlist_length =\n+            src->tlsext_ecpointformatlist_length;\n+    }\n+    if (src->tlsext_ellipticcurvelist) {\n+        dest->tlsext_ellipticcurvelist =\n+            BUF_memdup(src->tlsext_ellipticcurvelist,\n+                       src->tlsext_ellipticcurvelist_length);\n+        if (dest->tlsext_ellipticcurvelist == NULL)\n+            goto err;\n+        dest->tlsext_ellipticcurvelist_length =\n+            src->tlsext_ellipticcurvelist_length;\n+    }\n+# endif\n+#endif\n+\n+    if (ticket != 0) {\n+        dest->tlsext_tick_lifetime_hint = src->tlsext_tick_lifetime_hint;\n+        dest->tlsext_ticklen = src->tlsext_ticklen;\n+        if((dest->tlsext_tick = OPENSSL_malloc(src->tlsext_ticklen)) == NULL) {\n+            goto err;\n+        }\n+    }\n+\n+#ifndef OPENSSL_NO_SRP\n+    dest->srp_username = NULL;\n+    if (src->srp_username) {\n+        dest->srp_username = BUF_strdup(src->srp_username);\n+        if (dest->srp_username == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->srp_username = NULL;\n+    }\n+#endif\n+\n+    return dest;\n+err:\n+    SSLerr(SSL_F_SSL_SESSION_DUP, ERR_R_MALLOC_FAILURE);\n+    SSL_SESSION_free(dest);\n+    return NULL;\n+}\n+\n const unsigned char *SSL_SESSION_get_id(const SSL_SESSION *s,\n                                         unsigned int *len)\n {""}","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;

    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
",C,"
    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

",,,"@@ -2238,6 +2238,38 @@ int ssl3_get_new_session_ticket(SSL *s)
     }
 
     p = d = (unsigned char *)s->init_msg;
+
+    if (s->session->session_id_length > 0) {
+        int i = s->session_ctx->session_cache_mode;
+        SSL_SESSION *new_sess;
+        /*
+         * We reused an existing session, so we need to replace it with a new
+         * one
+         */
+        if (i & SSL_SESS_CACHE_CLIENT) {
+            /*
+             * Remove the old session from the cache
+             */
+            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
+                if (s->session_ctx->remove_session_cb != NULL)
+                    s->session_ctx->remove_session_cb(s->session_ctx,
+                                                      s->session);
+            } else {
+                /* We carry on if this fails */
+                SSL_CTX_remove_session(s->session_ctx, s->session);
+            }
+        }
+
+        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
+            al = SSL_AD_INTERNAL_ERROR;
+            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
+            goto f_err;
+        }
+
+        SSL_SESSION_free(s->session);
+        s->session = new_sess;
+    }
+
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */",openssl,98ece4eebfb6cd45cc8d550c6ac0022965071afc,8c2b1d872b25f3ec78e04f5cd2de8f21e853c4a6,1,"int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//    if (s->session->session_id_length > 0) {
//fix_flaw_line_below:
//        int i = s->session_ctx->session_cache_mode;
//fix_flaw_line_below:
//        SSL_SESSION *new_sess;
//fix_flaw_line_below:
//        /*
//fix_flaw_line_below:
//         * We reused an existing session, so we need to replace it with a new
//fix_flaw_line_below:
//         * one
//fix_flaw_line_below:
//         */
//fix_flaw_line_below:
//        if (i & SSL_SESS_CACHE_CLIENT) {
//fix_flaw_line_below:
//            /*
//fix_flaw_line_below:
//             * Remove the old session from the cache
//fix_flaw_line_below:
//             */
//fix_flaw_line_below:
//            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
//fix_flaw_line_below:
//                if (s->session_ctx->remove_session_cb != NULL)
//fix_flaw_line_below:
//                    s->session_ctx->remove_session_cb(s->session_ctx,
//fix_flaw_line_below:
//                                                      s->session);
//fix_flaw_line_below:
//            } else {
//fix_flaw_line_below:
//                /* We carry on if this fails */
//fix_flaw_line_below:
//                SSL_CTX_remove_session(s->session_ctx, s->session);
//fix_flaw_line_below:
//            }
//fix_flaw_line_below:
//        }
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
//fix_flaw_line_below:
//            al = SSL_AD_INTERNAL_ERROR;
//fix_flaw_line_below:
//            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
//fix_flaw_line_below:
//            goto f_err;
//fix_flaw_line_below:
//        }
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//        SSL_SESSION_free(s->session);
//fix_flaw_line_below:
//        s->session = new_sess;
//fix_flaw_line_below:
//    }
//fix_flaw_line_below:
//
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
",179863,"int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;

    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","The `ssl3_get_new_session_ticket` function processes a new session ticket in an SSL/TLS connection. It begins by retrieving a message containing the session ticket and checks its validity. If the message length is insufficient, it triggers an error. The function then extracts the ticket lifetime hint and the ticket length, ensuring they match the expected size. It allocates memory for the new session ticket, copies the ticket data, and computes a session ID based on the ticket's hash. If any errors occur during these steps, appropriate alerts are sent, and the function returns an error code; otherwise, it returns success.","The function processes a new session ticket in an SSL/TLS connection by first retrieving a message and validating its length. It checks that the message contains sufficient data, including a lifetime hint and ticket length. If the data is valid, it allocates memory for the new session ticket, copies the ticket information, and computes a session identifier based on the ticket's content. In case of any errors during these operations, appropriate alerts are generated, and the function indicates failure; otherwise, it successfully updates the session with the new ticket information.","The vulnerability identified by CWE-362, which relates to a race condition, was present in the original code due to improper handling of session management when a new session ticket was received. Specifically, the code did not adequately manage the existing session, allowing potential concurrent modifications to the session state. In the fixed code, a check for the session ID length was added, and if an existing session was reused, it ensured that the old session was removed from the cache before duplicating it. This change prevents race conditions by ensuring that session data is consistently managed and updated, thereby enhancing the security of session handling.","The vulnerability identified by CWE-362, which pertains to race conditions, arose from inadequate management of session states during the processing of new session tickets. In the original code, there was a lack of synchronization when reusing existing sessions, potentially allowing concurrent modifications that could lead to inconsistent session data. The fixed code addressed this issue by implementing checks to ensure that existing sessions were properly removed from the cache before creating new duplicates. This change effectively mitigated the risk of race conditions by ensuring that session management operations were performed in a controlled and consistent manner.","The vulnerability was fixed by introducing checks for the session ID length and ensuring proper management of existing sessions. In the updated code, if the session ID length is greater than zero, it checks the session cache mode and removes the old session from the cache using either a callback function or by calling a specific removal function. Additionally, the code duplicates the existing session using a dedicated function, ensuring that the old session is freed before assigning the new one. This structured approach prevents race conditions by ensuring that session data is consistently updated and managed during the ticket processing.","The vulnerability was fixed by implementing a structured approach to manage existing sessions before processing a new session ticket. The updated code checks if there is an existing session and, if so, ensures it is properly removed from the session cache to prevent concurrent modifications. It also duplicates the existing session safely, ensuring that the old session is freed before assigning the new one. This careful management of session states prevents race conditions, ensuring that session data remains consistent and secure during the handling of new session tickets."
15,179914,179914,,Local,Not required,Complete,CVE-2015-0239,https://www.cvedetails.com/cve/CVE-2015-0239/,CWE-362,High,,Partial,,2015-03-02,4.7,"The em_sysenter function in arch/x86/kvm/emulate.c in the Linux kernel before 3.18.5, when the guest OS lacks SYSENTER MSR initialization, allows guest OS users to gain guest OS privileges or cause a denial of service (guest OS crash) by triggering use of a 16-bit code segment for emulation of a SYSENTER instruction.",2016-12-23,DoS +Priv ,8,https://github.com/torvalds/linux/commit/f3747379accba8e95d70cec0eae0582c8c182050,f3747379accba8e95d70cec0eae0582c8c182050,"KVM: x86: SYSENTER emulation is broken

SYSENTER emulation is broken in several ways:
1. It misses the case of 16-bit code segments completely (CVE-2015-0239).
2. MSR_IA32_SYSENTER_CS is checked in 64-bit mode incorrectly (bits 0 and 1 can
   still be set without causing #GP).
3. MSR_IA32_SYSENTER_EIP and MSR_IA32_SYSENTER_ESP are not masked in
   legacy-mode.
4. There is some unneeded code.

Fix it.

Cc: stable@vger.linux.org
Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>",19,arch/x86/kvm/emulate.c,"{""sha"": ""de12c1d379f16899645d96a2c3fd75663919c86d"", ""filename"": ""arch/x86/kvm/emulate.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 19, ""changes"": 27, ""blob_url"": ""https://github.com/torvalds/linux/blob/f3747379accba8e95d70cec0eae0582c8c182050/arch/x86/kvm/emulate.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/f3747379accba8e95d70cec0eae0582c8c182050/arch/x86/kvm/emulate.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kvm/emulate.c?ref=f3747379accba8e95d70cec0eae0582c8c182050"", ""patch"": ""@@ -2348,7 +2348,7 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \t * Not recognized on AMD in compat mode (but is recognized in legacy\n \t * mode).\n \t */\n-\tif ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)\n+\tif ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)\n \t    && !vendor_intel(ctxt))\n \t\treturn emulate_ud(ctxt);\n \n@@ -2359,25 +2359,13 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \tsetup_syscalls_segments(ctxt, &cs, &ss);\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n-\tswitch (ctxt->mode) {\n-\tcase X86EMUL_MODE_PROT32:\n-\t\tif ((msr_data & 0xfffc) == 0x0)\n-\t\t\treturn emulate_gp(ctxt, 0);\n-\t\tbreak;\n-\tcase X86EMUL_MODE_PROT64:\n-\t\tif (msr_data == 0x0)\n-\t\t\treturn emulate_gp(ctxt, 0);\n-\t\tbreak;\n-\tdefault:\n-\t\tbreak;\n-\t}\n+\tif ((msr_data & 0xfffc) == 0x0)\n+\t\treturn emulate_gp(ctxt, 0);\n \n \tctxt->eflags &= ~(EFLG_VM | EFLG_IF);\n-\tcs_sel = (u16)msr_data;\n-\tcs_sel &= ~SELECTOR_RPL_MASK;\n+\tcs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;\n \tss_sel = cs_sel + 8;\n-\tss_sel &= ~SELECTOR_RPL_MASK;\n-\tif (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {\n+\tif (efer & EFER_LMA) {\n \t\tcs.d = 0;\n \t\tcs.l = 1;\n \t}\n@@ -2386,10 +2374,11 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);\n-\tctxt->_eip = msr_data;\n+\tctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);\n-\t*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;\n+\t*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :\n+\t\t\t\t\t\t\t      (u32)msr_data;\n \n \treturn X86EMUL_CONTINUE;\n }""}","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
 
 	return X86EMUL_CONTINUE;
 }
",C,"	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
	if (efer & EFER_LMA) {
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
","	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
	ctxt->_eip = msr_data;
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
",,"@@ -2348,7 +2348,7 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
-	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
+	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
@@ -2359,25 +2359,13 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
-	switch (ctxt->mode) {
-	case X86EMUL_MODE_PROT32:
-		if ((msr_data & 0xfffc) == 0x0)
-			return emulate_gp(ctxt, 0);
-		break;
-	case X86EMUL_MODE_PROT64:
-		if (msr_data == 0x0)
-			return emulate_gp(ctxt, 0);
-		break;
-	default:
-		break;
-	}
+	if ((msr_data & 0xfffc) == 0x0)
+		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
-	cs_sel = (u16)msr_data;
-	cs_sel &= ~SELECTOR_RPL_MASK;
+	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
-	ss_sel &= ~SELECTOR_RPL_MASK;
-	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
+	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}
@@ -2386,10 +2374,11 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
-	ctxt->_eip = msr_data;
+	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
-	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
+	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
+							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }",linux,f3747379accba8e95d70cec0eae0582c8c182050,63ea0a49ae0b145b91ff2b070c01b66fc75854b9,1,"static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
//flaw_line_below:
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
//fix_flaw_line_below:
//	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
//flaw_line_below:
	switch (ctxt->mode) {
//flaw_line_below:
	case X86EMUL_MODE_PROT32:
//flaw_line_below:
		if ((msr_data & 0xfffc) == 0x0)
//flaw_line_below:
			return emulate_gp(ctxt, 0);
//flaw_line_below:
		break;
//flaw_line_below:
	case X86EMUL_MODE_PROT64:
//flaw_line_below:
		if (msr_data == 0x0)
//flaw_line_below:
			return emulate_gp(ctxt, 0);
//flaw_line_below:
		break;
//flaw_line_below:
	default:
//flaw_line_below:
		break;
//flaw_line_below:
	}
//fix_flaw_line_below:
//	if ((msr_data & 0xfffc) == 0x0)
//fix_flaw_line_below:
//		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
//flaw_line_below:
	cs_sel = (u16)msr_data;
//flaw_line_below:
	cs_sel &= ~SELECTOR_RPL_MASK;
//fix_flaw_line_below:
//	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
//flaw_line_below:
	ss_sel &= ~SELECTOR_RPL_MASK;
//flaw_line_below:
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
//fix_flaw_line_below:
//	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
//flaw_line_below:
	ctxt->_eip = msr_data;
//fix_flaw_line_below:
//	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
//flaw_line_below:
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
//fix_flaw_line_below:
//	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
//fix_flaw_line_below:
//							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
",179914,"static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","The `em_sysenter` function emulates the `SYSENTER` instruction in an x86 architecture context, handling different operating modes such as real mode, protected mode, and long mode. It first retrieves the Extended Feature Enable Register (EFER) and checks for compatibility issues, injecting a general protection fault if necessary. The function sets up the code and stack segment descriptors, ensuring proper privilege levels, and updates the instruction pointer and stack pointer based on the values from the corresponding model-specific registers (MSRs). Finally, it returns a status indicating whether to continue execution or handle an error.","The provided code emulates the execution of a specific instruction in an x86 architecture, managing various operating modes such as real, protected, and long mode. It begins by checking the current mode and retrieving necessary system configuration data, handling potential compatibility issues by generating appropriate fault responses. The code sets up segment descriptors for code and stack segments, ensuring correct privilege levels, and updates the instruction and stack pointers based on values from system registers. Ultimately, it determines whether to continue execution or signal an error based on the results of these operations.","The vulnerability identified as CWE-362, which relates to a race condition, was present in the original code due to improper handling of the instruction pointer and stack pointer when transitioning between different operating modes. In the fixed code, the handling of the instruction and stack pointers was improved by ensuring that the values retrieved from the model-specific registers are correctly cast based on the Extended Feature Enable Register (EFER) state. Specifically, the instruction pointer and stack pointer are now conditionally cast to 32-bit or 64-bit values, preventing potential inconsistencies and ensuring that the execution context is correctly maintained across mode transitions.","The vulnerability identified as CWE-362, which pertains to a race condition, arose from inadequate handling of context switching between different operating modes, leading to potential inconsistencies in the execution state. In the original code, the instruction and stack pointers were not properly managed when transitioning between 32-bit and 64-bit modes, risking incorrect execution flow. The fix involved implementing conditional casting based on the state of a specific feature register, ensuring that the pointers are accurately represented in the appropriate format. This adjustment mitigated the risk of race conditions by maintaining a consistent and valid execution context during mode transitions.","The vulnerability was fixed by enhancing the handling of the instruction pointer and stack pointer based on the state of the Extended Feature Enable Register (EFER). In the revised code, the values retrieved from the model-specific registers for the instruction pointer and stack pointer are conditionally cast to either 32-bit or 64-bit formats, depending on whether the EFER indicates long mode. Specifically, the instruction pointer is set using a conditional expression that checks the EFER state, ensuring that the correct data type is used. This change prevents inconsistencies during mode transitions, thereby eliminating the race condition vulnerability.","The vulnerability was addressed by implementing a more robust mechanism for managing the execution context during transitions between different operating modes. The fix involved introducing conditional checks that determine the appropriate data type for the instruction and stack pointers based on the state of a specific feature register. This ensured that the pointers are accurately represented in either 32-bit or 64-bit formats, depending on the mode. By enforcing this type consistency, the fix mitigated the risk of race conditions, thereby maintaining a valid and stable execution state throughout the mode transitions."
16,179955,179955,,Local,Not required,Complete,CVE-2014-9529,https://www.cvedetails.com/cve/CVE-2014-9529/,CWE-362,Low,Complete,Complete,,2015-01-09,7.2,Race condition in the key_gc_unused_keys function in security/keys/gc.c in the Linux kernel through 3.18.2 allows local users to cause a denial of service (memory corruption or panic) or possibly have unspecified other impact via keyctl commands that trigger access to a key structure member during garbage collection of a key.,2018-01-04,DoS Mem. Corr. ,2,https://github.com/torvalds/linux/commit/a3a8784454692dd72e5d5d34dcdab17b4420e74c,a3a8784454692dd72e5d5d34dcdab17b4420e74c,"KEYS: close race between key lookup and freeing

When a key is being garbage collected, it's key->user would get put before
the ->destroy() callback is called, where the key is removed from it's
respective tracking structures.

This leaves a key hanging in a semi-invalid state which leaves a window open
for a different task to try an access key->user. An example is
find_keyring_by_name() which would dereference key->user for a key that is
in the process of being garbage collected (where key->user was freed but
->destroy() wasn't called yet - so it's still present in the linked list).

This would cause either a panic, or corrupt memory.

Fixes CVE-2014-9529.

Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
Signed-off-by: David Howells <dhowells@redhat.com>",1,security/keys/gc.c,"{""sha"": ""c7952375ac5325cfb4c403fa1020671b5f31a150"", ""filename"": ""security/keys/gc.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 2, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/a3a8784454692dd72e5d5d34dcdab17b4420e74c/security/keys/gc.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/a3a8784454692dd72e5d5d34dcdab17b4420e74c/security/keys/gc.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/security/keys/gc.c?ref=a3a8784454692dd72e5d5d34dcdab17b4420e74c"", ""patch"": ""@@ -148,12 +148,12 @@ static noinline void key_gc_unused_keys(struct list_head *keys)\n \t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n \t\t\tatomic_dec(&key->user->nikeys);\n \n-\t\tkey_user_put(key->user);\n-\n \t\t/* now throw away the key memory */\n \t\tif (key->type->destroy)\n \t\t\tkey->type->destroy(key);\n \n+\t\tkey_user_put(key->user);\n+\n \t\tkfree(key->description);\n \n #ifdef KEY_DEBUGGING""}","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
		key_user_put(key->user);

 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
		key_user_put(key->user);
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
",C,"		key_user_put(key->user);

","		key_user_put(key->user);
",,"@@ -148,12 +148,12 @@ static noinline void key_gc_unused_keys(struct list_head *keys)
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
-		key_user_put(key->user);
-
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
+		key_user_put(key->user);
+
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING",linux,a3a8784454692dd72e5d5d34dcdab17b4420e74c,693a30b8f19a964087a3762d09fb2e1cbad6b0d4,1,"static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
//flaw_line_below:
		key_user_put(key->user);
//flaw_line_below:

 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
//fix_flaw_line_below:
//		key_user_put(key->user);
//fix_flaw_line_below:
//
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
",179955,"static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
		key_user_put(key->user);
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
		key_user_put(key->user);

 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","The function `key_gc_unused_keys` is designed to clean up and free unused keys from a linked list, referred to as `keys`. It iterates through the list, removing each key and performing several operations: logging the key's serial number, checking its validity, and freeing its associated security resources. If the key is tracked under a user's quota, it updates the user's key count and byte usage accordingly. The function also decrements the total number of keys for the user and, if applicable, the instantiated keys. Finally, it invokes the key's destruction method, frees its description, and releases the memory allocated for the key.","The provided code defines a function that manages the cleanup of unused keys from a linked list. It iteratively removes each key, logging its identification number and verifying its integrity before releasing associated security resources. The function adjusts the user's key and memory usage statistics if the key is part of a quota system. It also decreases the count of total and instantiated keys for the user. After invoking any necessary destruction procedures for the key, it frees the memory allocated for the key's description and ultimately deallocates the memory used for the key itself.","The vulnerability identified as CWE-362, which pertains to a race condition, arises from the improper handling of user key statistics during the cleanup process. In the vulnerable code, the decrementing of user key counts and memory usage occurs without adequate synchronization, potentially allowing concurrent access to the user's data structure. This could lead to inconsistent or incorrect updates if multiple threads modify the same user data simultaneously. The fixed code addresses this issue by ensuring that the critical section involving the user's lock is properly managed with `spin_lock` and `spin_unlock`, thus preventing race conditions during the updates to `qnkeys` and `qnbytes`.","The vulnerability identified as CWE-362, which involves a race condition, occurs when multiple threads access and modify shared data without proper synchronization. In the vulnerable code, the updates to user statistics related to key management are performed without locking mechanisms, allowing concurrent threads to potentially interfere with each other's operations. This can lead to inconsistent or incorrect values being recorded. The fixed code resolves this issue by implementing a locking mechanism around the critical section where these updates occur, ensuring that only one thread can modify the shared data at a time, thus preventing race conditions and ensuring data integrity.","The vulnerability was fixed by introducing proper synchronization around the critical section that modifies user statistics. In the updated code, the use of `spin_lock` and `spin_unlock` ensures that when the code checks and updates the user's key count and memory usage, only one thread can access this section at a time. This prevents concurrent modifications that could lead to inconsistent data. Specifically, the locking mechanism surrounds the decrement operations on `qnkeys` and `qnbytes`, ensuring that these updates are atomic and safe from race conditions, thereby maintaining the integrity of the user’s data during the cleanup process.","The vulnerability was fixed by implementing a locking mechanism to ensure that updates to shared user statistics are performed in a thread-safe manner. By introducing locks around the critical section where user data is modified, the code prevents multiple threads from concurrently accessing and altering the same data. This synchronization ensures that only one thread can execute the update operations at a time, thereby eliminating the risk of race conditions. As a result, the integrity of the user’s data is maintained, and the potential for inconsistent or incorrect values due to simultaneous modifications is effectively mitigated."
17,180092,180092,,Local,Not required,,CVE-2016-7916,https://www.cvedetails.com/cve/CVE-2016-7916/,CWE-362,Medium,Complete,,,2016-11-16,4.7,Race condition in the environ_read function in fs/proc/base.c in the Linux kernel before 4.5.4 allows local users to obtain sensitive information from kernel memory by reading a /proc/*/environ file during a process-setup time interval in which environment-variable copying is incomplete.,2017-01-17,+Info ,2,https://github.com/torvalds/linux/commit/8148a73c9901a8794a50f950083c00ccf97d43b3,8148a73c9901a8794a50f950083c00ccf97d43b3,"proc: prevent accessing /proc/<PID>/environ until it's ready

If /proc/<PID>/environ gets read before the envp[] array is fully set up
in create_{aout,elf,elf_fdpic,flat}_tables(), we might end up trying to
read more bytes than are actually written, as env_start will already be
set but env_end will still be zero, making the range calculation
underflow, allowing to read beyond the end of what has been written.

Fix this as it is done for /proc/<PID>/cmdline by testing env_end for
zero.  It is, apparently, intentionally set last in create_*_tables().

This bug was found by the PaX size_overflow plugin that detected the
arithmetic underflow of 'this_len = env_end - (env_start + src)' when
env_end is still zero.

The expected consequence is that userland trying to access
/proc/<PID>/environ of a not yet fully set up process may get
inconsistent data as we're in the middle of copying in the environment
variables.

Fixes: https://forums.grsecurity.net/viewtopic.php?f=3&t=4363
Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=116461
Signed-off-by: Mathias Krause <minipli@googlemail.com>
Cc: Emese Revfy <re.emese@gmail.com>
Cc: Pax Team <pageexec@freemail.hu>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Cc: Mateusz Guzik <mguzik@redhat.com>
Cc: Alexey Dobriyan <adobriyan@gmail.com>
Cc: Cyrill Gorcunov <gorcunov@openvz.org>
Cc: Jarod Wilson <jarod@redhat.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",1,fs/proc/base.c,"{""sha"": ""92e37e224cd22d66d0b9675f1790e110d5e7b415"", ""filename"": ""fs/proc/base.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 1, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/8148a73c9901a8794a50f950083c00ccf97d43b3/fs/proc/base.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8148a73c9901a8794a50f950083c00ccf97d43b3/fs/proc/base.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/base.c?ref=8148a73c9901a8794a50f950083c00ccf97d43b3"", ""patch"": ""@@ -955,7 +955,8 @@ static ssize_t environ_read(struct file *file, char __user *buf,\n \tstruct mm_struct *mm = file->private_data;\n \tunsigned long env_start, env_end;\n \n-\tif (!mm)\n+\t/* Ensure the process spawned far enough to have an environment. */\n+\tif (!mm || !mm->env_end)\n \t\treturn 0;\n \n \tpage = (char *)__get_free_page(GFP_TEMPORARY);""}","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	if (!mm)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
",C,"	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
","	if (!mm)
",,"@@ -955,7 +955,8 @@ static ssize_t environ_read(struct file *file, char __user *buf,
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
-	if (!mm)
+	/* Ensure the process spawned far enough to have an environment. */
+	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);",linux,8148a73c9901a8794a50f950083c00ccf97d43b3,32a4e169039927bfb6ee9f0ccbbe3a8aaf13a4bc,1,"static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
//flaw_line_below:
	if (!mm)
//fix_flaw_line_below:
//	/* Ensure the process spawned far enough to have an environment. */
//fix_flaw_line_below:
//	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
",180092,"static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	if (!mm)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","The `environ_read` function is designed to read the environment variables of a process in a Linux kernel module context. It takes a file structure, a user buffer, a count of bytes to read, and a position pointer as parameters. The function first retrieves the memory management structure of the target process and allocates a temporary page for data transfer. It then acquires a read lock on the process's memory, calculates the range of environment variables, and reads them into the allocated page using `access_remote_vm`. The data is subsequently copied to the user buffer, updating the position pointer accordingly, and finally, it releases resources and returns the number of bytes read or an error code.","The provided code implements a function that reads the environment variables of a specified process in a kernel module. It begins by checking the validity of the target process's memory structure and allocates a temporary buffer for data transfer. After acquiring a read lock on the process's memory, it determines the range of environment variables to be read. The function then retrieves the data from the target process's memory and copies it into a user-provided buffer, updating the read position accordingly. Finally, it releases allocated resources and returns the number of bytes successfully read or an error code if applicable.","The vulnerability in the original code, identified as CWE-362 (Race Condition), arises from the lack of a check to ensure that the target process has a valid environment before proceeding with memory access. Specifically, if the process's environment has not been initialized, the function could attempt to read from an invalid memory range, leading to undefined behavior or crashes. The fix introduces a condition to verify that the memory structure is valid and that the environment end is non-zero before proceeding, thereby preventing potential race conditions and ensuring safer access to the process's memory.","The vulnerability identified as CWE-362 (Race Condition) in the original code stems from the absence of a safeguard to ensure that the target process's environment is properly initialized before accessing its memory. This oversight could allow the function to read from an invalid or uninitialized memory area, potentially leading to crashes or security issues. The fix addresses this vulnerability by adding a check to confirm that the environment is valid and initialized before proceeding with memory access. This change effectively mitigates the risk of race conditions and enhances the overall safety of the memory access operation.","The vulnerability was fixed by adding a condition to check the validity of the memory structure and ensure that the environment end is non-zero before proceeding with memory access. Specifically, the code now includes a check for `mm->env_end` to confirm that the process has a valid environment. This prevents the function from attempting to read from an uninitialized or invalid memory range, which could lead to undefined behavior. By implementing this safeguard, the code reduces the risk of race conditions and ensures safer access to the target process's memory, thereby enhancing overall stability and security.","The vulnerability was fixed by introducing a validation check to ensure that the target process's environment is properly initialized before any memory access occurs. This additional condition verifies that the environment is valid and not empty, preventing the function from attempting to read from an uninitialized or invalid memory area. By implementing this safeguard, the code effectively mitigates the risk of race conditions that could lead to crashes or security issues, thereby enhancing the overall safety and reliability of the memory access operation within the function."
18,180189,180189,,Local,Not required,Partial,CVE-2016-6156,https://www.cvedetails.com/cve/CVE-2016-6156/,CWE-362,Medium,,,,2016-08-06,1.9,"Race condition in the ec_device_ioctl_xcmd function in drivers/platform/chrome/cros_ec_dev.c in the Linux kernel before 4.7 allows local users to cause a denial of service (out-of-bounds array access) by changing a certain size value, aka a *double fetch* vulnerability.",2016-11-28,DoS ,7,https://github.com/torvalds/linux/commit/096cdc6f52225835ff503f987a0d68ef770bb78e,096cdc6f52225835ff503f987a0d68ef770bb78e,"platform/chrome: cros_ec_dev - double fetch bug in ioctl

We verify ""u_cmd.outsize"" and ""u_cmd.insize"" but we need to make sure
that those values have not changed between the two copy_from_user()
calls.  Otherwise it could lead to a buffer overflow.

Additionally, cros_ec_cmd_xfer() can set s_cmd->insize to a lower value.
We should use the new smaller value so we don't copy too much data to
the user.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Fixes: a841178445bb ('mfd: cros_ec: Use a zero-length array for command data')
Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
Reviewed-by: Kees Cook <keescook@chromium.org>
Tested-by: Gwendal Grignou <gwendal@chromium.org>
Cc: <stable@vger.kernel.org> # v4.2+
Signed-off-by: Olof Johansson <olof@lixom.net>",1,drivers/platform/chrome/cros_ec_dev.c,"{""sha"": ""8abd80dbcbed7974b4ace4265dd9dad1bca89edd"", ""filename"": ""drivers/platform/chrome/cros_ec_dev.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 1, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/096cdc6f52225835ff503f987a0d68ef770bb78e/drivers/platform/chrome/cros_ec_dev.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/096cdc6f52225835ff503f987a0d68ef770bb78e/drivers/platform/chrome/cros_ec_dev.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/platform/chrome/cros_ec_dev.c?ref=096cdc6f52225835ff503f987a0d68ef770bb78e"", ""patch"": ""@@ -151,13 +151,19 @@ static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n \t\tgoto exit;\n \t}\n \n+\tif (u_cmd.outsize != s_cmd->outsize ||\n+\t    u_cmd.insize != s_cmd->insize) {\n+\t\tret = -EINVAL;\n+\t\tgoto exit;\n+\t}\n+\n \ts_cmd->command += ec->cmd_offset;\n \tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n \t/* Only copy data to userland if data was received. */\n \tif (ret < 0)\n \t\tgoto exit;\n \n-\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))\n+\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n \t\tret = -EFAULT;\n exit:\n \tkfree(s_cmd);""}","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
",C,"	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
","	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
",,"@@ -151,13 +151,19 @@ static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
 		goto exit;
 	}
 
+	if (u_cmd.outsize != s_cmd->outsize ||
+	    u_cmd.insize != s_cmd->insize) {
+		ret = -EINVAL;
+		goto exit;
+	}
+
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
-	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
+	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);",linux,096cdc6f52225835ff503f987a0d68ef770bb78e,33688abb2802ff3a230bd2441f765477b94cc89e,1,"static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
//fix_flaw_line_below:
//	if (u_cmd.outsize != s_cmd->outsize ||
//fix_flaw_line_below:
//	    u_cmd.insize != s_cmd->insize) {
//fix_flaw_line_below:
//		ret = -EINVAL;
//fix_flaw_line_below:
//		goto exit;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
//flaw_line_below:
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
//fix_flaw_line_below:
//	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
",180189,"static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","The `ec_device_ioctl_xcmd` function facilitates communication between a user-space application and a Chrome OS Embedded Controller (EC) device by handling IOCTL commands. It begins by copying user-provided command data into a local structure and validating the sizes of input and output buffers against predefined limits. If valid, it allocates memory for a command structure, copies the command data, and adjusts the command offset. The function then transfers the command to the EC device and, if successful, copies any received data back to the user space. Finally, it cleans up allocated memory and returns the result of the operation.","The provided code implements a function that facilitates communication between a user-space application and a hardware device by processing input/output control commands. It begins by retrieving command data from the user and validating the sizes of the input and output buffers to ensure they do not exceed predefined limits. If the sizes are acceptable, it allocates memory for the command structure and copies the command data. The function then adjusts the command for the device and attempts to send it. If the command is successfully processed, it copies any received data back to the user, ensuring proper memory management throughout.","The vulnerability in the original code, identified as CWE-362 (Race Condition), arose from the lack of validation for the sizes of the output and input buffers after copying data from user space. This oversight allowed a malicious user to manipulate the command structure, potentially leading to memory corruption or unauthorized access. The fixed code addresses this issue by adding checks to ensure that the sizes of the output and input buffers in the user-provided command match those in the allocated command structure. This validation prevents inconsistencies that could be exploited, thereby enhancing the security of the function.","The vulnerability identified as CWE-362 (Race Condition) in the original code stemmed from the absence of validation for the sizes of output and input buffers after data was copied from user space. This lack of checks allowed an attacker to manipulate the command structure, potentially leading to memory corruption or unauthorized access to sensitive data. The fixed code introduced additional validation to ensure that the sizes of the buffers provided by the user matched those in the allocated command structure. This enhancement effectively mitigated the risk of exploitation by preventing inconsistencies that could be leveraged by malicious users.","The vulnerability was fixed by adding validation checks for the sizes of the output and input buffers after copying data from user space into the command structure. Specifically, the code now compares `u_cmd.outsize` and `u_cmd.insize` with the corresponding sizes in the allocated command structure, ensuring they match. If there is a discrepancy, the function returns an error code, preventing further processing of potentially malicious data. This additional validation step, implemented before the command is sent to the device, effectively mitigates the risk of memory corruption and unauthorized access that could arise from manipulated input.","The vulnerability was addressed by incorporating additional validation checks to ensure that the sizes of the output and input buffers provided by the user match those in the allocated command structure. This was achieved by comparing the sizes after copying data from user space, and if any discrepancies were found, the function would return an error code, halting further processing. This enhancement prevents the possibility of processing manipulated or inconsistent data, thereby mitigating risks associated with memory corruption and unauthorized access, and ultimately strengthening the security of the function."
19,180191,180191,,Local,Not required,,CVE-2016-6136,https://www.cvedetails.com/cve/CVE-2016-6136/,CWE-362,Medium,,Partial,,2016-08-06,1.9,"Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a *double fetch* vulnerability.",2018-01-04,Bypass ,93,https://github.com/torvalds/linux/commit/43761473c254b45883a64441dd0bc85a42f3645c,43761473c254b45883a64441dd0bc85a42f3645c,"audit: fix a double fetch in audit_log_single_execve_arg()

There is a double fetch problem in audit_log_single_execve_arg()
where we first check the execve(2) argumnets for any ""bad"" characters
which would require hex encoding and then re-fetch the arguments for
logging in the audit record[1].  Of course this leaves a window of
opportunity for an unsavory application to munge with the data.

This patch reworks things by only fetching the argument data once[2]
into a buffer where it is scanned and logged into the audit
records(s).  In addition to fixing the double fetch, this patch
improves on the original code in a few other ways: better handling
of large arguments which require encoding, stricter record length
checking, and some performance improvements (completely unverified,
but we got rid of some strlen() calls, that's got to be a good
thing).

As part of the development of this patch, I've also created a basic
regression test for the audit-testsuite, the test can be tracked on
GitHub at the following link:

 * https://github.com/linux-audit/audit-testsuite/issues/25

[1] If you pay careful attention, there is actually a triple fetch
problem due to a strnlen_user() call at the top of the function.

[2] This is a tiny white lie, we do make a call to strnlen_user()
prior to fetching the argument data.  I don't like it, but due to the
way the audit record is structured we really have no choice unless we
copy the entire argument at once (which would require a rather
wasteful allocation).  The good news is that with this patch the
kernel no longer relies on this strnlen_user() value for anything
beyond recording it in the log, we also update it with a trustworthy
value whenever possible.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Paul Moore <paul@paul-moore.com>",117,kernel/auditsc.c,"{""sha"": ""c65af21a12d6d2e7b56e99ed2fdd6d284a9ec43a"", ""filename"": ""kernel/auditsc.c"", ""status"": ""modified"", ""additions"": 164, ""deletions"": 168, ""changes"": 332, ""blob_url"": ""https://github.com/torvalds/linux/blob/43761473c254b45883a64441dd0bc85a42f3645c/kernel/auditsc.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/43761473c254b45883a64441dd0bc85a42f3645c/kernel/auditsc.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/kernel/auditsc.c?ref=43761473c254b45883a64441dd0bc85a42f3645c"", ""patch"": ""@@ -73,6 +73,7 @@\n #include <linux/compat.h>\n #include <linux/ctype.h>\n #include <linux/string.h>\n+#include <linux/uaccess.h>\n #include <uapi/linux/limits.h>\n \n #include \""audit.h\""\n@@ -82,7 +83,8 @@\n #define AUDITSC_SUCCESS 1\n #define AUDITSC_FAILURE 2\n \n-/* no execve audit message should be longer than this (userspace limits) */\n+/* no execve audit message should be longer than this (userspace limits),\n+ * see the note near the top of audit_log_execve_info() about this value */\n #define MAX_EXECVE_AUDIT_LEN 7500\n \n /* max length to print of cmdline/proctitle value during audit */\n@@ -992,184 +994,178 @@ static int audit_log_pid_context(struct audit_context *context, pid_t pid,\n \treturn rc;\n }\n \n-/*\n- * to_send and len_sent accounting are very loose estimates.  We aren't\n- * really worried about a hard cap to MAX_EXECVE_AUDIT_LEN so much as being\n- * within about 500 bytes (next page boundary)\n- *\n- * why snprintf?  an int is up to 12 digits long.  if we just assumed when\n- * logging that a[%d]= was going to be 16 characters long we would be wasting\n- * space in every audit message.  In one 7500 byte message we can log up to\n- * about 1000 min size arguments.  That comes down to about 50% waste of space\n- * if we didn't do the snprintf to find out how long arg_num_len was.\n- */\n-static int audit_log_single_execve_arg(struct audit_context *context,\n-\t\t\t\t\tstruct audit_buffer **ab,\n-\t\t\t\t\tint arg_num,\n-\t\t\t\t\tsize_t *len_sent,\n-\t\t\t\t\tconst char __user *p,\n-\t\t\t\t\tchar *buf)\n+static void audit_log_execve_info(struct audit_context *context,\n+\t\t\t\t  struct audit_buffer **ab)\n {\n-\tchar arg_num_len_buf[12];\n-\tconst char __user *tmp_p = p;\n-\t/* how many digits are in arg_num? 5 is the length of ' a=\""\""' */\n-\tsize_t arg_num_len = snprintf(arg_num_len_buf, 12, \""%d\"", arg_num) + 5;\n-\tsize_t len, len_left, to_send;\n-\tsize_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;\n-\tunsigned int i, has_cntl = 0, too_long = 0;\n-\tint ret;\n-\n-\t/* strnlen_user includes the null we don't want to send */\n-\tlen_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n-\n-\t/*\n-\t * We just created this mm, if we can't find the strings\n-\t * we just copied into it something is _very_ wrong. Similar\n-\t * for strings that are too long, we should not have created\n-\t * any.\n-\t */\n-\tif (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {\n-\t\tsend_sig(SIGKILL, current, 0);\n-\t\treturn -1;\n+\tlong len_max;\n+\tlong len_rem;\n+\tlong len_full;\n+\tlong len_buf;\n+\tlong len_abuf;\n+\tlong len_tmp;\n+\tbool require_data;\n+\tbool encode;\n+\tunsigned int iter;\n+\tunsigned int arg;\n+\tchar *buf_head;\n+\tchar *buf;\n+\tconst char __user *p = (const char __user *)current->mm->arg_start;\n+\n+\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\n+\t *       data we put in the audit record for this argument (see the\n+\t *       code below) ... at this point in time 96 is plenty */\n+\tchar abuf[96];\n+\n+\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\n+\t *       current value of 7500 is not as important as the fact that it\n+\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\n+\t *       room if we go over a little bit in the logging below */\n+\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\n+\tlen_max = MAX_EXECVE_AUDIT_LEN;\n+\n+\t/* scratch buffer to hold the userspace args */\n+\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n+\tif (!buf_head) {\n+\t\taudit_panic(\""out of memory for argv string\"");\n+\t\treturn;\n \t}\n+\tbuf = buf_head;\n \n-\t/* walk the whole argument looking for non-ascii chars */\n+\taudit_log_format(*ab, \""argc=%d\"", context->execve.argc);\n+\n+\tlen_rem = len_max;\n+\tlen_buf = 0;\n+\tlen_full = 0;\n+\trequire_data = true;\n+\tencode = false;\n+\titer = 0;\n+\targ = 0;\n \tdo {\n-\t\tif (len_left > MAX_EXECVE_AUDIT_LEN)\n-\t\t\tto_send = MAX_EXECVE_AUDIT_LEN;\n-\t\telse\n-\t\t\tto_send = len_left;\n-\t\tret = copy_from_user(buf, tmp_p, to_send);\n-\t\t/*\n-\t\t * There is no reason for this copy to be short. We just\n-\t\t * copied them here, and the mm hasn't been exposed to user-\n-\t\t * space yet.\n-\t\t */\n-\t\tif (ret) {\n-\t\t\tWARN_ON(1);\n-\t\t\tsend_sig(SIGKILL, current, 0);\n-\t\t\treturn -1;\n-\t\t}\n-\t\tbuf[to_send] = '\\0';\n-\t\thas_cntl = audit_string_contains_control(buf, to_send);\n-\t\tif (has_cntl) {\n-\t\t\t/*\n-\t\t\t * hex messages get logged as 2 bytes, so we can only\n-\t\t\t * send half as much in each message\n-\t\t\t */\n-\t\t\tmax_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;\n-\t\t\tbreak;\n-\t\t}\n-\t\tlen_left -= to_send;\n-\t\ttmp_p += to_send;\n-\t} while (len_left > 0);\n-\n-\tlen_left = len;\n-\n-\tif (len > max_execve_audit_len)\n-\t\ttoo_long = 1;\n-\n-\t/* rewalk the argument actually logging the message */\n-\tfor (i = 0; len_left > 0; i++) {\n-\t\tint room_left;\n-\n-\t\tif (len_left > max_execve_audit_len)\n-\t\t\tto_send = max_execve_audit_len;\n-\t\telse\n-\t\t\tto_send = len_left;\n-\n-\t\t/* do we have space left to send this argument in this ab? */\n-\t\troom_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;\n-\t\tif (has_cntl)\n-\t\t\troom_left -= (to_send * 2);\n-\t\telse\n-\t\t\troom_left -= to_send;\n-\t\tif (room_left < 0) {\n-\t\t\t*len_sent = 0;\n-\t\t\taudit_log_end(*ab);\n-\t\t\t*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);\n-\t\t\tif (!*ab)\n-\t\t\t\treturn 0;\n-\t\t}\n+\t\t/* NOTE: we don't ever want to trust this value for anything\n+\t\t *       serious, but the audit record format insists we\n+\t\t *       provide an argument length for really long arguments,\n+\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\n+\t\t *       to use strncpy_from_user() to obtain this value for\n+\t\t *       recording in the log, although we don't use it\n+\t\t *       anywhere here to avoid a double-fetch problem */\n+\t\tif (len_full == 0)\n+\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n+\n+\t\t/* read more data from userspace */\n+\t\tif (require_data) {\n+\t\t\t/* can we make more room in the buffer? */\n+\t\t\tif (buf != buf_head) {\n+\t\t\t\tmemmove(buf_head, buf, len_buf);\n+\t\t\t\tbuf = buf_head;\n+\t\t\t}\n+\n+\t\t\t/* fetch as much as we can of the argument */\n+\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\n+\t\t\t\t\t\t    len_max - len_buf);\n+\t\t\tif (len_tmp == -EFAULT) {\n+\t\t\t\t/* unable to copy from userspace */\n+\t\t\t\tsend_sig(SIGKILL, current, 0);\n+\t\t\t\tgoto out;\n+\t\t\t} else if (len_tmp == (len_max - len_buf)) {\n+\t\t\t\t/* buffer is not large enough */\n+\t\t\t\trequire_data = true;\n+\t\t\t\t/* NOTE: if we are going to span multiple\n+\t\t\t\t *       buffers force the encoding so we stand\n+\t\t\t\t *       a chance at a sane len_full value and\n+\t\t\t\t *       consistent record encoding */\n+\t\t\t\tencode = true;\n+\t\t\t\tlen_full = len_full * 2;\n+\t\t\t\tp += len_tmp;\n+\t\t\t} else {\n+\t\t\t\trequire_data = false;\n+\t\t\t\tif (!encode)\n+\t\t\t\t\tencode = audit_string_contains_control(\n+\t\t\t\t\t\t\t\tbuf, len_tmp);\n+\t\t\t\t/* try to use a trusted value for len_full */\n+\t\t\t\tif (len_full < len_max)\n+\t\t\t\t\tlen_full = (encode ?\n+\t\t\t\t\t\t    len_tmp * 2 : len_tmp);\n+\t\t\t\tp += len_tmp + 1;\n+\t\t\t}\n+\t\t\tlen_buf += len_tmp;\n+\t\t\tbuf_head[len_buf] = '\\0';\n \n-\t\t/*\n-\t\t * first record needs to say how long the original string was\n-\t\t * so we can be sure nothing was lost.\n-\t\t */\n-\t\tif ((i == 0) && (too_long))\n-\t\t\taudit_log_format(*ab, \"" a%d_len=%zu\"", arg_num,\n-\t\t\t\t\t has_cntl ? 2*len : len);\n-\n-\t\t/*\n-\t\t * normally arguments are small enough to fit and we already\n-\t\t * filled buf above when we checked for control characters\n-\t\t * so don't bother with another copy_from_user\n-\t\t */\n-\t\tif (len >= max_execve_audit_len)\n-\t\t\tret = copy_from_user(buf, p, to_send);\n-\t\telse\n-\t\t\tret = 0;\n-\t\tif (ret) {\n-\t\t\tWARN_ON(1);\n-\t\t\tsend_sig(SIGKILL, current, 0);\n-\t\t\treturn -1;\n+\t\t\t/* length of the buffer in the audit record? */\n+\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\n \t\t}\n-\t\tbuf[to_send] = '\\0';\n-\n-\t\t/* actually log it */\n-\t\taudit_log_format(*ab, \"" a%d\"", arg_num);\n-\t\tif (too_long)\n-\t\t\taudit_log_format(*ab, \""[%d]\"", i);\n-\t\taudit_log_format(*ab, \""=\"");\n-\t\tif (has_cntl)\n-\t\t\taudit_log_n_hex(*ab, buf, to_send);\n-\t\telse\n-\t\t\taudit_log_string(*ab, buf);\n-\n-\t\tp += to_send;\n-\t\tlen_left -= to_send;\n-\t\t*len_sent += arg_num_len;\n-\t\tif (has_cntl)\n-\t\t\t*len_sent += to_send * 2;\n-\t\telse\n-\t\t\t*len_sent += to_send;\n-\t}\n-\t/* include the null we didn't log */\n-\treturn len + 1;\n-}\n \n-static void audit_log_execve_info(struct audit_context *context,\n-\t\t\t\t  struct audit_buffer **ab)\n-{\n-\tint i, len;\n-\tsize_t len_sent = 0;\n-\tconst char __user *p;\n-\tchar *buf;\n+\t\t/* write as much as we can to the audit log */\n+\t\tif (len_buf > 0) {\n+\t\t\t/* NOTE: some magic numbers here - basically if we\n+\t\t\t *       can't fit a reasonable amount of data into the\n+\t\t\t *       existing audit buffer, flush it and start with\n+\t\t\t *       a new buffer */\n+\t\t\tif ((sizeof(abuf) + 8) > len_rem) {\n+\t\t\t\tlen_rem = len_max;\n+\t\t\t\taudit_log_end(*ab);\n+\t\t\t\t*ab = audit_log_start(context,\n+\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);\n+\t\t\t\tif (!*ab)\n+\t\t\t\t\tgoto out;\n+\t\t\t}\n \n-\tp = (const char __user *)current->mm->arg_start;\n+\t\t\t/* create the non-arg portion of the arg record */\n+\t\t\tlen_tmp = 0;\n+\t\t\tif (require_data || (iter > 0) ||\n+\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\n+\t\t\t\tif (iter == 0) {\n+\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t\t\"" a%d_len=%lu\"",\n+\t\t\t\t\t\t\targ, len_full);\n+\t\t\t\t}\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \"" a%d[%d]=\"", arg, iter++);\n+\t\t\t} else\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \"" a%d=\"", arg);\n+\t\t\tWARN_ON(len_tmp >= sizeof(abuf));\n+\t\t\tabuf[sizeof(abuf) - 1] = '\\0';\n+\n+\t\t\t/* log the arg in the audit record */\n+\t\t\taudit_log_format(*ab, \""%s\"", abuf);\n+\t\t\tlen_rem -= len_tmp;\n+\t\t\tlen_tmp = len_buf;\n+\t\t\tif (encode) {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */\n+\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp * 2;\n+\t\t\t\tlen_abuf -= len_tmp * 2;\n+\t\t\t} else {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */\n+\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp + 2;\n+\t\t\t\t/* don't subtract the \""2\"" because we still need\n+\t\t\t\t * to add quotes to the remaining string */\n+\t\t\t\tlen_abuf -= len_tmp;\n+\t\t\t}\n+\t\t\tlen_buf -= len_tmp;\n+\t\t\tbuf += len_tmp;\n+\t\t}\n \n-\taudit_log_format(*ab, \""argc=%d\"", context->execve.argc);\n+\t\t/* ready to move to the next argument? */\n+\t\tif ((len_buf == 0) && !require_data) {\n+\t\t\targ++;\n+\t\t\titer = 0;\n+\t\t\tlen_full = 0;\n+\t\t\trequire_data = true;\n+\t\t\tencode = false;\n+\t\t}\n+\t} while (arg < context->execve.argc);\n \n-\t/*\n-\t * we need some kernel buffer to hold the userspace args.  Just\n-\t * allocate one big one rather than allocating one of the right size\n-\t * for every single argument inside audit_log_single_execve_arg()\n-\t * should be <8k allocation so should be pretty safe.\n-\t */\n-\tbuf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n-\tif (!buf) {\n-\t\taudit_panic(\""out of memory for argv string\"");\n-\t\treturn;\n-\t}\n+\t/* NOTE: the caller handles the final audit_log_end() call */\n \n-\tfor (i = 0; i < context->execve.argc; i++) {\n-\t\tlen = audit_log_single_execve_arg(context, ab, i,\n-\t\t\t\t\t\t  &len_sent, p, buf);\n-\t\tif (len <= 0)\n-\t\t\tbreak;\n-\t\tp += len;\n-\t}\n-\tkfree(buf);\n+out:\n+\tkfree(buf_head);\n }\n \n static void show_special(struct audit_context *context, int *call_panic)""}","static int audit_log_single_execve_arg(struct audit_context *context,
static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
 {
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
 	}
	buf = buf_head;
 
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
 	do {
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
 
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
","static int audit_log_single_execve_arg(struct audit_context *context,
					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
 {
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
 	}
 
	/* walk the whole argument looking for non-ascii chars */
 	do {
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
 
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
 		}
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
",C,"static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
	buf = buf_head;
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
","					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
	/* walk the whole argument looking for non-ascii chars */
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
",,"@@ -73,6 +73,7 @@
 #include <linux/compat.h>
 #include <linux/ctype.h>
 #include <linux/string.h>
+#include <linux/uaccess.h>
 #include <uapi/linux/limits.h>
 
 #include ""audit.h""
@@ -82,7 +83,8 @@
 #define AUDITSC_SUCCESS 1
 #define AUDITSC_FAILURE 2
 
-/* no execve audit message should be longer than this (userspace limits) */
+/* no execve audit message should be longer than this (userspace limits),
+ * see the note near the top of audit_log_execve_info() about this value */
 #define MAX_EXECVE_AUDIT_LEN 7500
 
 /* max length to print of cmdline/proctitle value during audit */
@@ -992,184 +994,178 @@ static int audit_log_pid_context(struct audit_context *context, pid_t pid,
 	return rc;
 }
 
-/*
- * to_send and len_sent accounting are very loose estimates.  We aren't
- * really worried about a hard cap to MAX_EXECVE_AUDIT_LEN so much as being
- * within about 500 bytes (next page boundary)
- *
- * why snprintf?  an int is up to 12 digits long.  if we just assumed when
- * logging that a[%d]= was going to be 16 characters long we would be wasting
- * space in every audit message.  In one 7500 byte message we can log up to
- * about 1000 min size arguments.  That comes down to about 50% waste of space
- * if we didn't do the snprintf to find out how long arg_num_len was.
- */
-static int audit_log_single_execve_arg(struct audit_context *context,
-					struct audit_buffer **ab,
-					int arg_num,
-					size_t *len_sent,
-					const char __user *p,
-					char *buf)
+static void audit_log_execve_info(struct audit_context *context,
+				  struct audit_buffer **ab)
 {
-	char arg_num_len_buf[12];
-	const char __user *tmp_p = p;
-	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
-	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
-	size_t len, len_left, to_send;
-	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
-	unsigned int i, has_cntl = 0, too_long = 0;
-	int ret;
-
-	/* strnlen_user includes the null we don't want to send */
-	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
-
-	/*
-	 * We just created this mm, if we can't find the strings
-	 * we just copied into it something is _very_ wrong. Similar
-	 * for strings that are too long, we should not have created
-	 * any.
-	 */
-	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
-		send_sig(SIGKILL, current, 0);
-		return -1;
+	long len_max;
+	long len_rem;
+	long len_full;
+	long len_buf;
+	long len_abuf;
+	long len_tmp;
+	bool require_data;
+	bool encode;
+	unsigned int iter;
+	unsigned int arg;
+	char *buf_head;
+	char *buf;
+	const char __user *p = (const char __user *)current->mm->arg_start;
+
+	/* NOTE: this buffer needs to be large enough to hold all the non-arg
+	 *       data we put in the audit record for this argument (see the
+	 *       code below) ... at this point in time 96 is plenty */
+	char abuf[96];
+
+	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
+	 *       current value of 7500 is not as important as the fact that it
+	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
+	 *       room if we go over a little bit in the logging below */
+	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
+	len_max = MAX_EXECVE_AUDIT_LEN;
+
+	/* scratch buffer to hold the userspace args */
+	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
+	if (!buf_head) {
+		audit_panic(""out of memory for argv string"");
+		return;
 	}
+	buf = buf_head;
 
-	/* walk the whole argument looking for non-ascii chars */
+	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
+
+	len_rem = len_max;
+	len_buf = 0;
+	len_full = 0;
+	require_data = true;
+	encode = false;
+	iter = 0;
+	arg = 0;
 	do {
-		if (len_left > MAX_EXECVE_AUDIT_LEN)
-			to_send = MAX_EXECVE_AUDIT_LEN;
-		else
-			to_send = len_left;
-		ret = copy_from_user(buf, tmp_p, to_send);
-		/*
-		 * There is no reason for this copy to be short. We just
-		 * copied them here, and the mm hasn't been exposed to user-
-		 * space yet.
-		 */
-		if (ret) {
-			WARN_ON(1);
-			send_sig(SIGKILL, current, 0);
-			return -1;
-		}
-		buf[to_send] = '\0';
-		has_cntl = audit_string_contains_control(buf, to_send);
-		if (has_cntl) {
-			/*
-			 * hex messages get logged as 2 bytes, so we can only
-			 * send half as much in each message
-			 */
-			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
-			break;
-		}
-		len_left -= to_send;
-		tmp_p += to_send;
-	} while (len_left > 0);
-
-	len_left = len;
-
-	if (len > max_execve_audit_len)
-		too_long = 1;
-
-	/* rewalk the argument actually logging the message */
-	for (i = 0; len_left > 0; i++) {
-		int room_left;
-
-		if (len_left > max_execve_audit_len)
-			to_send = max_execve_audit_len;
-		else
-			to_send = len_left;
-
-		/* do we have space left to send this argument in this ab? */
-		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
-		if (has_cntl)
-			room_left -= (to_send * 2);
-		else
-			room_left -= to_send;
-		if (room_left < 0) {
-			*len_sent = 0;
-			audit_log_end(*ab);
-			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
-			if (!*ab)
-				return 0;
-		}
+		/* NOTE: we don't ever want to trust this value for anything
+		 *       serious, but the audit record format insists we
+		 *       provide an argument length for really long arguments,
+		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
+		 *       to use strncpy_from_user() to obtain this value for
+		 *       recording in the log, although we don't use it
+		 *       anywhere here to avoid a double-fetch problem */
+		if (len_full == 0)
+			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;
+
+		/* read more data from userspace */
+		if (require_data) {
+			/* can we make more room in the buffer? */
+			if (buf != buf_head) {
+				memmove(buf_head, buf, len_buf);
+				buf = buf_head;
+			}
+
+			/* fetch as much as we can of the argument */
+			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
+						    len_max - len_buf);
+			if (len_tmp == -EFAULT) {
+				/* unable to copy from userspace */
+				send_sig(SIGKILL, current, 0);
+				goto out;
+			} else if (len_tmp == (len_max - len_buf)) {
+				/* buffer is not large enough */
+				require_data = true;
+				/* NOTE: if we are going to span multiple
+				 *       buffers force the encoding so we stand
+				 *       a chance at a sane len_full value and
+				 *       consistent record encoding */
+				encode = true;
+				len_full = len_full * 2;
+				p += len_tmp;
+			} else {
+				require_data = false;
+				if (!encode)
+					encode = audit_string_contains_control(
+								buf, len_tmp);
+				/* try to use a trusted value for len_full */
+				if (len_full < len_max)
+					len_full = (encode ?
+						    len_tmp * 2 : len_tmp);
+				p += len_tmp + 1;
+			}
+			len_buf += len_tmp;
+			buf_head[len_buf] = '\0';
 
-		/*
-		 * first record needs to say how long the original string was
-		 * so we can be sure nothing was lost.
-		 */
-		if ((i == 0) && (too_long))
-			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
-					 has_cntl ? 2*len : len);
-
-		/*
-		 * normally arguments are small enough to fit and we already
-		 * filled buf above when we checked for control characters
-		 * so don't bother with another copy_from_user
-		 */
-		if (len >= max_execve_audit_len)
-			ret = copy_from_user(buf, p, to_send);
-		else
-			ret = 0;
-		if (ret) {
-			WARN_ON(1);
-			send_sig(SIGKILL, current, 0);
-			return -1;
+			/* length of the buffer in the audit record? */
+			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
-		buf[to_send] = '\0';
-
-		/* actually log it */
-		audit_log_format(*ab, "" a%d"", arg_num);
-		if (too_long)
-			audit_log_format(*ab, ""[%d]"", i);
-		audit_log_format(*ab, ""="");
-		if (has_cntl)
-			audit_log_n_hex(*ab, buf, to_send);
-		else
-			audit_log_string(*ab, buf);
-
-		p += to_send;
-		len_left -= to_send;
-		*len_sent += arg_num_len;
-		if (has_cntl)
-			*len_sent += to_send * 2;
-		else
-			*len_sent += to_send;
-	}
-	/* include the null we didn't log */
-	return len + 1;
-}
 
-static void audit_log_execve_info(struct audit_context *context,
-				  struct audit_buffer **ab)
-{
-	int i, len;
-	size_t len_sent = 0;
-	const char __user *p;
-	char *buf;
+		/* write as much as we can to the audit log */
+		if (len_buf > 0) {
+			/* NOTE: some magic numbers here - basically if we
+			 *       can't fit a reasonable amount of data into the
+			 *       existing audit buffer, flush it and start with
+			 *       a new buffer */
+			if ((sizeof(abuf) + 8) > len_rem) {
+				len_rem = len_max;
+				audit_log_end(*ab);
+				*ab = audit_log_start(context,
+						      GFP_KERNEL, AUDIT_EXECVE);
+				if (!*ab)
+					goto out;
+			}
 
-	p = (const char __user *)current->mm->arg_start;
+			/* create the non-arg portion of the arg record */
+			len_tmp = 0;
+			if (require_data || (iter > 0) ||
+			    ((len_abuf + sizeof(abuf)) > len_rem)) {
+				if (iter == 0) {
+					len_tmp += snprintf(&abuf[len_tmp],
+							sizeof(abuf) - len_tmp,
+							"" a%d_len=%lu"",
+							arg, len_full);
+				}
+				len_tmp += snprintf(&abuf[len_tmp],
+						    sizeof(abuf) - len_tmp,
+						    "" a%d[%d]="", arg, iter++);
+			} else
+				len_tmp += snprintf(&abuf[len_tmp],
+						    sizeof(abuf) - len_tmp,
+						    "" a%d="", arg);
+			WARN_ON(len_tmp >= sizeof(abuf));
+			abuf[sizeof(abuf) - 1] = '\0';
+
+			/* log the arg in the audit record */
+			audit_log_format(*ab, ""%s"", abuf);
+			len_rem -= len_tmp;
+			len_tmp = len_buf;
+			if (encode) {
+				if (len_abuf > len_rem)
+					len_tmp = len_rem / 2; /* encoding */
+				audit_log_n_hex(*ab, buf, len_tmp);
+				len_rem -= len_tmp * 2;
+				len_abuf -= len_tmp * 2;
+			} else {
+				if (len_abuf > len_rem)
+					len_tmp = len_rem - 2; /* quotes */
+				audit_log_n_string(*ab, buf, len_tmp);
+				len_rem -= len_tmp + 2;
+				/* don't subtract the ""2"" because we still need
+				 * to add quotes to the remaining string */
+				len_abuf -= len_tmp;
+			}
+			len_buf -= len_tmp;
+			buf += len_tmp;
+		}
 
-	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
+		/* ready to move to the next argument? */
+		if ((len_buf == 0) && !require_data) {
+			arg++;
+			iter = 0;
+			len_full = 0;
+			require_data = true;
+			encode = false;
+		}
+	} while (arg < context->execve.argc);
 
-	/*
-	 * we need some kernel buffer to hold the userspace args.  Just
-	 * allocate one big one rather than allocating one of the right size
-	 * for every single argument inside audit_log_single_execve_arg()
-	 * should be <8k allocation so should be pretty safe.
-	 */
-	buf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
-	if (!buf) {
-		audit_panic(""out of memory for argv string"");
-		return;
-	}
+	/* NOTE: the caller handles the final audit_log_end() call */
 
-	for (i = 0; i < context->execve.argc; i++) {
-		len = audit_log_single_execve_arg(context, ab, i,
-						  &len_sent, p, buf);
-		if (len <= 0)
-			break;
-		p += len;
-	}
-	kfree(buf);
+out:
+	kfree(buf_head);
 }
 
 static void show_special(struct audit_context *context, int *call_panic)",linux,43761473c254b45883a64441dd0bc85a42f3645c,0b7a0fdb29715e38641beb39db4d01695b22b5aa,1,"static int audit_log_single_execve_arg(struct audit_context *context,
//flaw_line_below:
					struct audit_buffer **ab,
//flaw_line_below:
					int arg_num,
//flaw_line_below:
					size_t *len_sent,
//flaw_line_below:
					const char __user *p,
//flaw_line_below:
					char *buf)
//fix_flaw_line_below:
//static void audit_log_execve_info(struct audit_context *context,
//fix_flaw_line_below:
//				  struct audit_buffer **ab)
 {
//flaw_line_below:
	char arg_num_len_buf[12];
//flaw_line_below:
	const char __user *tmp_p = p;
//flaw_line_below:
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
//flaw_line_below:
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
//flaw_line_below:
	size_t len, len_left, to_send;
//flaw_line_below:
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
//flaw_line_below:
	unsigned int i, has_cntl = 0, too_long = 0;
//flaw_line_below:
	int ret;
//flaw_line_below:

//flaw_line_below:
	/* strnlen_user includes the null we don't want to send */
//flaw_line_below:
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
//flaw_line_below:

//flaw_line_below:
	/*
//flaw_line_below:
	 * We just created this mm, if we can't find the strings
//flaw_line_below:
	 * we just copied into it something is _very_ wrong. Similar
//flaw_line_below:
	 * for strings that are too long, we should not have created
//flaw_line_below:
	 * any.
//flaw_line_below:
	 */
//flaw_line_below:
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
//flaw_line_below:
		send_sig(SIGKILL, current, 0);
//flaw_line_below:
		return -1;
//fix_flaw_line_below:
//	long len_max;
//fix_flaw_line_below:
//	long len_rem;
//fix_flaw_line_below:
//	long len_full;
//fix_flaw_line_below:
//	long len_buf;
//fix_flaw_line_below:
//	long len_abuf;
//fix_flaw_line_below:
//	long len_tmp;
//fix_flaw_line_below:
//	bool require_data;
//fix_flaw_line_below:
//	bool encode;
//fix_flaw_line_below:
//	unsigned int iter;
//fix_flaw_line_below:
//	unsigned int arg;
//fix_flaw_line_below:
//	char *buf_head;
//fix_flaw_line_below:
//	char *buf;
//fix_flaw_line_below:
//	const char __user *p = (const char __user *)current->mm->arg_start;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* NOTE: this buffer needs to be large enough to hold all the non-arg
//fix_flaw_line_below:
//	 *       data we put in the audit record for this argument (see the
//fix_flaw_line_below:
//	 *       code below) ... at this point in time 96 is plenty */
//fix_flaw_line_below:
//	char abuf[96];
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
//fix_flaw_line_below:
//	 *       current value of 7500 is not as important as the fact that it
//fix_flaw_line_below:
//	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
//fix_flaw_line_below:
//	 *       room if we go over a little bit in the logging below */
//fix_flaw_line_below:
//	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
//fix_flaw_line_below:
//	len_max = MAX_EXECVE_AUDIT_LEN;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* scratch buffer to hold the userspace args */
//fix_flaw_line_below:
//	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
//fix_flaw_line_below:
//	if (!buf_head) {
//fix_flaw_line_below:
//		audit_panic(""out of memory for argv string"");
//fix_flaw_line_below:
//		return;
 	}
//fix_flaw_line_below:
//	buf = buf_head;
 
//flaw_line_below:
	/* walk the whole argument looking for non-ascii chars */
//fix_flaw_line_below:
//	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	len_rem = len_max;
//fix_flaw_line_below:
//	len_buf = 0;
//fix_flaw_line_below:
//	len_full = 0;
//fix_flaw_line_below:
//	require_data = true;
//fix_flaw_line_below:
//	encode = false;
//fix_flaw_line_below:
//	iter = 0;
//fix_flaw_line_below:
//	arg = 0;
 	do {
//flaw_line_below:
		if (len_left > MAX_EXECVE_AUDIT_LEN)
//flaw_line_below:
			to_send = MAX_EXECVE_AUDIT_LEN;
//flaw_line_below:
		else
//flaw_line_below:
			to_send = len_left;
//flaw_line_below:
		ret = copy_from_user(buf, tmp_p, to_send);
//flaw_line_below:
		/*
//flaw_line_below:
		 * There is no reason for this copy to be short. We just
//flaw_line_below:
		 * copied them here, and the mm hasn't been exposed to user-
//flaw_line_below:
		 * space yet.
//flaw_line_below:
		 */
//flaw_line_below:
		if (ret) {
//flaw_line_below:
			WARN_ON(1);
//flaw_line_below:
			send_sig(SIGKILL, current, 0);
//flaw_line_below:
			return -1;
//flaw_line_below:
		}
//flaw_line_below:
		buf[to_send] = '\0';
//flaw_line_below:
		has_cntl = audit_string_contains_control(buf, to_send);
//flaw_line_below:
		if (has_cntl) {
//flaw_line_below:
			/*
//flaw_line_below:
			 * hex messages get logged as 2 bytes, so we can only
//flaw_line_below:
			 * send half as much in each message
//flaw_line_below:
			 */
//flaw_line_below:
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
//flaw_line_below:
			break;
//flaw_line_below:
		}
//flaw_line_below:
		len_left -= to_send;
//flaw_line_below:
		tmp_p += to_send;
//flaw_line_below:
	} while (len_left > 0);
//flaw_line_below:

//flaw_line_below:
	len_left = len;
//flaw_line_below:

//flaw_line_below:
	if (len > max_execve_audit_len)
//flaw_line_below:
		too_long = 1;
//flaw_line_below:

//flaw_line_below:
	/* rewalk the argument actually logging the message */
//flaw_line_below:
	for (i = 0; len_left > 0; i++) {
//flaw_line_below:
		int room_left;
//flaw_line_below:

//flaw_line_below:
		if (len_left > max_execve_audit_len)
//flaw_line_below:
			to_send = max_execve_audit_len;
//flaw_line_below:
		else
//flaw_line_below:
			to_send = len_left;
//flaw_line_below:

//flaw_line_below:
		/* do we have space left to send this argument in this ab? */
//flaw_line_below:
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			room_left -= (to_send * 2);
//flaw_line_below:
		else
//flaw_line_below:
			room_left -= to_send;
//flaw_line_below:
		if (room_left < 0) {
//flaw_line_below:
			*len_sent = 0;
//flaw_line_below:
			audit_log_end(*ab);
//flaw_line_below:
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
//flaw_line_below:
			if (!*ab)
//flaw_line_below:
				return 0;
//flaw_line_below:
		}
//fix_flaw_line_below:
//		/* NOTE: we don't ever want to trust this value for anything
//fix_flaw_line_below:
//		 *       serious, but the audit record format insists we
//fix_flaw_line_below:
//		 *       provide an argument length for really long arguments,
//fix_flaw_line_below:
//		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
//fix_flaw_line_below:
//		 *       to use strncpy_from_user() to obtain this value for
//fix_flaw_line_below:
//		 *       recording in the log, although we don't use it
//fix_flaw_line_below:
//		 *       anywhere here to avoid a double-fetch problem */
//fix_flaw_line_below:
//		if (len_full == 0)
//fix_flaw_line_below:
//			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/* read more data from userspace */
//fix_flaw_line_below:
//		if (require_data) {
//fix_flaw_line_below:
//			/* can we make more room in the buffer? */
//fix_flaw_line_below:
//			if (buf != buf_head) {
//fix_flaw_line_below:
//				memmove(buf_head, buf, len_buf);
//fix_flaw_line_below:
//				buf = buf_head;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//			/* fetch as much as we can of the argument */
//fix_flaw_line_below:
//			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
//fix_flaw_line_below:
//						    len_max - len_buf);
//fix_flaw_line_below:
//			if (len_tmp == -EFAULT) {
//fix_flaw_line_below:
//				/* unable to copy from userspace */
//fix_flaw_line_below:
//				send_sig(SIGKILL, current, 0);
//fix_flaw_line_below:
//				goto out;
//fix_flaw_line_below:
//			} else if (len_tmp == (len_max - len_buf)) {
//fix_flaw_line_below:
//				/* buffer is not large enough */
//fix_flaw_line_below:
//				require_data = true;
//fix_flaw_line_below:
//				/* NOTE: if we are going to span multiple
//fix_flaw_line_below:
//				 *       buffers force the encoding so we stand
//fix_flaw_line_below:
//				 *       a chance at a sane len_full value and
//fix_flaw_line_below:
//				 *       consistent record encoding */
//fix_flaw_line_below:
//				encode = true;
//fix_flaw_line_below:
//				len_full = len_full * 2;
//fix_flaw_line_below:
//				p += len_tmp;
//fix_flaw_line_below:
//			} else {
//fix_flaw_line_below:
//				require_data = false;
//fix_flaw_line_below:
//				if (!encode)
//fix_flaw_line_below:
//					encode = audit_string_contains_control(
//fix_flaw_line_below:
//								buf, len_tmp);
//fix_flaw_line_below:
//				/* try to use a trusted value for len_full */
//fix_flaw_line_below:
//				if (len_full < len_max)
//fix_flaw_line_below:
//					len_full = (encode ?
//fix_flaw_line_below:
//						    len_tmp * 2 : len_tmp);
//fix_flaw_line_below:
//				p += len_tmp + 1;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//			len_buf += len_tmp;
//fix_flaw_line_below:
//			buf_head[len_buf] = '\0';
 
//flaw_line_below:
		/*
//flaw_line_below:
		 * first record needs to say how long the original string was
//flaw_line_below:
		 * so we can be sure nothing was lost.
//flaw_line_below:
		 */
//flaw_line_below:
		if ((i == 0) && (too_long))
//flaw_line_below:
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
//flaw_line_below:
					 has_cntl ? 2*len : len);
//flaw_line_below:

//flaw_line_below:
		/*
//flaw_line_below:
		 * normally arguments are small enough to fit and we already
//flaw_line_below:
		 * filled buf above when we checked for control characters
//flaw_line_below:
		 * so don't bother with another copy_from_user
//flaw_line_below:
		 */
//flaw_line_below:
		if (len >= max_execve_audit_len)
//flaw_line_below:
			ret = copy_from_user(buf, p, to_send);
//flaw_line_below:
		else
//flaw_line_below:
			ret = 0;
//flaw_line_below:
		if (ret) {
//flaw_line_below:
			WARN_ON(1);
//flaw_line_below:
			send_sig(SIGKILL, current, 0);
//flaw_line_below:
			return -1;
//fix_flaw_line_below:
//			/* length of the buffer in the audit record? */
//fix_flaw_line_below:
//			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
//flaw_line_below:
		buf[to_send] = '\0';
//flaw_line_below:

//flaw_line_below:
		/* actually log it */
//flaw_line_below:
		audit_log_format(*ab, "" a%d"", arg_num);
//flaw_line_below:
		if (too_long)
//flaw_line_below:
			audit_log_format(*ab, ""[%d]"", i);
//flaw_line_below:
		audit_log_format(*ab, ""="");
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			audit_log_n_hex(*ab, buf, to_send);
//flaw_line_below:
		else
//flaw_line_below:
			audit_log_string(*ab, buf);
//flaw_line_below:

//flaw_line_below:
		p += to_send;
//flaw_line_below:
		len_left -= to_send;
//flaw_line_below:
		*len_sent += arg_num_len;
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			*len_sent += to_send * 2;
//flaw_line_below:
		else
//flaw_line_below:
			*len_sent += to_send;
//flaw_line_below:
	}
//flaw_line_below:
	/* include the null we didn't log */
//flaw_line_below:
	return len + 1;
//flaw_line_below:
}
",180191,"static int audit_log_single_execve_arg(struct audit_context *context,
					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
 {
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
 	}
 
	/* walk the whole argument looking for non-ascii chars */
 	do {
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
 
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
 		}
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
","static int audit_log_single_execve_arg(struct audit_context *context,
static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
 {
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
 	}
	buf = buf_head;
 
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
 	do {
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
 
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
","The `audit_log_single_execve_arg` function is designed to log a single argument from an `execve` system call for auditing purposes. It takes various parameters, including the audit context, a pointer to an audit buffer, the argument number, and a user-space pointer to the argument string. The function first checks the length of the argument and ensures it does not exceed predefined limits. It then copies the argument from user space, checking for control characters, and logs the argument in segments if necessary. The function handles potential errors by sending a signal to terminate the process and returns the total length of the argument logged, including a null terminator.","The provided code is responsible for logging a single argument from an `execve` system call for auditing purposes. It begins by determining the length of the argument and validating it against predefined limits. The code then copies the argument from user space while checking for control characters that may require special handling. If the argument is too long, it is logged in segments, ensuring that all data is captured correctly. The function also manages potential errors by terminating the process if issues arise during copying, ultimately returning the total length of the logged argument, including a null terminator.","The vulnerability identified as CWE-362, which pertains to a race condition, arises from the original code's reliance on user-supplied data without adequate validation or synchronization. Specifically, the function copies data from user space using `copy_from_user`, which can lead to inconsistencies if the user space memory is modified concurrently. In the fixed code, the use of `strncpy_from_user` mitigates this risk by ensuring that the length of the data copied is controlled and that the buffer is managed more safely. Additionally, the introduction of checks for buffer overflow and proper handling of encoding ensures that the logging process remains consistent and secure, reducing the likelihood of exploitation.","The vulnerability identified as CWE-362, related to race conditions, occurs when the code interacts with user-supplied data without sufficient safeguards, allowing for potential inconsistencies if the data is modified concurrently. In the original implementation, the copying of data from user space lacked robust validation, which could lead to unexpected behavior or security issues. The fixed version addresses this by implementing safer data handling techniques, ensuring that the length of the copied data is controlled and that buffer overflows are prevented. This enhances the reliability of the logging process and reduces the risk of exploitation through concurrent modifications.","The vulnerability was fixed by replacing the original data copying mechanism with a more secure approach using `strncpy_from_user`, which ensures that the length of the copied data is properly managed and prevents buffer overflows. The revised code also introduces checks for the maximum buffer size, allowing for dynamic adjustments based on the data being processed. Additionally, the use of flags like `require_data` and `encode` helps manage the state of the data being logged, ensuring that control characters are handled appropriately and that the logging process remains consistent and secure throughout the execution.","The vulnerability was addressed by implementing a more secure method for copying data from user space, which includes better management of the data length to prevent buffer overflows. The revised code introduces checks to ensure that the buffer can accommodate the incoming data, dynamically adjusting as necessary. Additionally, it incorporates flags to track the state of the data being processed, allowing for proper handling of special characters and ensuring that the logging remains consistent. These changes enhance the overall reliability and security of the logging process, mitigating the risk of race conditions and data inconsistencies."
20,180192,180192,,Local,Not required,,CVE-2016-6130,https://www.cvedetails.com/cve/CVE-2016-6130/,CWE-362,Medium,Partial,,,2016-07-03,1.9,"Race condition in the sclp_ctl_ioctl_sccb function in drivers/s390/char/sclp_ctl.c in the Linux kernel before 4.6 allows local users to obtain sensitive information from kernel memory by changing a certain length value, aka a *double fetch* vulnerability.",2016-11-28,+Info ,7,https://github.com/torvalds/linux/commit/532c34b5fbf1687df63b3fcd5b2846312ac943c6,532c34b5fbf1687df63b3fcd5b2846312ac943c6,"s390/sclp_ctl: fix potential information leak with /dev/sclp

The sclp_ctl_ioctl_sccb function uses two copy_from_user calls to
retrieve the sclp request from user space. The first copy_from_user
fetches the length of the request which is stored in the first two
bytes of the request. The second copy_from_user gets the complete
sclp request, but this copies the length field a second time.
A malicious user may have changed the length in the meantime.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Reviewed-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>",5,drivers/s390/char/sclp_ctl.c,"{""sha"": ""ea607a4a1bddaf3e41165aebed1fd787b87d754e"", ""filename"": ""drivers/s390/char/sclp_ctl.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 5, ""changes"": 12, ""blob_url"": ""https://github.com/torvalds/linux/blob/532c34b5fbf1687df63b3fcd5b2846312ac943c6/drivers/s390/char/sclp_ctl.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/532c34b5fbf1687df63b3fcd5b2846312ac943c6/drivers/s390/char/sclp_ctl.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/s390/char/sclp_ctl.c?ref=532c34b5fbf1687df63b3fcd5b2846312ac943c6"", ""patch"": ""@@ -56,6 +56,7 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)\n {\n \tstruct sclp_ctl_sccb ctl_sccb;\n \tstruct sccb_header *sccb;\n+\tunsigned long copied;\n \tint rc;\n \n \tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n@@ -65,14 +66,15 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)\n \tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n \tif (!sccb)\n \t\treturn -ENOMEM;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {\n+\tcopied = PAGE_SIZE -\n+\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);\n+\tif (offsetof(struct sccb_header, length) +\n+\t    sizeof(sccb->length) > copied || sccb->length > copied) {\n \t\trc = -EFAULT;\n \t\tgoto out_free;\n \t}\n-\tif (sccb->length > PAGE_SIZE || sccb->length < 8)\n-\t\treturn -EINVAL;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {\n-\t\trc = -EFAULT;\n+\tif (sccb->length < 8) {\n+\t\trc = -EINVAL;\n \t\tgoto out_free;\n \t}\n \trc = sclp_sync_request(ctl_sccb.cmdw, sccb);""}","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length < 8) {
		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
",C,"	unsigned long copied;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
	if (sccb->length < 8) {
		rc = -EINVAL;
","	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
",,"@@ -56,6 +56,7 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
+	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
@@ -65,14 +66,15 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
-	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
+	copied = PAGE_SIZE -
+		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
+	if (offsetof(struct sccb_header, length) +
+	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
-	if (sccb->length > PAGE_SIZE || sccb->length < 8)
-		return -EINVAL;
-	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
-		rc = -EFAULT;
+	if (sccb->length < 8) {
+		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);",linux,532c34b5fbf1687df63b3fcd5b2846312ac943c6,723cacbd9dc79582e562c123a0bacf8bfc69e72a,1,"static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
//fix_flaw_line_below:
//	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
//flaw_line_below:
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
//fix_flaw_line_below:
//	copied = PAGE_SIZE -
//fix_flaw_line_below:
//		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
//fix_flaw_line_below:
//	if (offsetof(struct sccb_header, length) +
//fix_flaw_line_below:
//	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
//flaw_line_below:
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
//flaw_line_below:
		return -EINVAL;
//flaw_line_below:
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
//flaw_line_below:
		rc = -EFAULT;
//fix_flaw_line_below:
//	if (sccb->length < 8) {
//fix_flaw_line_below:
//		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
",180192,"static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length < 8) {
		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","The `sclp_ctl_ioctl_sccb` function handles an IOCTL request for a System Control Block (SCCB) in a kernel module. It begins by copying data from user space into a local structure and checks if the command word is supported. If valid, it allocates memory for an SCCB and copies the initial data from user space. The function then validates the SCCB's length, ensuring it falls within acceptable bounds. It processes the SCCB through a synchronous request and, upon success, copies the modified SCCB back to user space. Finally, it frees the allocated memory and returns the result of the operation.","The provided code implements a function that processes an IOCTL request related to a control block in a kernel module. It begins by retrieving data from user space and verifying the validity of a command. If the command is supported, it allocates memory for the control block and copies the necessary data from user space. The function checks the length of the control block to ensure it is within acceptable limits. After processing the request, it returns the results back to user space and cleans up by freeing the allocated memory, handling any errors that may arise during the process.","The vulnerability identified in the original code, classified as CWE-362 (Race Condition), arises from the lack of proper validation of the control block's length before using it. In the vulnerable code, the length of the control block was checked only after copying data from user space, which could allow an attacker to manipulate the data and potentially cause buffer overflows or other unintended behaviors. The fixed code addresses this by calculating the amount of data copied and ensuring that the length specified in the control block does not exceed this amount, thereby preventing exploitation through improper length validation.","The vulnerability identified in the original code, classified as CWE-362 (Race Condition), stems from inadequate validation of the length of data being processed, which could allow an attacker to manipulate input and cause buffer overflows or other unintended behaviors. In the fixed code, the implementation ensures that the length of the data copied is checked against the specified length in the control block before any further processing occurs. This change effectively mitigates the risk of exploitation by ensuring that the length is validated properly, preventing potential race conditions that could arise from untrusted user input.","The vulnerability was fixed by modifying the way the length of the data is validated after copying from user space. In the updated code, the amount of data copied is stored in the variable `copied`, and checks are performed to ensure that the length specified in the control block does not exceed this value. Specifically, the code verifies that the sum of the offset of the length field and its size does not exceed the copied amount, and it also checks that the length is greater than a minimum threshold. These changes prevent potential buffer overflows and ensure safer handling of user input.","The vulnerability was addressed by implementing a more robust validation process for the length of the data being processed. The updated code calculates the total amount of data successfully copied from user space and ensures that this amount is sufficient to accommodate the specified length in the control block. Additionally, it checks that the length meets a minimum requirement to prevent underflows. By performing these checks before further processing, the code mitigates the risk of buffer overflows and ensures that only valid and safe data is handled, thereby enhancing overall security."
21,180581,180581,,Local,Not required,Complete,CVE-2016-2544,https://www.cvedetails.com/cve/CVE-2016-2544/,CWE-362,Medium,,,,2016-04-27,4.7,Race condition in the queue_delete function in sound/core/seq/seq_queue.c in the Linux kernel before 4.4.1 allows local users to cause a denial of service (use-after-free and system crash) by making an ioctl call at a certain time.,2017-09-06,DoS ,2,https://github.com/torvalds/linux/commit/3567eb6af614dac436c4b16a8d426f9faed639b3,3567eb6af614dac436c4b16a8d426f9faed639b3,"ALSA: seq: Fix race at timer setup and close

ALSA sequencer code has an open race between the timer setup ioctl and
the close of the client.  This was triggered by syzkaller fuzzer, and
a use-after-free was caught there as a result.

This patch papers over it by adding a proper queue->timer_mutex lock
around the timer-related calls in the relevant code path.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Tested-by: Dmitry Vyukov <dvyukov@google.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Takashi Iwai <tiwai@suse.de>",0,sound/core/seq/seq_queue.c,"{""sha"": ""0bec02e89d5118b3dffe1e22e88e423baa037491"", ""filename"": ""sound/core/seq/seq_queue.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 0, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/3567eb6af614dac436c4b16a8d426f9faed639b3/sound/core/seq/seq_queue.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/3567eb6af614dac436c4b16a8d426f9faed639b3/sound/core/seq/seq_queue.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/sound/core/seq/seq_queue.c?ref=3567eb6af614dac436c4b16a8d426f9faed639b3"", ""patch"": ""@@ -142,8 +142,10 @@ static struct snd_seq_queue *queue_new(int owner, int locked)\n static void queue_delete(struct snd_seq_queue *q)\n {\n \t/* stop and release the timer */\n+\tmutex_lock(&q->timer_mutex);\n \tsnd_seq_timer_stop(q->timer);\n \tsnd_seq_timer_close(q);\n+\tmutex_unlock(&q->timer_mutex);\n \t/* wait until access free */\n \tsnd_use_lock_sync(&q->use_lock);\n \t/* release resources... */""}"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
",C,"	mutex_lock(&q->timer_mutex);
	mutex_unlock(&q->timer_mutex);
",,,"@@ -142,8 +142,10 @@ static struct snd_seq_queue *queue_new(int owner, int locked)
 static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
+	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
+	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */",linux,3567eb6af614dac436c4b16a8d426f9faed639b3,030e2c78d3a91dd0d27fef37e91950dde333eba1,1," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
//fix_flaw_line_below:
//	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
//fix_flaw_line_below:
//	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
",180581," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
","The `queue_delete` function is responsible for safely deallocating a sound sequence queue structure in a sound system. It first stops and closes the associated timer to ensure no ongoing operations are affected. The function then synchronizes access to the queue using a lock to prevent concurrent modifications. After ensuring safe access, it deletes priority queues for ticks and time, effectively releasing any resources tied to them. Finally, it frees the memory allocated for the queue structure itself, ensuring that all associated resources are properly cleaned up to prevent memory leaks.","The provided code defines a function that handles the deletion of a sound sequence queue. It begins by stopping and closing an associated timer to prevent any ongoing operations. The function then ensures that access to the queue is synchronized, allowing safe modifications without interference from other processes. After confirming safe access, it removes priority queues related to timing and ticks, effectively releasing their resources. Finally, the function frees the memory allocated for the queue itself, ensuring that all associated resources are properly cleaned up to avoid memory leaks and maintain system stability.","The vulnerability identified in the original code, classified under CWE-362, arises from a race condition due to the lack of proper synchronization when accessing shared resources. Specifically, the timer was stopped and closed without any locking mechanism, which could lead to concurrent access issues if multiple threads attempted to modify the queue simultaneously. The fixed code addresses this vulnerability by introducing a mutex lock around the timer operations, ensuring that only one thread can stop or close the timer at a time. This change effectively prevents race conditions and ensures the integrity of the queue's state during deletion.","The vulnerability identified in the original code, categorized under CWE-362, stems from a race condition that occurs when multiple threads access shared resources without proper synchronization. In the vulnerable version, critical operations were performed on a shared timer without any locking mechanism, allowing concurrent modifications that could lead to inconsistent states or crashes. The fixed code resolves this issue by implementing a mutex lock around the timer operations, ensuring that only one thread can access and modify the timer at a time. This change effectively prevents race conditions, thereby enhancing the stability and reliability of the system during resource management.","The vulnerability was fixed by introducing a mutex lock around the timer operations in the modified code. Specifically, the `mutex_lock` function is called before stopping and closing the timer, ensuring that these critical operations are performed in a thread-safe manner. This prevents multiple threads from concurrently accessing the timer, which could lead to race conditions. After the timer operations are completed, the `mutex_unlock` function is called to release the lock, allowing other threads to safely access the shared resources. This synchronization mechanism effectively mitigates the risk of inconsistent states and enhances the overall stability of the queue deletion process.","The vulnerability was fixed by implementing a synchronization mechanism that ensures safe access to shared resources during critical operations. By introducing a locking mechanism around the operations related to the timer, the code prevents multiple threads from concurrently modifying the same resource, which could lead to inconsistent states or crashes. This change ensures that only one thread can perform the necessary actions at a time, effectively eliminating the race condition. Once the critical operations are completed, the lock is released, allowing other threads to access the resources safely, thereby enhancing the overall stability and reliability of the system."
22,180625,180625,,Local,Not required,Complete,CVE-2016-0723,https://www.cvedetails.com/cve/CVE-2016-0723/,CWE-362,Low,Partial,,,2016-02-07,5.6,Race condition in the tty_ioctl function in drivers/tty/tty_io.c in the Linux kernel through 4.4.1 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free and system crash) by making a TIOCGETD ioctl call during processing of a TIOCSETD ioctl call.,2016-12-05,DoS +Info ,1,https://github.com/torvalds/linux/commit/5c17c861a357e9458001f021a7afa7aab9937439,5c17c861a357e9458001f021a7afa7aab9937439,"tty: Fix unsafe ldisc reference via ioctl(TIOCGETD)

ioctl(TIOCGETD) retrieves the line discipline id directly from the
ldisc because the line discipline id (c_line) in termios is untrustworthy;
userspace may have set termios via ioctl(TCSETS*) without actually
changing the line discipline via ioctl(TIOCSETD).

However, directly accessing the current ldisc via tty->ldisc is
unsafe; the ldisc ptr dereferenced may be stale if the line discipline
is changing via ioctl(TIOCSETD) or hangup.

Wait for the line discipline reference (just like read() or write())
to retrieve the ""current"" line discipline id.

Cc: <stable@vger.kernel.org>
Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",1,drivers/tty/tty_io.c,"{""sha"": ""5cec01c75691a6b7ee58ea65346c05572db1bab1"", ""filename"": ""drivers/tty/tty_io.c"", ""status"": ""modified"", ""additions"": 23, ""deletions"": 1, ""changes"": 24, ""blob_url"": ""https://github.com/torvalds/linux/blob/5c17c861a357e9458001f021a7afa7aab9937439/drivers/tty/tty_io.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/5c17c861a357e9458001f021a7afa7aab9937439/drivers/tty/tty_io.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/tty/tty_io.c?ref=5c17c861a357e9458001f021a7afa7aab9937439"", ""patch"": ""@@ -2658,6 +2658,28 @@ static int tiocsetd(struct tty_struct *tty, int __user *p)\n \treturn ret;\n }\n \n+/**\n+ *\ttiocgetd\t-\tget line discipline\n+ *\t@tty: tty device\n+ *\t@p: pointer to user data\n+ *\n+ *\tRetrieves the line discipline id directly from the ldisc.\n+ *\n+ *\tLocking: waits for ldisc reference (in case the line discipline\n+ *\t\tis changing or the tty is being hungup)\n+ */\n+\n+static int tiocgetd(struct tty_struct *tty, int __user *p)\n+{\n+\tstruct tty_ldisc *ld;\n+\tint ret;\n+\n+\tld = tty_ldisc_ref_wait(tty);\n+\tret = put_user(ld->ops->num, p);\n+\ttty_ldisc_deref(ld);\n+\treturn ret;\n+}\n+\n /**\n  *\tsend_break\t-\tperformed time break\n  *\t@tty: device to break on\n@@ -2884,7 +2906,7 @@ long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n \tcase TIOCGSID:\n \t\treturn tiocgsid(tty, real_tty, p);\n \tcase TIOCGETD:\n-\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n+\t\treturn tiocgetd(tty, p);\n \tcase TIOCSETD:\n \t\treturn tiocsetd(tty, p);\n \tcase TIOCVHANGUP:""}","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
",C,"		return tiocgetd(tty, p);
","		return put_user(tty->ldisc->ops->num, (int __user *)p);
",,"@@ -2658,6 +2658,28 @@ static int tiocsetd(struct tty_struct *tty, int __user *p)
 	return ret;
 }
 
+/**
+ *	tiocgetd	-	get line discipline
+ *	@tty: tty device
+ *	@p: pointer to user data
+ *
+ *	Retrieves the line discipline id directly from the ldisc.
+ *
+ *	Locking: waits for ldisc reference (in case the line discipline
+ *		is changing or the tty is being hungup)
+ */
+
+static int tiocgetd(struct tty_struct *tty, int __user *p)
+{
+	struct tty_ldisc *ld;
+	int ret;
+
+	ld = tty_ldisc_ref_wait(tty);
+	ret = put_user(ld->ops->num, p);
+	tty_ldisc_deref(ld);
+	return ret;
+}
+
 /**
  *	send_break	-	performed time break
  *	@tty: device to break on
@@ -2884,7 +2906,7 @@ long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
-		return put_user(tty->ldisc->ops->num, (int __user *)p);
+		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:",linux,5c17c861a357e9458001f021a7afa7aab9937439,7f22f6c935cda600660e623a411fe380015d28d9,1,"long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
//flaw_line_below:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
//fix_flaw_line_below:
//		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
",180625,"long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","The `tty_ioctl` function handles various terminal I/O control commands for a given file descriptor associated with a terminal (TTY). It first checks the validity of the TTY and prepares for command execution by handling specific commands like setting the line discipline or managing break signals. The function processes a range of commands, including window size adjustments, exclusive access settings, and device flushing. It also interacts with the terminal's line discipline for additional command handling. The function returns appropriate error codes or results based on the command executed, ensuring robust management of terminal I/O operations.","The provided code implements a function that processes various input/output control commands for terminal devices. It begins by validating the terminal and preparing for command execution, handling specific cases such as setting line discipline and managing break signals. The function supports a wide range of commands, including adjustments to window size, exclusive access settings, and flushing of buffers. It also interacts with the terminal's line discipline for additional command processing. The function returns appropriate results or error codes based on the command executed, ensuring effective management of terminal operations and user interactions.","The vulnerability identified as CWE-362, which pertains to a race condition, arises from the potential for concurrent access to shared resources without proper synchronization. In the vulnerable code, the function allows multiple commands to be processed without ensuring that the terminal state is stable, particularly when checking for changes or waiting for operations to complete. The fix introduces a check using `tty_check_change` to ensure that the terminal's state is valid before proceeding with certain commands, and it also includes a wait for the terminal to finish sending data, thereby preventing race conditions that could lead to inconsistent states or unexpected behavior.","The vulnerability identified as CWE-362, related to race conditions, occurs when multiple processes can access and modify shared resources simultaneously without proper synchronization, leading to unpredictable behavior. In the vulnerable code, certain commands could be executed without ensuring that the terminal's state was stable, allowing for potential conflicts during concurrent operations. The fix addresses this issue by implementing checks to validate the terminal's state before executing specific commands and ensuring that operations are completed before proceeding. This added synchronization prevents inconsistencies and ensures that the terminal behaves as expected, mitigating the risk of race conditions.","The vulnerability was fixed by introducing a validation step using `tty_check_change` to ensure the terminal's state is stable before executing certain commands, such as setting line discipline or managing break signals. Additionally, the code now includes a wait for the terminal to finish sending data with `tty_wait_until_sent`, which prevents further commands from being processed while the terminal is still busy. This synchronization ensures that operations do not interfere with each other, thereby eliminating the race condition that could lead to inconsistent terminal states or unexpected behavior during concurrent access.","The vulnerability was addressed by implementing a validation mechanism to ensure that the terminal's state is stable before executing specific commands. This involved adding checks to confirm that the terminal is not in the middle of an operation, thereby preventing concurrent access issues. Additionally, the code now includes a waiting mechanism to ensure that any ongoing data transmission is completed before allowing further commands to be processed. These changes effectively synchronize access to the terminal, reducing the risk of race conditions and ensuring that operations are executed in a predictable and consistent manner."
23,180738,180738,,Local,Not required,Complete,CVE-2015-4170,https://www.cvedetails.com/cve/CVE-2015-4170/,CWE-362,Medium,,,,2016-05-02,4.7,Race condition in the ldsem_cmpxchg function in drivers/tty/tty_ldsem.c in the Linux kernel before 3.13-rc4-next-20131218 allows local users to cause a denial of service (ldsem_down_read and ldsem_down_write deadlock) by establishing a new tty thread during shutdown of a previous tty thread.,2017-11-11,DoS ,8,https://github.com/torvalds/linux/commit/cf872776fc84128bb779ce2b83a37c884c3203ae,cf872776fc84128bb779ce2b83a37c884c3203ae,"tty: Fix hang at ldsem_down_read()

When a controlling tty is being hung up and the hang up is
waiting for a just-signalled tty reader or writer to exit, and a new tty
reader/writer tries to acquire an ldisc reference concurrently with the
ldisc reference release from the signalled reader/writer, the hangup
can hang. The new reader/writer is sleeping in ldsem_down_read() and the
hangup is sleeping in ldsem_down_write() [1].

The new reader/writer fails to wakeup the waiting hangup because the
wrong lock count value is checked (the old lock count rather than the new
lock count) to see if the lock is unowned.

Change helper function to return the new lock count if the cmpxchg was
successful; document this behavior.

[1] edited dmesg log from reporter

SysRq : Show Blocked State
  task                        PC stack   pid father
systemd         D ffff88040c4f0000     0     1      0 0x00000000
 ffff88040c49fbe0 0000000000000046 ffff88040c4a0000 ffff88040c49ffd8
 00000000001d3980 00000000001d3980 ffff88040c4a0000 ffff88040593d840
 ffff88040c49fb40 ffffffff810a4cc0 0000000000000006 0000000000000023
Call Trace:
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817a6649>] schedule+0x24/0x5e
 [<ffffffff817a588b>] schedule_timeout+0x15b/0x1ec
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817aa691>] ? _raw_spin_unlock_irq+0x24/0x26
 [<ffffffff817aa10c>] down_read_failed+0xe3/0x1b9
 [<ffffffff817aa26d>] ldsem_down_read+0x8b/0xa5
 [<ffffffff8142b5ca>] ? tty_ldisc_ref_wait+0x1b/0x44
 [<ffffffff8142b5ca>] tty_ldisc_ref_wait+0x1b/0x44
 [<ffffffff81423f5b>] tty_write+0x7d/0x28a
 [<ffffffff814241f5>] redirected_tty_write+0x8d/0x98
 [<ffffffff81424168>] ? tty_write+0x28a/0x28a
 [<ffffffff8115d03f>] do_loop_readv_writev+0x56/0x79
 [<ffffffff8115e604>] do_readv_writev+0x1b0/0x1ff
 [<ffffffff8116ea0b>] ? do_vfs_ioctl+0x32a/0x489
 [<ffffffff81167d9d>] ? final_putname+0x1d/0x3a
 [<ffffffff8115e6c7>] vfs_writev+0x2e/0x49
 [<ffffffff8115e7d3>] SyS_writev+0x47/0xaa
 [<ffffffff817ab822>] system_call_fastpath+0x16/0x1b
bash            D ffffffff81c104c0     0  5469   5302 0x00000082
 ffff8800cf817ac0 0000000000000046 ffff8804086b22a0 ffff8800cf817fd8
 00000000001d3980 00000000001d3980 ffff8804086b22a0 ffff8800cf817a48
 000000000000b9a0 ffff8800cf817a78 ffffffff81004675 ffff8800cf817a44
Call Trace:
 [<ffffffff81004675>] ? dump_trace+0x165/0x29c
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff8100edda>] ? save_stack_trace+0x26/0x41
 [<ffffffff817a6649>] schedule+0x24/0x5e
 [<ffffffff817a588b>] schedule_timeout+0x15b/0x1ec
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817a9f03>] ? down_write_failed+0xa3/0x1c9
 [<ffffffff817aa691>] ? _raw_spin_unlock_irq+0x24/0x26
 [<ffffffff817a9f0b>] down_write_failed+0xab/0x1c9
 [<ffffffff817aa300>] ldsem_down_write+0x79/0xb1
 [<ffffffff817aada3>] ? tty_ldisc_lock_pair_timeout+0xa5/0xd9
 [<ffffffff817aada3>] tty_ldisc_lock_pair_timeout+0xa5/0xd9
 [<ffffffff8142bf33>] tty_ldisc_hangup+0xc4/0x218
 [<ffffffff81423ab3>] __tty_hangup+0x2e2/0x3ed
 [<ffffffff81424a76>] disassociate_ctty+0x63/0x226
 [<ffffffff81078aa7>] do_exit+0x79f/0xa11
 [<ffffffff81086bdb>] ? get_signal_to_deliver+0x206/0x62f
 [<ffffffff810b4bfb>] ? lock_release_holdtime.part.8+0xf/0x16e
 [<ffffffff81079b05>] do_group_exit+0x47/0xb5
 [<ffffffff81086c16>] get_signal_to_deliver+0x241/0x62f
 [<ffffffff810020a7>] do_signal+0x43/0x59d
 [<ffffffff810f2af7>] ? __audit_syscall_exit+0x21a/0x2a8
 [<ffffffff810b4bfb>] ? lock_release_holdtime.part.8+0xf/0x16e
 [<ffffffff81002655>] do_notify_resume+0x54/0x6c
 [<ffffffff817abaf8>] int_signal+0x12/0x17

Reported-by: Sami Farin <sami.farin@gmail.com>
Cc: <stable@vger.kernel.org> # 3.12.x
Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",3,drivers/tty/tty_ldsem.c,"{""sha"": ""d8a55e87877f06f3141602e4f08cdcb668c465b0"", ""filename"": ""drivers/tty/tty_ldsem.c"", ""status"": ""modified"", ""additions"": 13, ""deletions"": 3, ""changes"": 16, ""blob_url"": ""https://github.com/torvalds/linux/blob/cf872776fc84128bb779ce2b83a37c884c3203ae/drivers/tty/tty_ldsem.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cf872776fc84128bb779ce2b83a37c884c3203ae/drivers/tty/tty_ldsem.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/tty/tty_ldsem.c?ref=cf872776fc84128bb779ce2b83a37c884c3203ae"", ""patch"": ""@@ -86,11 +86,21 @@ static inline long ldsem_atomic_update(long delta, struct ld_semaphore *sem)\n \treturn atomic_long_add_return(delta, (atomic_long_t *)&sem->count);\n }\n \n+/*\n+ * ldsem_cmpxchg() updates @*old with the last-known sem->count value.\n+ * Returns 1 if count was successfully changed; @*old will have @new value.\n+ * Returns 0 if count was not changed; @*old will have most recent sem->count\n+ */\n static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n {\n-\tlong tmp = *old;\n-\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n-\treturn *old == tmp;\n+\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n+\tif (tmp == *old) {\n+\t\t*old = new;\n+\t\treturn 1;\n+\t} else {\n+\t\t*old = tmp;\n+\t\treturn 0;\n+\t}\n }\n \n /*""}"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
 }
"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
 }
",C,"	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
","	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
",,"@@ -86,11 +86,21 @@ static inline long ldsem_atomic_update(long delta, struct ld_semaphore *sem)
 	return atomic_long_add_return(delta, (atomic_long_t *)&sem->count);
 }
 
+/*
+ * ldsem_cmpxchg() updates @*old with the last-known sem->count value.
+ * Returns 1 if count was successfully changed; @*old will have @new value.
+ * Returns 0 if count was not changed; @*old will have most recent sem->count
+ */
 static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
-	long tmp = *old;
-	*old = atomic_long_cmpxchg(&sem->count, *old, new);
-	return *old == tmp;
+	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
+	if (tmp == *old) {
+		*old = new;
+		return 1;
+	} else {
+		*old = tmp;
+		return 0;
+	}
 }
 
 /*",linux,cf872776fc84128bb779ce2b83a37c884c3203ae,319e2e3f63c348a9b66db4667efa73178e18b17d,1," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
//flaw_line_below:
	long tmp = *old;
//flaw_line_below:
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
//flaw_line_below:
	return *old == tmp;
//fix_flaw_line_below:
//	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
//fix_flaw_line_below:
//	if (tmp == *old) {
//fix_flaw_line_below:
//		*old = new;
//fix_flaw_line_below:
//		return 1;
//fix_flaw_line_below:
//	} else {
//fix_flaw_line_below:
//		*old = tmp;
//fix_flaw_line_below:
//		return 0;
//fix_flaw_line_below:
//	}
 }
",180738," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
 }
"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
 }
","The provided code defines a static inline function `ldsem_cmpxchg`, which performs an atomic compare-and-swap operation on a semaphore's count. It takes three parameters: a pointer to a long integer `old`, a new long integer `new`, and a pointer to a `ld_semaphore` structure `sem`. The function first stores the current value of `*old` in a temporary variable `tmp`. It then attempts to update the semaphore's count using `atomic_long_cmpxchg`, replacing it with `new` only if it matches `*old`. Finally, the function returns a boolean value indicating whether the update was successful by comparing the updated `*old` with `tmp`.","The code defines a function that performs an atomic compare-and-swap operation on a semaphore's internal counter. It takes a pointer to a value representing the expected current state, a new value to set, and a pointer to a semaphore structure. The function first captures the current state of the expected value. It then attempts to update the semaphore's counter to the new value only if it matches the expected state. Finally, the function returns a boolean indicating whether the update was successful by comparing the updated state with the initially captured value.","The vulnerability in the original code, identified as CWE-362 (Race Condition on Shared Resource), arises from the potential for a race condition during the atomic compare-and-swap operation. In the vulnerable version, the function retrieves the current value of `*old` and then performs the atomic operation, which could lead to inconsistencies if another thread modifies the semaphore's count in between these actions. The fixed code addresses this by directly assigning the result of the atomic operation to a temporary variable `tmp` and then checking if `tmp` matches `*old`. If they match, it safely updates `*old` to the new value, ensuring thread safety.","The vulnerability identified as CWE-362 (Race Condition on Shared Resource) in the original code stems from the potential for a race condition during the execution of an atomic operation. In the vulnerable version, the function retrieves a value and then performs an atomic update, which could allow another thread to modify the shared resource in between these actions, leading to inconsistent or incorrect results. The fixed code resolves this issue by directly capturing the result of the atomic operation and checking it against the expected value before making any updates, ensuring that the operation is both atomic and thread-safe.","The vulnerability was fixed by modifying the logic of the atomic operation in the updated code. Instead of first retrieving the value of `*old` and then performing the atomic compare-and-swap, the fixed code directly assigns the result of the atomic operation to a temporary variable `tmp`. It then checks if `tmp` equals `*old`, ensuring that the update only occurs if the expected value matches the current state. If they match, `*old` is updated to the new value, and the function returns a success indicator. This change ensures that the operation is atomic and prevents race conditions.","The vulnerability was fixed by restructuring the logic of the atomic operation to ensure thread safety. Instead of first retrieving the expected value and then performing the atomic update, the revised code directly captures the result of the atomic operation in a temporary variable. It then checks if this result matches the expected value before proceeding with any updates. This approach ensures that the update only occurs if the expected value is still valid, effectively preventing race conditions and ensuring that the operation remains atomic, thus maintaining the integrity of the shared resource."
24,180825,180825,,Local,Not required,Complete,CVE-2017-17712,https://www.cvedetails.com/cve/CVE-2017-17712/,CWE-362,Medium,Complete,Complete,,2017-12-15,6.9,The raw_sendmsg() function in net/ipv4/raw.c in the Linux kernel through 4.14.6 has a race condition in inet->hdrincl that leads to uninitialized stack pointer usage; this allows a local user to execute code and gain privileges.,2018-04-03,Exec Code +Priv ,10,https://github.com/torvalds/linux/commit/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,"net: ipv4: fix for a race condition in raw_sendmsg

inet->hdrincl is racy, and could lead to uninitialized stack pointer
usage, so its value should be read only once.

Fixes: c008ba5bdc9f (""ipv4: Avoid reading user iov twice after raw_probe_proto_opt"")
Signed-off-by: Mohamed Ghannam <simo.ghannam@gmail.com>
Reviewed-by: Eric Dumazet <edumazet@google.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",5,net/ipv4/raw.c,"{""sha"": ""125c1eab3eaa6d894804c3aa8918aa7fcc736ca0"", ""filename"": ""net/ipv4/raw.c"", ""status"": ""modified"", ""additions"": 10, ""deletions"": 5, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483/net/ipv4/raw.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483/net/ipv4/raw.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/ipv4/raw.c?ref=8f659a03a0ba9289b9aeb9b4470e6fb263d6f483"", ""patch"": ""@@ -513,11 +513,16 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \tint err;\n \tstruct ip_options_data opt_copy;\n \tstruct raw_frag_vec rfv;\n+\tint hdrincl;\n \n \terr = -EMSGSIZE;\n \tif (len > 0xFFFF)\n \t\tgoto out;\n \n+\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n+\t * but READ_ONCE() doesn't work with bit fields\n+\t */\n+\thdrincl = inet->hdrincl;\n \t/*\n \t *\tCheck the flags.\n \t */\n@@ -593,7 +598,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\t/* Linux does not mangle headers on raw sockets,\n \t\t * so that IP options + IP_HDRINCL is non-sense.\n \t\t */\n-\t\tif (inet->hdrincl)\n+\t\tif (hdrincl)\n \t\t\tgoto done;\n \t\tif (ipc.opt->opt.srr) {\n \t\t\tif (!daddr)\n@@ -615,12 +620,12 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \n \tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n \t\t\t   RT_SCOPE_UNIVERSE,\n-\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n+\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n \t\t\t   inet_sk_flowi_flags(sk) |\n-\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n+\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n \t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n \n-\tif (!inet->hdrincl) {\n+\tif (!hdrincl) {\n \t\trfv.msg = msg;\n \t\trfv.hlen = 0;\n \n@@ -645,7 +650,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\tgoto do_confirm;\n back_from_confirm:\n \n-\tif (inet->hdrincl)\n+\tif (hdrincl)\n \t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n \t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n ""}","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (inet->hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!inet->hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (inet->hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
",C,"	int hdrincl;
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
		if (hdrincl)
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
	if (!hdrincl) {
	if (hdrincl)
","		if (inet->hdrincl)
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
	if (!inet->hdrincl) {
	if (inet->hdrincl)
",,"@@ -513,11 +513,16 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
+	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
+	/* hdrincl should be READ_ONCE(inet->hdrincl)
+	 * but READ_ONCE() doesn't work with bit fields
+	 */
+	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */
@@ -593,7 +598,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
-		if (inet->hdrincl)
+		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
@@ -615,12 +620,12 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
-			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
+			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
-			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
+			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
-	if (!inet->hdrincl) {
+	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
@@ -645,7 +650,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 		goto do_confirm;
 back_from_confirm:
 
-	if (inet->hdrincl)
+	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 ",linux,8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,93c647643b48f0131f02e45da3bd367d80443291,1,"static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
//fix_flaw_line_below:
//	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
//fix_flaw_line_below:
//	/* hdrincl should be READ_ONCE(inet->hdrincl)
//fix_flaw_line_below:
//	 * but READ_ONCE() doesn't work with bit fields
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
//flaw_line_below:
		if (inet->hdrincl)
//fix_flaw_line_below:
//		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
//flaw_line_below:
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
//fix_flaw_line_below:
//			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
//flaw_line_below:
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
//fix_flaw_line_below:
//			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
//flaw_line_below:
	if (!inet->hdrincl) {
//fix_flaw_line_below:
//	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
//flaw_line_below:
	if (inet->hdrincl)
//fix_flaw_line_below:
//	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
",180825,"static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (inet->hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!inet->hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (inet->hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","The `raw_sendmsg` function is responsible for sending raw IP packets through a socket in a Linux networking context. It begins by validating the message length and flags, ensuring the socket is properly configured for IPv4 communication. The function retrieves the destination address and prepares various parameters, including IP options and routing information. It checks for multicast addresses and sets the appropriate output interface. The function then constructs a flow structure for routing and handles the transmission of the packet, including managing timestamps and potential fragmentation. Finally, it cleans up resources and returns the result of the send operation, indicating success or failure.","The provided code implements a function that facilitates the sending of raw IP packets through a socket in a networking environment. It begins by validating the input message's length and flags, ensuring compatibility with IPv4. The function retrieves the destination address and prepares necessary parameters, including options and routing details. It handles multicast addresses and constructs a flow structure for routing the packet. The function manages the transmission process, including potential fragmentation and timestamping, while ensuring proper resource cleanup. Ultimately, it returns the outcome of the send operation, indicating whether it was successful or encountered an error.","The vulnerability identified as CWE-362, which pertains to a race condition, was present in the original code due to the direct access of a bit field without proper synchronization. Specifically, the variable `inet->hdrincl` was accessed directly, which could lead to inconsistent values being read if it was modified concurrently by another thread. The fix introduced a local variable to store the value of `hdrincl`, ensuring that the same value is used throughout the function's execution. This change mitigates the risk of race conditions by preventing simultaneous reads and writes to the shared variable, thus enhancing the code's thread safety.","The vulnerability identified as CWE-362, related to race conditions, arose from the direct access to a shared variable that could be modified by multiple threads simultaneously. This led to potential inconsistencies in the value being read, as one thread could change the variable while another was using it, resulting in unpredictable behavior. The fix involved introducing a local copy of the variable, ensuring that a consistent value was utilized throughout the function's execution. This change effectively mitigated the risk of race conditions by preventing concurrent access issues, thereby enhancing the overall thread safety of the code.","The vulnerability was fixed by introducing a local variable to store the value of the bit field `hdrincl`, which was previously accessed directly from the structure. By assigning `hdrincl = inet->hdrincl`, the code ensures that a consistent value is used throughout the function, preventing any race conditions that could arise from concurrent modifications. This change allows the function to operate on a stable copy of the value, eliminating the risk of reading an inconsistent state while other threads might be altering the original variable. This enhancement significantly improves the thread safety of the code.","The vulnerability was addressed by creating a local copy of a critical shared value that was previously accessed directly from a structure. This change ensures that the function operates on a stable and consistent value throughout its execution, thereby preventing potential inconsistencies that could arise from concurrent modifications by multiple threads. By isolating the value in a local variable, the code effectively mitigates the risk of race conditions, enhancing the overall thread safety and reliability of the function. This approach allows for predictable behavior even in a multi-threaded environment."
25,180889,180889,,Local,Not required,Partial,CVE-2017-15649,https://www.cvedetails.com/cve/CVE-2017-15649/,CWE-362,Low,Partial,Partial,,2017-10-19,4.6,"net/packet/af_packet.c in the Linux kernel before 4.13.6 allows local users to gain privileges via crafted system calls that trigger mishandling of packet_fanout data structures, because of a race condition (involving fanout_add and packet_do_bind) that leads to a use-after-free, a different vulnerability than CVE-2017-6346.",2018-08-24,+Priv ,5,https://github.com/torvalds/linux/commit/4971613c1639d8e5f102c4e797c3bf8f83a5a69e,4971613c1639d8e5f102c4e797c3bf8f83a5a69e,"packet: in packet_do_bind, test fanout with bind_lock held

Once a socket has po->fanout set, it remains a member of the group
until it is destroyed. The prot_hook must be constant and identical
across sockets in the group.

If fanout_add races with packet_do_bind between the test of po->fanout
and taking the lock, the bind call may make type or dev inconsistent
with that of the fanout group.

Hold po->bind_lock when testing po->fanout to avoid this race.

I had to introduce artificial delay (local_bh_enable) to actually
observe the race.

Fixes: dc99f600698d (""packet: Add fanout support."")
Signed-off-by: Willem de Bruijn <willemb@google.com>
Reviewed-by: Eric Dumazet <edumazet@google.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",2,net/packet/af_packet.c,"{""sha"": ""a10c2836465cf499e2b723419da25bc3ad07d69f"", ""filename"": ""net/packet/af_packet.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 3, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/4971613c1639d8e5f102c4e797c3bf8f83a5a69e/net/packet/af_packet.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/4971613c1639d8e5f102c4e797c3bf8f83a5a69e/net/packet/af_packet.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/packet/af_packet.c?ref=4971613c1639d8e5f102c4e797c3bf8f83a5a69e"", ""patch"": ""@@ -3069,13 +3069,15 @@ static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n \tint ret = 0;\n \tbool unlisted = false;\n \n-\tif (po->fanout)\n-\t\treturn -EINVAL;\n-\n \tlock_sock(sk);\n \tspin_lock(&po->bind_lock);\n \trcu_read_lock();\n \n+\tif (po->fanout) {\n+\t\tret = -EINVAL;\n+\t\tgoto out_unlock;\n+\t}\n+\n \tif (name) {\n \t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n \t\tif (!dev) {""}","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
	if (po->fanout)
		return -EINVAL;
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
",C,"	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

","	if (po->fanout)
		return -EINVAL;
",,"@@ -3069,13 +3069,15 @@ static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
 	int ret = 0;
 	bool unlisted = false;
 
-	if (po->fanout)
-		return -EINVAL;
-
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
+	if (po->fanout) {
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {",linux,4971613c1639d8e5f102c4e797c3bf8f83a5a69e,1579f678fb4397f9e439d2e373d4ade036c673b4,1,"static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
//flaw_line_below:
	if (po->fanout)
//flaw_line_below:
		return -EINVAL;
//flaw_line_below:

 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
//fix_flaw_line_below:
//	if (po->fanout) {
//fix_flaw_line_below:
//		ret = -EINVAL;
//fix_flaw_line_below:
//		goto out_unlock;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
",180889,"static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
	if (po->fanout)
		return -EINVAL;
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","The `packet_do_bind` function is responsible for binding a socket to a specific network device and protocol in a Linux kernel networking context. It first checks if the socket is already associated with a fanout, returning an error if so. The function then attempts to retrieve the network device either by name or index, handling errors if the device is not found. It evaluates whether a rehook is necessary based on the current protocol and device. If a rehook is needed and the socket is running, it unregisters the current protocol hook. Finally, it either registers a new protocol hook if conditions are met or reports an error if the device is down, ensuring proper resource management throughout.","The function manages the binding of a socket to a specific network device and protocol within the kernel's networking framework. It first checks for existing configurations and retrieves the desired network device based on provided identifiers. If a change in protocol or device is detected, it unregisters the current settings if the socket is active. The function then updates the socket's configuration with the new device and protocol, ensuring that resources are properly managed. Finally, it registers the new settings if the device is operational, or reports an error if the device is not available, maintaining system stability.","The vulnerability identified as CWE-362, which pertains to a race condition, arises from the improper handling of shared resources in the original code. Specifically, the function did not adequately synchronize access to the protocol hook and network device, allowing concurrent threads to modify the state of the socket and its associated resources unpredictably. In the fixed code, the addition of proper locking mechanisms, such as ensuring that the socket is locked before checking conditions and modifying the protocol hook, mitigates this issue. This prevents simultaneous access that could lead to inconsistent states or crashes, thereby enhancing the overall stability and security of the code.","The vulnerability identified as CWE-362, related to race conditions, occurs when multiple threads access shared resources without proper synchronization, leading to unpredictable behavior and potential data corruption. In the original code, the lack of adequate locking mechanisms allowed concurrent modifications to the socket's state and associated resources, which could result in inconsistent or erroneous outcomes. The fixed code addresses this issue by implementing appropriate locking strategies to ensure that only one thread can modify the shared resources at a time. This change effectively prevents simultaneous access, thereby enhancing the stability and security of the code by ensuring consistent state management.","The vulnerability was fixed by enhancing the synchronization mechanisms within the function. The code now ensures that the socket is locked using `lock_sock` before accessing shared resources, preventing concurrent modifications. Additionally, the use of `spin_lock` on the binding lock ensures that only one thread can manipulate the protocol hook and network device at a time. The sequence of operations was adjusted to guarantee that the current device and protocol are checked and updated safely, particularly during the rehooking process. This careful management of locks and resource access effectively mitigates the race condition, ensuring consistent and stable behavior.","The vulnerability was fixed by implementing stricter synchronization controls to manage access to shared resources. The revised code ensures that a lock is acquired before any modifications or checks are made, preventing multiple threads from concurrently altering the state of the socket and its associated resources. Additionally, the sequence of operations was reorganized to ensure that any necessary updates occur only when it is safe to do so, particularly during critical transitions. This approach effectively eliminates the race condition, ensuring that the system maintains a consistent state and operates reliably under concurrent access scenarios."
26,181435,181435,,Local,Not required,Complete,CVE-2017-7533,https://www.cvedetails.com/cve/CVE-2017-7533/,CWE-362,Medium,Complete,Complete,,2017-08-05,6.9,Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions.,2018-01-04,DoS +Priv Mem. Corr. ,4,https://github.com/torvalds/linux/commit/49d31c2f389acfe83417083e1208422b4091cd9e,49d31c2f389acfe83417083e1208422b4091cd9e,"dentry name snapshots

take_dentry_name_snapshot() takes a safe snapshot of dentry name;
if the name is a short one, it gets copied into caller-supplied
structure, otherwise an extra reference to external name is grabbed
(those are never modified).  In either case the pointer to stable
string is stored into the same structure.

dentry must be held by the caller of take_dentry_name_snapshot(),
but may be freely dropped afterwards - the snapshot will stay
until destroyed by release_dentry_name_snapshot().

Intended use:
	struct name_snapshot s;

	take_dentry_name_snapshot(&s, dentry);
	...
	access s.name
	...
	release_dentry_name_snapshot(&s);

Replaces fsnotify_oldname_...(), gets used in fsnotify to obtain the name
to pass down with event.

Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>",4,fs/namei.c,"{""sha"": ""831f3a9a8f05282b8ba9aae9c6ed963334b9b8f3"", ""filename"": ""fs/dcache.c"", ""status"": ""modified"", ""additions"": 27, ""deletions"": 0, ""changes"": 27, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/dcache.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/dcache.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/dcache.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -277,6 +277,33 @@ static inline int dname_external(const struct dentry *dentry)\n \treturn dentry->d_name.name != dentry->d_iname;\n }\n \n+void take_dentry_name_snapshot(struct name_snapshot *name, struct dentry *dentry)\n+{\n+\tspin_lock(&dentry->d_lock);\n+\tif (unlikely(dname_external(dentry))) {\n+\t\tstruct external_name *p = external_name(dentry);\n+\t\tatomic_inc(&p->u.count);\n+\t\tspin_unlock(&dentry->d_lock);\n+\t\tname->name = p->name;\n+\t} else {\n+\t\tmemcpy(name->inline_name, dentry->d_iname, DNAME_INLINE_LEN);\n+\t\tspin_unlock(&dentry->d_lock);\n+\t\tname->name = name->inline_name;\n+\t}\n+}\n+EXPORT_SYMBOL(take_dentry_name_snapshot);\n+\n+void release_dentry_name_snapshot(struct name_snapshot *name)\n+{\n+\tif (unlikely(name->name != name->inline_name)) {\n+\t\tstruct external_name *p;\n+\t\tp = container_of(name->name, struct external_name, name[0]);\n+\t\tif (unlikely(atomic_dec_and_test(&p->u.count)))\n+\t\t\tkfree_rcu(p, u.head);\n+\t}\n+}\n+EXPORT_SYMBOL(release_dentry_name_snapshot);\n+\n static inline void __d_set_inode_and_type(struct dentry *dentry,\n \t\t\t\t\t  struct inode *inode,\n \t\t\t\t\t  unsigned type_flags)""}<_**next**_>{""sha"": ""acd3be2cc691bb577916aa552668d2c1d179ed69"", ""filename"": ""fs/debugfs/inode.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 5, ""changes"": 10, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/debugfs/inode.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/debugfs/inode.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/debugfs/inode.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -766,7 +766,7 @@ struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n {\n \tint error;\n \tstruct dentry *dentry = NULL, *trap;\n-\tconst char *old_name;\n+\tstruct name_snapshot old_name;\n \n \ttrap = lock_rename(new_dir, old_dir);\n \t/* Source or destination directories don't exist? */\n@@ -781,19 +781,19 @@ struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n \tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n \t\tgoto exit;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \n \terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n \t\t\t      dentry, 0);\n \tif (error) {\n-\t\tfsnotify_oldname_free(old_name);\n+\t\trelease_dentry_name_snapshot(&old_name);\n \t\tgoto exit;\n \t}\n \td_move(old_dentry, dentry);\n-\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,\n+\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,\n \t\td_is_dir(old_dentry),\n \t\tNULL, old_dentry);\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \tunlock_rename(new_dir, old_dir);\n \tdput(dentry);\n \treturn old_dentry;""}<_**next**_>{""sha"": ""c5588e837b1532bfc201a5b8d6bfcdedad9f107f"", ""filename"": ""fs/namei.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/namei.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/namei.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/namei.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -4362,11 +4362,11 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n {\n \tint error;\n \tbool is_dir = d_is_dir(old_dentry);\n-\tconst unsigned char *old_name;\n \tstruct inode *source = old_dentry->d_inode;\n \tstruct inode *target = new_dentry->d_inode;\n \tbool new_is_dir = false;\n \tunsigned max_links = new_dir->i_sb->s_max_links;\n+\tstruct name_snapshot old_name;\n \n \tif (source == target)\n \t\treturn 0;\n@@ -4413,7 +4413,7 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n \tif (error)\n \t\treturn error;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \tdget(new_dentry);\n \tif (!is_dir || (flags & RENAME_EXCHANGE))\n \t\tlock_two_nondirectories(source, target);\n@@ -4468,14 +4468,14 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n \t\tinode_unlock(target);\n \tdput(new_dentry);\n \tif (!error) {\n-\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n+\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,\n \t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n \t\tif (flags & RENAME_EXCHANGE) {\n \t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n \t\t\t\t      new_is_dir, NULL, new_dentry);\n \t\t}\n \t}\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \n \treturn error;\n }""}<_**next**_>{""sha"": ""0c4583b61717646e491a28d879b5762faeb8956e"", ""filename"": ""fs/notify/fsnotify.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 2, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/notify/fsnotify.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/notify/fsnotify.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/notify/fsnotify.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -161,16 +161,20 @@ int __fsnotify_parent(const struct path *path, struct dentry *dentry, __u32 mask\n \tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n \t\t__fsnotify_update_child_dentry_flags(p_inode);\n \telse if (p_inode->i_fsnotify_mask & mask) {\n+\t\tstruct name_snapshot name;\n+\n \t\t/* we are notifying a parent so come up with the new mask which\n \t\t * specifies these are events which came from a child. */\n \t\tmask |= FS_EVENT_ON_CHILD;\n \n+\t\ttake_dentry_name_snapshot(&name, dentry);\n \t\tif (path)\n \t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n \t\telse\n \t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n+\t\trelease_dentry_name_snapshot(&name);\n \t}\n \n \tdput(parent);""}<_**next**_>{""sha"": ""025727bf679745e507bb427f14198bf1e607ae8e"", ""filename"": ""include/linux/dcache.h"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 0, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/dcache.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/dcache.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/dcache.h?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -591,5 +591,11 @@ static inline struct inode *d_real_inode(const struct dentry *dentry)\n \treturn d_backing_inode(d_real((struct dentry *) dentry, NULL, 0));\n }\n \n+struct name_snapshot {\n+\tconst char *name;\n+\tchar inline_name[DNAME_INLINE_LEN];\n+};\n+void take_dentry_name_snapshot(struct name_snapshot *, struct dentry *);\n+void release_dentry_name_snapshot(struct name_snapshot *);\n \n #endif\t/* __LINUX_DCACHE_H */""}<_**next**_>{""sha"": ""b78aa7ac77ce1d6920aa196f4cd71d6d6a7aee7b"", ""filename"": ""include/linux/fsnotify.h"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 31, ""changes"": 31, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/fsnotify.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/fsnotify.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/fsnotify.h?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -293,35 +293,4 @@ static inline void fsnotify_change(struct dentry *dentry, unsigned int ia_valid)\n \t}\n }\n \n-#if defined(CONFIG_FSNOTIFY)\t/* notify helpers */\n-\n-/*\n- * fsnotify_oldname_init - save off the old filename before we change it\n- */\n-static inline const unsigned char *fsnotify_oldname_init(const unsigned char *name)\n-{\n-\treturn kstrdup(name, GFP_KERNEL);\n-}\n-\n-/*\n- * fsnotify_oldname_free - free the name we got from fsnotify_oldname_init\n- */\n-static inline void fsnotify_oldname_free(const unsigned char *old_name)\n-{\n-\tkfree(old_name);\n-}\n-\n-#else\t/* CONFIG_FSNOTIFY */\n-\n-static inline const char *fsnotify_oldname_init(const unsigned char *name)\n-{\n-\treturn NULL;\n-}\n-\n-static inline void fsnotify_oldname_free(const unsigned char *old_name)\n-{\n-}\n-\n-#endif\t/*  CONFIG_FSNOTIFY */\n-\n #endif\t/* _LINUX_FS_NOTIFY_H */""}","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	fsnotify_oldname_free(old_name);
 
 	return error;
 }
",C,"	struct name_snapshot old_name;
	take_dentry_name_snapshot(&old_name, old_dentry);
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
	release_dentry_name_snapshot(&old_name);
","	const unsigned char *old_name;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
	fsnotify_oldname_free(old_name);
",,"@@ -4362,11 +4362,11 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
-	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
+	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;
@@ -4413,7 +4413,7 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 	if (error)
 		return error;
 
-	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
+	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
@@ -4468,14 +4468,14 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
-		fsnotify_move(old_dir, new_dir, old_name, is_dir,
+		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
-	fsnotify_oldname_free(old_name);
+	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }",linux,49d31c2f389acfe83417083e1208422b4091cd9e,a8e2b6367794e6cee9eecba6d5ff425f338e0754,1,"int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
//flaw_line_below:
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
//fix_flaw_line_below:
//	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
//flaw_line_below:
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
//fix_flaw_line_below:
//	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
//flaw_line_below:
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
//fix_flaw_line_below:
//		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
//flaw_line_below:
	fsnotify_oldname_free(old_name);
//fix_flaw_line_below:
//	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
",181435,"int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	fsnotify_oldname_free(old_name);
 
 	return error;
 }
","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
","The `vfs_rename` function implements the renaming of files or directories within a virtual filesystem. It checks permissions for deleting the old entry and creating or replacing the new entry, handling cases for directory exchanges and ensuring that link limits are respected. The function also manages inode locking to prevent race conditions and checks for local mount points. If the rename operation is permitted, it invokes the appropriate rename operation defined in the inode's operations. Finally, it notifies the filesystem of the move operation and cleans up resources, returning any errors encountered during the process.","The code defines a function that facilitates the renaming of files or directories within a virtual filesystem. It performs checks to ensure the user has the necessary permissions to delete the old entry and create or replace the new one, while also managing potential conflicts with existing entries. The function handles special cases, such as directory exchanges and link limits, and ensures proper locking to prevent concurrent access issues. If all conditions are met, it executes the rename operation and notifies the filesystem of the changes, returning any errors encountered during the process.","The vulnerability identified as CWE-362, which pertains to race conditions, was present in the original code due to the lack of proper handling of the name of the old directory entry during the rename operation. This could allow a malicious actor to manipulate the filesystem state between the permission checks and the actual rename execution, potentially leading to unauthorized access or modification. The fix introduced a mechanism to take a snapshot of the old name using `take_dentry_name_snapshot`, ensuring that the name is preserved and consistent throughout the operation, thereby mitigating the risk of race conditions and enhancing security.","The vulnerability identified as CWE-362, related to race conditions, arose from the potential for a malicious actor to exploit the time gap between permission checks and the execution of the rename operation. This gap allowed for the filesystem state to be altered, potentially leading to unauthorized access or modifications. The fix addressed this issue by implementing a mechanism to capture and preserve the name of the old entry before any changes were made. This ensured that the rename operation could be executed with a consistent reference, effectively mitigating the risk of race conditions and enhancing overall security.","The vulnerability was fixed by introducing a mechanism to take a snapshot of the old name using the `take_dentry_name_snapshot` function before proceeding with the rename operation. This ensures that the name of the old directory entry is preserved and remains consistent throughout the execution of the function. By capturing the old name early in the process, the code mitigates the risk of race conditions that could allow an attacker to alter the filesystem state between permission checks and the actual rename execution. Additionally, the old name snapshot is released at the end of the operation, maintaining resource integrity.","The vulnerability was fixed by implementing a mechanism to capture and preserve the name of the old directory entry before any modifications were made. This approach ensures that the name remains consistent throughout the entire rename operation, effectively preventing any potential race conditions that could arise from changes in the filesystem state during the execution. By securing the old name early in the process, the code mitigates the risk of unauthorized access or modifications that could occur if an attacker manipulated the filesystem between permission checks and the actual rename action. This enhancement significantly improves the overall security of the operation."
27,181539,181539,,Remote,Not required,Complete,CVE-2017-6001,https://www.cvedetails.com/cve/CVE-2017-6001/,CWE-362,High,Complete,Complete,,2017-02-18,7.6,Race condition in kernel/events/core.c in the Linux kernel before 4.9.7 allows local users to gain privileges via a crafted application that makes concurrent perf_event_open system calls for moving a software group into a hardware context.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-6786.,2018-06-19,+Priv ,23,https://github.com/torvalds/linux/commit/321027c1fe77f892f4ea07846aeae08cefbbb290,321027c1fe77f892f4ea07846aeae08cefbbb290,"perf/core: Fix concurrent sys_perf_event_open() vs. 'move_group' race

Di Shen reported a race between two concurrent sys_perf_event_open()
calls where both try and move the same pre-existing software group
into a hardware context.

The problem is exactly that described in commit:

  f63a8daa5812 (""perf: Fix event->ctx locking"")

... where, while we wait for a ctx->mutex acquisition, the event->ctx
relation can have changed under us.

That very same commit failed to recognise sys_perf_event_context() as an
external access vector to the events and thereby didn't apply the
established locking rules correctly.

So while one sys_perf_event_open() call is stuck waiting on
mutex_lock_double(), the other (which owns said locks) moves the group
about. So by the time the former sys_perf_event_open() acquires the
locks, the context we've acquired is stale (and possibly dead).

Apply the established locking rules as per perf_event_ctx_lock_nested()
to the mutex_lock_double() for the 'move_group' case. This obviously means
we need to validate state after we acquire the locks.

Reported-by: Di Shen (Keen Lab)
Tested-by: John Dias <joaodias@google.com>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
Cc: Jiri Olsa <jolsa@redhat.com>
Cc: Kees Cook <keescook@chromium.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Min Chong <mchong@google.com>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Stephane Eranian <eranian@google.com>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Vince Weaver <vincent.weaver@maine.edu>
Fixes: f63a8daa5812 (""perf: Fix event->ctx locking"")
Link: http://lkml.kernel.org/r/20170106131444.GZ3174@twins.programming.kicks-ass.net
Signed-off-by: Ingo Molnar <mingo@kernel.org>",4,kernel/events/core.c,"{""sha"": ""cbc5937265da842bc714cbccf8091484caa5f536"", ""filename"": ""kernel/events/core.c"", ""status"": ""modified"", ""additions"": 54, ""deletions"": 4, ""changes"": 58, ""blob_url"": ""https://github.com/torvalds/linux/blob/321027c1fe77f892f4ea07846aeae08cefbbb290/kernel/events/core.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/321027c1fe77f892f4ea07846aeae08cefbbb290/kernel/events/core.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/kernel/events/core.c?ref=321027c1fe77f892f4ea07846aeae08cefbbb290"", ""patch"": ""@@ -9529,6 +9529,37 @@ static int perf_event_set_clock(struct perf_event *event, clockid_t clk_id)\n \treturn 0;\n }\n \n+/*\n+ * Variation on perf_event_ctx_lock_nested(), except we take two context\n+ * mutexes.\n+ */\n+static struct perf_event_context *\n+__perf_event_ctx_lock_double(struct perf_event *group_leader,\n+\t\t\t     struct perf_event_context *ctx)\n+{\n+\tstruct perf_event_context *gctx;\n+\n+again:\n+\trcu_read_lock();\n+\tgctx = READ_ONCE(group_leader->ctx);\n+\tif (!atomic_inc_not_zero(&gctx->refcount)) {\n+\t\trcu_read_unlock();\n+\t\tgoto again;\n+\t}\n+\trcu_read_unlock();\n+\n+\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\n+\tif (group_leader->ctx != gctx) {\n+\t\tmutex_unlock(&ctx->mutex);\n+\t\tmutex_unlock(&gctx->mutex);\n+\t\tput_ctx(gctx);\n+\t\tgoto again;\n+\t}\n+\n+\treturn gctx;\n+}\n+\n /**\n  * sys_perf_event_open - open a performance event, associate it to a task/cpu\n  *\n@@ -9772,12 +9803,31 @@ SYSCALL_DEFINE5(perf_event_open,\n \t}\n \n \tif (move_group) {\n-\t\tgctx = group_leader->ctx;\n-\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);\n+\n \t\tif (gctx->task == TASK_TOMBSTONE) {\n \t\t\terr = -ESRCH;\n \t\t\tgoto err_locked;\n \t\t}\n+\n+\t\t/*\n+\t\t * Check if we raced against another sys_perf_event_open() call\n+\t\t * moving the software group underneath us.\n+\t\t */\n+\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n+\t\t\t/*\n+\t\t\t * If someone moved the group out from under us, check\n+\t\t\t * if this new event wound up on the same ctx, if so\n+\t\t\t * its the regular !move_group case, otherwise fail.\n+\t\t\t */\n+\t\t\tif (gctx != ctx) {\n+\t\t\t\terr = -EINVAL;\n+\t\t\t\tgoto err_locked;\n+\t\t\t} else {\n+\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n+\t\t\t\tmove_group = 0;\n+\t\t\t}\n+\t\t}\n \t} else {\n \t\tmutex_lock(&ctx->mutex);\n \t}\n@@ -9879,7 +9929,7 @@ SYSCALL_DEFINE5(perf_event_open,\n \tperf_unpin_context(ctx);\n \n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n \n \tif (task) {\n@@ -9905,7 +9955,7 @@ SYSCALL_DEFINE5(perf_event_open,\n \n err_locked:\n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n /* err_file: */\n \tfput(event_file);""}","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = __perf_event_ctx_lock_double(group_leader, ctx);

 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}

		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
",C,"		gctx = __perf_event_ctx_lock_double(group_leader, ctx);


		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
		perf_event_ctx_unlock(group_leader, gctx);
		perf_event_ctx_unlock(group_leader, gctx);
","		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
		mutex_unlock(&gctx->mutex);
		mutex_unlock(&gctx->mutex);
",,"@@ -9529,6 +9529,37 @@ static int perf_event_set_clock(struct perf_event *event, clockid_t clk_id)
 	return 0;
 }
 
+/*
+ * Variation on perf_event_ctx_lock_nested(), except we take two context
+ * mutexes.
+ */
+static struct perf_event_context *
+__perf_event_ctx_lock_double(struct perf_event *group_leader,
+			     struct perf_event_context *ctx)
+{
+	struct perf_event_context *gctx;
+
+again:
+	rcu_read_lock();
+	gctx = READ_ONCE(group_leader->ctx);
+	if (!atomic_inc_not_zero(&gctx->refcount)) {
+		rcu_read_unlock();
+		goto again;
+	}
+	rcu_read_unlock();
+
+	mutex_lock_double(&gctx->mutex, &ctx->mutex);
+
+	if (group_leader->ctx != gctx) {
+		mutex_unlock(&ctx->mutex);
+		mutex_unlock(&gctx->mutex);
+		put_ctx(gctx);
+		goto again;
+	}
+
+	return gctx;
+}
+
 /**
  * sys_perf_event_open - open a performance event, associate it to a task/cpu
  *
@@ -9772,12 +9803,31 @@ SYSCALL_DEFINE5(perf_event_open,
 	}
 
 	if (move_group) {
-		gctx = group_leader->ctx;
-		mutex_lock_double(&gctx->mutex, &ctx->mutex);
+		gctx = __perf_event_ctx_lock_double(group_leader, ctx);
+
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
+
+		/*
+		 * Check if we raced against another sys_perf_event_open() call
+		 * moving the software group underneath us.
+		 */
+		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
+			/*
+			 * If someone moved the group out from under us, check
+			 * if this new event wound up on the same ctx, if so
+			 * its the regular !move_group case, otherwise fail.
+			 */
+			if (gctx != ctx) {
+				err = -EINVAL;
+				goto err_locked;
+			} else {
+				perf_event_ctx_unlock(group_leader, gctx);
+				move_group = 0;
+			}
+		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}
@@ -9879,7 +9929,7 @@ SYSCALL_DEFINE5(perf_event_open,
 	perf_unpin_context(ctx);
 
 	if (move_group)
-		mutex_unlock(&gctx->mutex);
+		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
@@ -9905,7 +9955,7 @@ SYSCALL_DEFINE5(perf_event_open,
 
 err_locked:
 	if (move_group)
-		mutex_unlock(&gctx->mutex);
+		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);",linux,321027c1fe77f892f4ea07846aeae08cefbbb290,63cae12bce9861cec309798d34701cf3da20bc71,1,"SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
//flaw_line_below:
		gctx = group_leader->ctx;
//flaw_line_below:
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
//fix_flaw_line_below:
//		gctx = __perf_event_ctx_lock_double(group_leader, ctx);
//fix_flaw_line_below:
//
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * Check if we raced against another sys_perf_event_open() call
//fix_flaw_line_below:
//		 * moving the software group underneath us.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
//fix_flaw_line_below:
//			/*
//fix_flaw_line_below:
//			 * If someone moved the group out from under us, check
//fix_flaw_line_below:
//			 * if this new event wound up on the same ctx, if so
//fix_flaw_line_below:
//			 * its the regular !move_group case, otherwise fail.
//fix_flaw_line_below:
//			 */
//fix_flaw_line_below:
//			if (gctx != ctx) {
//fix_flaw_line_below:
//				err = -EINVAL;
//fix_flaw_line_below:
//				goto err_locked;
//fix_flaw_line_below:
//			} else {
//fix_flaw_line_below:
//				perf_event_ctx_unlock(group_leader, gctx);
//fix_flaw_line_below:
//				move_group = 0;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
//flaw_line_below:
		mutex_unlock(&gctx->mutex);
//fix_flaw_line_below:
//		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
//flaw_line_below:
		mutex_unlock(&gctx->mutex);
//fix_flaw_line_below:
//		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
",181539,"SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = __perf_event_ctx_lock_double(group_leader, ctx);

 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}

		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","The provided code implements the `perf_event_open` system call, which is used to create and manage performance monitoring events in a Linux environment. It allows users to monitor various performance metrics of processes or CPUs by specifying attributes such as event type, sampling frequency, and target process. The function validates input parameters, checks permissions, and manages event grouping and context. It allocates a new performance event, associates it with a specified task or CPU, and handles potential errors during the process. Finally, it returns a file descriptor for the newly created event, enabling further interaction with it.","The code implements a system call that facilitates the creation and management of performance monitoring events in a Linux environment. It allows users to specify various attributes for monitoring, such as the type of event and sampling frequency, while ensuring proper validation and permission checks. The functionality includes managing event grouping, associating events with specific processes or CPUs, and handling potential errors throughout the process. Ultimately, it returns a file descriptor for the newly created event, enabling users to interact with and control the performance monitoring features effectively.","The vulnerability identified as CWE-362, which pertains to a race condition, was present in the original code due to improper synchronization when handling performance events. Specifically, the code allowed concurrent calls to modify shared data structures without adequate locking, leading to potential inconsistencies. The fix introduced a new locking mechanism, specifically using `__perf_event_ctx_lock_double`, to ensure that the context is properly locked during critical operations. This prevents simultaneous modifications by different threads, thereby eliminating the race condition and ensuring that the integrity of the performance event data structures is maintained throughout the execution.","The vulnerability identified as CWE-362, related to race conditions, arose from inadequate synchronization in the original code, allowing multiple threads to concurrently modify shared data structures. This could lead to inconsistent states and unpredictable behavior during performance event management. The fix implemented a more robust locking mechanism to ensure that critical sections of the code are properly protected, preventing simultaneous access by different threads. By enforcing this synchronization, the updated code ensures that shared resources are accessed in a controlled manner, thereby eliminating the race condition and maintaining data integrity throughout the execution process.","The vulnerability was fixed by introducing a more robust locking mechanism using `__perf_event_ctx_lock_double`, which ensures that the context is properly locked during critical operations involving performance events. This change prevents concurrent modifications by different threads, addressing the race condition that could lead to inconsistent states. Additionally, the code now checks for potential conflicts when moving groups, ensuring that the context remains valid and consistent. By implementing these synchronization measures, the updated code effectively safeguards shared data structures, maintaining their integrity during simultaneous access and modifications.","The vulnerability was fixed by implementing a more effective locking mechanism that ensures critical sections of the code are protected from concurrent access. This change prevents multiple threads from simultaneously modifying shared data structures, which could lead to inconsistent states. The updated code includes checks to handle potential conflicts during operations that involve moving groups, ensuring that the context remains valid throughout the process. By enforcing these synchronization measures, the revised implementation effectively eliminates the race condition, thereby maintaining the integrity and reliability of the performance event management system."
28,182360,182360,,Local,Not required,Complete,CVE-2018-12633,https://www.cvedetails.com/cve/CVE-2018-12633/,CWE-362,Medium,Complete,,,2018-06-21,6.3,"An issue was discovered in the Linux kernel through 4.17.2. vbg_misc_device_ioctl() in drivers/virt/vboxguest/vboxguest_linux.c reads the same user data twice with copy_from_user. The header part of the user data is double-fetched, and a malicious user thread can tamper with the critical variables (hdr.size_in and hdr.size_out) in the header between the two fetches because of a race condition, leading to severe kernel errors, such as buffer over-accesses. This bug can cause a local denial of service and information leakage.",2018-08-21,DoS +Info ,3,https://github.com/torvalds/linux/commit/bd23a7269834dc7c1f93e83535d16ebc44b75eba,bd23a7269834dc7c1f93e83535d16ebc44b75eba,"virt: vbox: Only copy_from_user the request-header once

In vbg_misc_device_ioctl(), the header of the ioctl argument is copied from
the userspace pointer 'arg' and saved to the kernel object 'hdr'. Then the
'version', 'size_in', and 'size_out' fields of 'hdr' are verified.

Before this commit, after the checks a buffer for the entire request would
be allocated and then all data including the verified header would be
copied from the userspace 'arg' pointer again.

Given that the 'arg' pointer resides in userspace, a malicious userspace
process can race to change the data pointed to by 'arg' between the two
copies. By doing so, the user can bypass the verifications on the ioctl
argument.

This commit fixes this by using the already checked copy of the header
to fill the header part of the allocated buffer and only copying the
remainder of the data from userspace.

Signed-off-by: Wenwen Wang <wang6495@umn.edu>
Reviewed-by: Hans de Goede <hdegoede@redhat.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",1,drivers/virt/vboxguest/vboxguest_linux.c,"{""sha"": ""6e2a9619192d2317f8f449fbb5f9c24d0699e3f8"", ""filename"": ""drivers/virt/vboxguest/vboxguest_linux.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 1, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/bd23a7269834dc7c1f93e83535d16ebc44b75eba/drivers/virt/vboxguest/vboxguest_linux.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/bd23a7269834dc7c1f93e83535d16ebc44b75eba/drivers/virt/vboxguest/vboxguest_linux.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/virt/vboxguest/vboxguest_linux.c?ref=bd23a7269834dc7c1f93e83535d16ebc44b75eba"", ""patch"": ""@@ -121,7 +121,9 @@ static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,\n \tif (!buf)\n \t\treturn -ENOMEM;\n \n-\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {\n+\t*((struct vbg_ioctl_hdr *)buf) = hdr;\n+\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),\n+\t\t\t   hdr.size_in - sizeof(hdr))) {\n \t\tret = -EFAULT;\n \t\tgoto out;\n \t}""}","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
",C,"	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
","	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
",,"@@ -121,7 +121,9 @@ static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
 	if (!buf)
 		return -ENOMEM;
 
-	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
+	*((struct vbg_ioctl_hdr *)buf) = hdr;
+	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
+			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}",linux,bd23a7269834dc7c1f93e83535d16ebc44b75eba,bdeeed098811b36d1f988521600a89a400830a4b,1,"static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
//flaw_line_below:
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
//fix_flaw_line_below:
//	*((struct vbg_ioctl_hdr *)buf) = hdr;
//fix_flaw_line_below:
//	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
//fix_flaw_line_below:
//			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
",182360,"static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","The `vbg_misc_device_ioctl` function handles IOCTL (Input/Output Control) requests for a virtual device in a Linux kernel module. It begins by validating the input data structure, ensuring the version and sizes are correct. The function allocates a buffer for the request, either using a specialized allocation for VMMDEV requests or standard kernel memory allocation. It then copies data from user space to this buffer, processes the request through `vbg_core_ioctl`, and retrieves the output size. Finally, it copies the response back to user space, cleans up allocated memory, and returns the appropriate status code based on the operations performed.","The provided code implements a function that processes IOCTL requests for a virtual device within a Linux kernel module. It begins by validating the input data structure to ensure compatibility and correct sizes. The function allocates a buffer for the request, using a specific allocation method for certain types of requests. It then copies data from user space into this buffer, processes the request, and retrieves the output size. Finally, it copies the response back to user space, manages memory cleanup, and returns an appropriate status code based on the success or failure of the operations performed.","The vulnerability identified in the original code, classified as CWE-362 (Race Condition), arises from the potential for concurrent access to shared resources without proper synchronization. In the vulnerable code, the function directly copied user data into a buffer without ensuring that the buffer was correctly initialized or that the data was safely managed, which could lead to inconsistent states if accessed simultaneously by multiple threads. The fixed code addresses this by explicitly copying the header structure into the allocated buffer before copying the user data, ensuring that the buffer is properly set up and reducing the risk of race conditions during concurrent operations.","The vulnerability identified in the original code, classified as CWE-362 (Race Condition), stems from the lack of proper synchronization when accessing shared resources, which can lead to inconsistent states during concurrent operations. In the vulnerable version, user data was copied directly into a buffer without ensuring that the buffer was correctly initialized, allowing multiple threads to potentially interfere with each other. The fixed code mitigates this risk by first copying the header information into the allocated buffer, ensuring that the buffer is properly set up before any user data is processed, thereby reducing the likelihood of race conditions.","The vulnerability was fixed by modifying the order of operations related to buffer management and data copying. In the updated code, the header structure is first copied into the allocated buffer using `*((struct vbg_ioctl_hdr *)buf) = hdr;`, ensuring that the buffer is properly initialized before any user data is processed. This change prevents potential race conditions by establishing a clear structure in the buffer before copying user data with `copy_from_user`. Additionally, this approach ensures that the buffer is consistently prepared, reducing the risk of concurrent access issues that could lead to inconsistent states.","The vulnerability was addressed by ensuring that the buffer is properly initialized before any user data is copied into it. The updated code first establishes the structure of the buffer by copying the header information, which sets a clear format for the data that will follow. This change prevents potential race conditions by ensuring that the buffer is in a consistent state before any concurrent access occurs. By organizing the data copying process in this manner, the risk of inconsistent states due to simultaneous operations is significantly reduced, enhancing the overall safety and reliability of the code."
29,182375,182375,,Remote,Not required,Complete,CVE-2018-12232,https://www.cvedetails.com/cve/CVE-2018-12232/,CWE-362,Medium,,,,2018-06-12,7.1,"In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",2018-10-31,,1,https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14,6d8c50dcb029872b298eea68cc6209c866fd3e14,"socket: close race condition between sock_close() and sockfs_setattr()

fchownat() doesn't even hold refcnt of fd until it figures out
fd is really needed (otherwise is ignored) and releases it after
it resolves the path. This means sock_close() could race with
sockfs_setattr(), which leads to a NULL pointer dereference
since typically we set sock->sk to NULL in ->release().

As pointed out by Al, this is unique to sockfs. So we can fix this
in socket layer by acquiring inode_lock in sock_close() and
checking against NULL in sockfs_setattr().

sock_release() is called in many places, only the sock_close()
path matters here. And fortunately, this should not affect normal
sock_close() as it is only called when the last fd refcnt is gone.
It only affects sock_close() with a parallel sockfs_setattr() in
progress, which is not common.

Fixes: 86741ec25462 (""net: core: Add a UID field to struct sock."")
Reported-by: shankarapailoor <shankarapailoor@gmail.com>
Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
Cc: Lorenzo Colitti <lorenzo@google.com>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/socket.c,"{""sha"": ""8a109012608a6132a65293c86cd175426b851cbe"", ""filename"": ""net/socket.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 3, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/socket.c?ref=6d8c50dcb029872b298eea68cc6209c866fd3e14"", ""patch"": ""@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n \tif (!err && (iattr->ia_valid & ATTR_UID)) {\n \t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n \n-\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\tif (sock->sk)\n+\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\telse\n+\t\t\terr = -ENOENT;\n \t}\n \n \treturn err;\n@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);\n  *\tan inode not a file.\n  */\n \n-void sock_release(struct socket *sock)\n+static void __sock_release(struct socket *sock, struct inode *inode)\n {\n \tif (sock->ops) {\n \t\tstruct module *owner = sock->ops->owner;\n \n+\t\tif (inode)\n+\t\t\tinode_lock(inode);\n \t\tsock->ops->release(sock);\n+\t\tif (inode)\n+\t\t\tinode_unlock(inode);\n \t\tsock->ops = NULL;\n \t\tmodule_put(owner);\n \t}\n@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)\n \t}\n \tsock->file = NULL;\n }\n+\n+void sock_release(struct socket *sock)\n+{\n+\t__sock_release(sock, NULL);\n+}\n EXPORT_SYMBOL(sock_release);\n \n void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)\n@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)\n \n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }\n ""}"," static int sock_close(struct inode *inode, struct file *filp)
 {
	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
"," static int sock_close(struct inode *inode, struct file *filp)
 {
	sock_release(SOCKET_I(inode));
 	return 0;
 }
",C,"	__sock_release(SOCKET_I(inode), inode);
","	sock_release(SOCKET_I(inode));
",,"@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
-		sock->sk->sk_uid = iattr->ia_uid;
+		if (sock->sk)
+			sock->sk->sk_uid = iattr->ia_uid;
+		else
+			err = -ENOENT;
 	}
 
 	return err;
@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);
  *	an inode not a file.
  */
 
-void sock_release(struct socket *sock)
+static void __sock_release(struct socket *sock, struct inode *inode)
 {
 	if (sock->ops) {
 		struct module *owner = sock->ops->owner;
 
+		if (inode)
+			inode_lock(inode);
 		sock->ops->release(sock);
+		if (inode)
+			inode_unlock(inode);
 		sock->ops = NULL;
 		module_put(owner);
 	}
@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)
 	}
 	sock->file = NULL;
 }
+
+void sock_release(struct socket *sock)
+{
+	__sock_release(sock, NULL);
+}
 EXPORT_SYMBOL(sock_release);
 
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)
@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)
 
 static int sock_close(struct inode *inode, struct file *filp)
 {
-	sock_release(SOCKET_I(inode));
+	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
 ",linux,6d8c50dcb029872b298eea68cc6209c866fd3e14,873aca2ee86e533665a73292c4e308ded1e9bafe,1," static int sock_close(struct inode *inode, struct file *filp)
 {
//flaw_line_below:
	sock_release(SOCKET_I(inode));
//fix_flaw_line_below:
//	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
",182375," static int sock_close(struct inode *inode, struct file *filp)
 {
	sock_release(SOCKET_I(inode));
 	return 0;
 }
"," static int sock_close(struct inode *inode, struct file *filp)
 {
	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
","The provided code defines a function `sock_close`, which is designed to handle the closing of a socket in a Linux kernel module. It takes two parameters: a pointer to an `inode` structure and a pointer to a `file` structure. The function retrieves the socket associated with the given inode using the `SOCKET_I` macro and then calls `sock_release` to properly release the socket resources. This ensures that any allocated memory and associated data structures are cleaned up, preventing memory leaks. The function returns 0, indicating successful completion of the socket closure operation.","The code defines a function that is responsible for closing a socket within a kernel module. It takes two parameters related to the file and its associated inode. The function retrieves the socket linked to the inode and invokes a method to release the resources allocated for that socket. This process ensures that all memory and data structures associated with the socket are properly cleaned up, thereby preventing potential memory leaks. The function concludes by returning a value that indicates the successful completion of the socket closure operation.","The vulnerability in the original code, identified as CWE-362, arises from the improper handling of socket resource release, which could lead to use-after-free issues. In the vulnerable code, the function directly calls `sock_release` without considering the context of the inode, potentially allowing access to freed resources. The fixed code addresses this by using `__sock_release`, which takes both the socket and the inode as parameters, ensuring that the release process is more robust and context-aware. This change helps prevent unintended access to deallocated memory, thereby enhancing the overall safety of the socket closure operation.","The vulnerability identified as CWE-362 in the original code stems from inadequate management of resource deallocation, which can lead to use-after-free conditions. This occurs when resources are released without proper context, allowing subsequent operations to access memory that has already been freed, potentially leading to undefined behavior or security breaches. The fixed code addresses this issue by implementing a more context-aware resource release mechanism, ensuring that the deallocation process considers the associated metadata. This enhancement significantly reduces the risk of accessing invalid memory, thereby improving the overall safety and stability of the socket closure operation.","The vulnerability was fixed by replacing the call to `sock_release` with `__sock_release`, which takes both the socket and the inode as parameters. This change ensures that the resource release process is more contextually aware, allowing the function to properly manage the relationship between the socket and its associated inode. By doing so, the fixed code mitigates the risk of use-after-free vulnerabilities, as it ensures that the socket is released in a manner that respects its current state and context. This enhancement improves memory safety and prevents potential access to freed resources during socket closure.","The vulnerability was addressed by implementing a more robust resource release mechanism that considers the relationship between the socket and its associated metadata. By enhancing the deallocation process to include context-specific information, the fix ensures that resources are released in a manner that prevents access to memory that has already been freed. This change significantly reduces the risk of use-after-free conditions, as it allows for safer management of memory and resources during the closure operation. Overall, the fix improves the stability and security of the code by ensuring proper handling of resource deallocation."
30,182377,182377,,Remote,Not required,Complete,CVE-2018-12232,https://www.cvedetails.com/cve/CVE-2018-12232/,CWE-362,Medium,,,,2018-06-12,7.1,"In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",2018-10-31,,4,https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14,6d8c50dcb029872b298eea68cc6209c866fd3e14,"socket: close race condition between sock_close() and sockfs_setattr()

fchownat() doesn't even hold refcnt of fd until it figures out
fd is really needed (otherwise is ignored) and releases it after
it resolves the path. This means sock_close() could race with
sockfs_setattr(), which leads to a NULL pointer dereference
since typically we set sock->sk to NULL in ->release().

As pointed out by Al, this is unique to sockfs. So we can fix this
in socket layer by acquiring inode_lock in sock_close() and
checking against NULL in sockfs_setattr().

sock_release() is called in many places, only the sock_close()
path matters here. And fortunately, this should not affect normal
sock_close() as it is only called when the last fd refcnt is gone.
It only affects sock_close() with a parallel sockfs_setattr() in
progress, which is not common.

Fixes: 86741ec25462 (""net: core: Add a UID field to struct sock."")
Reported-by: shankarapailoor <shankarapailoor@gmail.com>
Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
Cc: Lorenzo Colitti <lorenzo@google.com>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/socket.c,"{""sha"": ""8a109012608a6132a65293c86cd175426b851cbe"", ""filename"": ""net/socket.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 3, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/socket.c?ref=6d8c50dcb029872b298eea68cc6209c866fd3e14"", ""patch"": ""@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n \tif (!err && (iattr->ia_valid & ATTR_UID)) {\n \t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n \n-\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\tif (sock->sk)\n+\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\telse\n+\t\t\terr = -ENOENT;\n \t}\n \n \treturn err;\n@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);\n  *\tan inode not a file.\n  */\n \n-void sock_release(struct socket *sock)\n+static void __sock_release(struct socket *sock, struct inode *inode)\n {\n \tif (sock->ops) {\n \t\tstruct module *owner = sock->ops->owner;\n \n+\t\tif (inode)\n+\t\t\tinode_lock(inode);\n \t\tsock->ops->release(sock);\n+\t\tif (inode)\n+\t\t\tinode_unlock(inode);\n \t\tsock->ops = NULL;\n \t\tmodule_put(owner);\n \t}\n@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)\n \t}\n \tsock->file = NULL;\n }\n+\n+void sock_release(struct socket *sock)\n+{\n+\t__sock_release(sock, NULL);\n+}\n EXPORT_SYMBOL(sock_release);\n \n void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)\n@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)\n \n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }\n ""}","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
 	}
 
 	return err;
}
","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		sock->sk->sk_uid = iattr->ia_uid;
 	}
 
 	return err;
}
",C,"		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
","		sock->sk->sk_uid = iattr->ia_uid;
",,"@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
-		sock->sk->sk_uid = iattr->ia_uid;
+		if (sock->sk)
+			sock->sk->sk_uid = iattr->ia_uid;
+		else
+			err = -ENOENT;
 	}
 
 	return err;
@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);
  *	an inode not a file.
  */
 
-void sock_release(struct socket *sock)
+static void __sock_release(struct socket *sock, struct inode *inode)
 {
 	if (sock->ops) {
 		struct module *owner = sock->ops->owner;
 
+		if (inode)
+			inode_lock(inode);
 		sock->ops->release(sock);
+		if (inode)
+			inode_unlock(inode);
 		sock->ops = NULL;
 		module_put(owner);
 	}
@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)
 	}
 	sock->file = NULL;
 }
+
+void sock_release(struct socket *sock)
+{
+	__sock_release(sock, NULL);
+}
 EXPORT_SYMBOL(sock_release);
 
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)
@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)
 
 static int sock_close(struct inode *inode, struct file *filp)
 {
-	sock_release(SOCKET_I(inode));
+	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
 ",linux,6d8c50dcb029872b298eea68cc6209c866fd3e14,873aca2ee86e533665a73292c4e308ded1e9bafe,1,"static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
//flaw_line_below:
		sock->sk->sk_uid = iattr->ia_uid;
//fix_flaw_line_below:
//		if (sock->sk)
//fix_flaw_line_below:
//			sock->sk->sk_uid = iattr->ia_uid;
//fix_flaw_line_below:
//		else
//fix_flaw_line_below:
//			err = -ENOENT;
 	}
 
 	return err;
}
",182377,"static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		sock->sk->sk_uid = iattr->ia_uid;
 	}
 
 	return err;
}
","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
 	}
 
 	return err;
}
","The `sockfs_setattr` function is designed to update the attributes of a socket file in a Linux filesystem. It first calls `simple_setattr` to handle general attribute changes based on the provided `iattr` structure. If the operation is successful and the user ID attribute (`ATTR_UID`) is specified for modification, it retrieves the socket associated with the given `dentry` and updates its user ID (`sk_uid`) to the new value from `iattr`. The function ultimately returns any error code from the attribute setting process, ensuring that both general and specific updates are managed effectively.","The provided code defines a function that updates the attributes of a socket file within a filesystem. It first attempts to set general attributes based on the input parameters. If this operation is successful and the user ID attribute is specified for change, the function retrieves the associated socket and updates its user ID to the new value. The function ensures that both general and specific attribute modifications are handled appropriately, returning any error encountered during the process. This functionality is essential for managing socket permissions and attributes in a Linux environment.","The vulnerability identified in the original code, classified as CWE-362 (Race Condition), arises from the potential for a race condition when accessing the socket's structure. In the vulnerable code, after checking for a successful attribute update, the code directly accesses the socket's user ID without verifying if the socket structure is still valid, which could lead to undefined behavior if the socket is modified or deleted concurrently. The fixed code addresses this issue by adding a check to ensure that the socket pointer is valid before attempting to update the user ID, thereby preventing potential access to a null or invalid pointer and ensuring safer execution.","The vulnerability identified in the original code, categorized as CWE-362 (Race Condition), stems from the lack of proper synchronization when accessing shared resources. Specifically, after successfully updating attributes, the code directly modifies a user ID without confirming the validity of the associated structure, which could lead to inconsistencies if the resource is altered or deleted by another process concurrently. The fixed code mitigates this risk by introducing a validation check to ensure that the resource is still accessible before performing the update, thereby preventing potential access to invalid memory and enhancing the overall safety of the operation.","The vulnerability was fixed by adding a validation check for the socket structure before updating the user ID. In the modified code, after the successful call to `simple_setattr`, the code now checks if the socket pointer (`sock->sk`) is not null before attempting to assign the new user ID from `iattr->ia_uid`. If the socket pointer is null, the code sets the error variable to `-ENOENT`, indicating that the socket does not exist. This additional check ensures that the code does not attempt to access or modify an invalid or non-existent structure, thereby preventing potential race conditions and enhancing safety.","The vulnerability was addressed by implementing a validation step to ensure the integrity of the resource before making modifications. The updated code checks whether the associated structure is still valid and accessible after the initial attribute update. If the structure is found to be null or invalid, the code sets an appropriate error code to indicate that the resource does not exist. This additional safeguard prevents attempts to access or modify a potentially invalid resource, thereby mitigating the risk of race conditions and enhancing the overall robustness and safety of the operation."
31,182442,182442,,Local,Not required,Complete,CVE-2018-8897,https://www.cvedetails.com/cve/CVE-2018-8897/,CWE-362,Low,Complete,Complete,,2018-05-08,7.2,"A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",2019-10-02,,8,https://github.com/torvalds/linux/commit/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,"x86/entry/64: Don't use IST entry for #BP stack

There's nothing IST-worthy about #BP/int3.  We don't allow kprobes
in the small handful of places in the kernel that run at CPL0 with
an invalid stack, and 32-bit kernels have used normal interrupt
gates for #BP forever.

Furthermore, we don't allow kprobes in places that have usergs while
in kernel mode, so ""paranoid"" is also unnecessary.

Signed-off-by: Andy Lutomirski <luto@kernel.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Cc: stable@vger.kernel.org",6,arch/x86/kernel/traps.c,"{""sha"": ""9b114675fbc05e1f589e6838f4603de871c0565d"", ""filename"": ""arch/x86/entry/entry_64.S"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 1, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/entry/entry_64.S"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/entry/entry_64.S"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/entry/entry_64.S?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -1138,7 +1138,7 @@ apicinterrupt3 HYPERV_REENLIGHTENMENT_VECTOR \\\n #endif /* CONFIG_HYPERV */\n \n idtentry debug\t\t\tdo_debug\t\thas_error_code=0\tparanoid=1 shift_ist=DEBUG_STACK\n-idtentry int3\t\t\tdo_int3\t\t\thas_error_code=0\tparanoid=1 shift_ist=DEBUG_STACK\n+idtentry int3\t\t\tdo_int3\t\t\thas_error_code=0\n idtentry stack_segment\t\tdo_stack_segment\thas_error_code=1\n \n #ifdef CONFIG_XEN""}<_**next**_>{""sha"": ""50bee5fe114013622ee858e4a3b7b16d1ffa8a05"", ""filename"": ""arch/x86/kernel/idt.c"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 2, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/idt.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/idt.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kernel/idt.c?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -160,7 +160,6 @@ static const __initconst struct idt_data early_pf_idts[] = {\n  */\n static const __initconst struct idt_data dbg_idts[] = {\n \tINTG(X86_TRAP_DB,\tdebug),\n-\tINTG(X86_TRAP_BP,\tint3),\n };\n #endif\n \n@@ -183,7 +182,6 @@ gate_desc debug_idt_table[IDT_ENTRIES] __page_aligned_bss;\n static const __initconst struct idt_data ist_idts[] = {\n \tISTG(X86_TRAP_DB,\tdebug,\t\tDEBUG_STACK),\n \tISTG(X86_TRAP_NMI,\tnmi,\t\tNMI_STACK),\n-\tSISTG(X86_TRAP_BP,\tint3,\t\tDEBUG_STACK),\n \tISTG(X86_TRAP_DF,\tdouble_fault,\tDOUBLEFAULT_STACK),\n #ifdef CONFIG_X86_MCE\n \tISTG(X86_TRAP_MC,\t&machine_check,\tMCE_STACK),""}<_**next**_>{""sha"": ""03f3d7695daccae1af7265f2aab221da81573ce5"", ""filename"": ""arch/x86/kernel/traps.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 7, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/traps.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/traps.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kernel/traps.c?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -577,7 +577,6 @@ do_general_protection(struct pt_regs *regs, long error_code)\n }\n NOKPROBE_SYMBOL(do_general_protection);\n \n-/* May run on IST stack. */\n dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n {\n #ifdef CONFIG_DYNAMIC_FTRACE\n@@ -592,6 +591,13 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n \tif (poke_int3_handler(regs))\n \t\treturn;\n \n+\t/*\n+\t * Use ist_enter despite the fact that we don't use an IST stack.\n+\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel\n+\t * mode or even during context tracking state changes.\n+\t *\n+\t * This means that we can't schedule.  That's okay.\n+\t */\n \tist_enter(regs);\n \tRCU_LOCKDEP_WARN(!rcu_is_watching(), \""entry code didn't wake RCU\"");\n #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n@@ -609,15 +615,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n \t\t\tSIGTRAP) == NOTIFY_STOP)\n \t\tgoto exit;\n \n-\t/*\n-\t * Let others (NMI) know that the debug stack is in use\n-\t * as we may switch to the interrupt stack.\n-\t */\n-\tdebug_stack_usage_inc();\n \tcond_local_irq_enable(regs);\n \tdo_trap(X86_TRAP_BP, SIGTRAP, \""int3\"", regs, error_code, NULL);\n \tcond_local_irq_disable(regs);\n-\tdebug_stack_usage_dec();\n+\n exit:\n \tist_exit(regs);\n }""}"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);

 exit:
 	ist_exit(regs);
 }
"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
	debug_stack_usage_dec();
 exit:
 	ist_exit(regs);
 }
",C,"	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */

","	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
	debug_stack_usage_dec();
",,"@@ -577,7 +577,6 @@ do_general_protection(struct pt_regs *regs, long error_code)
 }
 NOKPROBE_SYMBOL(do_general_protection);
 
-/* May run on IST stack. */
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -592,6 +591,13 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	if (poke_int3_handler(regs))
 		return;
 
+	/*
+	 * Use ist_enter despite the fact that we don't use an IST stack.
+	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
+	 * mode or even during context tracking state changes.
+	 *
+	 * This means that we can't schedule.  That's okay.
+	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
@@ -609,15 +615,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
-	/*
-	 * Let others (NMI) know that the debug stack is in use
-	 * as we may switch to the interrupt stack.
-	 */
-	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
-	debug_stack_usage_dec();
+
 exit:
 	ist_exit(regs);
 }",linux,d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,06ace26f4e6fcf747e890a39193be811777a048a,1," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * Use ist_enter despite the fact that we don't use an IST stack.
//fix_flaw_line_below:
//	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
//fix_flaw_line_below:
//	 * mode or even during context tracking state changes.
//fix_flaw_line_below:
//	 *
//fix_flaw_line_below:
//	 * This means that we can't schedule.  That's okay.
//fix_flaw_line_below:
//	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
//flaw_line_below:
	/*
//flaw_line_below:
	 * Let others (NMI) know that the debug stack is in use
//flaw_line_below:
	 * as we may switch to the interrupt stack.
//flaw_line_below:
	 */
//flaw_line_below:
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
//flaw_line_below:
	debug_stack_usage_dec();
//fix_flaw_line_below:
//
 exit:
 	ist_exit(regs);
 }
",182442," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
	debug_stack_usage_dec();
 exit:
 	ist_exit(regs);
 }
"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);

 exit:
 	ist_exit(regs);
 }
","The provided code defines a function `do_int3`, which handles the INT3 (breakpoint) exception in a kernel environment. It first checks if dynamic ftrace is active and handles it accordingly to prevent recursive crashes. If not, it proceeds to handle the INT3 exception through various mechanisms, including KGDB for debugging and Kprobes for dynamic instrumentation. The function ensures that the RCU (Read-Copy-Update) mechanism is properly managed and increments the debug stack usage. It then enables local interrupts, processes the trap, and finally disables interrupts before exiting, ensuring proper cleanup and state management throughout the process.","The code defines a function that manages the handling of a specific type of exception related to breakpoints in a kernel environment. It begins by checking for certain debugging features to prevent potential crashes during execution. If these checks pass, it processes the exception through various debugging and instrumentation mechanisms. The function ensures proper management of a concurrency control mechanism and tracks the usage of a debugging stack. It then enables and disables local interrupts while handling the exception, ensuring that the system state is maintained correctly throughout the process before exiting cleanly.","The vulnerability identified as CWE-362, which relates to a race condition, was present in the original code due to improper handling of concurrent access during the breakpoint exception processing. Specifically, the lack of synchronization mechanisms allowed multiple threads to potentially modify shared state simultaneously, leading to unpredictable behavior. The fixed code introduces a more structured approach by ensuring that the function checks for conditions like `modifying_ftrace_code` and uses `ist_enter` to manage the entry into the interrupt service routine safely. This change helps prevent concurrent modifications and ensures that the system state remains consistent during exception handling.","The vulnerability identified as CWE-362, which pertains to race conditions, arose from inadequate synchronization in the handling of breakpoint exceptions, allowing concurrent access to shared resources. This lack of proper management could lead to unpredictable behavior and system instability, as multiple threads might modify the same state simultaneously. The fixed code addresses this issue by implementing structured checks and entry management, ensuring that the execution context is properly controlled. This enhancement prevents concurrent modifications during critical operations, thereby maintaining system integrity and consistency throughout the exception handling process.","The vulnerability was fixed by introducing structured checks and proper management of the execution context within the exception handling routine. The code now first checks the state of `modifying_ftrace_code` and calls the `ftrace_int3_handler` to prevent recursive crashes. Additionally, the use of `ist_enter` ensures that the function safely manages the entry into the interrupt service routine, even when called from various contexts. This approach, combined with the checks for `kgdb_ll_trap` and `kprobe_int3_handler`, ensures that shared resources are accessed in a controlled manner, effectively mitigating the race condition.","The vulnerability was addressed by implementing structured checks and enhancing the management of the execution context during exception handling. The revised code introduces safeguards to prevent recursive crashes and ensures that the entry into the interrupt service routine is handled safely, even when invoked from various contexts. Additionally, the implementation includes checks for specific debugging and instrumentation mechanisms, which helps control access to shared resources. This comprehensive approach effectively mitigates the risk of concurrent modifications, thereby maintaining system stability and integrity during critical operations."
32,182476,182476,,Remote,Not required,Partial,CVE-2018-7998,https://www.cvedetails.com/cve/CVE-2018-7998/,CWE-362,High,Partial,Partial,,2018-03-09,5.1,"In libvips before 8.6.3, a NULL function pointer dereference vulnerability was found in the vips_region_generate function in region.c, which allows remote attackers to cause a denial of service or possibly have unspecified other impact via a crafted image file. This occurs because of a race condition involving a failed delayed load and other worker threads.",2018-03-27,DoS ,17,https://github.com/jcupitt/libvips/commit/20d840e6da15c1574b3ed998bc92f91d1e36c2a5,20d840e6da15c1574b3ed998bc92f91d1e36c2a5,"fix a crash with delayed load

If a delayed load failed, it could leave the pipeline only half-set up.
Sebsequent threads could then segv.

Set a load-has-failed flag and test before generate.

See https://github.com/jcupitt/libvips/issues/893",5,libvips/foreign/foreign.c,"{""sha"": ""08aaab8c21980577a74498f3b19bc3a155cbec9f"", ""filename"": ""ChangeLog"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/ChangeLog"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/ChangeLog"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/ChangeLog?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -4,6 +4,7 @@\n   writing twice to memory\n - better rounding behaviour in convolution means we hit the vector path more\n   often\n+- fix a crash if a delayed load failed [gsharpsh00ter]\n \n 5/1/18 started 8.6.2\n - vips_sink_screen() keeps a ref to the input image ... stops a rare race""}<_**next**_>{""sha"": ""fb03fd746990f1e8f7ff129a4e4db1e7320d3bb2"", ""filename"": ""libvips/foreign/foreign.c"", ""status"": ""modified"", ""additions"": 19, ""deletions"": 6, ""changes"": 25, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/foreign/foreign.c"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/foreign/foreign.c"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/libvips/foreign/foreign.c?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -18,6 +18,8 @@\n  * \t- transform cmyk->rgb if there's an embedded profile\n  * 16/6/17\n  * \t- add page_height\n+ * 5/3/18\n+ * \t- block _start if one start fails, see #893\n  */\n \n /*\n@@ -796,6 +798,11 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )\n \tVipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );\n \tVipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );\n \n+\t/* If this start has failed before in another thread, we can fail now.\n+\t */\n+\tif( load->error )\n+\t\treturn( NULL );\n+\n \tif( !load->real ) {\n \t\tif( !(load->real = vips_foreign_load_temp( load )) )\n \t\t\treturn( NULL );\n@@ -819,19 +826,25 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )\n \t\tg_object_set_qdata( G_OBJECT( load->real ), \n \t\t\tvips__foreign_load_operation, load ); \n \n-\t\tif( class->load( load ) ||\n-\t\t\tvips_image_pio_input( load->real ) ) \n-\t\t\treturn( NULL );\n-\n-\t\t/* ->header() read the header into @out, load has read the\n+\t\t/* Load the image and check the result.\n+\t\t *\n+\t\t * ->header() read the header into @out, load has read the\n \t\t * image into @real. They must match exactly in size, bands,\n \t\t * format and coding for the copy to work.  \n \t\t *\n \t\t * Some versions of ImageMagick give different results between\n \t\t * Ping and Load for some formats, for example.\n+\t\t *\n+\t\t * If the load fails, we need to stop\n \t\t */\n-\t\tif( !vips_foreign_load_iscompat( load->real, out ) )\n+\t\tif( class->load( load ) ||\n+\t\t\tvips_image_pio_input( load->real ) || \n+\t\t\tvips_foreign_load_iscompat( load->real, out ) ) {\n+\t\t\tvips_operation_invalidate( VIPS_OPERATION( load ) ); \n+\t\t\tload->error = TRUE;\n+\n \t\t\treturn( NULL );\n+\t\t}\n \n \t\t/* We have to tell vips that out depends on real. We've set\n \t\t * the demand hint below, but not given an input there.""}<_**next**_>{""sha"": ""dcbf21c82c7b70bba5714a9be2b33ee59aa6d51b"", ""filename"": ""libvips/include/vips/foreign.h"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 0, ""changes"": 5, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/include/vips/foreign.h"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/include/vips/foreign.h"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/libvips/include/vips/foreign.h?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -158,6 +158,11 @@ typedef struct _VipsForeignLoad {\n \t * TRUE.\n \t */\n \tgboolean disc;\n+\n+\t/* Set if a start function fails. We want to prevent the other starts\n+\t * from also triggering the load.\n+\t */\n+\tgboolean error;\n } VipsForeignLoad;\n \n typedef struct _VipsForeignLoadClass {""}","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
		 *
		 * If the load fails, we need to stop
 		 */
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

 			return( NULL );
		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
 		 */
		if( !vips_foreign_load_iscompat( load->real, out ) )
 			return( NULL );
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
",C,"	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
		 *
		 * If the load fails, we need to stop
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

		}
","		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
		if( !vips_foreign_load_iscompat( load->real, out ) )
",,"@@ -18,6 +18,8 @@
  * 	- transform cmyk->rgb if there's an embedded profile
  * 16/6/17
  * 	- add page_height
+ * 5/3/18
+ * 	- block _start if one start fails, see #893
  */
 
 /*
@@ -796,6 +798,11 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
+	/* If this start has failed before in another thread, we can fail now.
+	 */
+	if( load->error )
+		return( NULL );
+
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );
@@ -819,19 +826,25 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
-		if( class->load( load ) ||
-			vips_image_pio_input( load->real ) ) 
-			return( NULL );
-
-		/* ->header() read the header into @out, load has read the
+		/* Load the image and check the result.
+		 *
+		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
+		 *
+		 * If the load fails, we need to stop
 		 */
-		if( !vips_foreign_load_iscompat( load->real, out ) )
+		if( class->load( load ) ||
+			vips_image_pio_input( load->real ) || 
+			vips_foreign_load_iscompat( load->real, out ) ) {
+			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
+			load->error = TRUE;
+
 			return( NULL );
+		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.",libvips,20d840e6da15c1574b3ed998bc92f91d1e36c2a5,eefb2dad980c12922ad15d987be7c4b42a44d5cf,1,"vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
//fix_flaw_line_below:
//	/* If this start has failed before in another thread, we can fail now.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	if( load->error )
//fix_flaw_line_below:
//		return( NULL );
//fix_flaw_line_below:
//
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
//flaw_line_below:
		if( class->load( load ) ||
//flaw_line_below:
			vips_image_pio_input( load->real ) ) 
//flaw_line_below:
			return( NULL );
//flaw_line_below:

//flaw_line_below:
		/* ->header() read the header into @out, load has read the
//fix_flaw_line_below:
//		/* Load the image and check the result.
//fix_flaw_line_below:
//		 *
//fix_flaw_line_below:
//		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
//fix_flaw_line_below:
//		 *
//fix_flaw_line_below:
//		 * If the load fails, we need to stop
 		 */
//flaw_line_below:
		if( !vips_foreign_load_iscompat( load->real, out ) )
//fix_flaw_line_below:
//		if( class->load( load ) ||
//fix_flaw_line_below:
//			vips_image_pio_input( load->real ) || 
//fix_flaw_line_below:
//			vips_foreign_load_iscompat( load->real, out ) ) {
//fix_flaw_line_below:
//			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
//fix_flaw_line_below:
//			load->error = TRUE;
//fix_flaw_line_below:
//
 			return( NULL );
//fix_flaw_line_below:
//		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
",182476,"vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
 		 */
		if( !vips_foreign_load_iscompat( load->real, out ) )
 			return( NULL );
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
		 *
		 * If the load fails, we need to stop
 		 */
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

 			return( NULL );
		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","The `vips_foreign_load_start` function initiates the loading of an image using the VIPS image processing library. It checks if the image has already been loaded; if not, it attempts to load the image into a temporary storage. The function sets up progress signals to allow the caller to monitor the loading process and associates the load object with the image for error handling. It verifies that the loaded image's properties match those of the output image, ensuring compatibility. Finally, it establishes a pipeline connection between the output and the loaded image, returning a new region representing the loaded image data.","The provided code defines a function that begins the process of loading an image from a foreign format into a specified output structure. It first checks if the image has already been loaded; if not, it attempts to load it into temporary storage while setting up progress tracking for the loading operation. The function ensures that the properties of the loaded image match those of the output image to maintain compatibility. Finally, it establishes a connection between the output and the loaded image, allowing for efficient data handling and retrieval of the image information.","The vulnerability identified as CWE-362, which pertains to a race condition, arises when multiple threads access shared data without proper synchronization, leading to inconsistent states. In the vulnerable code, if one thread initiated the loading process and encountered an error, another thread could still attempt to load the image, potentially causing further issues. The fix introduces a check for the `load->error` variable at the beginning of the function, ensuring that if a loading error has previously occurred, the function will immediately return without proceeding, thus preventing concurrent threads from acting on an invalid state and enhancing thread safety.","The vulnerability identified as CWE-362, which involves a race condition, occurs when multiple threads access shared resources without adequate synchronization, leading to unpredictable behavior. In the original code, if one thread encountered an error during the loading process, another thread could still attempt to initiate the load, potentially resulting in inconsistent states or further errors. The fix addresses this issue by introducing a preliminary check to determine if a loading error has already occurred. This ensures that any subsequent attempts to load are halted, thereby preventing multiple threads from acting on an invalid or erroneous state and enhancing overall thread safety.","The vulnerability was fixed by adding a check for the `load->error` variable at the beginning of the function. This modification ensures that if a loading error has previously occurred, the function will immediately return `NULL`, preventing any further processing. By doing this, the code effectively halts any subsequent attempts to load the image if an error has been flagged, thus avoiding potential race conditions where multiple threads could act on an invalid state. Additionally, if the loading process fails, the code sets `load->error` to `TRUE`, providing a clear indication of the failure for any future operations.","The vulnerability was addressed by implementing an initial check to determine if a loading error had already occurred before proceeding with the image loading process. This safeguard ensures that if an error is detected, the function will immediately terminate and return an error indication, preventing any further attempts to load the image. By doing so, the code effectively prevents multiple threads from interacting with a potentially invalid state, thereby eliminating the risk of race conditions. This enhancement improves the overall stability and reliability of the image loading operation in a multi-threaded environment."
33,182551,182551,,Local,Not required,Partial,CVE-2017-18249,https://www.cvedetails.com/cve/CVE-2017-18249/,CWE-362,Medium,Partial,Partial,,2018-03-26,4.4,"The add_free_nid function in fs/f2fs/node.c in the Linux kernel before 4.12 does not properly track an allocated nid, which allows local users to cause a denial of service (race condition) or possibly have unspecified other impact via concurrent threads.",2019-04-02,DoS ,45,https://github.com/torvalds/linux/commit/30a61ddf8117c26ac5b295e1233eaa9629a94ca3,30a61ddf8117c26ac5b295e1233eaa9629a94ca3,"f2fs: fix race condition in between free nid allocator/initializer

In below concurrent case, allocated nid can be loaded into free nid cache
and be allocated again.

Thread A				Thread B
- f2fs_create
 - f2fs_new_inode
  - alloc_nid
   - __insert_nid_to_list(ALLOC_NID_LIST)
					- f2fs_balance_fs_bg
					 - build_free_nids
					  - __build_free_nids
					   - scan_nat_page
					    - add_free_nid
					     - __lookup_nat_cache
 - f2fs_add_link
  - init_inode_metadata
   - new_inode_page
    - new_node_page
     - set_node_addr
 - alloc_nid_done
  - __remove_nid_from_list(ALLOC_NID_LIST)
					     - __insert_nid_to_list(FREE_NID_LIST)

This patch makes nat cache lookup and free nid list operation being atomical
to avoid this race condition.

Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
Signed-off-by: Chao Yu <yuchao0@huawei.com>
Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>",17,fs/f2fs/node.c,"{""sha"": ""29dc996b573c7ecfbbb748a79ce8378784f635dc"", ""filename"": ""fs/f2fs/node.c"", ""status"": ""modified"", ""additions"": 45, ""deletions"": 18, ""changes"": 63, ""blob_url"": ""https://github.com/torvalds/linux/blob/30a61ddf8117c26ac5b295e1233eaa9629a94ca3/fs/f2fs/node.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/30a61ddf8117c26ac5b295e1233eaa9629a94ca3/fs/f2fs/node.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/f2fs/node.c?ref=30a61ddf8117c26ac5b295e1233eaa9629a94ca3"", ""patch"": ""@@ -1761,40 +1761,67 @@ static void __remove_nid_from_list(struct f2fs_sb_info *sbi,\n static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n {\n \tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n-\tstruct free_nid *i;\n+\tstruct free_nid *i, *e;\n \tstruct nat_entry *ne;\n-\tint err;\n+\tint err = -EINVAL;\n+\tbool ret = false;\n \n \t/* 0 nid should not be used */\n \tif (unlikely(nid == 0))\n \t\treturn false;\n \n-\tif (build) {\n-\t\t/* do not add allocated nids */\n-\t\tne = __lookup_nat_cache(nm_i, nid);\n-\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n-\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n-\t\t\treturn false;\n-\t}\n-\n \ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n \ti->nid = nid;\n \ti->state = NID_NEW;\n \n-\tif (radix_tree_preload(GFP_NOFS)) {\n-\t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n+\tif (radix_tree_preload(GFP_NOFS))\n+\t\tgoto err;\n \n \tspin_lock(&nm_i->nid_list_lock);\n+\n+\tif (build) {\n+\t\t/*\n+\t\t *   Thread A             Thread B\n+\t\t *  - f2fs_create\n+\t\t *   - f2fs_new_inode\n+\t\t *    - alloc_nid\n+\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)\n+\t\t *                     - f2fs_balance_fs_bg\n+\t\t *                      - build_free_nids\n+\t\t *                       - __build_free_nids\n+\t\t *                        - scan_nat_page\n+\t\t *                         - add_free_nid\n+\t\t *                          - __lookup_nat_cache\n+\t\t *  - f2fs_add_link\n+\t\t *   - init_inode_metadata\n+\t\t *    - new_inode_page\n+\t\t *     - new_node_page\n+\t\t *      - set_node_addr\n+\t\t *  - alloc_nid_done\n+\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)\n+\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)\n+\t\t */\n+\t\tne = __lookup_nat_cache(nm_i, nid);\n+\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n+\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n+\t\t\tgoto err_out;\n+\n+\t\te = __lookup_free_nid_list(nm_i, nid);\n+\t\tif (e) {\n+\t\t\tif (e->state == NID_NEW)\n+\t\t\t\tret = true;\n+\t\t\tgoto err_out;\n+\t\t}\n+\t}\n+\tret = true;\n \terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\n+err_out:\n \tspin_unlock(&nm_i->nid_list_lock);\n \tradix_tree_preload_end();\n-\tif (err) {\n+err:\n+\tif (err)\n \t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n-\treturn true;\n+\treturn ret;\n }\n \n static void remove_free_nid(struct f2fs_sb_info *sbi, nid_t nid)""}"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i, *e;
 	struct nat_entry *ne;
	int err = -EINVAL;
	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS))
		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
err:
	if (err)
 		kmem_cache_free(free_nid_slab, i);
	return ret;
 }
"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i;
 	struct nat_entry *ne;
	int err;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
 
 	spin_lock(&nm_i->nid_list_lock);
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
	if (err) {
 		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	return true;
 }
",C,"	struct free_nid *i, *e;
	int err = -EINVAL;
	bool ret = false;
	if (radix_tree_preload(GFP_NOFS))
		goto err;

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
err_out:
err:
	if (err)
	return ret;
","	struct free_nid *i;
	int err;
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	if (err) {
		return true;
	}
	return true;
",,"@@ -1761,40 +1761,67 @@ static void __remove_nid_from_list(struct f2fs_sb_info *sbi,
 static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
-	struct free_nid *i;
+	struct free_nid *i, *e;
 	struct nat_entry *ne;
-	int err;
+	int err = -EINVAL;
+	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
-	if (build) {
-		/* do not add allocated nids */
-		ne = __lookup_nat_cache(nm_i, nid);
-		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
-				nat_get_blkaddr(ne) != NULL_ADDR))
-			return false;
-	}
-
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
-	if (radix_tree_preload(GFP_NOFS)) {
-		kmem_cache_free(free_nid_slab, i);
-		return true;
-	}
+	if (radix_tree_preload(GFP_NOFS))
+		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);
+
+	if (build) {
+		/*
+		 *   Thread A             Thread B
+		 *  - f2fs_create
+		 *   - f2fs_new_inode
+		 *    - alloc_nid
+		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
+		 *                     - f2fs_balance_fs_bg
+		 *                      - build_free_nids
+		 *                       - __build_free_nids
+		 *                        - scan_nat_page
+		 *                         - add_free_nid
+		 *                          - __lookup_nat_cache
+		 *  - f2fs_add_link
+		 *   - init_inode_metadata
+		 *    - new_inode_page
+		 *     - new_node_page
+		 *      - set_node_addr
+		 *  - alloc_nid_done
+		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
+		 *                         - __insert_nid_to_list(FREE_NID_LIST)
+		 */
+		ne = __lookup_nat_cache(nm_i, nid);
+		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
+				nat_get_blkaddr(ne) != NULL_ADDR))
+			goto err_out;
+
+		e = __lookup_free_nid_list(nm_i, nid);
+		if (e) {
+			if (e->state == NID_NEW)
+				ret = true;
+			goto err_out;
+		}
+	}
+	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
+err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
-	if (err) {
+err:
+	if (err)
 		kmem_cache_free(free_nid_slab, i);
-		return true;
-	}
-	return true;
+	return ret;
 }
 
 static void remove_free_nid(struct f2fs_sb_info *sbi, nid_t nid)",linux,30a61ddf8117c26ac5b295e1233eaa9629a94ca3,5f4c3dec225e03273e1f74816e676cccc1a54329,1," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
//flaw_line_below:
	struct free_nid *i;
//fix_flaw_line_below:
//	struct free_nid *i, *e;
 	struct nat_entry *ne;
//flaw_line_below:
	int err;
//fix_flaw_line_below:
//	int err = -EINVAL;
//fix_flaw_line_below:
//	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
//flaw_line_below:
	if (build) {
//flaw_line_below:
		/* do not add allocated nids */
//flaw_line_below:
		ne = __lookup_nat_cache(nm_i, nid);
//flaw_line_below:
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
//flaw_line_below:
				nat_get_blkaddr(ne) != NULL_ADDR))
//flaw_line_below:
			return false;
//flaw_line_below:
	}
//flaw_line_below:

 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
//flaw_line_below:
	if (radix_tree_preload(GFP_NOFS)) {
//flaw_line_below:
		kmem_cache_free(free_nid_slab, i);
//flaw_line_below:
		return true;
//flaw_line_below:
	}
//fix_flaw_line_below:
//	if (radix_tree_preload(GFP_NOFS))
//fix_flaw_line_below:
//		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	if (build) {
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 *   Thread A             Thread B
//fix_flaw_line_below:
//		 *  - f2fs_create
//fix_flaw_line_below:
//		 *   - f2fs_new_inode
//fix_flaw_line_below:
//		 *    - alloc_nid
//fix_flaw_line_below:
//		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
//fix_flaw_line_below:
//		 *                     - f2fs_balance_fs_bg
//fix_flaw_line_below:
//		 *                      - build_free_nids
//fix_flaw_line_below:
//		 *                       - __build_free_nids
//fix_flaw_line_below:
//		 *                        - scan_nat_page
//fix_flaw_line_below:
//		 *                         - add_free_nid
//fix_flaw_line_below:
//		 *                          - __lookup_nat_cache
//fix_flaw_line_below:
//		 *  - f2fs_add_link
//fix_flaw_line_below:
//		 *   - init_inode_metadata
//fix_flaw_line_below:
//		 *    - new_inode_page
//fix_flaw_line_below:
//		 *     - new_node_page
//fix_flaw_line_below:
//		 *      - set_node_addr
//fix_flaw_line_below:
//		 *  - alloc_nid_done
//fix_flaw_line_below:
//		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
//fix_flaw_line_below:
//		 *                         - __insert_nid_to_list(FREE_NID_LIST)
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		ne = __lookup_nat_cache(nm_i, nid);
//fix_flaw_line_below:
//		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
//fix_flaw_line_below:
//				nat_get_blkaddr(ne) != NULL_ADDR))
//fix_flaw_line_below:
//			goto err_out;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		e = __lookup_free_nid_list(nm_i, nid);
//fix_flaw_line_below:
//		if (e) {
//fix_flaw_line_below:
//			if (e->state == NID_NEW)
//fix_flaw_line_below:
//				ret = true;
//fix_flaw_line_below:
//			goto err_out;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
//fix_flaw_line_below:
//err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
//flaw_line_below:
	if (err) {
//fix_flaw_line_below:
//err:
//fix_flaw_line_below:
//	if (err)
 		kmem_cache_free(free_nid_slab, i);
//flaw_line_below:
		return true;
//flaw_line_below:
	}
//flaw_line_below:
	return true;
//fix_flaw_line_below:
//	return ret;
 }
",182551," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i;
 	struct nat_entry *ne;
	int err;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
 
 	spin_lock(&nm_i->nid_list_lock);
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
	if (err) {
 		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	return true;
 }
"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i, *e;
 	struct nat_entry *ne;
	int err = -EINVAL;
	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS))
		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
err:
	if (err)
 		kmem_cache_free(free_nid_slab, i);
	return ret;
 }
","The `add_free_nid` function is designed to manage the addition of free node IDs (NIDs) in a file system's namespace management structure. It first checks if the provided NID is valid (not zero) and, if the `build` flag is set, ensures that the NID is not already allocated by looking it up in the NAT cache. If the NID is valid and not allocated, the function allocates memory for a new `free_nid` structure, initializes it, and attempts to preload a radix tree. It then acquires a lock on the NID list, inserts the new NID into the list, and releases the lock. If any errors occur during this process, it frees the allocated memory.","The function is responsible for adding a free node identifier to a file system's namespace management structure. It first verifies that the provided identifier is valid and, if a specific flag is set, checks that the identifier is not already in use. If the identifier is valid and available, the function allocates memory for a new structure, initializes it, and attempts to prepare a data structure for efficient access. It then locks the list of identifiers, inserts the new identifier into this list, and unlocks it. If any issues arise during this process, it ensures that allocated memory is properly freed.","The vulnerability identified by CWE-362, which pertains to a race condition, arises from the concurrent access to shared resources without proper synchronization. In the vulnerable code, multiple threads could simultaneously call the function to add a free node identifier, leading to inconsistent states if one thread modifies the list while another is reading or writing to it. The fixed code addresses this issue by ensuring that the critical section, where the list is accessed and modified, is properly protected by acquiring a lock before any operations are performed. This prevents race conditions by ensuring that only one thread can manipulate the list at a time, thus maintaining data integrity.","The vulnerability identified by CWE-362, which relates to race conditions, occurs when multiple threads access shared resources concurrently without adequate synchronization, leading to unpredictable behavior and potential data corruption. In the vulnerable code, the lack of proper locking mechanisms allowed simultaneous modifications and reads of a shared list, resulting in inconsistent states. The fixed code resolves this issue by implementing a locking mechanism that ensures only one thread can access and modify the shared resource at a time. This change effectively prevents concurrent threads from interfering with each other, thereby maintaining data integrity and consistency within the system.","The vulnerability was fixed by introducing a locking mechanism around the critical section where the shared list is accessed. In the updated code, a lock is acquired using `spin_lock` before checking and modifying the free node identifier list. This ensures that only one thread can execute the section that involves looking up the NAT cache and inserting the new identifier into the list, thus preventing race conditions. After the operations are completed, the lock is released with `spin_unlock`, ensuring that other threads must wait their turn to access the shared resource, thereby maintaining data integrity and consistency.","The vulnerability was fixed by implementing a locking mechanism to control access to the shared resource, ensuring that only one thread can modify or read from it at a time. This was achieved by acquiring a lock before entering the critical section where the shared list is accessed, which prevents concurrent threads from interfering with each other. After completing the necessary operations, the lock is released, allowing other threads to access the resource in a controlled manner. This change effectively eliminates the race condition, ensuring data integrity and consistency within the system during concurrent operations."
34,182583,182583,,Local,Not required,Partial,CVE-2017-18203,https://www.cvedetails.com/cve/CVE-2017-18203/,CWE-362,Medium,,,,2018-02-27,1.9,The dm_get_from_kobject function in drivers/md/dm.c in the Linux kernel before 4.14.3 allow local users to cause a denial of service (BUG) by leveraging a race condition with __dm_destroy during creation and removal of DM devices.,2018-06-19,DoS ,8,https://github.com/torvalds/linux/commit/b9a41d21dceadf8104812626ef85dc56ee8a60ed,b9a41d21dceadf8104812626ef85dc56ee8a60ed,"dm: fix race between dm_get_from_kobject() and __dm_destroy()

The following BUG_ON was hit when testing repeat creation and removal of
DM devices:

    kernel BUG at drivers/md/dm.c:2919!
    CPU: 7 PID: 750 Comm: systemd-udevd Not tainted 4.1.44
    Call Trace:
     [<ffffffff81649e8b>] dm_get_from_kobject+0x34/0x3a
     [<ffffffff81650ef1>] dm_attr_show+0x2b/0x5e
     [<ffffffff817b46d1>] ? mutex_lock+0x26/0x44
     [<ffffffff811df7f5>] sysfs_kf_seq_show+0x83/0xcf
     [<ffffffff811de257>] kernfs_seq_show+0x23/0x25
     [<ffffffff81199118>] seq_read+0x16f/0x325
     [<ffffffff811de994>] kernfs_fop_read+0x3a/0x13f
     [<ffffffff8117b625>] __vfs_read+0x26/0x9d
     [<ffffffff8130eb59>] ? security_file_permission+0x3c/0x44
     [<ffffffff8117bdb8>] ? rw_verify_area+0x83/0xd9
     [<ffffffff8117be9d>] vfs_read+0x8f/0xcf
     [<ffffffff81193e34>] ? __fdget_pos+0x12/0x41
     [<ffffffff8117c686>] SyS_read+0x4b/0x76
     [<ffffffff817b606e>] system_call_fastpath+0x12/0x71

The bug can be easily triggered, if an extra delay (e.g. 10ms) is added
between the test of DMF_FREEING & DMF_DELETING and dm_get() in
dm_get_from_kobject().

To fix it, we need to ensure the test of DMF_FREEING & DMF_DELETING and
dm_get() are done in an atomic way, so _minor_lock is used.

The other callers of dm_get() have also been checked to be OK: some
callers invoke dm_get() under _minor_lock, some callers invoke it under
_hash_lock, and dm_start_request() invoke it after increasing
md->open_count.

Cc: stable@vger.kernel.org
Signed-off-by: Hou Tao <houtao1@huawei.com>
Signed-off-by: Mike Snitzer <snitzer@redhat.com>",3,drivers/md/dm.c,"{""sha"": ""dcfa1a8c93909cad5bc07c257b0b70af8f3a722e"", ""filename"": ""drivers/md/dm.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 4, ""changes"": 12, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a41d21dceadf8104812626ef85dc56ee8a60ed/drivers/md/dm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a41d21dceadf8104812626ef85dc56ee8a60ed/drivers/md/dm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/md/dm.c?ref=b9a41d21dceadf8104812626ef85dc56ee8a60ed"", ""patch"": ""@@ -2711,11 +2711,15 @@ struct mapped_device *dm_get_from_kobject(struct kobject *kobj)\n \n \tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n \n-\tif (test_bit(DMF_FREEING, &md->flags) ||\n-\t    dm_deleting_md(md))\n-\t\treturn NULL;\n-\n+\tspin_lock(&_minor_lock);\n+\tif (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {\n+\t\tmd = NULL;\n+\t\tgoto out;\n+\t}\n \tdm_get(md);\n+out:\n+\tspin_unlock(&_minor_lock);\n+\n \treturn md;\n }\n ""}","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
 	dm_get(md);
out:
	spin_unlock(&_minor_lock);

 	return md;
 }
","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
 	dm_get(md);
 	return md;
 }
",C,"	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
out:
	spin_unlock(&_minor_lock);

","	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
",,"@@ -2711,11 +2711,15 @@ struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
-	if (test_bit(DMF_FREEING, &md->flags) ||
-	    dm_deleting_md(md))
-		return NULL;
-
+	spin_lock(&_minor_lock);
+	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
+		md = NULL;
+		goto out;
+	}
 	dm_get(md);
+out:
+	spin_unlock(&_minor_lock);
+
 	return md;
 }
 ",linux,b9a41d21dceadf8104812626ef85dc56ee8a60ed,856eb0916d181da6d043cc33e03f54d5c5bbe54a,1,"struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
//flaw_line_below:
	if (test_bit(DMF_FREEING, &md->flags) ||
//flaw_line_below:
	    dm_deleting_md(md))
//flaw_line_below:
		return NULL;
//flaw_line_below:

//fix_flaw_line_below:
//	spin_lock(&_minor_lock);
//fix_flaw_line_below:
//	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
//fix_flaw_line_below:
//		md = NULL;
//fix_flaw_line_below:
//		goto out;
//fix_flaw_line_below:
//	}
 	dm_get(md);
//fix_flaw_line_below:
//out:
//fix_flaw_line_below:
//	spin_unlock(&_minor_lock);
//fix_flaw_line_below:
//
 	return md;
 }
",182583,"struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
 	dm_get(md);
 	return md;
 }
","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
 	dm_get(md);
out:
	spin_unlock(&_minor_lock);

 	return md;
 }
","The function `dm_get_from_kobject` retrieves a pointer to a `mapped_device` structure from a given kernel object (`kobject`). It uses the `container_of` macro to obtain the `mapped_device` instance associated with the provided `kobject`. Before returning the pointer, it checks if the device is in the process of being freed or deleted by evaluating specific flags. If either condition is true, the function returns `NULL` to indicate that the device is no longer valid. If the device is valid, it increments the reference count of the `mapped_device` and returns the pointer to it.","The provided code defines a function that retrieves a specific structure associated with a kernel object. It first determines the structure's address based on the kernel object provided. Before proceeding, the function checks if the structure is marked for freeing or deletion, returning a null pointer if either condition is met, indicating that the structure is no longer valid. If the structure is valid, the function increments its reference count to ensure it remains accessible and then returns a pointer to the structure, allowing further operations to be performed on it safely.","The vulnerability in the original code, identified as CWE-362, arises from a race condition where the state of the `mapped_device` structure could change between the checks for freeing or deletion and the subsequent reference count increment. This could lead to dereferencing an invalid pointer, resulting in potential use-after-free errors. The fixed code addresses this issue by introducing a spin lock around the critical section, ensuring that the checks for the `DMF_FREEING` flag and the deletion status are performed atomically. This prevents concurrent modifications to the `mapped_device` state, thereby enhancing the safety of the reference counting operation.","The vulnerability identified as CWE-362 in the original code stems from a race condition that occurs when checking the state of a structure before incrementing its reference count. This situation allows for the possibility that the structure could be freed or deleted by another thread after the initial check but before the reference count is updated, leading to potential use-after-free errors. The fixed code resolves this issue by introducing a locking mechanism that ensures the checks and the reference count increment are performed atomically, thereby preventing concurrent modifications and ensuring the structure remains valid throughout the operation.","The vulnerability was fixed by introducing a spin lock around the critical section of the code. In the modified version, the spin lock is acquired before checking the `DMF_FREEING` flag and the deletion status of the `mapped_device`. This ensures that the checks and the subsequent call to `dm_get` are performed atomically, preventing any race conditions that could occur if another thread modifies the state of the `mapped_device` during this process. After the checks and reference count increment, the spin lock is released, ensuring thread safety and maintaining the integrity of the structure throughout the operation.","The vulnerability was fixed by implementing a locking mechanism that ensures exclusive access to the critical section of the code. By acquiring a lock before checking the state of the structure, the code prevents other threads from modifying it concurrently. This guarantees that the checks for freeing or deletion and the subsequent reference count increment occur without interruption, thus eliminating the risk of a race condition. Once the necessary operations are completed, the lock is released, ensuring that the structure remains valid and safe for use throughout the entire process."
35,182607,182607,,Local,Not required,Complete,CVE-2017-2616,https://www.cvedetails.com/cve/CVE-2017-2616/,CWE-362,Medium,,,,2018-07-27,4.7,A race condition was found in util-linux before 2.32.1 in the way su handled the management of child processes. A local authenticated attacker could use this flaw to kill other processes with root privileges under specific conditions.,2019-10-09,,10,https://github.com/karelzak/util-linux/commit/dffab154d29a288aa171ff50263ecc8f2e14a891,dffab154d29a288aa171ff50263ecc8f2e14a891,"su: properly clear child PID

Reported-by: Tobias Stöckmann <tobias@stoeckmann.org>
Signed-off-by: Karel Zak <kzak@redhat.com>",4,login-utils/su-common.c,"{""sha"": ""696adc8888ad901efbb98f7878c31e4a21c4cf27"", ""filename"": ""login-utils/su-common.c"", ""status"": ""modified"", ""additions"": 10, ""deletions"": 4, ""changes"": 14, ""blob_url"": ""https://github.com/karelzak/util-linux/blob/dffab154d29a288aa171ff50263ecc8f2e14a891/login-utils/su-common.c"", ""raw_url"": ""https://github.com/karelzak/util-linux/raw/dffab154d29a288aa171ff50263ecc8f2e14a891/login-utils/su-common.c"", ""contents_url"": ""https://api.github.com/repos/karelzak/util-linux/contents/login-utils/su-common.c?ref=dffab154d29a288aa171ff50263ecc8f2e14a891"", ""patch"": ""@@ -368,6 +368,9 @@ create_watching_parent (void)\n             }\n           else\n             status = WEXITSTATUS (status);\n+\n+\t  /* child is gone, don't use the PID anymore */\n+\t  child = (pid_t) -1;\n         }\n       else if (caught_signal)\n         status = caught_signal + 128;\n@@ -377,7 +380,7 @@ create_watching_parent (void)\n   else\n     status = 1;\n \n-  if (caught_signal)\n+  if (caught_signal && child != (pid_t)-1)\n     {\n       fprintf (stderr, _(\""\\nSession terminated, killing shell...\""));\n       kill (child, SIGTERM);\n@@ -387,9 +390,12 @@ create_watching_parent (void)\n \n   if (caught_signal)\n     {\n-      sleep (2);\n-      kill (child, SIGKILL);\n-      fprintf (stderr, _(\"" ...killed.\\n\""));\n+      if (child != (pid_t)-1)\n+\t{\n+\t  sleep (2);\n+\t  kill (child, SIGKILL);\n+\t  fprintf (stderr, _(\"" ...killed.\\n\""));\n+\t}\n \n       /* Let's terminate itself with the received signal.\n        *""}","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);

	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
",C,"
	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
  if (caught_signal && child != (pid_t)-1)
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
","  if (caught_signal)
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
",,"@@ -368,6 +368,9 @@ create_watching_parent (void)
             }
           else
             status = WEXITSTATUS (status);
+
+	  /* child is gone, don't use the PID anymore */
+	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
@@ -377,7 +380,7 @@ create_watching_parent (void)
   else
     status = 1;
 
-  if (caught_signal)
+  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
@@ -387,9 +390,12 @@ create_watching_parent (void)
 
   if (caught_signal)
     {
-      sleep (2);
-      kill (child, SIGKILL);
-      fprintf (stderr, _("" ...killed.\n""));
+      if (child != (pid_t)-1)
+	{
+	  sleep (2);
+	  kill (child, SIGKILL);
+	  fprintf (stderr, _("" ...killed.\n""));
+	}
 
       /* Let's terminate itself with the received signal.
        *",util-linux,dffab154d29a288aa171ff50263ecc8f2e14a891,904ffe1fd2ba6c2092e00b4b1b95fcc30be77b45,1,"create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	  /* child is gone, don't use the PID anymore */
//fix_flaw_line_below:
//	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
//flaw_line_below:
  if (caught_signal)
//fix_flaw_line_below:
//  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
//flaw_line_below:
      sleep (2);
//flaw_line_below:
      kill (child, SIGKILL);
//flaw_line_below:
      fprintf (stderr, _("" ...killed.\n""));
//fix_flaw_line_below:
//      if (child != (pid_t)-1)
//fix_flaw_line_below:
//	{
//fix_flaw_line_below:
//	  sleep (2);
//fix_flaw_line_below:
//	  kill (child, SIGKILL);
//fix_flaw_line_below:
//	  fprintf (stderr, _("" ...killed.\n""));
//fix_flaw_line_below:
//	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
",182607,"create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);

	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","The `create_watching_parent` function manages a child process that runs a shell while the parent process monitors it. Initially, it opens a PAM session and handles any errors. The function then forks a child process; if successful, the child executes the shell, while the parent changes its working directory to the root and sets up signal handlers for various termination signals. The parent continuously waits for the child to change state, handling signals appropriately. If a signal is caught, it terminates the child and cleans up the PAM session before exiting with the appropriate status code, ensuring proper resource management.","The code defines a function that creates a child process to run a shell while the parent process monitors its execution. It begins by establishing a session and handling any errors that arise. After forking, the child process executes the shell, while the parent changes its working directory and sets up signal handlers for termination signals. The parent continuously waits for the child to change state, managing signals as they occur. If a signal is caught, the parent terminates the child and performs cleanup operations before exiting, ensuring proper resource management and session termination.","The vulnerability identified by CWE-362, which relates to race conditions, was present in the original code due to improper handling of the child process's termination and signal management. Specifically, the parent process could access the child process's PID after it had already terminated, leading to potential inconsistencies and undefined behavior. In the fixed code, the child PID is explicitly set to an invalid value after it is no longer needed, preventing any further operations on it. Additionally, the signal handling logic is improved to ensure that signals are managed correctly, reducing the risk of race conditions during process termination.","The vulnerability associated with CWE-362, which pertains to race conditions, arose from the improper management of a child process's termination and signal handling. In the original code, the parent process could still reference the child process's identifier after it had terminated, leading to potential inconsistencies and unpredictable behavior. The fixed code addresses this issue by invalidating the child process identifier once it is no longer needed, ensuring that no further operations are attempted on it. Additionally, the signal handling logic is enhanced to manage signals more effectively, thereby reducing the risk of race conditions during the termination process.","The vulnerability was fixed by explicitly setting the child process identifier to an invalid value after it was no longer needed, specifically by assigning it to `-1` after checking its termination status. This change prevents any further operations on the child PID, mitigating the risk of race conditions. Additionally, the signal handling logic was improved by ensuring that the signal handlers for termination signals are set up correctly, and the cleanup process is more robust. The code now checks if the child PID is valid before attempting to kill it, enhancing overall stability and reliability during process management.","The vulnerability was addressed by ensuring that the identifier for the child process is invalidated after it is no longer needed, preventing any further operations on it that could lead to race conditions. Additionally, the signal handling mechanism was enhanced to ensure that signals are managed more effectively, reducing the likelihood of inconsistent behavior during process termination. The cleanup process was also made more robust, with checks in place to confirm the validity of the child process before any termination attempts, thereby improving the overall stability and reliability of the code during execution."
36,182851,182851,,Remote,Not required,Complete,CVE-2019-11815,https://www.cvedetails.com/cve/CVE-2019-11815/,CWE-362,Medium,Complete,Complete,,2019-05-08,9.3,"An issue was discovered in rds_tcp_kill_sock in net/rds/tcp.c in the Linux kernel before 5.0.8. There is a race condition leading to a use-after-free, related to net namespace cleanup.",2019-06-07,,1,https://github.com/torvalds/linux/commit/cb66ddd156203daefb8d71158036b27b0e2caf63,cb66ddd156203daefb8d71158036b27b0e2caf63,"net: rds: force to destroy connection if t_sock is NULL in rds_tcp_kill_sock().

When it is to cleanup net namespace, rds_tcp_exit_net() will call
rds_tcp_kill_sock(), if t_sock is NULL, it will not call
rds_conn_destroy(), rds_conn_path_destroy() and rds_tcp_conn_free() to free
connection, and the worker cp_conn_w is not stopped, afterwards the net is freed in
net_drop_ns(); While cp_conn_w rds_connect_worker() will call rds_tcp_conn_path_connect()
and reference 'net' which has already been freed.

In rds_tcp_conn_path_connect(), rds_tcp_set_callbacks() will set t_sock = sock before
sock->ops->connect, but if connect() is failed, it will call
rds_tcp_restore_callbacks() and set t_sock = NULL, if connect is always
failed, rds_connect_worker() will try to reconnect all the time, so
rds_tcp_kill_sock() will never to cancel worker cp_conn_w and free the
connections.

Therefore, the condition !tc->t_sock is not needed if it is going to do
cleanup_net->rds_tcp_exit_net->rds_tcp_kill_sock, because tc->t_sock is always
NULL, and there is on other path to cancel cp_conn_w and free
connection. So this patch is to fix this.

rds_tcp_kill_sock():
...
if (net != c_net || !tc->t_sock)
...
Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

==================================================================
BUG: KASAN: use-after-free in inet_create+0xbcc/0xd28
net/ipv4/af_inet.c:340
Read of size 4 at addr ffff8003496a4684 by task kworker/u8:4/3721

CPU: 3 PID: 3721 Comm: kworker/u8:4 Not tainted 5.1.0 #11
Hardware name: linux,dummy-virt (DT)
Workqueue: krdsd rds_connect_worker
Call trace:
 dump_backtrace+0x0/0x3c0 arch/arm64/kernel/time.c:53
 show_stack+0x28/0x38 arch/arm64/kernel/traps.c:152
 __dump_stack lib/dump_stack.c:77 [inline]
 dump_stack+0x120/0x188 lib/dump_stack.c:113
 print_address_description+0x68/0x278 mm/kasan/report.c:253
 kasan_report_error mm/kasan/report.c:351 [inline]
 kasan_report+0x21c/0x348 mm/kasan/report.c:409
 __asan_report_load4_noabort+0x30/0x40 mm/kasan/report.c:429
 inet_create+0xbcc/0xd28 net/ipv4/af_inet.c:340
 __sock_create+0x4f8/0x770 net/socket.c:1276
 sock_create_kern+0x50/0x68 net/socket.c:1322
 rds_tcp_conn_path_connect+0x2b4/0x690 net/rds/tcp_connect.c:114
 rds_connect_worker+0x108/0x1d0 net/rds/threads.c:175
 process_one_work+0x6e8/0x1700 kernel/workqueue.c:2153
 worker_thread+0x3b0/0xdd0 kernel/workqueue.c:2296
 kthread+0x2f0/0x378 kernel/kthread.c:255
 ret_from_fork+0x10/0x18 arch/arm64/kernel/entry.S:1117

Allocated by task 687:
 save_stack mm/kasan/kasan.c:448 [inline]
 set_track mm/kasan/kasan.c:460 [inline]
 kasan_kmalloc+0xd4/0x180 mm/kasan/kasan.c:553
 kasan_slab_alloc+0x14/0x20 mm/kasan/kasan.c:490
 slab_post_alloc_hook mm/slab.h:444 [inline]
 slab_alloc_node mm/slub.c:2705 [inline]
 slab_alloc mm/slub.c:2713 [inline]
 kmem_cache_alloc+0x14c/0x388 mm/slub.c:2718
 kmem_cache_zalloc include/linux/slab.h:697 [inline]
 net_alloc net/core/net_namespace.c:384 [inline]
 copy_net_ns+0xc4/0x2d0 net/core/net_namespace.c:424
 create_new_namespaces+0x300/0x658 kernel/nsproxy.c:107
 unshare_nsproxy_namespaces+0xa0/0x198 kernel/nsproxy.c:206
 ksys_unshare+0x340/0x628 kernel/fork.c:2577
 __do_sys_unshare kernel/fork.c:2645 [inline]
 __se_sys_unshare kernel/fork.c:2643 [inline]
 __arm64_sys_unshare+0x38/0x58 kernel/fork.c:2643
 __invoke_syscall arch/arm64/kernel/syscall.c:35 [inline]
 invoke_syscall arch/arm64/kernel/syscall.c:47 [inline]
 el0_svc_common+0x168/0x390 arch/arm64/kernel/syscall.c:83
 el0_svc_handler+0x60/0xd0 arch/arm64/kernel/syscall.c:129
 el0_svc+0x8/0xc arch/arm64/kernel/entry.S:960

Freed by task 264:
 save_stack mm/kasan/kasan.c:448 [inline]
 set_track mm/kasan/kasan.c:460 [inline]
 __kasan_slab_free+0x114/0x220 mm/kasan/kasan.c:521
 kasan_slab_free+0x10/0x18 mm/kasan/kasan.c:528
 slab_free_hook mm/slub.c:1370 [inline]
 slab_free_freelist_hook mm/slub.c:1397 [inline]
 slab_free mm/slub.c:2952 [inline]
 kmem_cache_free+0xb8/0x3a8 mm/slub.c:2968
 net_free net/core/net_namespace.c:400 [inline]
 net_drop_ns.part.6+0x78/0x90 net/core/net_namespace.c:407
 net_drop_ns net/core/net_namespace.c:406 [inline]
 cleanup_net+0x53c/0x6d8 net/core/net_namespace.c:569
 process_one_work+0x6e8/0x1700 kernel/workqueue.c:2153
 worker_thread+0x3b0/0xdd0 kernel/workqueue.c:2296
 kthread+0x2f0/0x378 kernel/kthread.c:255
 ret_from_fork+0x10/0x18 arch/arm64/kernel/entry.S:1117

The buggy address belongs to the object at ffff8003496a3f80
 which belongs to the cache net_namespace of size 7872
The buggy address is located 1796 bytes inside of
 7872-byte region [ffff8003496a3f80, ffff8003496a5e40)
The buggy address belongs to the page:
page:ffff7e000d25a800 count:1 mapcount:0 mapping:ffff80036ce4b000
index:0x0 compound_mapcount: 0
flags: 0xffffe0000008100(slab|head)
raw: 0ffffe0000008100 dead000000000100 dead000000000200 ffff80036ce4b000
raw: 0000000000000000 0000000080040004 00000001ffffffff 0000000000000000
page dumped because: kasan: bad access detected

Memory state around the buggy address:
 ffff8003496a4580: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
 ffff8003496a4600: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
>ffff8003496a4680: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
                   ^
 ffff8003496a4700: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
 ffff8003496a4780: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
==================================================================

Fixes: 467fa15356ac(""RDS-TCP: Support multiple RDS-TCP listen endpoints, one per netns."")
Reported-by: Hulk Robot <hulkci@huawei.com>
Signed-off-by: Mao Wenan <maowenan@huawei.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/rds/tcp.c,"{""sha"": ""faf726e00e27c75b11721dbc55518ca60bdf00a6"", ""filename"": ""net/rds/tcp.c"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 1, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/cb66ddd156203daefb8d71158036b27b0e2caf63/net/rds/tcp.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cb66ddd156203daefb8d71158036b27b0e2caf63/net/rds/tcp.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/tcp.c?ref=cb66ddd156203daefb8d71158036b27b0e2caf63"", ""patch"": ""@@ -608,7 +608,7 @@ static void rds_tcp_kill_sock(struct net *net)\n \tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n \t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n \n-\t\tif (net != c_net || !tc->t_sock)\n+\t\tif (net != c_net)\n \t\t\tcontinue;\n \t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n \t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);""}","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net || !tc->t_sock)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
",C,"		if (net != c_net)
","		if (net != c_net || !tc->t_sock)
",,"@@ -608,7 +608,7 @@ static void rds_tcp_kill_sock(struct net *net)
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
-		if (net != c_net || !tc->t_sock)
+		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);",linux,cb66ddd156203daefb8d71158036b27b0e2caf63,f28cd2af22a0c134e4aa1c64a70f70d815d473fb,1,"static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
//flaw_line_below:
		if (net != c_net || !tc->t_sock)
//fix_flaw_line_below:
//		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
",182851,"static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net || !tc->t_sock)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","The provided code defines a function `rds_tcp_kill_sock` that is responsible for terminating TCP connections associated with a specific network namespace. It first retrieves the listening socket and stops it, ensuring no new connections can be accepted. The function then acquires a lock to safely iterate through a list of active TCP connections, checking if each connection belongs to the specified network. If a connection is found, it is either moved to a temporary list for destruction or marked as detached if it already exists in the temporary list. Finally, the function cleans up by destroying the connections in the temporary list.","The code defines a function that manages the termination of TCP connections within a specific network context. It begins by stopping the listening socket to prevent new connections. The function then locks access to a list of active connections and iterates through them, checking if they belong to the specified network. For each relevant connection, it either moves it to a temporary list for later destruction or marks it as detached if it is already present. Finally, the function cleans up by destroying all connections stored in the temporary list, ensuring proper resource management.","The vulnerability identified by CWE-362, which pertains to a race condition, arises from the improper handling of shared resources in a multi-threaded environment. In the vulnerable code, the function iterates through a list of TCP connections while holding a lock, but it does not adequately manage the state of connections, potentially allowing concurrent modifications. The fixed code addresses this by ensuring that connections are safely moved to a temporary list or marked as detached only after confirming their ownership of the specified network. This change prevents inconsistencies and ensures that the connection list is not altered unexpectedly during iteration, thus mitigating the race condition.","The vulnerability identified by CWE-362, which involves a race condition, occurs when multiple threads access shared resources without proper synchronization, leading to unpredictable behavior. In the vulnerable code, the iteration over a list of connections while holding a lock did not adequately prevent concurrent modifications, risking inconsistencies in the connection state. The fixed code resolves this issue by ensuring that connections are either safely moved to a temporary list or marked as detached only after confirming their ownership of the relevant context. This careful management of connection states during iteration prevents unexpected alterations and enhances overall stability.","The vulnerability was fixed by enhancing the management of connections during iteration to prevent race conditions. In the updated code, the function first acquires a lock before iterating through the list of connections using `list_for_each_entry_safe`, which allows safe removal of entries while iterating. It checks if each connection belongs to the specified network and either moves it to a temporary list using `list_move_tail` or marks it as detached with `tc->t_tcp_node_detached`. This ensures that the connection list remains consistent and prevents concurrent modifications, thereby mitigating the race condition.","The vulnerability was fixed by implementing a more robust approach to managing connections during iteration, thereby preventing race conditions. The updated code ensures that a lock is acquired before iterating through the list of connections, allowing safe modifications while traversing the list. It introduces checks to confirm the ownership of each connection, allowing them to be either moved to a temporary storage for later cleanup or marked as detached if they are already present. This careful handling of connection states during iteration maintains consistency and prevents unexpected alterations, effectively mitigating the race condition."
37,182856,182856,,Local,Not required,Complete,CVE-2019-11599,https://www.cvedetails.com/cve/CVE-2019-11599/,CWE-362,Medium,Complete,Complete,,2019-04-29,6.9,"The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",2019-05-28,DoS +Info ,3,https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a,04f5866e41fb70690e28397487d8bd8eea7d712a,"coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping

The core dumping code has always run without holding the mmap_sem for
writing, despite that is the only way to ensure that the entire vma
layout will not change from under it.  Only using some signal
serialization on the processes belonging to the mm is not nearly enough.
This was pointed out earlier.  For example in Hugh's post from Jul 2017:

  https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils

  ""Not strictly relevant here, but a related note: I was very surprised
   to discover, only quite recently, how handle_mm_fault() may be called
   without down_read(mmap_sem) - when core dumping. That seems a
   misguided optimization to me, which would also be nice to correct""

In particular because the growsdown and growsup can move the
vm_start/vm_end the various loops the core dump does around the vma will
not be consistent if page faults can happen concurrently.

Pretty much all users calling mmget_not_zero()/get_task_mm() and then
taking the mmap_sem had the potential to introduce unexpected side
effects in the core dumping code.

Adding mmap_sem for writing around the ->core_dump invocation is a
viable long term fix, but it requires removing all copy user and page
faults and to replace them with get_dump_page() for all binary formats
which is not suitable as a short term fix.

For the time being this solution manually covers the places that can
confuse the core dump either by altering the vma layout or the vma flags
while it runs.  Once ->core_dump runs under mmap_sem for writing the
function mmget_still_valid() can be dropped.

Allowing mmap_sem protected sections to run in parallel with the
coredump provides some minor parallelism advantage to the swapoff code
(which seems to be safe enough by never mangling any vma field and can
keep doing swapins in parallel to the core dumping) and to some other
corner case.

In order to facilitate the backporting I added ""Fixes: 86039bd3b4e6""
however the side effect of this same race condition in /proc/pid/mem
should be reproducible since before 2.6.12-rc2 so I couldn't add any
other ""Fixes:"" because there's no hash beyond the git genesis commit.

Because find_extend_vma() is the only location outside of the process
context that could modify the ""mm"" structures under mmap_sem for
reading, by adding the mmget_still_valid() check to it, all other cases
that take the mmap_sem for reading don't need the new check after
mmget_not_zero()/get_task_mm().  The expand_stack() in page fault
context also doesn't need the new check, because all tasks under core
dumping are frozen.

Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com
Fixes: 86039bd3b4e6 (""userfaultfd: add new syscall to provide memory externalization"")
Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Reported-by: Jann Horn <jannh@google.com>
Suggested-by: Oleg Nesterov <oleg@redhat.com>
Acked-by: Peter Xu <peterx@redhat.com>
Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
Reviewed-by: Oleg Nesterov <oleg@redhat.com>
Reviewed-by: Jann Horn <jannh@google.com>
Acked-by: Jason Gunthorpe <jgg@mellanox.com>
Acked-by: Michal Hocko <mhocko@suse.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,drivers/infiniband/core/uverbs_main.c,"{""sha"": ""f2e7ffe6fc546612f62da9cde853b9c1bf37d8bb"", ""filename"": ""drivers/infiniband/core/uverbs_main.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 0, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/infiniband/core/uverbs_main.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t * will only be one mm, so no big deal.\n \t\t */\n \t\tdown_write(&mm->mmap_sem);\n+\t\tif (!mmget_still_valid(mm))\n+\t\t\tgoto skip_mm;\n \t\tmutex_lock(&ufile->umap_lock);\n \t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n \t\t\t\t\t  list) {\n@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n \t\t}\n \t\tmutex_unlock(&ufile->umap_lock);\n+\tskip_mm:\n \t\tup_write(&mm->mmap_sem);\n \t\tmmput(mm);\n \t}""}<_**next**_>{""sha"": ""95ca1fe7283cff265247c6f3a84e5fa573299fca"", ""filename"": ""fs/proc/task_mmu.c"", ""status"": ""modified"", ""additions"": 18, ""deletions"": 0, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/task_mmu.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,\n \t\t\t\t\tcount = -EINTR;\n \t\t\t\t\tgoto out_mm;\n \t\t\t\t}\n+\t\t\t\t/*\n+\t\t\t\t * Avoid to modify vma->vm_flags\n+\t\t\t\t * without locked ops while the\n+\t\t\t\t * coredump reads the vm_flags.\n+\t\t\t\t */\n+\t\t\t\tif (!mmget_still_valid(mm)) {\n+\t\t\t\t\t/*\n+\t\t\t\t\t * Silently return \""count\""\n+\t\t\t\t\t * like if get_task_mm()\n+\t\t\t\t\t * failed. FIXME: should this\n+\t\t\t\t\t * function have returned\n+\t\t\t\t\t * -ESRCH if get_task_mm()\n+\t\t\t\t\t * failed like if\n+\t\t\t\t\t * get_proc_task() fails?\n+\t\t\t\t\t */\n+\t\t\t\t\tup_write(&mm->mmap_sem);\n+\t\t\t\t\tgoto out_mm;\n+\t\t\t\t}\n \t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n \t\t\t\t\tvma_set_page_prot(vma);""}<_**next**_>{""sha"": ""f5de1e726356a51c27ff529f98d99032650eb839"", ""filename"": ""fs/userfaultfd.c"", ""status"": ""modified"", ""additions"": 9, ""deletions"": 0, ""changes"": 9, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/userfaultfd.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -629,6 +629,8 @@ static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n \n \t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n \t\tdown_write(&mm->mmap_sem);\n+\t\t/* no task can run (and in turn coredump) yet */\n+\t\tVM_WARN_ON(!mmget_still_valid(mm));\n \t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n \t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n \t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n@@ -883,6 +885,8 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t * taking the mmap_sem for writing.\n \t */\n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto skip_mm;\n \tprev = NULL;\n \tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\tcond_resched();\n@@ -905,6 +909,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t\tvma->vm_flags = new_flags;\n \t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n \t}\n+skip_mm:\n \tup_write(&mm->mmap_sem);\n \tmmput(mm);\n wakeup:\n@@ -1333,6 +1338,8 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;\n@@ -1520,6 +1527,8 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;""}<_**next**_>{""sha"": ""a3fda9f024c3c1988b6ff60954d7f7e74a9c1ecf"", ""filename"": ""include/linux/sched/mm.h"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/sched/mm.h?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -49,6 +49,27 @@ static inline void mmdrop(struct mm_struct *mm)\n \t\t__mmdrop(mm);\n }\n \n+/*\n+ * This has to be called after a get_task_mm()/mmget_not_zero()\n+ * followed by taking the mmap_sem for writing before modifying the\n+ * vmas or anything the coredump pretends not to change from under it.\n+ *\n+ * NOTE: find_extend_vma() called from GUP context is the only place\n+ * that can modify the \""mm\"" (notably the vm_start/end) under mmap_sem\n+ * for reading and outside the context of the process, so it is also\n+ * the only case that holds the mmap_sem for reading that must call\n+ * this function. Generally if the mmap_sem is hold for reading\n+ * there's no need of this check after get_task_mm()/mmget_not_zero().\n+ *\n+ * This function can be obsoleted and the check can be removed, after\n+ * the coredump code will hold the mmap_sem for writing before\n+ * invoking the ->core_dump methods.\n+ */\n+static inline bool mmget_still_valid(struct mm_struct *mm)\n+{\n+\treturn likely(!mm->core_state);\n+}\n+\n /**\n  * mmget() - Pin the address space associated with a &struct mm_struct.\n  * @mm: The address space to pin.""}<_**next**_>{""sha"": ""bd7b9f293b391f22b85810e48bc7c0679b217f05"", ""filename"": ""mm/mmap.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 1, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/mmap.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -45,6 +45,7 @@\n #include <linux/moduleparam.h>\n #include <linux/pkeys.h>\n #include <linux/oom.h>\n+#include <linux/sched/mm.h>\n \n #include <linux/uaccess.h>\n #include <asm/cacheflush.h>\n@@ -2525,7 +2526,8 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \tvma = find_vma_prev(mm, addr, &prev);\n \tif (vma && (vma->vm_start <= addr))\n \t\treturn vma;\n-\tif (!prev || expand_stack(prev, addr))\n+\t/* don't alter vm_end if the coredump is running */\n+\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n \t\treturn NULL;\n \tif (prev->vm_flags & VM_LOCKED)\n \t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n@@ -2551,6 +2553,9 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \t\treturn vma;\n \tif (!(vma->vm_flags & VM_GROWSDOWN))\n \t\treturn NULL;\n+\t/* don't alter vm_start if the coredump is running */\n+\tif (!mmget_still_valid(mm))\n+\t\treturn NULL;\n \tstart = vma->vm_start;\n \tif (expand_stack(vma, addr))\n \t\treturn NULL;""}","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
		if (!mmget_still_valid(mm))
			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
",C,"		if (!mmget_still_valid(mm))
			goto skip_mm;
	skip_mm:
",,,"@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
+		if (!mmget_still_valid(mm))
+			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
+	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}",linux,04f5866e41fb70690e28397487d8bd8eea7d712a,dce5b0bdeec61bdbee56121ceb1d014151d5cab1,1,"void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
//fix_flaw_line_below:
//		if (!mmget_still_valid(mm))
//fix_flaw_line_below:
//			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
//fix_flaw_line_below:
//	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
",182856,"void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
		if (!mmget_still_valid(mm))
			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","The function `uverbs_user_mmap_disassociate` is designed to disassociate user memory mappings from an RDMA (Remote Direct Memory Access) file structure in a concurrent environment. It first acquires a lock to ensure thread safety while iterating through a list of user mappings associated with the file. For each mapping, it checks if the corresponding memory area (mm_struct) is still valid; if not, it removes the mapping from the list. Once a valid memory area is found, it acquires a write lock on the memory's semaphore, cleans up the associated virtual memory areas (VMAs) by removing page table entries, and updates the VMA flags to prevent sharing. Finally, it releases the locks and decrements the reference count on the memory area.","The provided code implements a function that disassociates user memory mappings from a specific file structure in a multi-threaded environment. It ensures thread safety by acquiring necessary locks while iterating through a list of memory mappings. The function checks the validity of each memory area and removes any invalid mappings. Upon finding a valid memory area, it acquires a write lock to safely clean up associated virtual memory areas by removing page table entries and updating flags to prevent sharing. Finally, it releases the locks and decrements the reference count on the memory area, ensuring proper resource management.","The vulnerability identified as CWE-362, which pertains to race conditions, arises from the improper handling of memory area references in the original code. Specifically, the function did not adequately verify the validity of the memory area after acquiring the write lock on `mm->mmap_sem`, potentially allowing concurrent modifications to the memory area while it was being processed. The fix introduced a check using `mmget_still_valid(mm)` before proceeding with the cleanup, ensuring that the memory area remains valid throughout the operation. This change prevents race conditions by ensuring that the memory area is not released or modified by another thread during critical operations.","The vulnerability identified as CWE-362, related to race conditions, occurs when a function fails to properly manage concurrent access to shared resources, leading to potential inconsistencies. In the original code, after acquiring a lock on a memory area, there was no verification to ensure that the memory remained valid before proceeding with cleanup operations. This oversight allowed other threads to modify or release the memory area, resulting in unpredictable behavior. The fix introduced an additional validity check before performing critical operations, ensuring that the memory area was still valid and preventing race conditions during concurrent access.","The vulnerability was fixed by introducing a validity check for the memory area after acquiring the write lock on `mm->mmap_sem`. The original code lacked a mechanism to ensure that the memory area remained valid during critical operations, which could lead to race conditions. The fix added a call to `mmget_still_valid(mm)` immediately after acquiring the lock, allowing the function to verify that the memory area had not been released or modified by another thread. If the memory area was no longer valid, the function would skip the cleanup process, thus preventing potential inconsistencies and ensuring safe concurrent access.","The vulnerability was addressed by adding a critical validity check for the memory area after acquiring the necessary lock. This change ensures that the memory area remains valid before proceeding with any cleanup operations, thereby preventing potential race conditions. By verifying the memory's validity at this stage, the code can safely skip the cleanup process if the memory has been released or modified by another thread. This enhancement effectively mitigates the risk of inconsistencies and ensures that concurrent access to shared resources is managed correctly, maintaining the integrity of the operations performed."
38,182857,182857,,Local,Not required,Complete,CVE-2019-11599,https://www.cvedetails.com/cve/CVE-2019-11599/,CWE-362,Medium,Complete,Complete,,2019-04-29,6.9,"The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",2019-05-28,DoS +Info ,18,https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a,04f5866e41fb70690e28397487d8bd8eea7d712a,"coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping

The core dumping code has always run without holding the mmap_sem for
writing, despite that is the only way to ensure that the entire vma
layout will not change from under it.  Only using some signal
serialization on the processes belonging to the mm is not nearly enough.
This was pointed out earlier.  For example in Hugh's post from Jul 2017:

  https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils

  ""Not strictly relevant here, but a related note: I was very surprised
   to discover, only quite recently, how handle_mm_fault() may be called
   without down_read(mmap_sem) - when core dumping. That seems a
   misguided optimization to me, which would also be nice to correct""

In particular because the growsdown and growsup can move the
vm_start/vm_end the various loops the core dump does around the vma will
not be consistent if page faults can happen concurrently.

Pretty much all users calling mmget_not_zero()/get_task_mm() and then
taking the mmap_sem had the potential to introduce unexpected side
effects in the core dumping code.

Adding mmap_sem for writing around the ->core_dump invocation is a
viable long term fix, but it requires removing all copy user and page
faults and to replace them with get_dump_page() for all binary formats
which is not suitable as a short term fix.

For the time being this solution manually covers the places that can
confuse the core dump either by altering the vma layout or the vma flags
while it runs.  Once ->core_dump runs under mmap_sem for writing the
function mmget_still_valid() can be dropped.

Allowing mmap_sem protected sections to run in parallel with the
coredump provides some minor parallelism advantage to the swapoff code
(which seems to be safe enough by never mangling any vma field and can
keep doing swapins in parallel to the core dumping) and to some other
corner case.

In order to facilitate the backporting I added ""Fixes: 86039bd3b4e6""
however the side effect of this same race condition in /proc/pid/mem
should be reproducible since before 2.6.12-rc2 so I couldn't add any
other ""Fixes:"" because there's no hash beyond the git genesis commit.

Because find_extend_vma() is the only location outside of the process
context that could modify the ""mm"" structures under mmap_sem for
reading, by adding the mmget_still_valid() check to it, all other cases
that take the mmap_sem for reading don't need the new check after
mmget_not_zero()/get_task_mm().  The expand_stack() in page fault
context also doesn't need the new check, because all tasks under core
dumping are frozen.

Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com
Fixes: 86039bd3b4e6 (""userfaultfd: add new syscall to provide memory externalization"")
Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Reported-by: Jann Horn <jannh@google.com>
Suggested-by: Oleg Nesterov <oleg@redhat.com>
Acked-by: Peter Xu <peterx@redhat.com>
Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
Reviewed-by: Oleg Nesterov <oleg@redhat.com>
Reviewed-by: Jann Horn <jannh@google.com>
Acked-by: Jason Gunthorpe <jgg@mellanox.com>
Acked-by: Michal Hocko <mhocko@suse.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,fs/proc/task_mmu.c,"{""sha"": ""f2e7ffe6fc546612f62da9cde853b9c1bf37d8bb"", ""filename"": ""drivers/infiniband/core/uverbs_main.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 0, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/infiniband/core/uverbs_main.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t * will only be one mm, so no big deal.\n \t\t */\n \t\tdown_write(&mm->mmap_sem);\n+\t\tif (!mmget_still_valid(mm))\n+\t\t\tgoto skip_mm;\n \t\tmutex_lock(&ufile->umap_lock);\n \t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n \t\t\t\t\t  list) {\n@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n \t\t}\n \t\tmutex_unlock(&ufile->umap_lock);\n+\tskip_mm:\n \t\tup_write(&mm->mmap_sem);\n \t\tmmput(mm);\n \t}""}<_**next**_>{""sha"": ""95ca1fe7283cff265247c6f3a84e5fa573299fca"", ""filename"": ""fs/proc/task_mmu.c"", ""status"": ""modified"", ""additions"": 18, ""deletions"": 0, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/task_mmu.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,\n \t\t\t\t\tcount = -EINTR;\n \t\t\t\t\tgoto out_mm;\n \t\t\t\t}\n+\t\t\t\t/*\n+\t\t\t\t * Avoid to modify vma->vm_flags\n+\t\t\t\t * without locked ops while the\n+\t\t\t\t * coredump reads the vm_flags.\n+\t\t\t\t */\n+\t\t\t\tif (!mmget_still_valid(mm)) {\n+\t\t\t\t\t/*\n+\t\t\t\t\t * Silently return \""count\""\n+\t\t\t\t\t * like if get_task_mm()\n+\t\t\t\t\t * failed. FIXME: should this\n+\t\t\t\t\t * function have returned\n+\t\t\t\t\t * -ESRCH if get_task_mm()\n+\t\t\t\t\t * failed like if\n+\t\t\t\t\t * get_proc_task() fails?\n+\t\t\t\t\t */\n+\t\t\t\t\tup_write(&mm->mmap_sem);\n+\t\t\t\t\tgoto out_mm;\n+\t\t\t\t}\n \t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n \t\t\t\t\tvma_set_page_prot(vma);""}<_**next**_>{""sha"": ""f5de1e726356a51c27ff529f98d99032650eb839"", ""filename"": ""fs/userfaultfd.c"", ""status"": ""modified"", ""additions"": 9, ""deletions"": 0, ""changes"": 9, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/userfaultfd.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -629,6 +629,8 @@ static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n \n \t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n \t\tdown_write(&mm->mmap_sem);\n+\t\t/* no task can run (and in turn coredump) yet */\n+\t\tVM_WARN_ON(!mmget_still_valid(mm));\n \t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n \t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n \t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n@@ -883,6 +885,8 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t * taking the mmap_sem for writing.\n \t */\n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto skip_mm;\n \tprev = NULL;\n \tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\tcond_resched();\n@@ -905,6 +909,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t\tvma->vm_flags = new_flags;\n \t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n \t}\n+skip_mm:\n \tup_write(&mm->mmap_sem);\n \tmmput(mm);\n wakeup:\n@@ -1333,6 +1338,8 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;\n@@ -1520,6 +1527,8 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;""}<_**next**_>{""sha"": ""a3fda9f024c3c1988b6ff60954d7f7e74a9c1ecf"", ""filename"": ""include/linux/sched/mm.h"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/sched/mm.h?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -49,6 +49,27 @@ static inline void mmdrop(struct mm_struct *mm)\n \t\t__mmdrop(mm);\n }\n \n+/*\n+ * This has to be called after a get_task_mm()/mmget_not_zero()\n+ * followed by taking the mmap_sem for writing before modifying the\n+ * vmas or anything the coredump pretends not to change from under it.\n+ *\n+ * NOTE: find_extend_vma() called from GUP context is the only place\n+ * that can modify the \""mm\"" (notably the vm_start/end) under mmap_sem\n+ * for reading and outside the context of the process, so it is also\n+ * the only case that holds the mmap_sem for reading that must call\n+ * this function. Generally if the mmap_sem is hold for reading\n+ * there's no need of this check after get_task_mm()/mmget_not_zero().\n+ *\n+ * This function can be obsoleted and the check can be removed, after\n+ * the coredump code will hold the mmap_sem for writing before\n+ * invoking the ->core_dump methods.\n+ */\n+static inline bool mmget_still_valid(struct mm_struct *mm)\n+{\n+\treturn likely(!mm->core_state);\n+}\n+\n /**\n  * mmget() - Pin the address space associated with a &struct mm_struct.\n  * @mm: The address space to pin.""}<_**next**_>{""sha"": ""bd7b9f293b391f22b85810e48bc7c0679b217f05"", ""filename"": ""mm/mmap.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 1, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/mmap.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -45,6 +45,7 @@\n #include <linux/moduleparam.h>\n #include <linux/pkeys.h>\n #include <linux/oom.h>\n+#include <linux/sched/mm.h>\n \n #include <linux/uaccess.h>\n #include <asm/cacheflush.h>\n@@ -2525,7 +2526,8 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \tvma = find_vma_prev(mm, addr, &prev);\n \tif (vma && (vma->vm_start <= addr))\n \t\treturn vma;\n-\tif (!prev || expand_stack(prev, addr))\n+\t/* don't alter vm_end if the coredump is running */\n+\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n \t\treturn NULL;\n \tif (prev->vm_flags & VM_LOCKED)\n \t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n@@ -2551,6 +2553,9 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \t\treturn vma;\n \tif (!(vma->vm_flags & VM_GROWSDOWN))\n \t\treturn NULL;\n+\t/* don't alter vm_start if the coredump is running */\n+\tif (!mmget_still_valid(mm))\n+\t\treturn NULL;\n \tstart = vma->vm_start;\n \tif (expand_stack(vma, addr))\n \t\treturn NULL;""}","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
",C,"				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
",,,"@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 					count = -EINTR;
 					goto out_mm;
 				}
+				/*
+				 * Avoid to modify vma->vm_flags
+				 * without locked ops while the
+				 * coredump reads the vm_flags.
+				 */
+				if (!mmget_still_valid(mm)) {
+					/*
+					 * Silently return ""count""
+					 * like if get_task_mm()
+					 * failed. FIXME: should this
+					 * function have returned
+					 * -ESRCH if get_task_mm()
+					 * failed like if
+					 * get_proc_task() fails?
+					 */
+					up_write(&mm->mmap_sem);
+					goto out_mm;
+				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);",linux,04f5866e41fb70690e28397487d8bd8eea7d712a,dce5b0bdeec61bdbee56121ceb1d014151d5cab1,1,"static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
//fix_flaw_line_below:
//				/*
//fix_flaw_line_below:
//				 * Avoid to modify vma->vm_flags
//fix_flaw_line_below:
//				 * without locked ops while the
//fix_flaw_line_below:
//				 * coredump reads the vm_flags.
//fix_flaw_line_below:
//				 */
//fix_flaw_line_below:
//				if (!mmget_still_valid(mm)) {
//fix_flaw_line_below:
//					/*
//fix_flaw_line_below:
//					 * Silently return ""count""
//fix_flaw_line_below:
//					 * like if get_task_mm()
//fix_flaw_line_below:
//					 * failed. FIXME: should this
//fix_flaw_line_below:
//					 * function have returned
//fix_flaw_line_below:
//					 * -ESRCH if get_task_mm()
//fix_flaw_line_below:
//					 * failed like if
//fix_flaw_line_below:
//					 * get_proc_task() fails?
//fix_flaw_line_below:
//					 */
//fix_flaw_line_below:
//					up_write(&mm->mmap_sem);
//fix_flaw_line_below:
//					goto out_mm;
//fix_flaw_line_below:
//				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
",182857,"static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","The `clear_refs_write` function is designed to handle write operations to a specific file in the `/proc` filesystem, allowing users to clear reference types in a process's memory management. It begins by reading user input to determine the type of reference to clear, validating the input against predefined types. If the type is valid, it retrieves the associated task and its memory management structure. Depending on the reference type, it may reset the peak resident set size or clear soft dirty flags from virtual memory areas. The function utilizes synchronization mechanisms to ensure safe access to memory structures and performs necessary cleanup before returning the number of bytes processed or an error code.","The provided code implements a function that processes write requests to a specific file in the system's process directory, enabling the clearing of memory reference types for a specified process. It begins by reading and validating user input to determine the reference type. Upon confirming the validity, it retrieves the target process's memory management structure and performs operations based on the reference type, such as resetting memory usage metrics or clearing specific memory flags. The function ensures safe access to memory structures through synchronization mechanisms and concludes by returning the number of bytes processed or an error code if applicable.","The vulnerability identified by CWE-362, which pertains to a race condition, arises from the potential for concurrent access to shared resources without proper synchronization. In the vulnerable code, the function modifies memory flags of virtual memory areas while holding a read lock on the memory management structure, which could lead to inconsistencies if another thread simultaneously accesses these flags. The fix introduces a check using `mmget_still_valid(mm)` to ensure that the memory management structure remains valid before modifying the flags, thereby preventing unsafe operations and ensuring that the memory state is consistent during concurrent accesses.","The vulnerability identified by CWE-362 involves a race condition that occurs when multiple threads access shared resources without adequate synchronization, leading to potential inconsistencies. In the original code, modifications to memory flags could happen while another thread was reading those flags, risking data corruption or unexpected behavior. The fix addresses this issue by introducing a validation check to ensure that the memory management structure remains valid before any modifications are made. This additional safeguard prevents unsafe operations during concurrent access, thereby maintaining the integrity of the memory state and ensuring safe interactions among threads.","The vulnerability was fixed by adding a validation check using `mmget_still_valid(mm)` before modifying the memory flags of virtual memory areas. This ensures that the memory management structure remains valid and has not been released by another thread while the current thread holds a read lock. If the validation fails, the code safely releases the write lock with `up_write(&mm->mmap_sem)` and exits the modification section, preventing any unsafe operations. This change effectively mitigates the race condition by ensuring that modifications to memory flags occur only when it is safe to do so, thus preserving data integrity.","The vulnerability was fixed by introducing a validation step to ensure that the memory management structure remains valid before any modifications are made to the memory flags. This check prevents the code from proceeding with changes if the structure has been released by another thread, thereby avoiding potential data corruption. Additionally, the fix includes proper handling of locks to ensure that any write operations are only performed when it is safe to do so. This approach effectively mitigates the race condition, ensuring that concurrent access to shared resources does not lead to inconsistent or unexpected behavior."
39,182913,182913,,Remote,Not required,Partial,CVE-2019-6974,https://www.cvedetails.com/cve/CVE-2019-6974/,CWE-362,Medium,Partial,Partial,,2019-02-15,6.8,"In the Linux kernel before 4.20.8, kvm_ioctl_create_device in virt/kvm/kvm_main.c mishandles reference counting because of a race condition, leading to a use-after-free.",2019-09-20,,2,https://github.com/torvalds/linux/commit/cfa39381173d5f969daf43582c95ad679189cbc9,cfa39381173d5f969daf43582c95ad679189cbc9,"kvm: fix kvm_ioctl_create_device() reference counting (CVE-2019-6974)

kvm_ioctl_create_device() does the following:

1. creates a device that holds a reference to the VM object (with a borrowed
   reference, the VM's refcount has not been bumped yet)
2. initializes the device
3. transfers the reference to the device to the caller's file descriptor table
4. calls kvm_get_kvm() to turn the borrowed reference to the VM into a real
   reference

The ownership transfer in step 3 must not happen before the reference to the VM
becomes a proper, non-borrowed reference, which only happens in step 4.
After step 3, an attacker can close the file descriptor and drop the borrowed
reference, which can cause the refcount of the kvm object to drop to zero.

This means that we need to grab a reference for the device before
anon_inode_getfd(), otherwise the VM can disappear from under us.

Fixes: 852b6d57dc7f (""kvm: add device control API"")
Cc: stable@kernel.org
Signed-off-by: Jann Horn <jannh@google.com>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>",1,virt/kvm/kvm_main.c,"{""sha"": ""585845203db89e5d2bc90722fd75bfcf595671d9"", ""filename"": ""virt/kvm/kvm_main.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 1, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/cfa39381173d5f969daf43582c95ad679189cbc9/virt/kvm/kvm_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cfa39381173d5f969daf43582c95ad679189cbc9/virt/kvm/kvm_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/virt/kvm/kvm_main.c?ref=cfa39381173d5f969daf43582c95ad679189cbc9"", ""patch"": ""@@ -3000,16 +3000,17 @@ static int kvm_ioctl_create_device(struct kvm *kvm,\n \tif (ops->init)\n \t\tops->init(dev);\n \n+\tkvm_get_kvm(kvm);\n \tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n \tif (ret < 0) {\n+\t\tkvm_put_kvm(kvm);\n \t\tmutex_lock(&kvm->lock);\n \t\tlist_del(&dev->vm_node);\n \t\tmutex_unlock(&kvm->lock);\n \t\tops->destroy(dev);\n \t\treturn ret;\n \t}\n \n-\tkvm_get_kvm(kvm);\n \tcd->fd = ret;\n \treturn 0;\n }""}","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
 	cd->fd = ret;
 	return 0;
 }
","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
",C,"	kvm_get_kvm(kvm);
		kvm_put_kvm(kvm);
","	kvm_get_kvm(kvm);
",,"@@ -3000,16 +3000,17 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 	if (ops->init)
 		ops->init(dev);
 
+	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
+		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
-	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }",linux,cfa39381173d5f969daf43582c95ad679189cbc9,8834f5600cf3c8db365e18a3d5cac2c2780c81e5,1,"static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
//fix_flaw_line_below:
//	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
//fix_flaw_line_below:
//		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
//flaw_line_below:
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
",182913,"static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
 	cd->fd = ret;
 	return 0;
 }
","The `kvm_ioctl_create_device` function is responsible for creating a virtual device within a Kernel-based Virtual Machine (KVM) environment. It first checks if the requested device type is valid and retrieves the corresponding device operations. If the function is called in test mode, it returns immediately without creating a device. Otherwise, it allocates memory for the device structure and locks the KVM instance to safely create the device using the specified operations. If successful, it initializes the device, adds it to the KVM's device list, and retrieves a file descriptor for the device, returning it to the caller.","The provided code defines a function that facilitates the creation of a virtual device within a virtual machine environment. It begins by validating the requested device type and checking if the operation is in test mode, in which case it exits early. If not in test mode, it allocates memory for the device and locks the virtual machine instance to ensure thread safety. The function then attempts to create the device using specified operations, initializes it if successful, and adds it to the device list. Finally, it retrieves a file descriptor for the device and returns it, signaling successful creation.","The vulnerability identified by CWE-362, which relates to a race condition, arises from the improper handling of resource management and locking in the original code. Specifically, if the device creation operation fails after the device has been added to the device list, the code does not properly manage the reference count for the virtual machine, potentially leading to a use-after-free scenario. The fixed code addresses this by ensuring that the reference count is decremented with `kvm_put_kvm(kvm)` before the device is removed from the list and destroyed, thus preventing access to a potentially invalid memory reference and ensuring safer resource management.","The vulnerability identified by CWE-362, which pertains to race conditions, occurs when resource management is not properly synchronized, leading to potential inconsistencies in the state of shared resources. In the original code, if the creation of a device fails after it has been added to a list, the reference count for the associated virtual machine is not decremented, risking access to invalid memory. The fixed code resolves this issue by ensuring that the reference count is appropriately managed before removing the device from the list, thereby preventing the possibility of accessing freed memory and enhancing overall stability and safety.","The vulnerability was fixed by adding a call to decrement the reference count for the virtual machine before the device is removed from the device list. In the original code, if the device creation failed after the device was added, the reference count was not adjusted, leading to potential use-after-free issues. The fixed code introduces `kvm_put_kvm(kvm)` right before the device is removed with `list_del(&dev->vm_node)`, ensuring that the reference count is properly managed. This change prevents access to invalid memory and ensures that resources are safely released, enhancing the overall stability of the system.","The vulnerability was addressed by ensuring proper management of resource references in the event of a device creation failure. The fix involved adding a step to decrement the reference count for the virtual machine before removing the device from the list. This adjustment ensures that the system does not attempt to access or manipulate memory that has already been freed, thereby preventing potential use-after-free issues. By implementing this change, the code enhances stability and safety, ensuring that resources are correctly released and that the system remains robust against race conditions."
40,183036,183036,,Local,Not required,Complete,CVE-2016-10741,https://www.cvedetails.com/cve/CVE-2016-10741/,CWE-362,Medium,,,,2019-02-01,4.7,"In the Linux kernel before 4.9.3, fs/xfs/xfs_aops.c allows local users to cause a denial of service (system crash) because there is a race condition between direct and memory-mapped I/O (associated with a hole) that is handled with BUG_ON instead of an I/O failure.",2019-04-18,DoS ,20,https://github.com/torvalds/linux/commit/04197b341f23b908193308b8d63d17ff23232598,04197b341f23b908193308b8d63d17ff23232598,"xfs: don't BUG() on mixed direct and mapped I/O

We've had reports of generic/095 causing XFS to BUG() in
__xfs_get_blocks() due to the existence of delalloc blocks on a
direct I/O read. generic/095 issues a mix of various types of I/O,
including direct and memory mapped I/O to a single file. This is
clearly not supported behavior and is known to lead to such
problems. E.g., the lack of exclusion between the direct I/O and
write fault paths means that a write fault can allocate delalloc
blocks in a region of a file that was previously a hole after the
direct read has attempted to flush/inval the file range, but before
it actually reads the block mapping. In turn, the direct read
discovers a delalloc extent and cannot proceed.

While the appropriate solution here is to not mix direct and memory
mapped I/O to the same regions of the same file, the current
BUG_ON() behavior is probably overkill as it can crash the entire
system.  Instead, localize the failure to the I/O in question by
returning an error for a direct I/O that cannot be handled safely
due to delalloc blocks. Be careful to allow the case of a direct
write to post-eof delalloc blocks. This can occur due to speculative
preallocation and is safe as post-eof blocks are not accompanied by
dirty pages in pagecache (conversely, preallocation within eof must
have been zeroed, and thus dirtied, before the inode size could have
been increased beyond said blocks).

Finally, provide an additional warning if a direct I/O write occurs
while the file is memory mapped. This may not catch all problematic
scenarios, but provides a hint that some known-to-be-problematic I/O
methods are in use.

Signed-off-by: Brian Foster <bfoster@redhat.com>
Reviewed-by: Dave Chinner <dchinner@redhat.com>
Signed-off-by: Dave Chinner <david@fromorbit.com>",1,fs/xfs/xfs_aops.c,"{""sha"": ""2693ba84ec2541072396d138fbf970bca90a597c"", ""filename"": ""fs/xfs/xfs_aops.c"", ""status"": ""modified"", ""additions"": 20, ""deletions"": 2, ""changes"": 22, ""blob_url"": ""https://github.com/torvalds/linux/blob/04197b341f23b908193308b8d63d17ff23232598/fs/xfs/xfs_aops.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04197b341f23b908193308b8d63d17ff23232598/fs/xfs/xfs_aops.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/xfs/xfs_aops.c?ref=04197b341f23b908193308b8d63d17ff23232598"", ""patch"": ""@@ -1361,6 +1361,26 @@ __xfs_get_blocks(\n \tif (error)\n \t\tgoto out_unlock;\n \n+\t/*\n+\t * The only time we can ever safely find delalloc blocks on direct I/O\n+\t * is a dio write to post-eof speculative preallocation. All other\n+\t * scenarios are indicative of a problem or misuse (such as mixing\n+\t * direct and mapped I/O).\n+\t *\n+\t * The file may be unmapped by the time we get here so we cannot\n+\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this\n+\t * is a read or a write within eof. Otherwise, carry on but warn as a\n+\t * precuation if the file happens to be mapped.\n+\t */\n+\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {\n+\t\tif (!create || offset < i_size_read(VFS_I(ip))) {\n+\t\t\tWARN_ON_ONCE(1);\n+\t\t\terror = -EIO;\n+\t\t\tgoto out_unlock;\n+\t\t}\n+\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));\n+\t}\n+\n \t/* for DAX, we convert unwritten extents directly */\n \tif (create &&\n \t    (!nimaps ||\n@@ -1450,8 +1470,6 @@ __xfs_get_blocks(\n \t     (new || ISUNWRITTEN(&imap))))\n \t\tset_buffer_new(bh_result);\n \n-\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);\n-\n \treturn 0;\n \n out_unlock:""}","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
",C,"	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

","	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
",,"@@ -1361,6 +1361,26 @@ __xfs_get_blocks(
 	if (error)
 		goto out_unlock;
 
+	/*
+	 * The only time we can ever safely find delalloc blocks on direct I/O
+	 * is a dio write to post-eof speculative preallocation. All other
+	 * scenarios are indicative of a problem or misuse (such as mixing
+	 * direct and mapped I/O).
+	 *
+	 * The file may be unmapped by the time we get here so we cannot
+	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
+	 * is a read or a write within eof. Otherwise, carry on but warn as a
+	 * precuation if the file happens to be mapped.
+	 */
+	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
+		if (!create || offset < i_size_read(VFS_I(ip))) {
+			WARN_ON_ONCE(1);
+			error = -EIO;
+			goto out_unlock;
+		}
+		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
+	}
+
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
@@ -1450,8 +1470,6 @@ __xfs_get_blocks(
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
-	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
-
 	return 0;
 
 out_unlock:",linux,04197b341f23b908193308b8d63d17ff23232598,399372349a7f9b2d7e56e4fa4467c69822d07024,1,"__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * The only time we can ever safely find delalloc blocks on direct I/O
//fix_flaw_line_below:
//	 * is a dio write to post-eof speculative preallocation. All other
//fix_flaw_line_below:
//	 * scenarios are indicative of a problem or misuse (such as mixing
//fix_flaw_line_below:
//	 * direct and mapped I/O).
//fix_flaw_line_below:
//	 *
//fix_flaw_line_below:
//	 * The file may be unmapped by the time we get here so we cannot
//fix_flaw_line_below:
//	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
//fix_flaw_line_below:
//	 * is a read or a write within eof. Otherwise, carry on but warn as a
//fix_flaw_line_below:
//	 * precuation if the file happens to be mapped.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
//fix_flaw_line_below:
//		if (!create || offset < i_size_read(VFS_I(ip))) {
//fix_flaw_line_below:
//			WARN_ON_ONCE(1);
//fix_flaw_line_below:
//			error = -EIO;
//fix_flaw_line_below:
//			goto out_unlock;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
//flaw_line_below:
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
//flaw_line_below:

 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
",183036,"__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","The `__xfs_get_blocks` function is responsible for managing block allocation and mapping in the XFS filesystem. It takes an inode, block index, and a buffer head as parameters, determining whether to create new blocks or read existing ones based on the provided flags. The function handles direct I/O and CoW (Copy-on-Write) scenarios, ensuring proper locking and error handling. It translates logical block addresses to physical ones, manages unwritten extents, and updates the buffer head with the appropriate mapping information. Additionally, it ensures compatibility with DAX (Direct Access) and real-time files, returning errors as necessary while maintaining filesystem integrity.","The provided code implements a function that manages block allocation and mapping within a filesystem. It determines whether to create new blocks or read existing ones based on input parameters, while handling various scenarios such as direct I/O and Copy-on-Write. The function translates logical addresses to physical ones, manages unwritten extents, and updates a buffer with the appropriate mapping information. It also ensures compatibility with specific file types and handles error conditions, maintaining the integrity of the filesystem throughout the process. The function ultimately returns an error code or success status based on the operations performed.","The vulnerability identified as CWE-362, which pertains to a race condition, was present in the original code due to the improper handling of direct I/O operations when the file was unmapped. Specifically, the code allowed for the possibility of mixing direct and mapped I/O, which could lead to inconsistent states and unexpected behavior. The fix introduced a check that raises a warning if a direct I/O operation encounters a delayed allocation block while ensuring that the operation is only allowed under specific conditions, such as when creating a new block beyond the end of the file. This change mitigates the risk of race conditions by enforcing stricter checks on the state of the file during I/O operations.","The vulnerability identified as CWE-362, related to race conditions, arose from the improper handling of direct I/O operations in the original code, which allowed for potential conflicts when the file's state was altered concurrently. This could lead to inconsistent data being read or written, especially when mixing direct and mapped I/O. The fix implemented stricter checks to ensure that direct I/O operations only proceed under specific conditions, such as when creating new blocks beyond the end of the file. This change effectively mitigates the risk of race conditions by enforcing a more controlled and predictable state during I/O operations.","The vulnerability was fixed by introducing checks that prevent improper direct I/O operations when the file is unmapped. Specifically, the code now verifies if a direct I/O operation encounters a delayed allocation block, raising a warning if this occurs. The condition checks whether the operation is a creation and if the offset is less than the file's size, using the `WARN_ON_ONCE` function to log potential misuse. This ensures that direct I/O is only allowed under safe circumstances, thereby reducing the risk of race conditions and ensuring consistent file state during operations.","The vulnerability was addressed by implementing stricter checks to prevent unsafe direct I/O operations when the file's state could be altered concurrently. The fix introduced conditions that ensure direct I/O is only permitted under specific scenarios, such as when creating new blocks beyond the end of the file. Additionally, warnings are generated if a direct I/O operation encounters a delayed allocation block, which helps identify potential misuse. This approach effectively mitigates the risk of race conditions by enforcing a more controlled execution flow, ensuring that file operations maintain consistency and integrity."
41,184595,184595,,Remote,Not required,Complete,CVE-2011-3961,https://www.cvedetails.com/cve/CVE-2011-3961/,CWE-362,Medium,Complete,Complete,,2012-02-08,9.3,Race condition in Google Chrome before 17.0.963.46 allows remote attackers to execute arbitrary code via vectors that trigger a crash of a utility process.,2017-09-18,Exec Code ,4,https://github.com/chromium/chromium/commit/b712795852f9d6073e062680e280634290c4ba5d,b712795852f9d6073e062680e280634290c4ba5d,"Fix uninitialized variables in HarfBuzzShaperBase
https://bugs.webkit.org/show_bug.cgi?id=79546

Reviewed by Dirk Pranke.

These were introduced in r108733.

* platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp:
(WebCore::HarfBuzzShaperBase::HarfBuzzShaperBase):

git-svn-id: svn://svn.chromium.org/blink/trunk@108871 bbb929c8-8fbe-4397-9dbb-9b2b20218538",0,third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp,"{""sha"": ""47cc92f4eecf9c468b793df4067b1908ccbcfe30"", ""filename"": ""third_party/WebKit/Source/WebCore/ChangeLog"", ""status"": ""modified"", ""additions"": 12, ""deletions"": 0, ""changes"": 12, ""blob_url"": ""https://github.com/chromium/chromium/blob/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/ChangeLog"", ""raw_url"": ""https://github.com/chromium/chromium/raw/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/ChangeLog"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/WebCore/ChangeLog?ref=b712795852f9d6073e062680e280634290c4ba5d"", ""patch"": ""@@ -1,3 +1,15 @@\n+2012-02-24  Adrienne Walker  <enne@google.com>\n+\n+        Fix uninitialized variables in HarfBuzzShaperBase\n+        https://bugs.webkit.org/show_bug.cgi?id=79546\n+\n+        Reviewed by Dirk Pranke.\n+\n+        These were introduced in r108733.\n+\n+        * platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp:\n+        (WebCore::HarfBuzzShaperBase::HarfBuzzShaperBase):\n+\n 2012-02-24  Noel Gordon  <noel.gordon@gmail.com>\n \n         [chromium] JPEG RGB image with Adode Marker fails to turbo swizzle decode""}<_**next**_>{""sha"": ""b8daff9e3a07c5118ba92b6d6a8096b93a2beccc"", ""filename"": ""third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 0, ""changes"": 4, ""blob_url"": ""https://github.com/chromium/chromium/blob/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""raw_url"": ""https://github.com/chromium/chromium/raw/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp?ref=b712795852f9d6073e062680e280634290c4ba5d"", ""patch"": ""@@ -42,8 +42,12 @@ namespace WebCore {\n \n HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)\n     : m_font(font)\n+    , m_normalizedBufferLength(0)\n     , m_run(run)\n     , m_wordSpacingAdjustment(font->wordSpacing())\n+    , m_padding(0)\n+    , m_padPerWordBreak(0)\n+    , m_padError(0)\n     , m_letterSpacing(font->letterSpacing())\n {\n }""}"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
     , m_letterSpacing(font->letterSpacing())
 {
 }
",C,"    , m_normalizedBufferLength(0)
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
",,,"@@ -42,8 +42,12 @@ namespace WebCore {
 
 HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
+    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
+    , m_padding(0)
+    , m_padPerWordBreak(0)
+    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }",Chrome,b712795852f9d6073e062680e280634290c4ba5d,617f382e9c9abab47e325c5d2e319524a5440faa,1," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
//fix_flaw_line_below:
//    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
//fix_flaw_line_below:
//    , m_padding(0)
//fix_flaw_line_below:
//    , m_padPerWordBreak(0)
//fix_flaw_line_below:
//    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
",184595," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
     , m_letterSpacing(font->letterSpacing())
 {
 }
"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
","The provided code snippet defines a constructor for the `HarfBuzzShaperBase` class, which initializes an instance with specific attributes related to font and text rendering. It takes two parameters: a pointer to a `Font` object and a `TextRun` object. The constructor initializes member variables `m_font` and `m_run` with the provided arguments, while also setting `m_wordSpacingAdjustment` and `m_letterSpacing` using the respective properties from the `Font` object. This setup is essential for managing text shaping and rendering, allowing for adjustments in word and letter spacing based on the font's characteristics.","The code snippet defines a constructor for a class that handles text shaping using a specific font and text run. It initializes the class instance by accepting a font object and a text run object as parameters. During initialization, it sets up essential attributes related to the font, including adjustments for spacing between words and letters. This functionality is crucial for rendering text accurately, as it allows the class to manage how text is displayed, ensuring that the visual output adheres to the specified typographic settings of the font being used.","The vulnerability identified in the original code, classified under CWE-362, arises from the potential use of uninitialized member variables, which can lead to undefined behavior. In the vulnerable code, the member variable `m_normalizedBufferLength` was not initialized, leaving it with an indeterminate value. This could result in unpredictable behavior when the object is used. The fixed code addresses this issue by explicitly initializing `m_normalizedBufferLength` to zero, along with other padding-related variables. This ensures that all member variables have defined values upon object construction, thereby enhancing the stability and security of the code.","The vulnerability associated with the original code, categorized under CWE-362, stems from the risk of using uninitialized member variables, which can lead to unpredictable behavior and security issues. In the vulnerable version, certain member variables were left uninitialized, potentially allowing them to contain garbage values. The fixed code mitigates this risk by ensuring that all relevant member variables are explicitly initialized to defined values during object construction. This change enhances the reliability of the code by preventing the use of indeterminate values, thereby improving both stability and security in the handling of the object's state.","The vulnerability was fixed by explicitly initializing previously uninitialized member variables in the constructor of the class. In the original code, the variable `m_normalizedBufferLength` was not set, which could lead to undefined behavior. The updated code initializes `m_normalizedBufferLength` to zero, along with other padding-related variables such as `m_padding`, `m_padPerWordBreak`, and `m_padError`. This ensures that all member variables have defined values upon object creation, thereby preventing the use of indeterminate values and enhancing the overall stability and security of the code during text shaping operations.","The vulnerability was addressed by ensuring that all member variables are explicitly initialized in the constructor. Previously, some member variables were left uninitialized, which could lead to unpredictable behavior when the object was used. The revised code initializes these variables to defined values, thereby eliminating the risk of using indeterminate data. This change guarantees that the object's state is consistent and reliable from the moment it is created, significantly enhancing the stability and security of the code by preventing potential issues related to uninitialized memory."
42,184867,184867,,Remote,Not required,Partial,CVE-2013-2906,https://www.cvedetails.com/cve/CVE-2013-2906/,CWE-362,Medium,Partial,Partial,,2013-10-02,6.8,"Multiple race conditions in the Web Audio implementation in Blink, as used in Google Chrome before 30.0.1599.66, allow remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to threading in core/html/HTMLMediaElement.cpp, core/platform/audio/AudioDSPKernelProcessor.cpp, core/platform/audio/HRTFElevation.cpp, and modules/webaudio/ConvolverNode.cpp.",2017-09-18,DoS ,4,https://github.com/chromium/chromium/commit/c2364e0ce42878a2177c6f4cf7adb3c715b777c1,c2364e0ce42878a2177c6f4cf7adb3c715b777c1,"[OriginChip] Re-enable the chip as necessary when switching tabs.

BUG=369500

Review URL: https://codereview.chromium.org/292493003

git-svn-id: svn://svn.chromium.org/chrome/trunk/src@271161 0039d316-1c4b-4281-b951-d872f2087c98",1,chrome/browser/ui/omnibox/omnibox_edit_model.cc,"{""sha"": ""587dd72386e3a10288ba621f07704dd623f37872"", ""filename"": ""chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 1, ""changes"": 5, ""blob_url"": ""https://github.com/chromium/chromium/blob/c2364e0ce42878a2177c6f4cf7adb3c715b777c1/chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/c2364e0ce42878a2177c6f4cf7adb3c715b777c1/chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/chrome/browser/ui/omnibox/omnibox_edit_model.cc?ref=c2364e0ce42878a2177c6f4cf7adb3c715b777c1"", ""patch"": ""@@ -247,8 +247,11 @@ const OmniboxEditModel::State OmniboxEditModel::GetStateForTabSwitch() {\n void OmniboxEditModel::RestoreState(const State* state) {\n   // We need to update the permanent text correctly and revert the view\n   // regardless of whether there is saved state.\n+  bool url_replacement_enabled = !state || state->url_replacement_enabled;\n   controller_->GetToolbarModel()->set_url_replacement_enabled(\n-      !state || state->url_replacement_enabled);\n+      url_replacement_enabled);\n+  controller_->GetToolbarModel()->set_origin_chip_enabled(\n+      url_replacement_enabled);\n   permanent_text_ = controller_->GetToolbarModel()->GetText();\n   // Don't muck with the search term replacement state, as we've just set it\n   // correctly.""}"," void OmniboxEditModel::RestoreState(const State* state) {
  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
"," void OmniboxEditModel::RestoreState(const State* state) {
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      !state || state->url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
",C,"  bool url_replacement_enabled = !state || state->url_replacement_enabled;
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
","      !state || state->url_replacement_enabled);
",,"@@ -247,8 +247,11 @@ const OmniboxEditModel::State OmniboxEditModel::GetStateForTabSwitch() {
 void OmniboxEditModel::RestoreState(const State* state) {
   // We need to update the permanent text correctly and revert the view
   // regardless of whether there is saved state.
+  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
-      !state || state->url_replacement_enabled);
+      url_replacement_enabled);
+  controller_->GetToolbarModel()->set_origin_chip_enabled(
+      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
   // Don't muck with the search term replacement state, as we've just set it
   // correctly.",Chrome,c2364e0ce42878a2177c6f4cf7adb3c715b777c1,9016f5323611c8bb8d24f40cddeb38f75990fdc0,1," void OmniboxEditModel::RestoreState(const State* state) {
   // We need to update the permanent text correctly and revert the view
   // regardless of whether there is saved state.
//fix_flaw_line_below:
//  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
//flaw_line_below:
      !state || state->url_replacement_enabled);
//fix_flaw_line_below:
//      url_replacement_enabled);
//fix_flaw_line_below:
//  controller_->GetToolbarModel()->set_origin_chip_enabled(
//fix_flaw_line_below:
//      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
   // Don't muck with the search term replacement state, as we've just set it
   // correctly.
  view_->RevertWithoutResettingSearchTermReplacement();
  // Restore the autocomplete controller's input, or clear it if this is a new
  // tab.
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  // Restore any user editing.
  if (state->user_input_in_progress) {
    // NOTE: Be sure and set keyword-related state BEFORE invoking
    // DisplayTextFromUserText(), as its result depends upon this state.
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
",184867," void OmniboxEditModel::RestoreState(const State* state) {
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      !state || state->url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
"," void OmniboxEditModel::RestoreState(const State* state) {
  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
","The `RestoreState` function in the `OmniboxEditModel` class is designed to restore the state of the omnibox based on a provided `State` object. It first checks if the state is valid to determine whether URL replacement should be enabled. The function then retrieves and sets the current text from the toolbar model, reverts the view without resetting the search term, and initializes the autocomplete input. If the state is valid, it updates the focus state and source, and if user input is in progress, it sets the keyword, hint status, and updates the view with the user's text and gray text autocompletion.","The function is responsible for restoring the state of a user interface component based on a given state object. It determines whether certain features should be enabled and retrieves the current text from a model. The function then reverts the display to a previous state without altering the search term. If the state object is valid, it updates the focus and source of user interaction. Additionally, if there is ongoing user input, it sets relevant parameters and updates the display with the user's text and any associated suggestions, ensuring the interface reflects the user's previous interactions accurately.","The vulnerability identified as CWE-362, which pertains to race conditions, was present in the original code due to the lack of synchronization when accessing shared resources. Specifically, the function directly set the URL replacement state based on the provided state object without ensuring that the state was not modified concurrently by another thread. This could lead to inconsistent behavior if multiple threads accessed or modified the state simultaneously. The fix introduced a local variable to store the URL replacement state before using it, ensuring that the value is consistent and reducing the risk of race conditions during concurrent access.","The vulnerability identified as CWE-362, which relates to race conditions, arose from the original code's direct manipulation of shared resources without proper synchronization. This lack of control allowed multiple threads to potentially access and modify the same state simultaneously, leading to unpredictable behavior and inconsistent results. The fix addressed this issue by introducing a local variable to store the relevant state information before any operations were performed. This change ensured that the value used in subsequent operations remained consistent, thereby mitigating the risk of race conditions and enhancing the overall stability and reliability of the code.","The vulnerability was fixed by introducing a local boolean variable to store the URL replacement state before it was used in subsequent function calls. In the updated code, the expression that checks the state object is assigned to the variable `url_replacement_enabled`, which is then passed to both `set_url_replacement_enabled` and `set_origin_chip_enabled`. This change ensures that both functions operate on a consistent value, preventing any race conditions that could arise from concurrent modifications to the state object. By isolating the state retrieval in this manner, the code enhances its stability and reliability during concurrent access.","The vulnerability was fixed by introducing a local variable to capture the state information before it was used in any operations. This approach ensures that the value remains consistent throughout the function, preventing any potential race conditions that could occur if multiple threads accessed or modified the state simultaneously. By storing the relevant state in a local variable, the code guarantees that subsequent operations rely on a stable and unchanging value, thereby enhancing the overall reliability and stability of the function during concurrent execution. This change effectively mitigates the risks associated with shared resource access."
43,184868,184868,,Remote,Not required,Partial,CVE-2013-2906,https://www.cvedetails.com/cve/CVE-2013-2906/,CWE-362,Medium,Partial,Partial,,2013-10-02,6.8,"Multiple race conditions in the Web Audio implementation in Blink, as used in Google Chrome before 30.0.1599.66, allow remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to threading in core/html/HTMLMediaElement.cpp, core/platform/audio/AudioDSPKernelProcessor.cpp, core/platform/audio/HRTFElevation.cpp, and modules/webaudio/ConvolverNode.cpp.",2017-09-18,DoS ,5,https://github.com/chromium/chromium/commit/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,"Suspend shared timers while blockingly closing databases

BUG=388771
R=michaeln@chromium.org

Review URL: https://codereview.chromium.org/409863002

git-svn-id: svn://svn.chromium.org/chrome/trunk/src@284785 0039d316-1c4b-4281-b951-d872f2087c98",0,content/renderer/render_thread_impl.cc,"{""sha"": ""5116fac7442aeafd9f36622a135f8b2669235f42"", ""filename"": ""content/renderer/render_thread_impl.cc"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 0, ""changes"": 5, ""blob_url"": ""https://github.com/chromium/chromium/blob/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2/content/renderer/render_thread_impl.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2/content/renderer/render_thread_impl.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/renderer/render_thread_impl.cc?ref=c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2"", ""patch"": ""@@ -574,8 +574,13 @@ void RenderThreadImpl::Shutdown() {\n \n   // Wait for all databases to be closed.\n   if (webkit_platform_support_) {\n+    // WaitForAllDatabasesToClose might run a nested message loop. To avoid\n+    // processing timer events while we're already in the process of shutting\n+    // down blink, put a ScopePageLoadDeferrer on the stack.\n+    WebView::willEnterModalLoop();\n     webkit_platform_support_->web_database_observer_impl()->\n         WaitForAllDatabasesToClose();\n+    WebView::didExitModalLoop();\n   }\n \n   // Shutdown in reverse of the initialization order.""}","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
    WebView::didExitModalLoop();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
",C,"    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
    WebView::didExitModalLoop();
",,,"@@ -574,8 +574,13 @@ void RenderThreadImpl::Shutdown() {
 
   // Wait for all databases to be closed.
   if (webkit_platform_support_) {
+    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
+    // processing timer events while we're already in the process of shutting
+    // down blink, put a ScopePageLoadDeferrer on the stack.
+    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
+    WebView::didExitModalLoop();
   }
 
   // Shutdown in reverse of the initialization order.",Chrome,c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,6e44c30f2dc7371711503b0a942da01180206c47,1,"void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   // Wait for all databases to be closed.
   if (webkit_platform_support_) {
//fix_flaw_line_below:
//    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
//fix_flaw_line_below:
//    // processing timer events while we're already in the process of shutting
//fix_flaw_line_below:
//    // down blink, put a ScopePageLoadDeferrer on the stack.
//fix_flaw_line_below:
//    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
//fix_flaw_line_below:
//    WebView::didExitModalLoop();
   }
 
   // Shutdown in reverse of the initialization order.
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  // Shutdown the file thread if it's running.
  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  // RemoveEmbeddedWorkerRoute may be called while deleting
  // EmbeddedWorkerDispatcher. So it must be deleted before deleting
  // RenderThreadImpl.
  embedded_worker_dispatcher_.reset();

  // Ramp down IDB before we ramp down WebKit (and V8), since IDB classes might
  // hold pointers to V8 objects (e.g., via pending requests).
  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

  // TODO(port)
#if defined(OS_WIN)
  // Clean up plugin channels before this thread goes away.
  NPChannelBase::CleanupChannels();
#endif
}
",184868,"void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
    WebView::didExitModalLoop();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","The `Shutdown` function in the `RenderThreadImpl` class is responsible for gracefully terminating various components and observers associated with the rendering process. It first notifies all registered observers of the shutdown event, then proceeds to clean up memory observers and database connections, ensuring all databases are closed. The function removes various message filters related to audio, video capture, and input events, and resets associated resources. Additionally, it stops any active threads, including file and media threads, and performs platform-specific cleanup. Finally, it invokes a shutdown routine for the Blink framework and cleans up any remaining resources, ensuring a complete shutdown.","The provided code implements a shutdown procedure for a rendering process, ensuring that all associated components and observers are properly terminated. It begins by notifying observers of the shutdown event, followed by the removal of various message filters related to audio, video, and input handling. The code also ensures that all database connections are closed and cleans up any active threads. Additionally, it resets resources related to media handling and platform support, while performing necessary cleanup specific to the operating system. This comprehensive approach guarantees that all resources are released and the rendering process is safely concluded.","The vulnerability identified as CWE-362, which pertains to a race condition, was present in the original shutdown code due to the potential for nested message loops during the database closure process. Specifically, the function `WaitForAllDatabasesToClose` could inadvertently allow timer events to be processed while the system was already in the shutdown phase, leading to unpredictable behavior. The fix introduced a mechanism to defer page load events by using `WebView::willEnterModalLoop()` and `WebView::didExitModalLoop()`, ensuring that no timer events are processed during the critical shutdown sequence, thus preventing the race condition and ensuring a safe and orderly shutdown.","The vulnerability identified as CWE-362, related to a race condition, arose from the potential for concurrent execution during the shutdown process, particularly when closing database connections. This situation could lead to unexpected behavior, as timer events might be processed while the system was already in the midst of shutting down, resulting in inconsistent states. The fix addressed this issue by implementing a mechanism to defer certain events during the critical shutdown phase, ensuring that no conflicting operations could occur simultaneously. This change effectively synchronized the shutdown process, preventing the race condition and promoting a safe and orderly termination of resources.","The vulnerability was fixed by introducing a mechanism to defer event processing during the shutdown sequence. Specifically, the code now utilizes `WebView::willEnterModalLoop()` before calling `WaitForAllDatabasesToClose()` to prevent timer events from being processed while the system is shutting down. This ensures that the shutdown process is not interrupted by concurrent operations that could lead to inconsistent states. After the database closure, `WebView::didExitModalLoop()` is called to signal the end of the deferral, effectively synchronizing the shutdown process and eliminating the race condition that could arise from nested message loops.","The vulnerability was fixed by implementing a mechanism to defer event processing during the shutdown sequence, which prevents concurrent operations from interfering with the critical shutdown tasks. This was achieved by introducing a control structure that temporarily suspends the handling of certain events while the system is closing database connections. By ensuring that no conflicting operations can occur during this phase, the fix effectively synchronizes the shutdown process, thereby eliminating the race condition that could lead to unpredictable behavior and ensuring a safe and orderly termination of resources."
44,185106,185106,,Remote,Not required,Partial,CVE-2013-0893,https://www.cvedetails.com/cve/CVE-2013-0893/,CWE-362,Medium,Partial,Partial,,2013-02-23,6.8,"Race condition in Google Chrome before 25.0.1364.97 on Windows and Linux, and before 25.0.1364.99 on Mac OS X, allows remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to media.",2018-10-30,DoS ,6,https://github.com/chromium/chromium/commit/ed6f4545a2a345697e07908c887333f5bdcc97a3,ed6f4545a2a345697e07908c887333f5bdcc97a3,"Apply 'x-content-type-options' check to dynamically inserted script.

BUG=348581

Review URL: https://codereview.chromium.org/185593011

git-svn-id: svn://svn.chromium.org/blink/trunk@168570 bbb929c8-8fbe-4397-9dbb-9b2b20218538",3,third_party/WebKit/Source/core/dom/ScriptLoader.cpp,"{""sha"": ""dafdb9606e69e9b0318994a254c8b73622ff304d"", ""filename"": ""third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""status"": ""added"", ""additions"": 11, ""deletions"": 0, ""changes"": 11, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -0,0 +1,11 @@\n+CONSOLE ERROR: Refused to execute script from 'http://127.0.0.1:8000/security/contentTypeOptions/resources/script-with-header.pl?mime=application/json' because its MIME type ('application/json') is not executable, and strict MIME type checking is enabled.\n+Check that script sent with an 'X-Content-Type-Options: nosniff' header is correctly blocked if the MIME type isn't scripty.\n+\n+On success, you will see a series of \""PASS\"" messages, followed by \""TEST COMPLETE\"".\n+\n+\n+PASS window.scriptsSuccessfullyLoaded is 0\n+PASS successfullyParsed is true\n+\n+TEST COMPLETE\n+""}<_**next**_>{""sha"": ""6d0c62d8e7c1e4cf8369e76e93c00a08b7fa755b"", ""filename"": ""third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""status"": ""added"", ""additions"": 23, ""deletions"": 0, ""changes"": 23, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -0,0 +1,23 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+    <title>'X-Content-Type-Options: nosniff;' blocks scripts!</title>\n+<body>\n+    <script src=\""/js-test-resources/js-test.js\""></script>\n+    <script>\n+        description('Check that script sent with an \\'X-Content-Type-Options: nosniff\\' header is correctly blocked if the MIME type isn\\'t scripty.');\n+        window.jsTestIsAsync = true;\n+\n+        window.scriptsSuccessfullyLoaded = 0;\n+\n+        var s = document.createElement('script');\n+        s.src = './resources/script-with-header.pl?mime=application/json';\n+        document.querySelector('head').appendChild(s);\n+\n+        window.onload = function () {\n+            shouldBe('window.scriptsSuccessfullyLoaded', '0');\n+            finishJSTest();\n+        };\n+    </script>\n+</body>\n+</html>""}<_**next**_>{""sha"": ""0ee2e25f4d5c4590e7c67948543e1b56ead8b167"", ""filename"": ""third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 3, ""changes"": 9, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/core/dom/ScriptLoader.cpp?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -308,9 +308,12 @@ void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)\n     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))\n         return;\n \n-    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {\n-        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, \""Refused to execute script from '\"" + m_resource->url().elidedString() + \""' because its MIME type ('\"" + m_resource->mimeType() + \""') is not executable, and strict MIME type checking is enabled.\"");\n-        return;\n+    if (m_isExternalScript) {\n+        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();\n+        if (resource && !resource->mimeTypeAllowedByNosniff()) {\n+            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, \""Refused to execute script from '\"" + resource->url().elidedString() + \""' because its MIME type ('\"" + resource->mimeType() + \""') is not executable, and strict MIME type checking is enabled.\"");\n+            return;\n+        }\n     }\n \n     if (frame) {""}","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
",C,"    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
","    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
",,"@@ -308,9 +308,12 @@ void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
-    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
-        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
-        return;
+    if (m_isExternalScript) {
+        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
+        if (resource && !resource->mimeTypeAllowedByNosniff()) {
+            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
+            return;
+        }
     }
 
     if (frame) {",Chrome,ed6f4545a2a345697e07908c887333f5bdcc97a3,2c29c343bc7bff16fb552abf162d17d26a9d9563,1,"void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
//flaw_line_below:
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
//flaw_line_below:
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
//flaw_line_below:
        return;
//fix_flaw_line_below:
//    if (m_isExternalScript) {
//fix_flaw_line_below:
//        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
//fix_flaw_line_below:
//        if (resource && !resource->mimeTypeAllowedByNosniff()) {
//fix_flaw_line_below:
//            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
//fix_flaw_line_below:
//            return;
//fix_flaw_line_below:
//        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        // http://www.whatwg.org/specs/web-apps/current-work/#execute-the-script-block step 2.3
        // with additional support for HTML imports.
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        // Create a script from the script element node, using the script
        // block's source and the script block's type.
        // Note: This is where the script is compiled and actually executed.
        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
",185106,"void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","The `executeScript` function in the `ScriptLoader` class is responsible for executing a given script represented by `sourceCode`. It first checks if the script execution has already started and whether the source code is empty. It retrieves the associated document and frame, then evaluates content security policies to determine if the script can be executed, considering factors like inline scripts and MIME type restrictions. If the script passes these checks, it executes the script in the main world of the frame, handling cross-origin resource sharing (CORS) appropriately. Finally, it manages the current script context for HTML script elements.","The function is designed to execute a script only if certain conditions are met, such as ensuring that the script execution has not already begun and that the script source is not empty. It checks the associated document and frame, evaluating security policies to determine if the script can run, including considerations for inline scripts and MIME type restrictions. If the script passes these security checks, it is executed within the main context of the frame, while also managing cross-origin resource sharing. The function ensures proper handling of the script context for HTML elements during execution.","The vulnerability identified by CWE-362, which relates to race conditions, was present in the original code due to insufficient checks on the execution context of scripts. Specifically, the code did not adequately verify the security policies before executing scripts, allowing potential execution of malicious scripts if the conditions were manipulated. The fix introduced a more robust validation process, particularly by ensuring that the MIME type of external scripts is checked against strict policies using `mimeTypeAllowedByNosniff()`. This change helps prevent unauthorized script execution by enforcing stricter content security policies and ensuring that only safe scripts are executed in the appropriate context.","The vulnerability identified by CWE-362, related to race conditions, arose from inadequate synchronization in the script execution process, allowing for potential manipulation of the execution context. In the original code, insufficient checks on security policies could lead to the execution of unauthorized scripts, especially when external resources were involved. The fix improved the validation mechanism by implementing stricter checks on MIME types and content security policies before executing scripts. This enhancement ensures that only scripts meeting specific safety criteria are executed, thereby mitigating the risk of race conditions and unauthorized script execution.","The vulnerability was fixed by enhancing the validation checks for script execution, particularly for external scripts. The updated code introduces a check using `mimeTypeAllowedByNosniff()` to ensure that the MIME type of the resource is executable before allowing the script to run. Additionally, the code now verifies the content security policy more rigorously, specifically through `allowInlineScript()` and `allowScriptNonce()`, to prevent unauthorized script execution. These changes ensure that only scripts that comply with strict security policies are executed, thereby mitigating the risk of race conditions and enhancing overall security.","The vulnerability was addressed by implementing stricter validation checks for script execution, particularly focusing on external scripts. The updated code ensures that the MIME type of resources is verified to confirm that they are executable before allowing execution. Additionally, the content security policies were reinforced to rigorously assess whether scripts meet specific safety criteria, including checks for inline scripts and nonce attributes. These enhancements collectively prevent unauthorized script execution and mitigate the risk of race conditions, thereby improving the overall security of the script execution process."
45,186239,186239,,Remote,Not required,,CVE-2016-1670,https://www.cvedetails.com/cve/CVE-2016-1670/,CWE-362,High,,Partial,,2016-05-14,2.6,Race condition in the ResourceDispatcherHostImpl::BeginRequest function in content/browser/loader/resource_dispatcher_host_impl.cc in Google Chrome before 50.0.2661.102 allows remote attackers to make arbitrary HTTP requests by leveraging access to a renderer process and reusing a request ID.,2018-10-30,,7,https://github.com/chromium/chromium/commit/1af4fada49c4f3890f16daac31d38379a9d782b2,1af4fada49c4f3890f16daac31d38379a9d782b2,"Block a compromised renderer from reusing request ids.

BUG=578882

Review URL: https://codereview.chromium.org/1608573002

Cr-Commit-Position: refs/heads/master@{#372547}",0,content/browser/loader/resource_dispatcher_host_impl.cc,"{""sha"": ""8f0883073c4c5df0d87ebaef4685c987255b7f9a"", ""filename"": ""content/browser/bad_message.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/bad_message.h"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/bad_message.h"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/bad_message.h?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -129,6 +129,7 @@ enum BadMessageReason {\n   BDH_DEVICE_NOT_ALLOWED_FOR_ORIGIN = 105,\n   ACI_WRONG_STORAGE_PARTITION = 106,\n   RDHI_WRONG_STORAGE_PARTITION = 107,\n+  RDH_INVALID_REQUEST_ID = 108,\n \n   // Please add new elements here. The naming convention is abbreviated class\n   // name (e.g. RenderFrameHost becomes RFH) plus a unique description of the""}<_**next**_>{""sha"": ""1db3d98e157c29ae3455281fdbd72dbf65d12d4a"", ""filename"": ""content/browser/loader/resource_dispatcher_host_impl.cc"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/loader/resource_dispatcher_host_impl.cc?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -1187,6 +1187,20 @@ void ResourceDispatcherHostImpl::OnSyncLoad(\n                sync_result->routing_id());\n }\n \n+bool ResourceDispatcherHostImpl::IsRequestIDInUse(\n+    const GlobalRequestID& id) const {\n+  if (pending_loaders_.find(id) != pending_loaders_.end())\n+    return true;\n+  for (const auto& blocked_loaders : blocked_loaders_map_) {\n+    for (const auto& loader : *blocked_loaders.second.get()) {\n+      ResourceRequestInfoImpl* info = loader->GetRequestInfo();\n+      if (info->GetGlobalRequestID() == id)\n+        return true;\n+    }\n+  }\n+  return false;\n+}\n+\n void ResourceDispatcherHostImpl::UpdateRequestForTransfer(\n     int child_id,\n     int route_id,\n@@ -1281,6 +1295,13 @@ void ResourceDispatcherHostImpl::BeginRequest(\n   int process_type = filter_->process_type();\n   int child_id = filter_->child_id();\n \n+  // Reject request id that's currently in use.\n+  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {\n+    bad_message::ReceivedBadMessage(filter_,\n+                                    bad_message::RDH_INVALID_REQUEST_ID);\n+    return;\n+  }\n+\n   // PlzNavigate: reject invalid renderer main resource request.\n   if (IsBrowserSideNavigationEnabled() &&\n       IsResourceTypeFrame(request_data.resource_type) &&""}<_**next**_>{""sha"": ""29a82b248a109c7890fec7e0c2f5b6d2ff717d4c"", ""filename"": ""content/browser/loader/resource_dispatcher_host_impl.h"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 0, ""changes"": 2, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.h"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.h"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/loader/resource_dispatcher_host_impl.h?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -422,6 +422,8 @@ class CONTENT_EXPORT ResourceDispatcherHostImpl\n                   const ResourceHostMsg_Request& request_data,\n                   IPC::Message* sync_result);\n \n+  bool IsRequestIDInUse(const GlobalRequestID& id) const;\n+\n   // Update the ResourceRequestInfo and internal maps when a request is\n   // transferred from one process to another.\n   void UpdateRequestForTransfer(int child_id,""}<_**next**_>{""sha"": ""6714e9a086a737607f6947d6fdf8d1608742ee9e"", ""filename"": ""content/browser/security_exploit_browsertest.cc"", ""status"": ""modified"", ""additions"": 75, ""deletions"": 8, ""changes"": 83, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/security_exploit_browsertest.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/security_exploit_browsertest.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/security_exploit_browsertest.cc?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -23,6 +23,7 @@\n #include \""content/public/browser/content_browser_client.h\""\n #include \""content/public/browser/interstitial_page.h\""\n #include \""content/public/browser/interstitial_page_delegate.h\""\n+#include \""content/public/browser/resource_dispatcher_host.h\""\n #include \""content/public/browser/storage_partition.h\""\n #include \""content/public/common/appcache_info.h\""\n #include \""content/public/common/browser_side_navigation_policy.h\""\n@@ -37,13 +38,19 @@\n #include \""ipc/ipc_security_test_util.h\""\n #include \""net/dns/mock_host_resolver.h\""\n #include \""net/test/embedded_test_server/embedded_test_server.h\""\n+#include \""net/test/url_request/url_request_slow_download_job.h\""\n \n using IPC::IpcSecurityTestUtil;\n \n namespace content {\n \n namespace {\n \n+// This request id is used by tests that craft a\n+// ResourceHostMsg_RequestResource. The id is sufficiently large that it doesn't\n+// collide with ids used by previous navigation requests.\n+const int kRequestIdNotPreviouslyUsed = 10000;\n+\n // This is a helper function for the tests which attempt to create a\n // duplicate RenderViewHost or RenderWidgetHost. It tries to create two objects\n // with the same process and routing ids, which causes a collision.\n@@ -98,13 +105,11 @@ RenderViewHostImpl* PrepareToDuplicateHosts(Shell* shell,\n   return next_rfh->render_view_host();\n }\n \n-ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n+ResourceHostMsg_Request CreateXHRRequest(const char* url) {\n   ResourceHostMsg_Request request;\n   request.method = \""GET\"";\n-  request.url = GURL(\""http://bar.com/simple_page.html\"");\n-  request.first_party_for_cookies = GURL(origin);\n+  request.url = GURL(url);\n   request.referrer_policy = blink::WebReferrerPolicyDefault;\n-  request.headers = base::StringPrintf(\""Origin: %s\\r\\n\"", origin);\n   request.load_flags = 0;\n   request.origin_pid = 0;\n   request.resource_type = RESOURCE_TYPE_XHR;\n@@ -120,6 +125,49 @@ ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n   return request;\n }\n \n+ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n+  ResourceHostMsg_Request request =\n+      CreateXHRRequest(\""http://bar.com/simple_page.html\"");\n+  request.first_party_for_cookies = GURL(origin);\n+  request.headers = base::StringPrintf(\""Origin: %s\\r\\n\"", origin);\n+  return request;\n+}\n+\n+void TryCreateDuplicateRequestIds(Shell* shell, bool block_loaders) {\n+  NavigateToURL(shell, GURL(\""http://foo.com/simple_page.html\""));\n+  RenderFrameHost* rfh = shell->web_contents()->GetMainFrame();\n+\n+  if (block_loaders) {\n+    // Test the case where loaders are placed into blocked_loaders_map_.\n+    int child_id = rfh->GetProcess()->GetID();\n+    BrowserThread::PostTask(\n+        BrowserThread::IO, FROM_HERE,\n+        base::Bind(&ResourceDispatcherHost::BlockRequestsForRoute,\n+                   base::Unretained(ResourceDispatcherHost::Get()), child_id,\n+                   rfh->GetRoutingID()));\n+  }\n+\n+  // URLRequestSlowDownloadJob waits for another request to kFinishDownloadUrl\n+  // to finish all pending requests. It is never sent, so the following URL\n+  // blocks indefinitely, which is good because the request stays alive and the\n+  // test can try to reuse the request id without a race.\n+  const char* blocking_url = net::URLRequestSlowDownloadJob::kUnknownSizeUrl;\n+  ResourceHostMsg_Request request(CreateXHRRequest(blocking_url));\n+\n+  // Use the same request id twice.\n+  RenderProcessHostWatcher process_killed(\n+      rfh->GetProcess(), RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n+  IPC::IpcSecurityTestUtil::PwnMessageReceived(\n+      rfh->GetProcess()->GetChannel(),\n+      ResourceHostMsg_RequestResource(rfh->GetRoutingID(),\n+                                      kRequestIdNotPreviouslyUsed, request));\n+  IPC::IpcSecurityTestUtil::PwnMessageReceived(\n+      rfh->GetProcess()->GetChannel(),\n+      ResourceHostMsg_RequestResource(rfh->GetRoutingID(),\n+                                      kRequestIdNotPreviouslyUsed, request));\n+  process_killed.Wait();\n+}\n+\n }  // namespace\n \n \n@@ -145,6 +193,12 @@ class SecurityExploitBrowserTest : public ContentBrowserTest {\n             \"",EXCLUDE localhost\"");\n   }\n \n+  void SetUpOnMainThread() override {\n+    BrowserThread::PostTask(\n+        BrowserThread::IO, FROM_HERE,\n+        base::Bind(&net::URLRequestSlowDownloadJob::AddUrlHandler));\n+  }\n+\n  protected:\n   // Tests that a given file path sent in a ViewHostMsg_RunFileChooser will\n   // cause renderer to be killed.\n@@ -384,7 +438,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         chrome_origin_msg));\n     web_process_killed.Wait();\n   }\n@@ -404,7 +459,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         embedder_isolated_origin_msg));\n     web_process_killed.Wait();\n     SetBrowserClientForTesting(old_client);\n@@ -418,7 +474,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         invalid_origin_msg));\n     web_process_killed.Wait();\n   }\n@@ -431,10 +488,20 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         invalid_scheme_origin_msg));\n     web_process_killed.Wait();\n   }\n }\n \n+// Renderer process should not be able to create multiple requests with the same\n+// id.\n+IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidRequestId) {\n+  // Existing loader in pending_loaders_.\n+  TryCreateDuplicateRequestIds(shell(), false);\n+  // Existing loader in blocked_loaders_map_.\n+  TryCreateDuplicateRequestIds(shell(), true);\n+}\n+\n }  // namespace content""}<_**next**_>{""sha"": ""9f330134f0b052a4f6a8c6aa85e90c8230edaae6"", ""filename"": ""tools/metrics/histograms/histograms.xml"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/tools/metrics/histograms/histograms.xml"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/tools/metrics/histograms/histograms.xml"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/tools/metrics/histograms/histograms.xml?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -57944,6 +57944,7 @@ http://cs/file:chrome/histograms.xml - but prefer this file for new entries.\n   <int value=\""105\"" label=\""BDH_DEVICE_NOT_ALLOWED_FOR_ORIGIN\""/>\n   <int value=\""106\"" label=\""ACI_WRONG_STORAGE_PARTITION\""/>\n   <int value=\""107\"" label=\""RDHI_WRONG_STORAGE_PARTITION\""/>\n+  <int value=\""108\"" label=\""RDH_INVALID_REQUEST_ID\""/>\n </enum>\n \n <enum name=\""BadMessageReasonExtensions\"" type=\""int\"">""}","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
",C,"  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

",,,"@@ -1187,6 +1187,20 @@ void ResourceDispatcherHostImpl::OnSyncLoad(
                sync_result->routing_id());
 }
 
+bool ResourceDispatcherHostImpl::IsRequestIDInUse(
+    const GlobalRequestID& id) const {
+  if (pending_loaders_.find(id) != pending_loaders_.end())
+    return true;
+  for (const auto& blocked_loaders : blocked_loaders_map_) {
+    for (const auto& loader : *blocked_loaders.second.get()) {
+      ResourceRequestInfoImpl* info = loader->GetRequestInfo();
+      if (info->GetGlobalRequestID() == id)
+        return true;
+    }
+  }
+  return false;
+}
+
 void ResourceDispatcherHostImpl::UpdateRequestForTransfer(
     int child_id,
     int route_id,
@@ -1281,6 +1295,13 @@ void ResourceDispatcherHostImpl::BeginRequest(
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
+  // Reject request id that's currently in use.
+  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
+    bad_message::ReceivedBadMessage(filter_,
+                                    bad_message::RDH_INVALID_REQUEST_ID);
+    return;
+  }
+
   // PlzNavigate: reject invalid renderer main resource request.
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&",Chrome,1af4fada49c4f3890f16daac31d38379a9d782b2,f5d1a9c599c7cd6833455eacb4ec694a47c1f1b0,1,"void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
//fix_flaw_line_below:
//  // Reject request id that's currently in use.
//fix_flaw_line_below:
//  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
//fix_flaw_line_below:
//    bad_message::ReceivedBadMessage(filter_,
//fix_flaw_line_below:
//                                    bad_message::RDH_INVALID_REQUEST_ID);
//fix_flaw_line_below:
//    return;
//fix_flaw_line_below:
//  }
//fix_flaw_line_below:
//
   // PlzNavigate: reject invalid renderer main resource request.
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  // Reject invalid priority.
  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  // If we crash here, figure out what URL the renderer was requesting.
  // http://crbug.com/91398
  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  // If the request that's coming in is being transferred from another process,
  // we want to reuse and resume the old loader rather than start a new one.
  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    // If the request is transferring to a new process, we can update our
    // state and let it resume with its existing ResourceHandlers.
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  // http://crbug.com/90971
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  // Parse the headers before calling ShouldServiceRequest, so that they are
  // available to be validated.
  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  // Allow the observer to block/handle the request.
  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  // Construct the request.
  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  // If the request is a MAIN_FRAME request, the first-party URL gets updated on
  // redirects.
  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  // Resolve elements from request_body and prepare upload data.
  if (request_data.request_body.get()) {
    // |blob_context| could be null when the request is from the plugins because
    // ResourceMessageFilters created in PluginProcessHost don't have the blob
    // context.
    if (blob_context) {
      // Attaches the BlobDataHandles to request_body not to free the blobs and
      // any attached shareable files until upload completion. These data will
      // be used in UploadDataStream and ServiceWorkerURLRequestJob.
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  // Raw headers are sensitive, as they include Cookie/Set-Cookie, so only
  // allow requesting them if requester has ReadRawCookies permission.
  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    // TODO: crbug.com/523063 can we call bad_message::ReceivedBadMessage here?
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    // Prevent third-party image content from prompting for login, as this
    // is often a scam to extract credentials for another domain from the user.
    // Only block image loads, as the attack applies largely to the ""src""
    // property of the <img> tag. It is common for web properties to allow
    // untrusted values for <img src>; this is considered a fair thing for an
    // HTML sanitizer to do. Conversely, any HTML sanitizer that didn't
    // filter sources for <script>, <link>, <embed>, <object>, <iframe> tags
    // would be considered vulnerable in and of itself.
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  // Sync loads should have maximum priority and should be the only
  // requets that have the ignore limits flag set.
  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  // Make extra info and read footer (contains request ID).
  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  // Request takes ownership.
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    // Hang on to a reference to ensure the blob is not released prior
    // to the job being started.
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  // Initialize the service worker handler for the request. We don't use
  // ServiceWorker for synchronous loads to avoid renderer deadlocks.
  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  // Have the appcache associate its extra info with the request.
  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
",186239,"void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","The `BeginRequest` function in the `ResourceDispatcherHostImpl` class initiates a resource request by validating input parameters, managing request priorities, and handling potential transfers of existing requests. It checks for browser-side navigation and URL validity, ensuring that the request adheres to specified priority limits. The function constructs a new `URLRequest` object, setting various properties such as method, headers, and upload data, while also managing security policies and load flags. Additionally, it initializes service worker handlers and app cache interceptors, ultimately creating a resource handler to process the request through the `BeginRequestInternal` method.","The provided code defines a method that initiates a resource request by validating various parameters, including the request's URL and priority. It checks for specific conditions related to navigation and resource types, ensuring compliance with security policies. The method constructs a new request, setting its properties such as method and headers, while managing potential uploads and handling service worker interactions. It also incorporates mechanisms for error handling and request transfer, ultimately creating a handler to process the request further, ensuring that all necessary contexts and configurations are appropriately set before execution.","The vulnerability identified as CWE-362, which pertains to a race condition, arises when a request ID is reused while a request is still in progress. In the vulnerable code, there was no check to ensure that the request ID was not already in use, potentially allowing multiple requests to be processed simultaneously with the same ID. The fixed code addresses this issue by introducing a validation step that rejects any request ID currently in use, thereby preventing the processing of duplicate requests and ensuring that each request is uniquely identified, thus mitigating the risk of race conditions and associated security issues.","The vulnerability identified as CWE-362 involves a race condition where a request ID could be reused while a request was still being processed, leading to potential conflicts and unpredictable behavior. In the original code, there was no mechanism to check if a request ID was already in use, allowing multiple requests to be initiated with the same ID. The fixed code introduces a validation step that explicitly rejects any request ID that is currently active, ensuring that each request is uniquely identified and processed sequentially, thereby eliminating the risk of race conditions and enhancing overall system stability.","The vulnerability was fixed by adding a check at the beginning of the request handling process to determine if the request ID is already in use. Specifically, the code now calls the function `IsRequestIDInUse` with the current request ID, which constructs a global request ID based on the child ID and request ID. If the check returns true, the code invokes `bad_message::ReceivedBadMessage` to reject the request, signaling an invalid request ID. This ensures that no new requests can be processed with an ID that is already active, effectively preventing race conditions.","The vulnerability was addressed by implementing a validation step that checks whether a request ID is currently in use before processing a new request. This involves introducing a mechanism to verify the uniqueness of the request ID, ensuring that no two requests can share the same identifier while one is still active. If the check determines that the request ID is already in use, the system rejects the new request and signals an error. This change effectively prevents race conditions by ensuring that each request is uniquely identified and processed in a controlled manner."
