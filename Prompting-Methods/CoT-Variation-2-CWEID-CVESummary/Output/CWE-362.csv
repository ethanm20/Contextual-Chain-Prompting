,Unnamed: 0.1,Unnamed: 0,Access Gained,Attack Origin,Authentication Required,Availability,CVE ID,CVE Page,CWE ID,Complexity,Confidentiality,Integrity,Known Exploits,Publish Date,Score,Summary,Update Date,Vulnerability Classification,add_lines,codeLink,commit_id,commit_message,del_lines,file_name,files_changed,func_after,func_before,lang,lines_after,lines_before,parentID,patch,project,project_after,project_before,vul,vul_func_with_fix,idx,primevul_func_before_fix,primevul_func_after_fix,C1_Description_of_Functionality_In_Context,C2_Description_of_Functionality_Generic,C3_Explanation_of_Vulnerability_In_Context,C4_Explanation_of_Vulnerability_Generic,C5_Explanation_Vulnerability_Fixed_In_Context,C6_Explanation_Vulnerability_Fixed_Generic
0,177805,177805,,Local,Not required,Partial,CVE-2018-19489,https://www.cvedetails.com/cve/CVE-2018-19489/,CWE-362,Low,,,,2018-12-13,2.1,v9fs_wstat in hw/9pfs/9p.c in QEMU allows guest OS users to cause a denial of service (crash) because of a race condition during file renaming.,2019-05-31,DoS,1,https://git.qemu.org/?p=qemu.git;a=commit;h=1d20398694a3b67a388d955b7a945ba4aa90a8a8,1d20398694a3b67a388d955b7a945ba4aa90a8a8,,0,,,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
",C,"    V9fsState *s = pdu->s;
",,5298f4d67a911dd9cefa4c4185eed242074d64c2,"@@ -2855,6 +2855,7 @@ static void coroutine_fn v9fs_wstat(void *opaque)
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
+    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
@@ -2920,7 +2921,9 @@ static void coroutine_fn v9fs_wstat(void *opaque)
         }
     }
     if (v9stat.name.size != 0) {
+        v9fs_path_write_lock(s);
         err = v9fs_complete_rename(pdu, fidp, -1, &v9stat.name);
+        v9fs_path_unlock(s);
         if (err < 0) {
             goto out;
         }",qemu,https://git.qemu.org/?p=qemu.git;a=blob;f=hw/9pfs/9p.c;h=bdf7919abfc517ae90b41c66d965137f1c7ee510;hb=bdf7919abfc517ae90b41c66d965137f1c7ee510,https://git.qemu.org/?p=qemu.git;a=blob;f=hw/9pfs/9p.c;h=267a25533b775b7e9a95c7bbd44a17914842e3cf;hb=267a25533b775b7e9a95c7bbd44a17914842e3cf,1,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
//fix_flaw_line_below:
//    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
",177805,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","The provided code defines a coroutine function named `v9fs_wstat`, which is designed to handle a specific operation related to file status updates in a V9FS (Virtual 9P File System) context. It initializes a `V9fsStat` structure and attempts to unmarshal data from a protocol data unit (PDU) starting at a specified offset. The function extracts a file identifier (`fid`), an unused value, and the `v9stat` structure from the PDU. If the unmarshalling process encounters an error, the function is set to jump to a label (`out_nofid`), indicating that further error handling or cleanup may be required.","The code defines a coroutine function that processes a file status update operation within a virtual file system. It initializes a structure to hold file status information and attempts to extract relevant data from a provided data packet, starting at a specific position. This extraction includes a file identifier and an unused value, along with the status information. If the extraction fails, the function is prepared to handle the error by transitioning to a designated cleanup or error handling section, ensuring that any necessary follow-up actions can be taken based on the outcome of the data processing.","The vulnerability in the original code arises from a race condition during file renaming, where concurrent access to shared resources could lead to inconsistent states, potentially causing a denial of service. Specifically, the lack of proper synchronization when accessing the file identifier (`fid`) and the associated state could allow multiple threads to manipulate the same resource simultaneously. In the fixed code, the addition of a reference to the `V9fsState` structure from the `pdu` ensures that the function has access to the correct state context, which likely includes mechanisms for managing concurrency and preventing race conditions, thereby enhancing stability and security during file operations.","The vulnerability in the original code stems from a race condition that occurs during file renaming, allowing multiple threads to access and modify shared resources simultaneously. This lack of synchronization can lead to inconsistent states, ultimately resulting in a denial of service, such as crashes. In the revised code, the introduction of a reference to a state structure enhances the context management, likely incorporating mechanisms to ensure proper synchronization and control over concurrent access. This change mitigates the risk of race conditions, thereby improving the stability and security of file operations within the system.","The vulnerability was fixed by adding a reference to the `V9fsState` structure from the `pdu` in the revised code. This change allows the function to access the correct state context, which is crucial for managing concurrent operations safely. By ensuring that the function has a clear reference to the state, it can implement necessary synchronization mechanisms to prevent race conditions when accessing shared resources, such as the file identifier (`fid`). This enhancement helps maintain data integrity during file renaming operations, thereby reducing the risk of crashes and improving overall system stability.","The vulnerability was addressed by incorporating a reference to a state management structure, which enhances the function's ability to handle concurrent operations safely. This addition allows the code to maintain proper context during file operations, ensuring that access to shared resources is managed effectively. By implementing necessary synchronization mechanisms, the revised code mitigates the risk of race conditions that could lead to inconsistent states and potential crashes. This improvement enhances the overall stability and security of the system during critical operations, particularly during file renaming processes."
1,177889,177889,,Remote,Not required,Partial,CVE-2015-3196,https://www.cvedetails.com/cve/CVE-2015-3196/,CWE-362,Medium,,,,2015-12-06,4.3,"ssl/s3_clnt.c in OpenSSL 1.0.0 before 1.0.0t, 1.0.1 before 1.0.1p, and 1.0.2 before 1.0.2d, when used for a multi-threaded client, writes the PSK identity hint to an incorrect data structure, which allows remote servers to cause a denial of service (race condition and double free) via a crafted ServerKeyExchange message.",2019-06-13,DoS,1,https://git.openssl.org/?p=openssl.git;a=commit;h=3c66a669dfc7b3792f7af0758ea26fe8502ce70c,3c66a669dfc7b3792f7af0758ea26fe8502ce70c,,1,,,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strdup((char *)p);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
",C,"        s->session->psk_identity = BUF_strndup((char *)p, i);
","        s->session->psk_identity = BUF_strdup((char *)p);
",f8f75a7e4a7746caba9cab843e14ed6a41dfad21,"@@ -2780,7 +2780,7 @@ int ssl3_get_client_key_exchange(SSL *s)
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
-        s->session->psk_identity = BUF_strdup((char *)p);
+        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
",openssl,https://git.openssl.org/?p=openssl.git;a=blob;f=ssl/s3_srvr.c;h=acd3b9e964a239b8b01f7aa84179c8e4f09a0e14;hb=3c66a669dfc7b3792f7af0758ea26fe8502ce70c,https://git.openssl.org/?p=openssl.git;a=blob;f=ssl/s3_srvr.c;h=8885694c02b34522d8acb980727c4a7841716993,1,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
//flaw_line_below:
        s->session->psk_identity = BUF_strdup((char *)p);
//fix_flaw_line_below:
//        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
",177889,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strdup((char *)p);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","The `ssl3_get_client_key_exchange` function is part of the SSL/TLS handshake process, responsible for handling the client's key exchange message. It begins by retrieving the message and determining the key exchange algorithm used (e.g., RSA, DH, ECDH, PSK). Depending on the algorithm, it performs specific operations such as decrypting the received key material, computing shared secrets, and generating a master secret. The function also includes error handling for various scenarios, ensuring that the handshake can proceed or fail gracefully. Ultimately, it establishes a secure session by generating a master key based on the exchanged keys.","The provided code implements a crucial part of the SSL/TLS handshake process, specifically handling the client's key exchange message. It identifies the key exchange method used and processes the received data accordingly, which may involve decrypting a session key or computing a shared secret based on the chosen algorithm. The function ensures secure communication by generating a master secret from the exchanged keys while incorporating error handling to manage various failure scenarios. Ultimately, it establishes a secure session, enabling encrypted communication between the client and server.","The vulnerability identified in the code relates to a race condition and potential double free error when handling the PSK identity hint in a multi-threaded environment. Specifically, the code incorrectly writes the PSK identity hint to a shared data structure, which can lead to inconsistencies and memory management issues when multiple threads access it simultaneously. In the fixed code, the handling of the PSK identity hint was modified to ensure that it is stored in a thread-safe manner, preventing concurrent access issues and ensuring proper memory allocation and deallocation, thus mitigating the risk of denial of service attacks.","The vulnerability arises from improper handling of the PSK identity hint in a multi-threaded environment, where concurrent access to shared data structures can lead to race conditions and double free errors. This flaw allows remote servers to exploit the situation by sending crafted messages, potentially causing a denial of service. The fix involved ensuring that the PSK identity hint is written to a correctly allocated and thread-safe data structure, thereby preventing simultaneous access issues and ensuring proper memory management. This change mitigates the risk of exploitation and enhances the stability of the application in multi-threaded scenarios.","The vulnerability was fixed by modifying how the PSK identity hint is managed within the code. Specifically, the fix ensures that the PSK identity is allocated and freed correctly, preventing race conditions and double free errors. The code now uses `BUF_strndup` to safely duplicate the PSK identity string, ensuring proper memory allocation. Additionally, the previous handling of the PSK identity hint was adjusted to ensure that it is not written to a shared structure without proper synchronization. This change enhances memory safety and stability, particularly in multi-threaded environments, by ensuring that the identity hint is managed correctly.","The vulnerability was addressed by implementing a more robust memory management strategy for handling the PSK identity hint. The fix involved ensuring that the identity hint is allocated in a way that prevents concurrent access issues, thereby eliminating the risk of race conditions and double free errors. This was achieved by using safer memory allocation techniques that guarantee proper duplication and cleanup of the identity string. Additionally, the code was modified to ensure that the identity hint is stored in a thread-safe manner, enhancing the overall stability and security of the application in multi-threaded environments."
2,178347,178347,,Remote,Not required,Partial,CVE-2014-3509,https://www.cvedetails.com/cve/CVE-2014-3509/,CWE-362,Medium,Partial,Partial,,2014-08-13,6.8,"Race condition in the ssl_parse_serverhello_tlsext function in t1_lib.c in OpenSSL 1.0.0 before 1.0.0n and 1.0.1 before 1.0.1i, when multithreading and session resumption are used, allows remote SSL servers to cause a denial of service (memory overwrite and client application crash) or possibly have unspecified other impact by sending Elliptic Curve (EC) Supported Point Formats Extension data.",2017-11-14,DoS,10,https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,,7,,,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       if (!s->hit)
                                {
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                {
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
",C,"                       if (!s->hit)
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
","                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
",0042fb5fd1c9d257d713b15a1f45da05cf5c1c87,"@@ -2647,15 +2647,18 @@ static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
-                       s->session->tlsext_ecpointformatlist_length = 0;
-                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
-                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
+                       if (!s->hit)
                                {
-                               *al = TLS1_AD_INTERNAL_ERROR;
-                               return 0;
+                               s->session->tlsext_ecpointformatlist_length = 0;
+                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
+                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
+                                       {
+                                       *al = TLS1_AD_INTERNAL_ERROR;
+                                       return 0;
+                                       }
+                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
+                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
-                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
-                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;",openssl,https://git.openssl.org/gitweb/?p=openssl.git;a=blob;f=ssl/t1_lib.c;h=749d88d1a9d9a6faa535437c236104497f0d1a66;hb=fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,https://git.openssl.org/gitweb/?p=openssl.git;a=blob;f=ssl/t1_lib.c;h=4374d6aadd3d5b0804d13d338b27408d8102e8b5,1,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
//flaw_line_below:
                       s->session->tlsext_ecpointformatlist_length = 0;
//flaw_line_below:
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
//flaw_line_below:
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
//fix_flaw_line_below:
//                       if (!s->hit)
                                {
//flaw_line_below:
                               *al = TLS1_AD_INTERNAL_ERROR;
//flaw_line_below:
                               return 0;
//fix_flaw_line_below:
//                               s->session->tlsext_ecpointformatlist_length = 0;
//fix_flaw_line_below:
//                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
//fix_flaw_line_below:
//                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
//fix_flaw_line_below:
//                                       {
//fix_flaw_line_below:
//                                       *al = TLS1_AD_INTERNAL_ERROR;
//fix_flaw_line_below:
//                                       return 0;
//fix_flaw_line_below:
//                                       }
//fix_flaw_line_below:
//                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
//fix_flaw_line_below:
//                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
//flaw_line_below:
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
//flaw_line_below:
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
",178347,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                {
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       if (!s->hit)
                                {
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","The `ssl_scan_serverhello_tlsext` function processes the TLS extensions present in a ServerHello message during the SSL/TLS handshake. It begins by initializing various flags and checking for the presence of specific extensions, such as server name indication, application layer protocol negotiation, and session tickets. The function validates the length and type of each extension, handling them accordingly, including memory management for dynamic data. It also checks for conditions that may lead to errors, such as unsupported extensions or incorrect sizes. Finally, it ensures that renegotiation is handled securely, returning appropriate error codes when necessary.","The provided code processes TLS extensions from a ServerHello message during the SSL/TLS handshake. It initializes various flags and checks for specific extensions, validating their lengths and types while managing memory for dynamic data. The code handles different extension types, ensuring proper responses for server name indications, application layer protocol negotiations, and session tickets, among others. It also includes error handling for unsupported or incorrectly formatted extensions. Additionally, the code enforces security measures related to renegotiation, returning appropriate error codes when necessary to maintain a secure connection.","The vulnerability, identified as a race condition, arises from improper handling of shared resources in a multithreaded environment, particularly when processing the Elliptic Curve Supported Point Formats Extension. In the vulnerable code, concurrent access to session data could lead to memory overwrites, causing application crashes or denial of service. The fixed code addresses this by ensuring that the session's extension data is only modified when the session is not in a resumed state, as indicated by the check for the `hit` variable. This prevents simultaneous modifications, thereby eliminating the race condition and enhancing overall stability and security.","The vulnerability is a race condition that occurs in a multithreaded environment during the processing of specific TLS extensions, which can lead to memory overwrites and application crashes. This happens when multiple threads access and modify shared session data simultaneously, particularly during session resumption. The fix addresses this issue by implementing checks to ensure that modifications to the session data only occur when the session is not in a resumed state. This prevents concurrent access and modifications, thereby eliminating the race condition and enhancing the stability and security of the application.","The vulnerability was fixed by adding a condition to check the `hit` variable before modifying the session's extension data. In the updated code, modifications to the Elliptic Curve Supported Point Formats Extension are only allowed if the session is not in a resumed state, as indicated by `if (!s->hit)`. This ensures that shared session data is not concurrently accessed or altered by multiple threads, thereby preventing race conditions. Additionally, the code includes proper memory management to handle the allocation and deallocation of extension data, further enhancing stability and security during multithreaded operations.","The vulnerability was fixed by implementing a check to ensure that modifications to session extension data only occur when the session is not in a resumed state. This prevents concurrent access to shared resources in a multithreaded environment, thereby eliminating the race condition that could lead to memory overwrites and application crashes. Additionally, the fix includes proper memory management practices to ensure that any allocated resources are appropriately handled, further enhancing the stability and security of the application during operations that involve session resumption and multithreading."
3,178408,178408,,Local,Not required,,CVE-2011-4029,https://www.cvedetails.com/cve/CVE-2011-4029/,CWE-362,Medium,Partial,,,2012-07-03,1.9,"The LockServer function in os/utils.c in X.Org xserver before 1.11.2 allows local users to change the permissions of arbitrary files to 444, read those files, and possibly cause a denial of service (removed execution permission) via a symlink attack on a temporary lock file.",2012-07-17,DoS,1,https://cgit.freedesktop.org/xorg/xserver/commit/?id=b67581cf825940fdf52bf2e0af4330e695d724a4,b67581cf825940fdf52bf2e0af4330e695d724a4,,1,,,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) chmod(tmp, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
",C,"  (void) fchmod(lfd, 0444);
","  (void) chmod(tmp, 0444);
",6ba44b91e37622ef8c146d8f2ac92d708a18ed34,"@@ -297,7 +297,7 @@ LockServer(void)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
-  (void) chmod(tmp, 0444);
+  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*",xserver,https://cgit.freedesktop.org/xorg/xserver/tree/os/utils.c?id=b67581cf825940fdf52bf2e0af4330e695d724a4,https://cgit.freedesktop.org/xorg/xserver/tree/os/utils.c?id=6ba44b91e37622ef8c146d8f2ac92d708a18ed34,1,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
//flaw_line_below:
  (void) chmod(tmp, 0444);
//fix_flaw_line_below:
//  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
",178408,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) chmod(tmp, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","The `LockServer` function is designed to manage a locking mechanism for a server process, ensuring that only one instance runs for a given display. It first constructs a temporary lock file path based on predefined constants and the display number. The function attempts to create this temporary file, writing the current process ID (PID) to it, and then changes its permissions. It then tries to create a hard link to establish the actual lock file, checking for existing locks by reading the PID from the lock file and verifying if the process is still active. If the lock cannot be established, it handles stale locks or reports an error if another instance is running.","The code implements a locking mechanism for a server process to ensure that only one instance operates for a specific display. It constructs a temporary file path and attempts to create a file that contains the current process ID. After successfully creating this file, it tries to establish a permanent lock by linking the temporary file to a designated lock file. The code checks for existing locks by reading the process ID from the lock file and verifying if the associated process is still active. If another instance is detected, it reports an error; otherwise, it finalizes the lock.","The vulnerability in the original code arises from the use of a temporary lock file that can be manipulated through a symlink attack, allowing an attacker to redirect the lock file creation to an arbitrary file. This could lead to unauthorized permission changes, such as setting the permissions of sensitive files to read-only. The fixed code addresses this issue by using the `O_NOFOLLOW` flag when opening the lock file, which prevents the following of symlinks. This ensures that the lock file is created only for the intended target, mitigating the risk of unauthorized access and potential denial of service.","The vulnerability in the original code stems from the use of a temporary lock file that can be exploited through a symlink attack, allowing an attacker to redirect the lock file creation to an arbitrary file. This could enable unauthorized users to change the permissions of sensitive files, potentially making them read-only and leading to a denial of service. The fixed code mitigates this risk by implementing a safeguard that prevents following symlinks when accessing the lock file, ensuring that the lock file is created only for the intended target and protecting against unauthorized access and manipulation.","The vulnerability was fixed by adding the `O_NOFOLLOW` flag when opening the lock file, which prevents the system from following symlinks. In the updated code, when the lock file is accessed, this flag ensures that if a symlink points to a different file, the operation will fail rather than inadvertently modifying the permissions of an arbitrary file. This change effectively protects against the symlink attack that could allow local users to change file permissions. Additionally, the use of `unlink` for the temporary file ensures that it is removed after the locking process, further enhancing security.","The vulnerability was fixed by implementing a safeguard that prevents the following of symbolic links when accessing the lock file. This change ensures that any attempt to manipulate the lock file through a symlink attack will fail, thereby protecting sensitive files from unauthorized permission changes. Additionally, the code was modified to ensure that temporary files are properly removed after their use, further reducing the risk of exploitation. These enhancements collectively strengthen the security of the locking mechanism, ensuring that it operates only on the intended files and preventing potential denial of service attacks."
4,178753,178753,,Local,Not required,Complete,CVE-2012-3511,https://www.cvedetails.com/cve/CVE-2012-3511/,CWE-362,High,Complete,Complete,,2012-10-03,6.2,Multiple race conditions in the madvise_remove function in mm/madvise.c in the Linux kernel before 3.4.5 allow local users to cause a denial of service (use-after-free and system crash) via vectors involving a (1) munmap or (2) close system call.,2013-10-23,DoS ,13,https://github.com/torvalds/linux/commit/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,"mm: Hold a file reference in madvise_remove

Otherwise the code races with munmap (causing a use-after-free
of the vma) or with close (causing a use-after-free of the struct
file).

The bug was introduced by commit 90ed52ebe481 (""[PATCH] holepunch: fix
mmap_sem i_mutex deadlock"")

Cc: Hugh Dickins <hugh@veritas.com>
Cc: Miklos Szeredi <mszeredi@suse.cz>
Cc: Badari Pulavarty <pbadari@us.ibm.com>
Cc: Nick Piggin <npiggin@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Andy Lutomirski <luto@amacapital.net>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",4,mm/madvise.c,"{""sha"": ""14d260fa0d17939a2279c244df91789cd30720e4"", ""filename"": ""mm/madvise.c"", ""status"": ""modified"", ""additions"": 14, ""deletions"": 4, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb/mm/madvise.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb/mm/madvise.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/madvise.c?ref=9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb"", ""patch"": ""@@ -15,6 +15,7 @@\n #include <linux/sched.h>\n #include <linux/ksm.h>\n #include <linux/fs.h>\n+#include <linux/file.h>\n \n /*\n  * Any behaviour which results in changes to the vma->vm_flags needs to\n@@ -204,14 +205,16 @@ static long madvise_remove(struct vm_area_struct *vma,\n {\n \tloff_t offset;\n \tint error;\n+\tstruct file *f;\n \n \t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n \n \tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n \t\treturn -EINVAL;\n \n-\tif (!vma->vm_file || !vma->vm_file->f_mapping\n-\t\t|| !vma->vm_file->f_mapping->host) {\n+\tf = vma->vm_file;\n+\n+\tif (!f || !f->f_mapping || !f->f_mapping->host) {\n \t\t\treturn -EINVAL;\n \t}\n \n@@ -221,11 +224,18 @@ static long madvise_remove(struct vm_area_struct *vma,\n \toffset = (loff_t)(start - vma->vm_start)\n \t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n \n-\t/* filesystem's fallocate may need to take i_mutex */\n+\t/*\n+\t * Filesystem's fallocate may need to take i_mutex.  We need to\n+\t * explicitly grab a reference because the vma (and hence the\n+\t * vma's reference to the file) can go away as soon as we drop\n+\t * mmap_sem.\n+\t */\n+\tget_file(f);\n \tup_read(&current->mm->mmap_sem);\n-\terror = do_fallocate(vma->vm_file,\n+\terror = do_fallocate(f,\n \t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n \t\t\t\toffset, end - start);\n+\tfput(f);\n \tdown_read(&current->mm->mmap_sem);\n \treturn error;\n }""}","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/* filesystem's fallocate may need to take i_mutex */
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(vma->vm_file,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
",C,"	struct file *f;
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
	error = do_fallocate(f,
	fput(f);
","	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
	/* filesystem's fallocate may need to take i_mutex */
	error = do_fallocate(vma->vm_file,
",,"@@ -15,6 +15,7 @@
 #include <linux/sched.h>
 #include <linux/ksm.h>
 #include <linux/fs.h>
+#include <linux/file.h>
 
 /*
  * Any behaviour which results in changes to the vma->vm_flags needs to
@@ -204,14 +205,16 @@ static long madvise_remove(struct vm_area_struct *vma,
 {
 	loff_t offset;
 	int error;
+	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
-	if (!vma->vm_file || !vma->vm_file->f_mapping
-		|| !vma->vm_file->f_mapping->host) {
+	f = vma->vm_file;
+
+	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
@@ -221,11 +224,18 @@ static long madvise_remove(struct vm_area_struct *vma,
 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
-	/* filesystem's fallocate may need to take i_mutex */
+	/*
+	 * Filesystem's fallocate may need to take i_mutex.  We need to
+	 * explicitly grab a reference because the vma (and hence the
+	 * vma's reference to the file) can go away as soon as we drop
+	 * mmap_sem.
+	 */
+	get_file(f);
 	up_read(&current->mm->mmap_sem);
-	error = do_fallocate(vma->vm_file,
+	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
+	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }",linux,9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,1b7fa4c27111757789b21bb78543317dad4cfd08,1,"static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
//fix_flaw_line_below:
//	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
//flaw_line_below:
	if (!vma->vm_file || !vma->vm_file->f_mapping
//flaw_line_below:
		|| !vma->vm_file->f_mapping->host) {
//fix_flaw_line_below:
//	f = vma->vm_file;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
//flaw_line_below:
	/* filesystem's fallocate may need to take i_mutex */
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * Filesystem's fallocate may need to take i_mutex.  We need to
//fix_flaw_line_below:
//	 * explicitly grab a reference because the vma (and hence the
//fix_flaw_line_below:
//	 * vma's reference to the file) can go away as soon as we drop
//fix_flaw_line_below:
//	 * mmap_sem.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	get_file(f);
 	up_read(&current->mm->mmap_sem);
//flaw_line_below:
	error = do_fallocate(vma->vm_file,
//fix_flaw_line_below:
//	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
//fix_flaw_line_below:
//	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
",178753,"static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/* filesystem's fallocate may need to take i_mutex */
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(vma->vm_file,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","The `madvise_remove` function is designed to handle memory management advice for a specific virtual memory area (VMA) in a Linux kernel context. It first checks the VMA's flags to ensure it is not locked, nonlinear, or using huge pages, returning an error if any of these conditions are met. The function also verifies that the VMA is associated with a valid file and that it has the appropriate shared and writable permissions. If these checks pass, it calculates the offset for the specified range and invokes `do_fallocate` to punch a hole in the file, effectively removing the specified memory range while maintaining the file size.","The provided code defines a function that manages memory advice for a specific area in a process's virtual memory. It begins by ensuring that the memory area is not locked or using special page types, returning an error if any restrictions apply. The function checks for a valid associated file and verifies that the memory area has the correct permissions for shared and writable access. If all conditions are met, it calculates the necessary offset and calls a system function to remove a specified range of memory from the file, while keeping the overall file size unchanged.","The vulnerability in the original code arises from a race condition where the reference to the file associated with the memory area could be released while the function is executing, leading to a use-after-free scenario. This occurs because the function drops the memory map semaphore before performing file operations, allowing other processes to potentially close the file or unmap the memory area. The fixed code addresses this issue by explicitly obtaining a reference to the file using `get_file(f)` before releasing the semaphore, ensuring that the file remains valid throughout the operation, and subsequently releasing the reference with `fput(f)` after the operation is complete.","The vulnerability in the original code stems from race conditions that occur when the function interacts with a file associated with a memory area. Specifically, the function could release its reference to the memory area, allowing other processes to close the file or unmap the memory, which could lead to a use-after-free situation and potentially crash the system. The fixed code mitigates this issue by ensuring that a reference to the file is obtained before releasing the memory map semaphore, thus maintaining the validity of the file throughout the operation and preventing any unintended access to freed memory.","The vulnerability was fixed by introducing a reference counting mechanism for the file associated with the memory area. In the updated code, a reference to the file is obtained using `get_file(f)` before the memory map semaphore is released with `up_read(&current->mm->mmap_sem)`. This ensures that the file remains valid during the execution of the `do_fallocate` function, preventing any race conditions that could lead to use-after-free errors. After the file operation is completed, the reference is released with `fput(f)`, ensuring proper resource management and enhancing the stability of the function.","The vulnerability was fixed by implementing a reference counting mechanism for the file associated with the memory area. The updated code ensures that a reference to the file is obtained before releasing the semaphore that controls access to the memory map. This change prevents the file from being closed or the memory area from being unmapped while the function is executing, thereby eliminating the risk of accessing freed memory. After the necessary operations are completed, the reference is properly released, ensuring that resources are managed correctly and enhancing the overall stability of the function."
5,179043,179043,,Local,Not required,Complete,CVE-2011-2183,https://www.cvedetails.com/cve/CVE-2011-2183/,CWE-362,High,,,,2012-06-13,4.0,"Race condition in the scan_get_next_rmap_item function in mm/ksm.c in the Linux kernel before 2.6.39.3, when Kernel SamePage Merging (KSM) is enabled, allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via a crafted application.",2012-06-14,DoS ,6,https://github.com/torvalds/linux/commit/2b472611a32a72f4a118c069c2d62a1a3f087afd,2b472611a32a72f4a118c069c2d62a1a3f087afd,"ksm: fix NULL pointer dereference in scan_get_next_rmap_item()

Andrea Righi reported a case where an exiting task can race against
ksmd::scan_get_next_rmap_item (http://lkml.org/lkml/2011/6/1/742) easily
triggering a NULL pointer dereference in ksmd.

ksm_scan.mm_slot == &ksm_mm_head with only one registered mm

CPU 1 (__ksm_exit)		CPU 2 (scan_get_next_rmap_item)
 				list_empty() is false
lock				slot == &ksm_mm_head
list_del(slot->mm_list)
(list now empty)
unlock
				lock
				slot = list_entry(slot->mm_list.next)
				(list is empty, so slot is still ksm_mm_head)
				unlock
				slot->mm == NULL ... Oops

Close this race by revalidating that the new slot is not simply the list
head again.

Andrea's test case:

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/mman.h>

#define BUFSIZE getpagesize()

int main(int argc, char **argv)
{
	void *ptr;

	if (posix_memalign(&ptr, getpagesize(), BUFSIZE) < 0) {
		perror(""posix_memalign"");
		exit(1);
	}
	if (madvise(ptr, BUFSIZE, MADV_MERGEABLE) < 0) {
		perror(""madvise"");
		exit(1);
	}
	*(char *)NULL = 0;

	return 0;
}

Reported-by: Andrea Righi <andrea@betterlinux.com>
Tested-by: Andrea Righi <andrea@betterlinux.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
Signed-off-by: Hugh Dickins <hughd@google.com>
Signed-off-by: Chris Wright <chrisw@sous-sol.org>
Cc: <stable@kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,mm/ksm.c,"{""sha"": ""9a68b0cf0a1c4c8009ee25d2990530d7e2927132"", ""filename"": ""mm/ksm.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 0, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/2b472611a32a72f4a118c069c2d62a1a3f087afd/mm/ksm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/2b472611a32a72f4a118c069c2d62a1a3f087afd/mm/ksm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/ksm.c?ref=2b472611a32a72f4a118c069c2d62a1a3f087afd"", ""patch"": ""@@ -1302,6 +1302,12 @@ static struct rmap_item *scan_get_next_rmap_item(struct page **page)\n \t\tslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\n \t\tksm_scan.mm_slot = slot;\n \t\tspin_unlock(&ksm_mmlist_lock);\n+\t\t/*\n+\t\t * Although we tested list_empty() above, a racing __ksm_exit\n+\t\t * of the last mm on the list may have removed it since then.\n+\t\t */\n+\t\tif (slot == &ksm_mm_head)\n+\t\t\treturn NULL;\n next_mm:\n \t\tksm_scan.address = 0;\n \t\tksm_scan.rmap_list = &slot->rmap_list;""}","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
",C,"		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
",,,"@@ -1302,6 +1302,12 @@ static struct rmap_item *scan_get_next_rmap_item(struct page **page)
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
+		/*
+		 * Although we tested list_empty() above, a racing __ksm_exit
+		 * of the last mm on the list may have removed it since then.
+		 */
+		if (slot == &ksm_mm_head)
+			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;",linux,2b472611a32a72f4a118c069c2d62a1a3f087afd,c7cbb02222eccb82bfd42696b01abceddae663f2,1,"static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * Although we tested list_empty() above, a racing __ksm_exit
//fix_flaw_line_below:
//		 * of the last mm on the list may have removed it since then.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (slot == &ksm_mm_head)
//fix_flaw_line_below:
//			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
",179043,"static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","The `scan_get_next_rmap_item` function is designed to traverse and manage memory mapping structures in a Linux kernel context, specifically for Kernel Same-page Merging (KSM). It begins by checking if there are any memory management slots available for scanning. If the current slot is the head, it drains any lingering pages and prepares for a new scan. The function iterates through virtual memory areas (VMAs) to identify mergeable pages, checking their properties and flushing them as necessary. It retrieves the next reverse mapping item (rmap_item) for eligible pages, updating the scanning state accordingly. If no mergeable pages are found, it cleans up the memory slot and prepares for the next iteration, ultimately returning the found rmap_item or NULL when complete.","The provided code implements a function that scans through memory management structures to identify and manage pages eligible for merging in a memory optimization process. It begins by checking for available memory slots and, if necessary, drains lingering pages. The function iterates through virtual memory areas to locate pages that can be merged, checking their properties and performing necessary operations on them. It retrieves relevant mapping information for these pages while updating the scanning state. If no eligible pages are found, it cleans up the memory slot and prepares for the next iteration, ultimately returning the found mapping information or indicating completion.","The vulnerability in the original code arises from a race condition where the memory management slot could be removed by another thread after the initial check for an empty list, leading to a potential NULL pointer dereference when accessing the slot. This occurs because the code does not recheck the validity of the slot after acquiring a lock and before using it. The fix addresses this by adding a check immediately after acquiring the lock to ensure that the slot is still valid, returning NULL if it has been removed, thus preventing any subsequent dereference of a NULL pointer and ensuring thread safety.","The vulnerability in the original code stems from a race condition that occurs when multiple threads access shared memory management structures, potentially leading to a NULL pointer dereference. This happens because one thread can remove a memory slot after it has been checked for validity but before it is used, resulting in unsafe dereferencing. The fix addresses this issue by introducing an additional validity check after acquiring a lock, ensuring that the memory slot is still present before any operations are performed on it. This change effectively prevents the race condition and enhances the overall thread safety of the code.","The vulnerability was fixed by adding a check for the validity of the memory management slot immediately after acquiring the lock on the list. Specifically, after the initial check for an empty list, the code now verifies if the slot is still valid by comparing it to the head of the list. If the slot is found to be the head, indicating it has been removed by another thread, the function returns NULL, preventing any further dereferencing of a potentially NULL pointer. This ensures that the code safely handles concurrent modifications to the memory management structures, thus eliminating the race condition.","The vulnerability was fixed by introducing an additional validity check for the memory management slot after acquiring a lock on the list. This check ensures that the slot has not been removed by another thread since the initial verification. If the slot is found to be invalid, the function returns early, preventing any operations that could lead to dereferencing a NULL pointer. This change effectively safeguards against race conditions by ensuring that the code only proceeds with valid memory structures, thereby enhancing the overall stability and safety of the function in a multi-threaded environment."
6,179198,179198,,Local,Not required,Partial,CVE-2013-3302,https://www.cvedetails.com/cve/CVE-2013-3302/,CWE-362,Medium,Partial,Partial,,2013-04-29,4.4,Race condition in the smb_send_rqst function in fs/cifs/transport.c in the Linux kernel before 3.7.2 allows local users to cause a denial of service (NULL pointer dereference and OOPS) or possibly have unspecified other impact via vectors involving a reconnection event.,2013-05-03,DoS ,3,https://github.com/torvalds/linux/commit/ea702b80e0bbb2448e201472127288beb82ca2fe,ea702b80e0bbb2448e201472127288beb82ca2fe,"cifs: move check for NULL socket into smb_send_rqst

Cai reported this oops:

[90701.616664] BUG: unable to handle kernel NULL pointer dereference at 0000000000000028
[90701.625438] IP: [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.632167] PGD fea319067 PUD 103fda4067 PMD 0
[90701.637255] Oops: 0000 [#1] SMP
[90701.640878] Modules linked in: des_generic md4 nls_utf8 cifs dns_resolver binfmt_misc tun sg igb iTCO_wdt iTCO_vendor_support lpc_ich pcspkr i2c_i801 i2c_core i7core_edac edac_core ioatdma dca mfd_core coretemp kvm_intel kvm crc32c_intel microcode sr_mod cdrom ata_generic sd_mod pata_acpi crc_t10dif ata_piix libata megaraid_sas dm_mirror dm_region_hash dm_log dm_mod
[90701.677655] CPU 10
[90701.679808] Pid: 9627, comm: ls Tainted: G        W    3.7.1+ #10 QCI QSSC-S4R/QSSC-S4R
[90701.688950] RIP: 0010:[<ffffffff814a343e>]  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.698383] RSP: 0018:ffff88177b431bb8  EFLAGS: 00010206
[90701.704309] RAX: ffff88177b431fd8 RBX: 00007ffffffff000 RCX: ffff88177b431bec
[90701.712271] RDX: 0000000000000003 RSI: 0000000000000006 RDI: 0000000000000000
[90701.720223] RBP: ffff88177b431bc8 R08: 0000000000000004 R09: 0000000000000000
[90701.728185] R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000001
[90701.736147] R13: ffff88184ef92000 R14: 0000000000000023 R15: ffff88177b431c88
[90701.744109] FS:  00007fd56a1a47c0(0000) GS:ffff88105fc40000(0000) knlGS:0000000000000000
[90701.753137] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
[90701.759550] CR2: 0000000000000028 CR3: 000000104f15f000 CR4: 00000000000007e0
[90701.767512] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
[90701.775465] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
[90701.783428] Process ls (pid: 9627, threadinfo ffff88177b430000, task ffff88185ca4cb60)
[90701.792261] Stack:
[90701.794505]  0000000000000023 ffff88177b431c50 ffff88177b431c38 ffffffffa014fcb1
[90701.802809]  ffff88184ef921bc 0000000000000000 00000001ffffffff ffff88184ef921c0
[90701.811123]  ffff88177b431c08 ffffffff815ca3d9 ffff88177b431c18 ffff880857758000
[90701.819433] Call Trace:
[90701.822183]  [<ffffffffa014fcb1>] smb_send_rqst+0x71/0x1f0 [cifs]
[90701.828991]  [<ffffffff815ca3d9>] ? schedule+0x29/0x70
[90701.834736]  [<ffffffffa014fe6d>] smb_sendv+0x3d/0x40 [cifs]
[90701.841062]  [<ffffffffa014fe96>] smb_send+0x26/0x30 [cifs]
[90701.847291]  [<ffffffffa015801f>] send_nt_cancel+0x6f/0xd0 [cifs]
[90701.854102]  [<ffffffffa015075e>] SendReceive+0x18e/0x360 [cifs]
[90701.860814]  [<ffffffffa0134a78>] CIFSFindFirst+0x1a8/0x3f0 [cifs]
[90701.867724]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs]
[90701.875601]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs]
[90701.883477]  [<ffffffffa01578e6>] cifs_query_dir_first+0x26/0x30 [cifs]
[90701.890869]  [<ffffffffa015480d>] initiate_cifs_search+0xed/0x250 [cifs]
[90701.898354]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.904486]  [<ffffffffa01554cb>] cifs_readdir+0x45b/0x8f0 [cifs]
[90701.911288]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.917410]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.923533]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.929657]  [<ffffffff81195848>] vfs_readdir+0xb8/0xe0
[90701.935490]  [<ffffffff81195b9f>] sys_getdents+0x8f/0x110
[90701.941521]  [<ffffffff815d3b99>] system_call_fastpath+0x16/0x1b
[90701.948222] Code: 66 90 55 65 48 8b 04 25 f0 c6 00 00 48 89 e5 53 48 83 ec 08 83 fe 01 48 8b 98 48 e0 ff ff 48 c7 80 48 e0 ff ff ff ff ff ff 74 22 <48> 8b 47 28 ff 50 68 65 48 8b 14 25 f0 c6 00 00 48 89 9a 48 e0
[90701.970313] RIP  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.977125]  RSP <ffff88177b431bb8>
[90701.981018] CR2: 0000000000000028
[90701.984809] ---[ end trace 24bd602971110a43 ]---

This is likely due to a race vs. a reconnection event.

The current code checks for a NULL socket in smb_send_kvec, but that's
too late. By the time that check is done, the socket will already have
been passed to kernel_setsockopt. Move the check into smb_send_rqst, so
that it's checked earlier.

In truth, this is a bit of a half-assed fix. The -ENOTSOCK error
return here looks like it could bubble back up to userspace. The locking
rules around the ssocket pointer are really unclear as well. There are
cases where the ssocket pointer is changed without holding the srv_mutex,
but I'm not clear whether there's a potential race here yet or not.

This code seems like it could benefit from some fundamental re-think of
how the socket handling should behave. Until then though, this patch
should at least fix the above oops in most cases.

Cc: <stable@vger.kernel.org> # 3.7+
Reported-and-Tested-by: CAI Qian <caiqian@redhat.com>
Signed-off-by: Jeff Layton <jlayton@redhat.com>
Signed-off-by: Steve French <smfrench@gmail.com>",0,fs/cifs/transport.c,"{""sha"": ""1a528680ec5a29e4b59485fba5df54568a1e3787"", ""filename"": ""fs/cifs/transport.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 3, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/ea702b80e0bbb2448e201472127288beb82ca2fe/fs/cifs/transport.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/ea702b80e0bbb2448e201472127288beb82ca2fe/fs/cifs/transport.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/cifs/transport.c?ref=ea702b80e0bbb2448e201472127288beb82ca2fe"", ""patch"": ""@@ -144,9 +144,6 @@ smb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n \n \t*sent = 0;\n \n-\tif (ssocket == NULL)\n-\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */\n-\n \tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n \tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n \tsmb_msg.msg_control = NULL;\n@@ -291,6 +288,9 @@ smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)\n \tstruct socket *ssocket = server->ssocket;\n \tint val = 1;\n \n+\tif (ssocket == NULL)\n+\t\treturn -ENOTSOCK;\n+\n \tcFYI(1, \""Sending smb: smb_len=%u\"", smb_buf_length);\n \tdump_smb(iov[0].iov_base, iov[0].iov_len);\n ""}","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
	if (ssocket == NULL)
		return -ENOTSOCK;

 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
",C,"	if (ssocket == NULL)
		return -ENOTSOCK;

",,,"@@ -144,9 +144,6 @@ smb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,
 
 	*sent = 0;
 
-	if (ssocket == NULL)
-		return -ENOTSOCK; /* BB eventually add reconnect code here */
-
 	smb_msg.msg_name = (struct sockaddr *) &server->dstaddr;
 	smb_msg.msg_namelen = sizeof(struct sockaddr);
 	smb_msg.msg_control = NULL;
@@ -291,6 +288,9 @@ smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
+	if (ssocket == NULL)
+		return -ENOTSOCK;
+
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 ",linux,ea702b80e0bbb2448e201472127288beb82ca2fe,ecccd1248d6e6986130ffcc3b0d003cb46a485c0,1,"smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
//fix_flaw_line_below:
//	if (ssocket == NULL)
//fix_flaw_line_below:
//		return -ENOTSOCK;
//fix_flaw_line_below:
//
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
",179198,"smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
	if (ssocket == NULL)
		return -ENOTSOCK;

 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","The `smb_send_rqst` function is responsible for sending an SMB (Server Message Block) request over a TCP socket to a specified server. It begins by preparing the socket for sending data using TCP_CORK to optimize transmission. The function sends the initial SMB request data and then iterates through any additional pages of data, sending each one sequentially. After sending, it uncorks the socket to allow normal data flow. If the total bytes sent do not match the expected length, it marks the server's TCP status for reconnection to handle potential data corruption. Finally, it returns the result of the send operation, indicating success or failure.","The function facilitates the transmission of a request over a TCP connection to a server. It begins by preparing the socket for efficient data transfer and sends the initial request data. Following this, it processes and sends additional data pages sequentially. After completing the data transmission, it ensures the socket is returned to normal operation. If the total amount of data sent does not match the expected size, it indicates a potential issue, prompting a status update for reconnection to prevent data corruption. The function concludes by returning a status code reflecting the success or failure of the operation.","The vulnerability in the original code arises from a race condition where the socket could be null during the transmission process, leading to a potential null pointer dereference and subsequent denial of service. This occurs if a reconnection event happens while the function is executing, causing the socket to become invalid. The fixed code addresses this issue by adding a check for a null socket at the beginning of the function. If the socket is null, the function immediately returns an error code, preventing any further operations that could lead to dereferencing a null pointer and ensuring safer execution.","The vulnerability in the original code stems from a race condition that occurs when a reconnection event happens while the function is executing, potentially leading to a null pointer dereference. This situation can cause the program to crash, resulting in a denial of service. The fixed code mitigates this issue by introducing a check at the beginning of the function to ensure that the socket is not null. If the socket is found to be null, the function immediately returns an error, thereby preventing any further operations that could lead to dereferencing a null pointer and enhancing overall stability.","The vulnerability was fixed by adding a check for a null socket at the beginning of the function. Specifically, the code now verifies if the socket variable is null and, if so, immediately returns an error code indicating that the socket is not valid. This prevents any subsequent operations that could lead to dereferencing a null pointer, which was the root cause of the race condition. By ensuring that the socket is valid before proceeding with data transmission, the code enhances stability and prevents potential crashes during reconnection events.","The vulnerability was addressed by implementing a safeguard that checks whether the socket is null at the start of the function. If the socket is found to be null, the function immediately returns an error code, preventing any further execution that could lead to dereferencing a null pointer. This proactive measure ensures that the function does not attempt to perform operations on an invalid socket, thereby eliminating the risk of a crash or denial of service during reconnection events. This fix enhances the overall robustness and stability of the code in handling socket operations."
7,179380,179380,,Remote,Not required,Complete,CVE-2011-4348,https://www.cvedetails.com/cve/CVE-2011-4348/,CWE-362,Medium,,,,2013-06-08,7.1,"Race condition in the sctp_rcv function in net/sctp/input.c in the Linux kernel before 2.6.29 allows remote attackers to cause a denial of service (system hang) via SCTP packets.  NOTE: in some environments, this issue exists because of an incomplete fix for CVE-2011-2482.",2013-07-25,DoS ,13,https://github.com/torvalds/linux/commit/ae53b5bd77719fed58086c5be60ce4f22bffe1c6,ae53b5bd77719fed58086c5be60ce4f22bffe1c6,"sctp: Fix another socket race during accept/peeloff

There is a race between sctp_rcv() and sctp_accept() where we
have moved the association from the listening socket to the
accepted socket, but sctp_rcv() processing cached the old
socket and continues to use it.

The easy solution is to check for the socket mismatch once we've
grabed the socket lock.  If we hit a mis-match, that means
that were are currently holding the lock on the listening socket,
but the association is refrencing a newly accepted socket.  We need
to drop the lock on the old socket and grab the lock on the new one.

A more proper solution might be to create accepted sockets when
the new association is established, similar to TCP.  That would
eliminate the race for 1-to-1 style sockets, but it would still
existing for 1-to-many sockets where a user wished to peeloff an
association.  For now, we'll live with this easy solution as
it addresses the problem.

Reported-by: Michal Hocko <mhocko@suse.cz>
Reported-by: Karsten Keil <kkeil@suse.de>
Signed-off-by: Vlad Yasevich <vladislav.yasevich@hp.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",0,net/sctp/input.c,"{""sha"": ""2e4a8646dbc389dcb55fcce1cf6b3ad0f608af0d"", ""filename"": ""net/sctp/input.c"", ""status"": ""modified"", ""additions"": 13, ""deletions"": 0, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/ae53b5bd77719fed58086c5be60ce4f22bffe1c6/net/sctp/input.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/ae53b5bd77719fed58086c5be60ce4f22bffe1c6/net/sctp/input.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/sctp/input.c?ref=ae53b5bd77719fed58086c5be60ce4f22bffe1c6"", ""patch"": ""@@ -249,6 +249,19 @@ int sctp_rcv(struct sk_buff *skb)\n \t */\n \tsctp_bh_lock_sock(sk);\n \n+\tif (sk != rcvr->sk) {\n+\t\t/* Our cached sk is different from the rcvr->sk.  This is\n+\t\t * because migrate()/accept() may have moved the association\n+\t\t * to a new socket and released all the sockets.  So now we\n+\t\t * are holding a lock on the old socket while the user may\n+\t\t * be doing something with the new socket.  Switch our veiw\n+\t\t * of the current sk.\n+\t\t */\n+\t\tsctp_bh_unlock_sock(sk);\n+\t\tsk = rcvr->sk;\n+\t\tsctp_bh_lock_sock(sk);\n+\t}\n+\n \tif (sock_owned_by_user(sk)) {\n \t\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);\n \t\tsctp_add_backlog(sk, skb);""}","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
",C,"	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

",,,"@@ -249,6 +249,19 @@ int sctp_rcv(struct sk_buff *skb)
 	 */
 	sctp_bh_lock_sock(sk);
 
+	if (sk != rcvr->sk) {
+		/* Our cached sk is different from the rcvr->sk.  This is
+		 * because migrate()/accept() may have moved the association
+		 * to a new socket and released all the sockets.  So now we
+		 * are holding a lock on the old socket while the user may
+		 * be doing something with the new socket.  Switch our veiw
+		 * of the current sk.
+		 */
+		sctp_bh_unlock_sock(sk);
+		sk = rcvr->sk;
+		sctp_bh_lock_sock(sk);
+	}
+
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);",linux,ae53b5bd77719fed58086c5be60ce4f22bffe1c6,759af00ebef858015eb68876ac1f383bcb6a1774,1,"int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
//fix_flaw_line_below:
//	if (sk != rcvr->sk) {
//fix_flaw_line_below:
//		/* Our cached sk is different from the rcvr->sk.  This is
//fix_flaw_line_below:
//		 * because migrate()/accept() may have moved the association
//fix_flaw_line_below:
//		 * to a new socket and released all the sockets.  So now we
//fix_flaw_line_below:
//		 * are holding a lock on the old socket while the user may
//fix_flaw_line_below:
//		 * be doing something with the new socket.  Switch our veiw
//fix_flaw_line_below:
//		 * of the current sk.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		sctp_bh_unlock_sock(sk);
//fix_flaw_line_below:
//		sk = rcvr->sk;
//fix_flaw_line_below:
//		sctp_bh_lock_sock(sk);
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
",179380,"int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","The `sctp_rcv` function processes incoming SCTP (Stream Control Transmission Protocol) packets by first validating the packet type and performing checksum verification. It extracts the SCTP header and addresses, ensuring they are valid and unicast. The function then attempts to identify the associated SCTP association or endpoint. If the packet is unrecognized, it checks for ""out of the blue"" packets and applies necessary security policies. Valid packets are chunkified and queued for processing, while maintaining socket locks to manage concurrent access. Finally, it releases any references held during lookups and discards invalid packets, updating statistics accordingly.","The code processes incoming packets for a specific transport protocol by validating their type and ensuring they meet checksum requirements. It extracts necessary headers and addresses, confirming they are valid and unicast. The function attempts to identify the relevant communication association or endpoint, handling cases where the packet is unrecognized. It applies security policies and manages concurrent access to resources. Valid packets are organized into manageable structures and queued for further processing, while invalid packets are discarded, and statistics are updated to reflect the handling of packets, including successful receptions and discards.","The vulnerability in the original code arises from a race condition where the socket state could change between the time it is locked and when it is accessed, potentially leading to a system hang or denial of service. Specifically, if the socket associated with a communication changes due to operations like migration or acceptance, the code could still hold a lock on the old socket, causing inconsistencies. The fixed code addresses this by checking if the current socket differs from the receiver's socket after acquiring the lock. If they differ, it unlocks the old socket and locks the new one, ensuring safe access and preventing race conditions.","The vulnerability in the original code is a race condition that occurs when the state of a socket changes while it is being accessed, potentially leading to a denial of service or system hang. This happens when the code locks a socket but does not verify if it is still valid before performing operations, allowing an attacker to exploit this timing issue. The fix addresses this by introducing a check to ensure that the socket being accessed is the same as the one that was initially locked. If a change is detected, the code safely unlocks the old socket and locks the new one, preventing inconsistencies and ensuring stable operation.","The vulnerability was fixed by adding a check after acquiring the socket lock to determine if the current socket differs from the receiver's socket. If the sockets are not the same, the code first unlocks the old socket and then locks the new socket associated with the receiver. This is done using the `sctp_bh_unlock_sock` and `sctp_bh_lock_sock` functions, ensuring that the code always operates on the correct socket. This change prevents race conditions that could lead to inconsistencies and potential denial of service, thereby enhancing the stability and security of the function.","The vulnerability was fixed by implementing a verification step after acquiring the socket lock to ensure that the socket being accessed is still the correct one. If a change in the socket is detected, the code safely releases the lock on the old socket and acquires a lock on the new socket associated with the communication. This adjustment prevents race conditions by ensuring that the code operates on a valid socket throughout its execution, thereby maintaining consistency and preventing potential denial of service or system hangs caused by accessing an outdated or invalid socket state."
8,179565,179565,,Remote,Not required,Complete,CVE-2014-2706,https://www.cvedetails.com/cve/CVE-2014-2706/,CWE-362,Medium,,,,2014-04-14,7.1,"Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c.",2017-07-10,DoS ,15,https://github.com/torvalds/linux/commit/1d147bfa64293b2723c4fec50922168658e613ba,1d147bfa64293b2723c4fec50922168658e613ba,"mac80211: fix AP powersave TX vs. wakeup race

There is a race between the TX path and the STA wakeup: while
a station is sleeping, mac80211 buffers frames until it wakes
up, then the frames are transmitted. However, the RX and TX
path are concurrent, so the packet indicating wakeup can be
processed while a packet is being transmitted.

This can lead to a situation where the buffered frames list
is emptied on the one side, while a frame is being added on
the other side, as the station is still seen as sleeping in
the TX path.

As a result, the newly added frame will not be send anytime
soon. It might be sent much later (and out of order) when the
station goes to sleep and wakes up the next time.

Additionally, it can lead to the crash below.

Fix all this by synchronising both paths with a new lock.
Both path are not fastpath since they handle PS situations.

In a later patch we'll remove the extra skb queue locks to
reduce locking overhead.

BUG: unable to handle kernel
NULL pointer dereference at 000000b0
IP: [<ff6f1791>] ieee80211_report_used_skb+0x11/0x3e0 [mac80211]
*pde = 00000000
Oops: 0000 [#1] SMP DEBUG_PAGEALLOC
EIP: 0060:[<ff6f1791>] EFLAGS: 00210282 CPU: 1
EIP is at ieee80211_report_used_skb+0x11/0x3e0 [mac80211]
EAX: e5900da0 EBX: 00000000 ECX: 00000001 EDX: 00000000
ESI: e41d00c0 EDI: e5900da0 EBP: ebe458e4 ESP: ebe458b0
 DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
CR0: 8005003b CR2: 000000b0 CR3: 25a78000 CR4: 000407d0
DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
DR6: ffff0ff0 DR7: 00000400
Process iperf (pid: 3934, ti=ebe44000 task=e757c0b0 task.ti=ebe44000)
iwlwifi 0000:02:00.0: I iwl_pcie_enqueue_hcmd Sending command LQ_CMD (#4e), seq: 0x0903, 92 bytes at 3[3]:9
Stack:
 e403b32c ebe458c4 00200002 00200286 e403b338 ebe458cc c10960bb e5900da0
 ff76a6ec ebe458d8 00000000 e41d00c0 e5900da0 ebe458f0 ff6f1b75 e403b210
 ebe4598c ff723dc1 00000000 ff76a6ec e597c978 e403b758 00000002 00000002
Call Trace:
 [<ff6f1b75>] ieee80211_free_txskb+0x15/0x20 [mac80211]
 [<ff723dc1>] invoke_tx_handlers+0x1661/0x1780 [mac80211]
 [<ff7248a5>] ieee80211_tx+0x75/0x100 [mac80211]
 [<ff7249bf>] ieee80211_xmit+0x8f/0xc0 [mac80211]
 [<ff72550e>] ieee80211_subif_start_xmit+0x4fe/0xe20 [mac80211]
 [<c149ef70>] dev_hard_start_xmit+0x450/0x950
 [<c14b9aa9>] sch_direct_xmit+0xa9/0x250
 [<c14b9c9b>] __qdisc_run+0x4b/0x150
 [<c149f732>] dev_queue_xmit+0x2c2/0xca0

Cc: stable@vger.kernel.org
Reported-by: Yaara Rozenblum <yaara.rozenblum@intel.com>
Signed-off-by: Emmanuel Grumbach <emmanuel.grumbach@intel.com>
Reviewed-by: Stanislaw Gruszka <sgruszka@redhat.com>
[reword commit log, use a separate lock]
Signed-off-by: Johannes Berg <johannes.berg@intel.com>",0,net/mac80211/tx.c,"{""sha"": ""62a5f0889583437203a1a580b608829997c70ad1"", ""filename"": ""net/mac80211/sta_info.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 0, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/sta_info.c?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -330,6 +330,7 @@ struct sta_info *sta_info_alloc(struct ieee80211_sub_if_data *sdata,\n \trcu_read_unlock();\n \n \tspin_lock_init(&sta->lock);\n+\tspin_lock_init(&sta->ps_lock);\n \tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n \tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n \tmutex_init(&sta->ampdu_mlme.mtx);\n@@ -1109,6 +1110,8 @@ void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n \n \tskb_queue_head_init(&pending);\n \n+\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n+\tspin_lock(&sta->ps_lock);\n \t/* Send all buffered frames to the station */\n \tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n \t\tint count = skb_queue_len(&pending), tmp;\n@@ -1128,6 +1131,7 @@ void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n \t}\n \n \tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n+\tspin_unlock(&sta->ps_lock);\n \n \t/* This station just woke up and isn't aware of our SMPS state */\n \tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,""}<_**next**_>{""sha"": ""d3a6d8208f2f85f7db331238f41c0da2938f0f8d"", ""filename"": ""net/mac80211/sta_info.h"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 4, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/sta_info.h?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -267,6 +267,7 @@ struct ieee80211_tx_latency_stat {\n  * @drv_unblock_wk: used for driver PS unblocking\n  * @listen_interval: listen interval of this station, when we're acting as AP\n  * @_flags: STA flags, see &enum ieee80211_sta_info_flags, do not use directly\n+ * @ps_lock: used for powersave (when mac80211 is the AP) related locking\n  * @ps_tx_buf: buffers (per AC) of frames to transmit to this station\n  *\twhen it leaves power saving state or polls\n  * @tx_filtered: buffers (per AC) of frames we already tried to\n@@ -356,10 +357,8 @@ struct sta_info {\n \t/* use the accessors defined below */\n \tunsigned long _flags;\n \n-\t/*\n-\t * STA powersave frame queues, no more than the internal\n-\t * locking required.\n-\t */\n+\t/* STA powersave lock and frame queues */\n+\tspinlock_t ps_lock;\n \tstruct sk_buff_head ps_tx_buf[IEEE80211_NUM_ACS];\n \tstruct sk_buff_head tx_filtered[IEEE80211_NUM_ACS];\n \tunsigned long driver_buffered_tids;""}<_**next**_>{""sha"": ""4080c615636fabf3d430ecd898d4349ccd213464"", ""filename"": ""net/mac80211/tx.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 0, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/tx.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/tx.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/tx.c?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -478,6 +478,20 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n \t\t       sta->sta.addr, sta->sta.aid, ac);\n \t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n \t\t\tpurge_old_ps_buffers(tx->local);\n+\n+\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n+\t\tspin_lock(&sta->ps_lock);\n+\t\t/*\n+\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n+\t\t * been queued to pending queue. No reordering can happen, go\n+\t\t * ahead and Tx the packet.\n+\t\t */\n+\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n+\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n+\t\t\tspin_unlock(&sta->ps_lock);\n+\t\t\treturn TX_CONTINUE;\n+\t\t}\n+\n \t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n \t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n \t\t\tps_dbg(tx->sdata,\n@@ -492,6 +506,7 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n \t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n \t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n \t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n+\t\tspin_unlock(&sta->ps_lock);\n \n \t\tif (!timer_pending(&local->sta_cleanup))\n \t\t\tmod_timer(&local->sta_cleanup,""}","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);

		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
",C,"
		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

		spin_unlock(&sta->ps_lock);
",,,"@@ -478,6 +478,20 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
+
+		/* sync with ieee80211_sta_ps_deliver_wakeup */
+		spin_lock(&sta->ps_lock);
+		/*
+		 * STA woke up the meantime and all the frames on ps_tx_buf have
+		 * been queued to pending queue. No reordering can happen, go
+		 * ahead and Tx the packet.
+		 */
+		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
+		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
+			spin_unlock(&sta->ps_lock);
+			return TX_CONTINUE;
+		}
+
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
@@ -492,6 +506,7 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
+		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,",linux,1d147bfa64293b2723c4fec50922168658e613ba,50c11eb9982554e9f99b7bab322c517cbe5ce1a1,1,"ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/* sync with ieee80211_sta_ps_deliver_wakeup */
//fix_flaw_line_below:
//		spin_lock(&sta->ps_lock);
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * STA woke up the meantime and all the frames on ps_tx_buf have
//fix_flaw_line_below:
//		 * been queued to pending queue. No reordering can happen, go
//fix_flaw_line_below:
//		 * ahead and Tx the packet.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
//fix_flaw_line_below:
//		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
//fix_flaw_line_below:
//			spin_unlock(&sta->ps_lock);
//fix_flaw_line_below:
//			return TX_CONTINUE;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
//fix_flaw_line_below:
//		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
",179565,"ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);

		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","The function `ieee80211_tx_h_unicast_ps_buf` manages the transmission of unicast packets for stations in power-saving (PS) mode within a wireless network. It first checks if the station (STA) is valid and whether it is in PS mode, allowing for buffering of packets unless explicitly disabled. If the STA's transmission buffer for a specific access category (AC) is full, it purges the oldest packet to make space for new ones. The function updates transmission information, queues the packet for later transmission, and ensures that a cleanup timer is set if necessary. It also recalculates the Traffic Indication Map (TIM) to reflect the buffered packets.","The code manages the transmission of unicast packets for devices in power-saving mode within a wireless network. It first verifies the validity of the device and checks if it is allowed to buffer packets. If the device's transmission buffer for a specific category is full, it removes the oldest packet to accommodate new ones. The code updates transmission details, queues the packet for future sending, and sets a cleanup timer if needed. Additionally, it recalculates the Traffic Indication Map to reflect the status of buffered packets, ensuring efficient management of power-saving operations.","The vulnerability in the original code arises from a race condition related to the power-saving state of a device, where simultaneous access to shared resources could lead to inconsistent states and potential system crashes. Specifically, the lack of proper synchronization when checking the power-saving flags allowed for scenarios where packets could be queued or processed incorrectly. The fixed code addresses this by introducing a spin lock (`sta->ps_lock`) to synchronize access to the power-saving state, ensuring that the state is checked and modified safely. This prevents race conditions by ensuring that only one thread can access the critical section at a time, thus maintaining system stability.","The vulnerability in the original code stems from a race condition that occurs when multiple threads access shared resources related to the power-saving state of a device. This lack of synchronization can lead to inconsistent states, allowing attackers to exploit the situation and potentially cause system crashes through crafted network traffic. The fixed code addresses this issue by implementing a locking mechanism to ensure that access to the critical sections of the code is serialized. This prevents simultaneous modifications and checks of the power-saving state, thereby eliminating the race condition and enhancing the stability and security of the system.","The vulnerability was fixed by introducing a spin lock (`sta->ps_lock`) to synchronize access to the power-saving state of the device. In the modified code, the lock is acquired before checking the power-saving flags, ensuring that no other thread can modify the state while it is being accessed. This prevents race conditions by ensuring that the checks for the power-saving state and the subsequent actions, such as queuing packets or dropping old frames, are performed atomically. The lock is released after these operations, maintaining thread safety and enhancing the overall stability of the system.","The vulnerability was fixed by implementing a locking mechanism to ensure synchronized access to the shared resources related to the power-saving state. This change prevents multiple threads from simultaneously modifying or checking the state, which could lead to inconsistent behavior. By acquiring the lock before performing any operations that depend on the power-saving state, the code ensures that these operations are executed atomically. This effectively eliminates the race condition, enhancing the stability and security of the system by preventing potential crashes caused by improper interactions with the power-saving functionality."
9,179567,179567,,Remote,Not required,Complete,CVE-2014-2672,https://www.cvedetails.com/cve/CVE-2014-2672/,CWE-362,Medium,,,,2014-04-01,7.1,Race condition in the ath_tx_aggr_sleep function in drivers/net/wireless/ath/ath9k/xmit.c in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via a large amount of network traffic that triggers certain list deletions.,2014-04-19,DoS ,5,https://github.com/torvalds/linux/commit/21f8aaee0c62708654988ce092838aa7df4d25d8,21f8aaee0c62708654988ce092838aa7df4d25d8,"ath9k: protect tid->sched check

We check tid->sched without a lock taken on ath_tx_aggr_sleep(). That
is race condition which can result of doing list_del(&tid->list) twice
(second time with poisoned list node) and cause crash like shown below:

[424271.637220] BUG: unable to handle kernel paging request at 00100104
[424271.637328] IP: [<f90fc072>] ath_tx_aggr_sleep+0x62/0xe0 [ath9k]
...
[424271.639953] Call Trace:
[424271.639998]  [<f90f6900>] ? ath9k_get_survey+0x110/0x110 [ath9k]
[424271.640083]  [<f90f6942>] ath9k_sta_notify+0x42/0x50 [ath9k]
[424271.640177]  [<f809cfef>] sta_ps_start+0x8f/0x1c0 [mac80211]
[424271.640258]  [<c10f730e>] ? free_compound_page+0x2e/0x40
[424271.640346]  [<f809e915>] ieee80211_rx_handlers+0x9d5/0x2340 [mac80211]
[424271.640437]  [<c112f048>] ? kmem_cache_free+0x1d8/0x1f0
[424271.640510]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640578]  [<c10fc23c>] ? put_page+0x2c/0x40
[424271.640640]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640706]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640787]  [<f809dde3>] ? ieee80211_rx_handlers_result+0x73/0x1d0 [mac80211]
[424271.640897]  [<f80a07a0>] ieee80211_prepare_and_rx_handle+0x520/0xad0 [mac80211]
[424271.641009]  [<f809e22d>] ? ieee80211_rx_handlers+0x2ed/0x2340 [mac80211]
[424271.641104]  [<c13846ce>] ? ip_output+0x7e/0xd0
[424271.641182]  [<f80a1057>] ieee80211_rx+0x307/0x7c0 [mac80211]
[424271.641266]  [<f90fa6ee>] ath_rx_tasklet+0x88e/0xf70 [ath9k]
[424271.641358]  [<f80a0f2c>] ? ieee80211_rx+0x1dc/0x7c0 [mac80211]
[424271.641445]  [<f90f82db>] ath9k_tasklet+0xcb/0x130 [ath9k]

Bug report:
https://bugzilla.kernel.org/show_bug.cgi?id=70551

Reported-and-tested-by: Max Sydorenko <maxim.stargazer@gmail.com>
Cc: stable@vger.kernel.org
Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
Signed-off-by: John W. Linville <linville@tuxdriver.com>",2,drivers/net/wireless/ath/ath9k/xmit.c,"{""sha"": ""4f4ce83f7ab4afd69fb4ed4ae937953711d3343a"", ""filename"": ""drivers/net/wireless/ath/ath9k/xmit.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 3, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/21f8aaee0c62708654988ce092838aa7df4d25d8/drivers/net/wireless/ath/ath9k/xmit.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/21f8aaee0c62708654988ce092838aa7df4d25d8/drivers/net/wireless/ath/ath9k/xmit.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/net/wireless/ath/ath9k/xmit.c?ref=21f8aaee0c62708654988ce092838aa7df4d25d8"", ""patch"": ""@@ -1444,14 +1444,16 @@ void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,\n \tfor (tidno = 0, tid = &an->tid[tidno];\n \t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n \n-\t\tif (!tid->sched)\n-\t\t\tcontinue;\n-\n \t\tac = tid->ac;\n \t\ttxq = ac->txq;\n \n \t\tath_txq_lock(sc, txq);\n \n+\t\tif (!tid->sched) {\n+\t\t\tath_txq_unlock(sc, txq);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tbuffered = ath_tid_has_buffered(tid);\n \n \t\ttid->sched = false;""}","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
		if (!tid->sched)
			continue;
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
",C,"		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

","		if (!tid->sched)
			continue;
",,"@@ -1444,14 +1444,16 @@ void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
-		if (!tid->sched)
-			continue;
-
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
+		if (!tid->sched) {
+			ath_txq_unlock(sc, txq);
+			continue;
+		}
+
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;",linux,21f8aaee0c62708654988ce092838aa7df4d25d8,1c97560f6d751a620978504a4a888c631192b71a,1,"void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
//flaw_line_below:
		if (!tid->sched)
//flaw_line_below:
			continue;
//flaw_line_below:

 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
//fix_flaw_line_below:
//		if (!tid->sched) {
//fix_flaw_line_below:
//			ath_txq_unlock(sc, txq);
//fix_flaw_line_below:
//			continue;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
",179567,"void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
		if (!tid->sched)
			continue;
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","The function `ath_tx_aggr_sleep` is designed to manage the transmission aggregation state of a specific station (`sta`) in a wireless network, particularly when the station is entering a sleep mode. It iterates through all possible Traffic Identifier (TID) values associated with the station, checking if each TID is scheduled for transmission. If a TID is scheduled, it locks the corresponding transmission queue, marks the TID and its associated Access Category (AC) as unscheduled, and removes them from their respective lists. Finally, it updates the station's buffered status based on whether there were any buffered packets for that TID before the operation.","The provided code manages the transmission aggregation state of a wireless station when it enters sleep mode. It iterates through all possible traffic identifiers associated with the station, checking if any are scheduled for transmission. For each scheduled identifier, it locks the corresponding transmission queue, marks the identifier and its associated category as unscheduled, and removes them from their respective lists. After unlocking the queue, it updates the station's status to reflect whether there were any buffered packets for the identifier prior to this operation, ensuring proper handling of data during the station's sleep state.","The vulnerability in the original code arises from a race condition where multiple threads could concurrently modify the scheduling state of traffic identifiers, leading to inconsistent states and potential system crashes. Specifically, if one thread was in the process of deleting a scheduled identifier from its list while another thread attempted to access or modify the same identifier, it could result in undefined behavior. The fixed code addresses this issue by introducing a check for the scheduling state of the identifier before locking the transmission queue. This ensures that only scheduled identifiers are processed, preventing concurrent modifications and enhancing stability.","The vulnerability in the original code is a race condition that occurs when multiple threads attempt to modify the scheduling state of traffic identifiers simultaneously. This can lead to inconsistent states and potential system crashes, especially when a high volume of network traffic triggers concurrent deletions from lists. The fixed code resolves this issue by implementing a check for the scheduling state before proceeding with any operations that require locking. This ensures that only identifiers that are actually scheduled are processed, thereby preventing concurrent modifications and enhancing the overall stability of the system.","The vulnerability was fixed by adding a check for the scheduling state of each traffic identifier before locking the transmission queue. In the modified code, the condition `if (!tid->sched)` is evaluated right after determining the associated access category and transmission queue. If the identifier is not scheduled, the code unlocks the transmission queue and continues to the next iteration, preventing any further processing of that identifier. This change ensures that only scheduled identifiers are handled, thereby reducing the risk of race conditions and ensuring that list deletions occur safely without concurrent modifications.","The vulnerability was fixed by introducing a preliminary check to determine if a traffic identifier is scheduled before proceeding with any operations that require locking a transmission queue. This adjustment ensures that only identifiers that are actively scheduled are processed, thereby preventing unnecessary locking and potential race conditions. If an identifier is not scheduled, the code immediately unlocks the queue and continues to the next identifier, effectively avoiding any modifications to the list that could lead to inconsistent states. This change enhances the stability of the system by ensuring safe access to shared resources."
10,179745,179745,,Local,Not required,Complete,CVE-2015-7990,https://www.cvedetails.com/cve/CVE-2015-7990/,CWE-362,Medium,Partial,Partial,,2015-12-28,5.9,Race condition in the rds_sendmsg function in net/rds/sendmsg.c in the Linux kernel before 4.3.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2015-6937.,2018-10-16,DoS ,3,https://github.com/torvalds/linux/commit/8c7188b23474cca017b3ef354c4a58456f68303a,8c7188b23474cca017b3ef354c4a58456f68303a,"RDS: fix race condition when sending a message on unbound socket

Sasha's found a NULL pointer dereference in the RDS connection code when
sending a message to an apparently unbound socket.  The problem is caused
by the code checking if the socket is bound in rds_sendmsg(), which checks
the rs_bound_addr field without taking a lock on the socket.  This opens a
race where rs_bound_addr is temporarily set but where the transport is not
in rds_bind(), leading to a NULL pointer dereference when trying to
dereference 'trans' in __rds_conn_create().

Vegard wrote a reproducer for this issue, so kindly ask him to share if
you're interested.

I cannot reproduce the NULL pointer dereference using Vegard's reproducer
with this patch, whereas I could without.

Complete earlier incomplete fix to CVE-2015-6937:

  74e98eb08588 (""RDS: verify the underlying transport exists before creating a connection"")

Cc: David S. Miller <davem@davemloft.net>
Cc: stable@vger.kernel.org

Reviewed-by: Vegard Nossum <vegard.nossum@oracle.com>
Reviewed-by: Sasha Levin <sasha.levin@oracle.com>
Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Signed-off-by: Quentin Casasnovas <quentin.casasnovas@oracle.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/rds/send.c,"{""sha"": ""e3b118cae81d5e859e0244df1bf323aaa1798b8e"", ""filename"": ""net/rds/connection.c"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 6, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/connection.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/connection.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/connection.c?ref=8c7188b23474cca017b3ef354c4a58456f68303a"", ""patch"": ""@@ -186,12 +186,6 @@ static struct rds_connection *__rds_conn_create(struct net *net,\n \t\t}\n \t}\n \n-\tif (trans == NULL) {\n-\t\tkmem_cache_free(rds_conn_slab, conn);\n-\t\tconn = ERR_PTR(-ENODEV);\n-\t\tgoto out;\n-\t}\n-\n \tconn->c_trans = trans;\n \n \tret = trans->conn_alloc(conn, gfp);""}<_**next**_>{""sha"": ""c9cdb358ea885e3e356cc675b579f1313ed94ff9"", ""filename"": ""net/rds/send.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 1, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/send.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/send.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/send.c?ref=8c7188b23474cca017b3ef354c4a58456f68303a"", ""patch"": ""@@ -1013,11 +1013,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n \t\trelease_sock(sk);\n \t}\n \n-\t/* racing with another thread binding seems ok here */\n+\tlock_sock(sk);\n \tif (daddr == 0 || rs->rs_bound_addr == 0) {\n+\t\trelease_sock(sk);\n \t\tret = -ENOTCONN; /* XXX not a great errno */\n \t\tgoto out;\n \t}\n+\trelease_sock(sk);\n \n \tif (payload_len > rds_sk_sndbuf(rs)) {\n \t\tret = -EMSGSIZE;""}","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	/* racing with another thread binding seems ok here */
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
",C,"	lock_sock(sk);
		release_sock(sk);
	release_sock(sk);
","	/* racing with another thread binding seems ok here */
",,"@@ -1013,11 +1013,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 		release_sock(sk);
 	}
 
-	/* racing with another thread binding seems ok here */
+	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
+		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
+	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;",linux,8c7188b23474cca017b3ef354c4a58456f68303a,20f795666d3accbb75969730019aeb03f50ef0ec,1,"int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
//flaw_line_below:
	/* racing with another thread binding seems ok here */
//fix_flaw_line_below:
//	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
//fix_flaw_line_below:
//		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
//fix_flaw_line_below:
//	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
",179745,"int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	/* racing with another thread binding seems ok here */
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","The `rds_sendmsg` function is responsible for sending messages over a socket using the Reliable Datagram Sockets (RDS) protocol. It begins by validating the message parameters and extracting the destination address and port. The function checks for connection status and buffer size constraints before allocating a message structure and copying the user data into it. It manages control messages and ensures that the connection is active. The function then attempts to queue the message for sending, handling potential congestion and non-blocking conditions. Finally, it either sends the message or cleans up resources in case of errors, returning the length of the payload or an error code.","The function facilitates the transmission of messages over a specific socket using a specialized protocol. It begins by validating the message's parameters and extracting the destination details. The function checks the connection status and ensures that the message size does not exceed the allowed limits. After allocating necessary resources and copying the message data, it manages any control messages and verifies that the connection is active. The function then attempts to queue the message for sending, addressing potential congestion and non-blocking scenarios, before either successfully sending the message or cleaning up resources in case of errors.","The vulnerability in the original code arises from a race condition where the socket's connection state could be modified by another thread while the function checks the connection status, leading to potential NULL pointer dereferences and system crashes. Specifically, the checks for the destination address and bound address were not adequately protected by locking mechanisms, allowing concurrent access. In the fixed code, the addition of a lock around the checks for the destination address and bound address ensures that the connection state remains consistent during these critical operations, effectively preventing the race condition and enhancing the stability of the function.","The vulnerability in the original code stems from a race condition that occurs when multiple threads access and modify the socket's connection state simultaneously. This can lead to scenarios where a thread checks the connection status while another thread alters it, resulting in potential NULL pointer dereferences and system crashes. The fixed code addresses this issue by implementing proper locking mechanisms around critical sections that check the connection state, ensuring that the connection information remains consistent and protected from concurrent modifications. This change effectively mitigates the risk of denial of service and enhances the overall stability of the function.","The vulnerability was fixed by introducing a locking mechanism around the checks for the destination address and bound address. In the modified code, the `lock_sock` function is called before verifying the connection status, ensuring that no other thread can alter the connection state while these checks are performed. This prevents race conditions that could lead to NULL pointer dereferences. Additionally, the `release_sock` function is used appropriately to release the lock after the checks, maintaining thread safety and ensuring that the socket's state remains consistent throughout the execution of the function.","The vulnerability was fixed by implementing a locking mechanism to ensure that critical sections of the code, which check the connection state, are protected from concurrent access by multiple threads. By acquiring a lock before performing these checks, the code prevents other threads from modifying the connection state while it is being evaluated. This change ensures that the connection information remains consistent and valid during the execution of the function, effectively mitigating the risk of race conditions that could lead to NULL pointer dereferences and potential system crashes. The proper release of the lock after the checks further enhances thread safety."
11,179750,179750,,Local,Not required,Complete,CVE-2015-7613,https://www.cvedetails.com/cve/CVE-2015-7613/,CWE-362,Medium,Complete,Complete,,2015-10-19,6.9,"Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",2016-12-07,+Priv ,7,https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf,b9a532277938798b53178d5a66af6e2915cb27cf,"Initialize msg/shm IPC objects before doing ipc_addid()

As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before
having initialized the IPC object state.  Yes, we initialize the IPC
object in a locked state, but with all the lockless RCU lookup work,
that IPC object lock no longer means that the state cannot be seen.

We already did this for the IPC semaphore code (see commit e8577d1f0329:
""ipc/sem.c: fully initialize sem_array before making it visible"") but we
clearly forgot about msg and shm.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Cc: Manfred Spraul <manfred@colorfullife.com>
Cc: Davidlohr Bueso <dbueso@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",6,ipc/msg.c,"{""sha"": ""1471db9a7e6112b3316ae887b50c6d8d1352f171"", ""filename"": ""ipc/msg.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 7, ""changes"": 14, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/msg.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \t\treturn retval;\n \t}\n \n-\t/* ipc_addid() locks msq upon success. */\n-\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n-\tif (id < 0) {\n-\t\tipc_rcu_putref(msq, msg_rcu_free);\n-\t\treturn id;\n-\t}\n-\n \tmsq->q_stime = msq->q_rtime = 0;\n \tmsq->q_ctime = get_seconds();\n \tmsq->q_cbytes = msq->q_qnum = 0;\n@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \tINIT_LIST_HEAD(&msq->q_receivers);\n \tINIT_LIST_HEAD(&msq->q_senders);\n \n+\t/* ipc_addid() locks msq upon success. */\n+\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n+\tif (id < 0) {\n+\t\tipc_rcu_putref(msq, msg_rcu_free);\n+\t\treturn id;\n+\t}\n+\n \tipc_unlock_object(&msq->q_perm);\n \trcu_read_unlock();\n ""}<_**next**_>{""sha"": ""41787276e14170af7de8261181721991fde528bf"", ""filename"": ""ipc/shm.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 6, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/shm.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -551,12 +551,6 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -565,6 +559,13 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*""}<_**next**_>{""sha"": ""0f401d94b7c657d5e7126fe78f149c94ffea8e24"", ""filename"": ""ipc/util.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/util.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \trcu_read_lock();\n \tspin_lock(&new->lock);\n \n+\tcurrent_euid_egid(&euid, &egid);\n+\tnew->cuid = new->uid = euid;\n+\tnew->gid = new->cgid = egid;\n+\n \tid = idr_alloc(&ids->ipcs_idr, new,\n \t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n \t\t       GFP_NOWAIT);\n@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \n \tids->in_use++;\n \n-\tcurrent_euid_egid(&euid, &egid);\n-\tnew->cuid = new->uid = euid;\n-\tnew->gid = new->cgid = egid;\n-\n \tif (next_id < 0) {\n \t\tnew->seq = ids->seq++;\n \t\tif (ids->seq > IPCID_SEQ_MAX)""}","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
",C,"	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

","	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
",,"@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)
 		return retval;
 	}
 
-	/* ipc_addid() locks msq upon success. */
-	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
-	if (id < 0) {
-		ipc_rcu_putref(msq, msg_rcu_free);
-		return id;
-	}
-
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
+	/* ipc_addid() locks msq upon success. */
+	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
+	if (id < 0) {
+		ipc_rcu_putref(msq, msg_rcu_free);
+		return id;
+	}
+
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 ",linux,b9a532277938798b53178d5a66af6e2915cb27cf,3225031fbeb1e32b269a82eccd815128267a4bfe,1,"static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
//flaw_line_below:
	/* ipc_addid() locks msq upon success. */
//flaw_line_below:
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
//flaw_line_below:
	if (id < 0) {
//flaw_line_below:
		ipc_rcu_putref(msq, msg_rcu_free);
//flaw_line_below:
		return id;
//flaw_line_below:
	}
//flaw_line_below:

 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
//fix_flaw_line_below:
//	/* ipc_addid() locks msq upon success. */
//fix_flaw_line_below:
//	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
//fix_flaw_line_below:
//	if (id < 0) {
//fix_flaw_line_below:
//		ipc_rcu_putref(msq, msg_rcu_free);
//fix_flaw_line_below:
//		return id;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
",179750,"static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","The provided code defines a function `newque` that creates a new message queue within a specified IPC (Inter-Process Communication) namespace. It begins by allocating memory for a message queue structure and initializes its permissions based on provided flags and a key. The function then calls a security function to allocate the message queue, handling any errors that arise. If successful, it registers the message queue ID, initializes various timestamps and counters, and sets up linked lists for managing messages and senders/receivers. Finally, it unlocks the IPC object and returns the ID of the newly created message queue.","The code defines a function that creates a new message queue within a specified communication namespace. It begins by allocating memory for the message queue and initializing its permissions based on provided parameters. The function then checks for security constraints before proceeding to register the message queue, handling any errors that may occur during this process. Upon successful registration, it initializes various attributes related to timestamps and message counts, sets up linked lists for managing messages and participants, and finally unlocks the associated object before returning the identifier of the newly created message queue.","The vulnerability in the original code arises from a race condition where the message queue's permissions could be modified by another thread after the security check but before the message queue is registered with `ipc_addid`. This could lead to unauthorized access, as the UID and GID comparisons might use uninitialized data. The fixed code addresses this issue by ensuring that the message queue is fully initialized, including setting timestamps and counters, before calling `ipc_addid`. This change guarantees that the message queue's state is stable and secure during the registration process, thus preventing potential privilege escalation.","The vulnerability in the original code stems from a race condition that allows unauthorized users to exploit timing issues between security checks and the registration of an IPC object. This could result in comparisons against uninitialized data, potentially granting elevated privileges. The fixed code mitigates this vulnerability by ensuring that all necessary attributes of the IPC object are fully initialized before the registration process occurs. This change stabilizes the object's state, preventing any modifications that could occur during the critical section, thereby eliminating the risk of privilege escalation through uninitialized data comparisons.","The vulnerability was fixed by restructuring the order of operations in the code to ensure that the message queue is fully initialized before the critical registration call. In the updated code, after the security check with `security_msg_queue_alloc`, the attributes of the message queue, such as timestamps and message counts, are set, and linked lists for messages and participants are initialized. This initialization occurs before the call to `ipc_addid`, which locks the message queue upon success. By ensuring that the message queue is in a stable state before registration, the risk of race conditions and unauthorized access is effectively mitigated.","The vulnerability was fixed by reorganizing the code to ensure that all necessary attributes of the IPC object are fully initialized before it is registered. This involved moving the initialization of timestamps, message counts, and linked lists to occur immediately after the security checks and before the critical registration step. By doing so, the code guarantees that the object's state is stable and complete, preventing any potential race conditions that could allow unauthorized access. This change effectively eliminates the risk of comparisons against uninitialized data during the registration process, thereby enhancing the overall security of the implementation."
12,179752,179752,,Local,Not required,Complete,CVE-2015-7613,https://www.cvedetails.com/cve/CVE-2015-7613/,CWE-362,Medium,Complete,Complete,,2015-10-19,6.9,"Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",2016-12-07,+Priv ,4,https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf,b9a532277938798b53178d5a66af6e2915cb27cf,"Initialize msg/shm IPC objects before doing ipc_addid()

As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before
having initialized the IPC object state.  Yes, we initialize the IPC
object in a locked state, but with all the lockless RCU lookup work,
that IPC object lock no longer means that the state cannot be seen.

We already did this for the IPC semaphore code (see commit e8577d1f0329:
""ipc/sem.c: fully initialize sem_array before making it visible"") but we
clearly forgot about msg and shm.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Cc: Manfred Spraul <manfred@colorfullife.com>
Cc: Davidlohr Bueso <dbueso@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",3,ipc/util.c,"{""sha"": ""1471db9a7e6112b3316ae887b50c6d8d1352f171"", ""filename"": ""ipc/msg.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 7, ""changes"": 14, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/msg.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \t\treturn retval;\n \t}\n \n-\t/* ipc_addid() locks msq upon success. */\n-\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n-\tif (id < 0) {\n-\t\tipc_rcu_putref(msq, msg_rcu_free);\n-\t\treturn id;\n-\t}\n-\n \tmsq->q_stime = msq->q_rtime = 0;\n \tmsq->q_ctime = get_seconds();\n \tmsq->q_cbytes = msq->q_qnum = 0;\n@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \tINIT_LIST_HEAD(&msq->q_receivers);\n \tINIT_LIST_HEAD(&msq->q_senders);\n \n+\t/* ipc_addid() locks msq upon success. */\n+\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n+\tif (id < 0) {\n+\t\tipc_rcu_putref(msq, msg_rcu_free);\n+\t\treturn id;\n+\t}\n+\n \tipc_unlock_object(&msq->q_perm);\n \trcu_read_unlock();\n ""}<_**next**_>{""sha"": ""41787276e14170af7de8261181721991fde528bf"", ""filename"": ""ipc/shm.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 6, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/shm.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -551,12 +551,6 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -565,6 +559,13 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*""}<_**next**_>{""sha"": ""0f401d94b7c657d5e7126fe78f149c94ffea8e24"", ""filename"": ""ipc/util.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/util.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \trcu_read_lock();\n \tspin_lock(&new->lock);\n \n+\tcurrent_euid_egid(&euid, &egid);\n+\tnew->cuid = new->uid = euid;\n+\tnew->gid = new->cgid = egid;\n+\n \tid = idr_alloc(&ids->ipcs_idr, new,\n \t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n \t\t       GFP_NOWAIT);\n@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \n \tids->in_use++;\n \n-\tcurrent_euid_egid(&euid, &egid);\n-\tnew->cuid = new->uid = euid;\n-\tnew->gid = new->cgid = egid;\n-\n \tif (next_id < 0) {\n \t\tnew->seq = ids->seq++;\n \t\tif (ids->seq > IPCID_SEQ_MAX)""}","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
",C,"	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

","	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
",,"@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
+	current_euid_egid(&euid, &egid);
+	new->cuid = new->uid = euid;
+	new->gid = new->cgid = egid;
+
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
 
 	ids->in_use++;
 
-	current_euid_egid(&euid, &egid);
-	new->cuid = new->uid = euid;
-	new->gid = new->cgid = egid;
-
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)",linux,b9a532277938798b53178d5a66af6e2915cb27cf,3225031fbeb1e32b269a82eccd815128267a4bfe,1,"int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
//fix_flaw_line_below:
//	current_euid_egid(&euid, &egid);
//fix_flaw_line_below:
//	new->cuid = new->uid = euid;
//fix_flaw_line_below:
//	new->gid = new->cgid = egid;
//fix_flaw_line_below:
//
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
//flaw_line_below:
	current_euid_egid(&euid, &egid);
//flaw_line_below:
	new->cuid = new->uid = euid;
//flaw_line_below:
	new->gid = new->cgid = egid;
//flaw_line_below:

 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
",179752,"int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","The `ipc_addid` function is responsible for adding a new IPC (Inter-Process Communication) identifier to a given set of IPC identifiers, ensuring that the system does not exceed predefined limits. It first checks if the current number of identifiers in use is less than the maximum allowed size. If space is available, it initializes a lock for the new IPC permission structure, allocates a unique identifier using an IDR (ID Radix Tree), and updates the usage count. The function also sets the creator's user and group IDs, assigns a sequence number, and constructs a unique IPC ID before returning the newly allocated identifier or an error code if allocation fails.","The code defines a function that manages the addition of a new identifier to a collection of inter-process communication identifiers. It first checks if there is available space for a new identifier and initializes necessary locks for thread safety. If space is available, it allocates a unique identifier and updates the count of currently used identifiers. The function also records the user and group information of the creator, assigns a sequence number, and constructs a unique identifier based on the allocation. Finally, it returns the new identifier or an error code if the allocation fails.","The vulnerability in the original code arises from a race condition where the user and group IDs are set after the allocation of the new IPC identifier, potentially allowing uninitialized data to be used for these comparisons. This could lead to privilege escalation if an attacker exploits the timing of the `ipc_addid` call. In the fixed code, the assignment of the creator's user and group IDs is moved before the identifier allocation, ensuring that valid and initialized values are used during the entire process. This change effectively mitigates the risk of using uninitialized data, thus addressing the race condition vulnerability.","The vulnerability stems from a race condition that occurs when user and group IDs are assigned after the allocation of a new IPC identifier, which may lead to comparisons against uninitialized data. This timing issue can be exploited by local users to escalate privileges. The fix addresses this vulnerability by ensuring that the user and group IDs are set before the identifier allocation process begins. By initializing these values earlier, the code guarantees that valid data is used throughout the operation, effectively preventing the exploitation of uninitialized data and mitigating the associated risks.","The vulnerability was fixed by moving the assignment of user and group IDs to occur before the allocation of the new IPC identifier. In the changed code, the calls to `current_euid_egid` are placed before the `idr_alloc` function, ensuring that the creator's user and group IDs are initialized and valid prior to any identifier allocation. This change prevents the use of uninitialized data during the identifier allocation process, thereby eliminating the race condition that could lead to privilege escalation. As a result, the integrity of the UID and GID comparisons is maintained throughout the operation.","The vulnerability was fixed by reordering the operations within the code to ensure that user and group IDs are assigned before the allocation of the new identifier. By initializing these IDs earlier in the process, the code guarantees that valid and initialized data is used during the entire operation, thus preventing any comparisons against uninitialized values. This adjustment effectively mitigates the race condition that could be exploited for privilege escalation, ensuring that the integrity of the identifier allocation process is maintained and that security checks are based on properly initialized data."
13,179797,179797,,Local,Not required,Complete,CVE-2015-3339,https://www.cvedetails.com/cve/CVE-2015-3339/,CWE-362,High,Complete,Complete,,2015-05-27,6.2,"Race condition in the prepare_binprm function in fs/exec.c in the Linux kernel before 3.19.6 allows local users to gain privileges by executing a setuid program at a time instant when a chown to root is in progress, and the ownership is changed but the setuid bit is not yet stripped.",2016-12-30,+Priv ,1,https://github.com/torvalds/linux/commit/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,"fs: take i_mutex during prepare_binprm for set[ug]id executables

This prevents a race between chown() and execve(), where chowning a
setuid-user binary to root would momentarily make the binary setuid
root.

This patch was mostly written by Linus Torvalds.

Signed-off-by: Jann Horn <jann@thejh.net>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",25,fs/exec.c,"{""sha"": ""49a1c61433b73722683cad25eef1fb92045e265a"", ""filename"": ""fs/exec.c"", ""status"": ""modified"", ""additions"": 48, ""deletions"": 28, ""changes"": 76, ""blob_url"": ""https://github.com/torvalds/linux/blob/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543/fs/exec.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543/fs/exec.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/exec.c?ref=8b01fc86b9f425899f8a3a8fc1c47d73c2c20543"", ""patch"": ""@@ -1275,6 +1275,53 @@ static void check_unsafe_exec(struct linux_binprm *bprm)\n \tspin_unlock(&p->fs->lock);\n }\n \n+static void bprm_fill_uid(struct linux_binprm *bprm)\n+{\n+\tstruct inode *inode;\n+\tunsigned int mode;\n+\tkuid_t uid;\n+\tkgid_t gid;\n+\n+\t/* clear any previous set[ug]id data from a previous binary */\n+\tbprm->cred->euid = current_euid();\n+\tbprm->cred->egid = current_egid();\n+\n+\tif (bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID)\n+\t\treturn;\n+\n+\tif (task_no_new_privs(current))\n+\t\treturn;\n+\n+\tinode = file_inode(bprm->file);\n+\tmode = READ_ONCE(inode->i_mode);\n+\tif (!(mode & (S_ISUID|S_ISGID)))\n+\t\treturn;\n+\n+\t/* Be careful if suid/sgid is set */\n+\tmutex_lock(&inode->i_mutex);\n+\n+\t/* reload atomically mode/uid/gid now that lock held */\n+\tmode = inode->i_mode;\n+\tuid = inode->i_uid;\n+\tgid = inode->i_gid;\n+\tmutex_unlock(&inode->i_mutex);\n+\n+\t/* We ignore suid/sgid if there are no mappings for them in the ns */\n+\tif (!kuid_has_mapping(bprm->cred->user_ns, uid) ||\n+\t\t !kgid_has_mapping(bprm->cred->user_ns, gid))\n+\t\treturn;\n+\n+\tif (mode & S_ISUID) {\n+\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n+\t\tbprm->cred->euid = uid;\n+\t}\n+\n+\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n+\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n+\t\tbprm->cred->egid = gid;\n+\t}\n+}\n+\n /*\n  * Fill the binprm structure from the inode.\n  * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes\n@@ -1283,36 +1330,9 @@ static void check_unsafe_exec(struct linux_binprm *bprm)\n  */\n int prepare_binprm(struct linux_binprm *bprm)\n {\n-\tstruct inode *inode = file_inode(bprm->file);\n-\tumode_t mode = inode->i_mode;\n \tint retval;\n \n-\n-\t/* clear any previous set[ug]id data from a previous binary */\n-\tbprm->cred->euid = current_euid();\n-\tbprm->cred->egid = current_egid();\n-\n-\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n-\t    !task_no_new_privs(current) &&\n-\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n-\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n-\t\t/* Set-uid? */\n-\t\tif (mode & S_ISUID) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->euid = inode->i_uid;\n-\t\t}\n-\n-\t\t/* Set-gid? */\n-\t\t/*\n-\t\t * If setgid is set but no group execute bit then this\n-\t\t * is a candidate for mandatory locking, not a setgid\n-\t\t * executable.\n-\t\t */\n-\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->egid = inode->i_gid;\n-\t\t}\n-\t}\n+\tbprm_fill_uid(bprm);\n \n \t/* fill in binprm security blob */\n \tretval = security_bprm_set_creds(bprm);""}"," int prepare_binprm(struct linux_binprm *bprm)
 {
 	int retval;
 
	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
"," int prepare_binprm(struct linux_binprm *bprm)
 {
	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
 	int retval;
 
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
",C,"	bprm_fill_uid(bprm);
","	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
",,"@@ -1275,6 +1275,53 @@ static void check_unsafe_exec(struct linux_binprm *bprm)
 	spin_unlock(&p->fs->lock);
 }
 
+static void bprm_fill_uid(struct linux_binprm *bprm)
+{
+	struct inode *inode;
+	unsigned int mode;
+	kuid_t uid;
+	kgid_t gid;
+
+	/* clear any previous set[ug]id data from a previous binary */
+	bprm->cred->euid = current_euid();
+	bprm->cred->egid = current_egid();
+
+	if (bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID)
+		return;
+
+	if (task_no_new_privs(current))
+		return;
+
+	inode = file_inode(bprm->file);
+	mode = READ_ONCE(inode->i_mode);
+	if (!(mode & (S_ISUID|S_ISGID)))
+		return;
+
+	/* Be careful if suid/sgid is set */
+	mutex_lock(&inode->i_mutex);
+
+	/* reload atomically mode/uid/gid now that lock held */
+	mode = inode->i_mode;
+	uid = inode->i_uid;
+	gid = inode->i_gid;
+	mutex_unlock(&inode->i_mutex);
+
+	/* We ignore suid/sgid if there are no mappings for them in the ns */
+	if (!kuid_has_mapping(bprm->cred->user_ns, uid) ||
+		 !kgid_has_mapping(bprm->cred->user_ns, gid))
+		return;
+
+	if (mode & S_ISUID) {
+		bprm->per_clear |= PER_CLEAR_ON_SETID;
+		bprm->cred->euid = uid;
+	}
+
+	if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
+		bprm->per_clear |= PER_CLEAR_ON_SETID;
+		bprm->cred->egid = gid;
+	}
+}
+
 /*
  * Fill the binprm structure from the inode.
  * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes
@@ -1283,36 +1330,9 @@ static void check_unsafe_exec(struct linux_binprm *bprm)
  */
 int prepare_binprm(struct linux_binprm *bprm)
 {
-	struct inode *inode = file_inode(bprm->file);
-	umode_t mode = inode->i_mode;
 	int retval;
 
-
-	/* clear any previous set[ug]id data from a previous binary */
-	bprm->cred->euid = current_euid();
-	bprm->cred->egid = current_egid();
-
-	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
-	    !task_no_new_privs(current) &&
-	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
-	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
-		/* Set-uid? */
-		if (mode & S_ISUID) {
-			bprm->per_clear |= PER_CLEAR_ON_SETID;
-			bprm->cred->euid = inode->i_uid;
-		}
-
-		/* Set-gid? */
-		/*
-		 * If setgid is set but no group execute bit then this
-		 * is a candidate for mandatory locking, not a setgid
-		 * executable.
-		 */
-		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
-			bprm->per_clear |= PER_CLEAR_ON_SETID;
-			bprm->cred->egid = inode->i_gid;
-		}
-	}
+	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);",linux,8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,5224b9613b91d937c6948fe977023247afbcc04e,1," int prepare_binprm(struct linux_binprm *bprm)
 {
//flaw_line_below:
	struct inode *inode = file_inode(bprm->file);
//flaw_line_below:
	umode_t mode = inode->i_mode;
 	int retval;
 
//flaw_line_below:

//flaw_line_below:
	/* clear any previous set[ug]id data from a previous binary */
//flaw_line_below:
	bprm->cred->euid = current_euid();
//flaw_line_below:
	bprm->cred->egid = current_egid();
//flaw_line_below:

//flaw_line_below:
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
//flaw_line_below:
	    !task_no_new_privs(current) &&
//flaw_line_below:
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
//flaw_line_below:
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
//flaw_line_below:
		/* Set-uid? */
//flaw_line_below:
		if (mode & S_ISUID) {
//flaw_line_below:
			bprm->per_clear |= PER_CLEAR_ON_SETID;
//flaw_line_below:
			bprm->cred->euid = inode->i_uid;
//flaw_line_below:
		}
//flaw_line_below:

//flaw_line_below:
		/* Set-gid? */
//flaw_line_below:
		/*
//flaw_line_below:
		 * If setgid is set but no group execute bit then this
//flaw_line_below:
		 * is a candidate for mandatory locking, not a setgid
//flaw_line_below:
		 * executable.
//flaw_line_below:
		 */
//flaw_line_below:
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
//flaw_line_below:
			bprm->per_clear |= PER_CLEAR_ON_SETID;
//flaw_line_below:
			bprm->cred->egid = inode->i_gid;
//flaw_line_below:
		}
//flaw_line_below:
	}
//fix_flaw_line_below:
//	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
",179797," int prepare_binprm(struct linux_binprm *bprm)
 {
	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
 	int retval;
 
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
"," int prepare_binprm(struct linux_binprm *bprm)
 {
 	int retval;
 
	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
","The `prepare_binprm` function initializes a binary program's parameters for execution in a Linux environment. It begins by retrieving the inode of the file associated with the binary and its mode. The function clears any previous set-user or set-group ID data, then checks if the binary can inherit these IDs based on its permissions and the current task's settings. If the binary has set-user or set-group ID attributes, it updates the effective user and group IDs accordingly. The function also prepares a security context for the binary and reads its content into a buffer, returning any errors encountered during these operations.","The code initializes the parameters necessary for executing a binary file in a Linux system. It starts by retrieving the file's metadata and clearing any previous user or group ID settings. The function checks if the binary can inherit specific privileges based on its permissions and the current task's configuration. If applicable, it updates the effective user and group IDs to those specified in the file's metadata. Additionally, it sets up a security context for the binary and reads its content into a designated buffer, handling any errors that may arise during these processes.","The vulnerability in the original code arises from a race condition where a local user could exploit the timing of a file ownership change while executing a setuid program. Specifically, if a user executed a setuid binary during the brief window when the ownership was being changed to root but before the setuid bit was stripped, they could gain elevated privileges. The fixed code addresses this issue by introducing the `bprm_fill_uid` function, which ensures that the effective user ID is set correctly before any security checks are performed, thereby preventing the race condition and ensuring that privilege escalation cannot occur during the ownership transition.","The vulnerability stems from a race condition that allows local users to exploit the timing of file ownership changes while executing a setuid program. During the brief period when the ownership is being changed to root but before the setuid bit is removed, a user could gain elevated privileges by executing the binary. The fix addresses this issue by ensuring that the effective user ID is set correctly before any security checks are conducted. This change prevents the race condition, thereby eliminating the opportunity for privilege escalation during the ownership transition process.","The vulnerability was fixed by introducing the `bprm_fill_uid` function, which is called at the beginning of the modified code. This function ensures that the effective user ID and group ID are set correctly before any security checks are performed. By doing this, the code eliminates the race condition that previously allowed a user to execute a setuid program during a file ownership change. Consequently, the security checks that follow, including the call to `security_bprm_set_creds`, now operate on a properly initialized context, preventing unauthorized privilege escalation during the critical ownership transition period.","The vulnerability was fixed by ensuring that the effective user and group IDs are set correctly before any security checks are performed. This adjustment prevents the race condition that allowed a user to exploit the timing of file ownership changes while executing a setuid program. By establishing the correct user context early in the process, the code eliminates the opportunity for privilege escalation during the critical period when ownership is being modified. This change ensures that security checks operate on a properly initialized state, thereby safeguarding against unauthorized access and privilege elevation."
14,179863,179863,,Remote,Not required,Partial,CVE-2015-1791,https://www.cvedetails.com/cve/CVE-2015-1791/,CWE-362,Medium,Partial,Partial,,2015-06-12,6.8,"Race condition in the ssl3_get_new_session_ticket function in ssl/s3_clnt.c in OpenSSL before 0.9.8zg, 1.0.0 before 1.0.0s, 1.0.1 before 1.0.1n, and 1.0.2 before 1.0.2b, when used for a multi-threaded client, allows remote attackers to cause a denial of service (double free and application crash) or possibly have unspecified other impact by providing a NewSessionTicket during an attempt to reuse a ticket that had been obtained earlier.",2017-11-14,DoS ,32,https://github.com/openssl/openssl/commit/98ece4eebfb6cd45cc8d550c6ac0022965071afc,98ece4eebfb6cd45cc8d550c6ac0022965071afc,"Fix race condition in NewSessionTicket

If a NewSessionTicket is received by a multi-threaded client when
attempting to reuse a previous ticket then a race condition can occur
potentially leading to a double free of the ticket data.

CVE-2015-1791

This also fixes RT#3808 where a session ID is changed for a session already
in the client session cache. Since the session ID is the key to the cache
this breaks the cache access.

Parts of this patch were inspired by this Akamai change:
https://github.com/akamai/openssl/commit/c0bf69a791239ceec64509f9f19fcafb2461b0d3

Reviewed-by: Rich Salz <rsalz@openssl.org>",0,ssl/s3_clnt.c,"{""sha"": ""4e18b65605431b7ef0e7b0fec99604d9c77b404c"", ""filename"": ""include/openssl/ssl.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/include/openssl/ssl.h"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/include/openssl/ssl.h"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/include/openssl/ssl.h?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -2048,6 +2048,7 @@ void ERR_load_SSL_strings(void);\n # define SSL_F_SSL_READ                                   223\n # define SSL_F_SSL_SCAN_CLIENTHELLO_TLSEXT                320\n # define SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT                321\n+# define SSL_F_SSL_SESSION_DUP                            348\n # define SSL_F_SSL_SESSION_NEW                            189\n # define SSL_F_SSL_SESSION_PRINT_FP                       190\n # define SSL_F_SSL_SESSION_SET1_ID_CONTEXT                312""}<_**next**_>{""sha"": ""d6f53b0dea846bba4e7e93b37768faea1dfa879f"", ""filename"": ""ssl/s3_clnt.c"", ""status"": ""modified"", ""additions"": 32, ""deletions"": 0, ""changes"": 32, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/s3_clnt.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/s3_clnt.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/s3_clnt.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -2238,6 +2238,38 @@ int ssl3_get_new_session_ticket(SSL *s)\n     }\n \n     p = d = (unsigned char *)s->init_msg;\n+\n+    if (s->session->session_id_length > 0) {\n+        int i = s->session_ctx->session_cache_mode;\n+        SSL_SESSION *new_sess;\n+        /*\n+         * We reused an existing session, so we need to replace it with a new\n+         * one\n+         */\n+        if (i & SSL_SESS_CACHE_CLIENT) {\n+            /*\n+             * Remove the old session from the cache\n+             */\n+            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {\n+                if (s->session_ctx->remove_session_cb != NULL)\n+                    s->session_ctx->remove_session_cb(s->session_ctx,\n+                                                      s->session);\n+            } else {\n+                /* We carry on if this fails */\n+                SSL_CTX_remove_session(s->session_ctx, s->session);\n+            }\n+        }\n+\n+        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {\n+            al = SSL_AD_INTERNAL_ERROR;\n+            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);\n+            goto f_err;\n+        }\n+\n+        SSL_SESSION_free(s->session);\n+        s->session = new_sess;\n+    }\n+\n     n2l(p, s->session->tlsext_tick_lifetime_hint);\n     n2s(p, ticklen);\n     /* ticket_lifetime_hint + ticket_length + ticket */""}<_**next**_>{""sha"": ""4b4d89ce7a1e8c6a596224a60588f1a6b314b051"", ""filename"": ""ssl/ssl_err.c"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_err.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_err.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_err.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -274,6 +274,7 @@ static ERR_STRING_DATA SSL_str_functs[] = {\n      \""SSL_SCAN_CLIENTHELLO_TLSEXT\""},\n     {ERR_FUNC(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT),\n      \""SSL_SCAN_SERVERHELLO_TLSEXT\""},\n+    {ERR_FUNC(SSL_F_SSL_SESSION_DUP), \""ssl_session_dup\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_NEW), \""SSL_SESSION_new\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_PRINT_FP), \""SSL_SESSION_print_fp\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_SET1_ID_CONTEXT),""}<_**next**_>{""sha"": ""3252631e1c8981e39a828c59b98c7e63931e4a25"", ""filename"": ""ssl/ssl_locl.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_locl.h"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_locl.h"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_locl.h?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -1860,6 +1860,7 @@ __owur int ssl_set_peer_cert_type(SESS_CERT *c, int type);\n __owur int ssl_get_new_session(SSL *s, int session);\n __owur int ssl_get_prev_session(SSL *s, unsigned char *session, int len,\n                          const unsigned char *limit);\n+__owur SSL_SESSION *ssl_session_dup(SSL_SESSION *src, int ticket);\n __owur int ssl_cipher_id_cmp(const SSL_CIPHER *a, const SSL_CIPHER *b);\n DECLARE_OBJ_BSEARCH_GLOBAL_CMP_FN(SSL_CIPHER, SSL_CIPHER, ssl_cipher_id);\n __owur int ssl_cipher_ptr_id_cmp(const SSL_CIPHER *const *ap,""}<_**next**_>{""sha"": ""fd940541d5bc0fdb02c298da55170b40d8397eb4"", ""filename"": ""ssl/ssl_sess.c"", ""status"": ""modified"", ""additions"": 116, ""deletions"": 0, ""changes"": 116, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_sess.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_sess.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_sess.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -225,6 +225,122 @@ SSL_SESSION *SSL_SESSION_new(void)\n     return (ss);\n }\n \n+/*\n+ * Create a new SSL_SESSION and duplicate the contents of |src| into it. If\n+ * ticket == 0 then no ticket information is duplicated, otherwise it is.\n+ */\n+SSL_SESSION *ssl_session_dup(SSL_SESSION *src, int ticket)\n+{\n+    SSL_SESSION *dest;\n+\n+    dest = OPENSSL_malloc(sizeof(*src));\n+    if (dest == NULL) {\n+        goto err;\n+    }\n+    memcpy(dest, src, sizeof(*dest));\n+\n+#ifndef OPENSSL_NO_PSK\n+    if (src->psk_identity_hint) {\n+        dest->psk_identity_hint = BUF_strdup(src->psk_identity_hint);\n+        if (dest->psk_identity_hint == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->psk_identity_hint = NULL;\n+    }\n+    if (src->psk_identity) {\n+        dest->psk_identity = BUF_strdup(src->psk_identity);\n+        if (dest->psk_identity == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->psk_identity = NULL;\n+    }\n+#endif\n+\n+    if (src->sess_cert != NULL)\n+        CRYPTO_add(&src->sess_cert->references, 1, CRYPTO_LOCK_SSL_SESS_CERT);\n+\n+    if (src->peer != NULL)\n+        CRYPTO_add(&src->peer->references, 1, CRYPTO_LOCK_X509);\n+\n+    dest->references = 1;\n+\n+    if(src->ciphers != NULL) {\n+        dest->ciphers = sk_SSL_CIPHER_dup(src->ciphers);\n+        if (dest->ciphers == NULL)\n+            goto err;\n+    } else {\n+        dest->ciphers = NULL;\n+    }\n+\n+    if (!CRYPTO_dup_ex_data(CRYPTO_EX_INDEX_SSL_SESSION,\n+                                            &dest->ex_data, &src->ex_data)) {\n+        goto err;\n+    }\n+\n+    /* We deliberately don't copy the prev and next pointers */\n+    dest->prev = NULL;\n+    dest->next = NULL;\n+\n+#ifndef OPENSSL_NO_TLSEXT\n+    if (src->tlsext_hostname) {\n+        dest->tlsext_hostname = BUF_strdup(src->tlsext_hostname);\n+        if (dest->tlsext_hostname == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->tlsext_hostname = NULL;\n+    }\n+# ifndef OPENSSL_NO_EC\n+    if (src->tlsext_ecpointformatlist) {\n+        dest->tlsext_ecpointformatlist =\n+            BUF_memdup(src->tlsext_ecpointformatlist,\n+                       src->tlsext_ecpointformatlist_length);\n+        if (dest->tlsext_ecpointformatlist == NULL)\n+            goto err;\n+        dest->tlsext_ecpointformatlist_length =\n+            src->tlsext_ecpointformatlist_length;\n+    }\n+    if (src->tlsext_ellipticcurvelist) {\n+        dest->tlsext_ellipticcurvelist =\n+            BUF_memdup(src->tlsext_ellipticcurvelist,\n+                       src->tlsext_ellipticcurvelist_length);\n+        if (dest->tlsext_ellipticcurvelist == NULL)\n+            goto err;\n+        dest->tlsext_ellipticcurvelist_length =\n+            src->tlsext_ellipticcurvelist_length;\n+    }\n+# endif\n+#endif\n+\n+    if (ticket != 0) {\n+        dest->tlsext_tick_lifetime_hint = src->tlsext_tick_lifetime_hint;\n+        dest->tlsext_ticklen = src->tlsext_ticklen;\n+        if((dest->tlsext_tick = OPENSSL_malloc(src->tlsext_ticklen)) == NULL) {\n+            goto err;\n+        }\n+    }\n+\n+#ifndef OPENSSL_NO_SRP\n+    dest->srp_username = NULL;\n+    if (src->srp_username) {\n+        dest->srp_username = BUF_strdup(src->srp_username);\n+        if (dest->srp_username == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->srp_username = NULL;\n+    }\n+#endif\n+\n+    return dest;\n+err:\n+    SSLerr(SSL_F_SSL_SESSION_DUP, ERR_R_MALLOC_FAILURE);\n+    SSL_SESSION_free(dest);\n+    return NULL;\n+}\n+\n const unsigned char *SSL_SESSION_get_id(const SSL_SESSION *s,\n                                         unsigned int *len)\n {""}","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;

    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
",C,"
    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

",,,"@@ -2238,6 +2238,38 @@ int ssl3_get_new_session_ticket(SSL *s)
     }
 
     p = d = (unsigned char *)s->init_msg;
+
+    if (s->session->session_id_length > 0) {
+        int i = s->session_ctx->session_cache_mode;
+        SSL_SESSION *new_sess;
+        /*
+         * We reused an existing session, so we need to replace it with a new
+         * one
+         */
+        if (i & SSL_SESS_CACHE_CLIENT) {
+            /*
+             * Remove the old session from the cache
+             */
+            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
+                if (s->session_ctx->remove_session_cb != NULL)
+                    s->session_ctx->remove_session_cb(s->session_ctx,
+                                                      s->session);
+            } else {
+                /* We carry on if this fails */
+                SSL_CTX_remove_session(s->session_ctx, s->session);
+            }
+        }
+
+        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
+            al = SSL_AD_INTERNAL_ERROR;
+            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
+            goto f_err;
+        }
+
+        SSL_SESSION_free(s->session);
+        s->session = new_sess;
+    }
+
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */",openssl,98ece4eebfb6cd45cc8d550c6ac0022965071afc,8c2b1d872b25f3ec78e04f5cd2de8f21e853c4a6,1,"int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//    if (s->session->session_id_length > 0) {
//fix_flaw_line_below:
//        int i = s->session_ctx->session_cache_mode;
//fix_flaw_line_below:
//        SSL_SESSION *new_sess;
//fix_flaw_line_below:
//        /*
//fix_flaw_line_below:
//         * We reused an existing session, so we need to replace it with a new
//fix_flaw_line_below:
//         * one
//fix_flaw_line_below:
//         */
//fix_flaw_line_below:
//        if (i & SSL_SESS_CACHE_CLIENT) {
//fix_flaw_line_below:
//            /*
//fix_flaw_line_below:
//             * Remove the old session from the cache
//fix_flaw_line_below:
//             */
//fix_flaw_line_below:
//            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
//fix_flaw_line_below:
//                if (s->session_ctx->remove_session_cb != NULL)
//fix_flaw_line_below:
//                    s->session_ctx->remove_session_cb(s->session_ctx,
//fix_flaw_line_below:
//                                                      s->session);
//fix_flaw_line_below:
//            } else {
//fix_flaw_line_below:
//                /* We carry on if this fails */
//fix_flaw_line_below:
//                SSL_CTX_remove_session(s->session_ctx, s->session);
//fix_flaw_line_below:
//            }
//fix_flaw_line_below:
//        }
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
//fix_flaw_line_below:
//            al = SSL_AD_INTERNAL_ERROR;
//fix_flaw_line_below:
//            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
//fix_flaw_line_below:
//            goto f_err;
//fix_flaw_line_below:
//        }
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//        SSL_SESSION_free(s->session);
//fix_flaw_line_below:
//        s->session = new_sess;
//fix_flaw_line_below:
//    }
//fix_flaw_line_below:
//
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
",179863,"int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;

    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","The function `ssl3_get_new_session_ticket` processes a new session ticket in an SSL/TLS context. It first retrieves a message containing the session ticket and checks its validity. If the message length is insufficient, it triggers an error. The function then extracts the ticket lifetime hint and the ticket length, ensuring they match the expected size. It allocates memory for the new session ticket, copies the ticket data, and computes a session ID based on the ticket's hash. If any errors occur during these steps, appropriate alerts are sent, and the function returns an error code; otherwise, it returns success.","The code handles the reception and processing of a new session ticket in an SSL/TLS protocol context. It begins by retrieving a message and validating its length to ensure it contains the necessary information. If the message is too short, an error is triggered. The code then extracts the lifetime hint and ticket length, checking for consistency. It allocates memory for the new ticket, copies the relevant data, and generates a session identifier based on the ticket's content. If any issues arise during these operations, appropriate error alerts are sent, and the function indicates failure; otherwise, it confirms success.","The vulnerability in the original code arises from a race condition that occurs when multiple threads attempt to reuse a session ticket simultaneously, leading to a potential double free of the session object. This happens because the code does not adequately manage the session state when a new session ticket is received while another thread is still processing the existing session. The fixed code addresses this issue by checking if the session ID length is greater than zero before attempting to duplicate the session. It ensures that the old session is properly removed from the cache and replaced with a new session, thus preventing the race condition and potential denial of service.","The vulnerability in the original code stems from a race condition that occurs in a multi-threaded environment, where simultaneous attempts to reuse a session ticket can lead to a double free of memory, resulting in application crashes or denial of service. This happens when one thread processes a new session ticket while another thread is still using the existing session, causing inconsistencies in memory management. The fixed code mitigates this issue by implementing checks to ensure that the session is properly managed and replaced only when it is safe to do so, thus preventing concurrent access issues and ensuring stable memory handling.","The vulnerability was fixed by introducing a check for the session ID length before attempting to duplicate the session. In the updated code, if the session ID length is greater than zero, it indicates that an existing session is in use. The code then safely removes the old session from the cache, either by calling a removal callback or using a specific function to delete it from the session context. After ensuring the old session is properly handled, the code duplicates the session and assigns it to the session pointer, thus preventing race conditions and potential double free errors.","The vulnerability was addressed by implementing a check to determine if an existing session is in use before proceeding with any modifications. This ensures that the code only attempts to replace the session when it is safe to do so, thereby preventing concurrent access issues. The fix includes mechanisms to properly remove the old session from the cache, ensuring that memory is managed correctly and reducing the risk of double freeing. By carefully managing the session state and ensuring that operations on the session are synchronized, the updated code effectively mitigates the race condition that could lead to application crashes."
15,179914,179914,,Local,Not required,Complete,CVE-2015-0239,https://www.cvedetails.com/cve/CVE-2015-0239/,CWE-362,High,,Partial,,2015-03-02,4.7,"The em_sysenter function in arch/x86/kvm/emulate.c in the Linux kernel before 3.18.5, when the guest OS lacks SYSENTER MSR initialization, allows guest OS users to gain guest OS privileges or cause a denial of service (guest OS crash) by triggering use of a 16-bit code segment for emulation of a SYSENTER instruction.",2016-12-23,DoS +Priv ,8,https://github.com/torvalds/linux/commit/f3747379accba8e95d70cec0eae0582c8c182050,f3747379accba8e95d70cec0eae0582c8c182050,"KVM: x86: SYSENTER emulation is broken

SYSENTER emulation is broken in several ways:
1. It misses the case of 16-bit code segments completely (CVE-2015-0239).
2. MSR_IA32_SYSENTER_CS is checked in 64-bit mode incorrectly (bits 0 and 1 can
   still be set without causing #GP).
3. MSR_IA32_SYSENTER_EIP and MSR_IA32_SYSENTER_ESP are not masked in
   legacy-mode.
4. There is some unneeded code.

Fix it.

Cc: stable@vger.linux.org
Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>",19,arch/x86/kvm/emulate.c,"{""sha"": ""de12c1d379f16899645d96a2c3fd75663919c86d"", ""filename"": ""arch/x86/kvm/emulate.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 19, ""changes"": 27, ""blob_url"": ""https://github.com/torvalds/linux/blob/f3747379accba8e95d70cec0eae0582c8c182050/arch/x86/kvm/emulate.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/f3747379accba8e95d70cec0eae0582c8c182050/arch/x86/kvm/emulate.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kvm/emulate.c?ref=f3747379accba8e95d70cec0eae0582c8c182050"", ""patch"": ""@@ -2348,7 +2348,7 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \t * Not recognized on AMD in compat mode (but is recognized in legacy\n \t * mode).\n \t */\n-\tif ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)\n+\tif ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)\n \t    && !vendor_intel(ctxt))\n \t\treturn emulate_ud(ctxt);\n \n@@ -2359,25 +2359,13 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \tsetup_syscalls_segments(ctxt, &cs, &ss);\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n-\tswitch (ctxt->mode) {\n-\tcase X86EMUL_MODE_PROT32:\n-\t\tif ((msr_data & 0xfffc) == 0x0)\n-\t\t\treturn emulate_gp(ctxt, 0);\n-\t\tbreak;\n-\tcase X86EMUL_MODE_PROT64:\n-\t\tif (msr_data == 0x0)\n-\t\t\treturn emulate_gp(ctxt, 0);\n-\t\tbreak;\n-\tdefault:\n-\t\tbreak;\n-\t}\n+\tif ((msr_data & 0xfffc) == 0x0)\n+\t\treturn emulate_gp(ctxt, 0);\n \n \tctxt->eflags &= ~(EFLG_VM | EFLG_IF);\n-\tcs_sel = (u16)msr_data;\n-\tcs_sel &= ~SELECTOR_RPL_MASK;\n+\tcs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;\n \tss_sel = cs_sel + 8;\n-\tss_sel &= ~SELECTOR_RPL_MASK;\n-\tif (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {\n+\tif (efer & EFER_LMA) {\n \t\tcs.d = 0;\n \t\tcs.l = 1;\n \t}\n@@ -2386,10 +2374,11 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);\n-\tctxt->_eip = msr_data;\n+\tctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);\n-\t*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;\n+\t*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :\n+\t\t\t\t\t\t\t      (u32)msr_data;\n \n \treturn X86EMUL_CONTINUE;\n }""}","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
 
 	return X86EMUL_CONTINUE;
 }
",C,"	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
	if (efer & EFER_LMA) {
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
","	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
	ctxt->_eip = msr_data;
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
",,"@@ -2348,7 +2348,7 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
-	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
+	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
@@ -2359,25 +2359,13 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
-	switch (ctxt->mode) {
-	case X86EMUL_MODE_PROT32:
-		if ((msr_data & 0xfffc) == 0x0)
-			return emulate_gp(ctxt, 0);
-		break;
-	case X86EMUL_MODE_PROT64:
-		if (msr_data == 0x0)
-			return emulate_gp(ctxt, 0);
-		break;
-	default:
-		break;
-	}
+	if ((msr_data & 0xfffc) == 0x0)
+		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
-	cs_sel = (u16)msr_data;
-	cs_sel &= ~SELECTOR_RPL_MASK;
+	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
-	ss_sel &= ~SELECTOR_RPL_MASK;
-	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
+	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}
@@ -2386,10 +2374,11 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
-	ctxt->_eip = msr_data;
+	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
-	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
+	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
+							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }",linux,f3747379accba8e95d70cec0eae0582c8c182050,63ea0a49ae0b145b91ff2b070c01b66fc75854b9,1,"static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
//flaw_line_below:
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
//fix_flaw_line_below:
//	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
//flaw_line_below:
	switch (ctxt->mode) {
//flaw_line_below:
	case X86EMUL_MODE_PROT32:
//flaw_line_below:
		if ((msr_data & 0xfffc) == 0x0)
//flaw_line_below:
			return emulate_gp(ctxt, 0);
//flaw_line_below:
		break;
//flaw_line_below:
	case X86EMUL_MODE_PROT64:
//flaw_line_below:
		if (msr_data == 0x0)
//flaw_line_below:
			return emulate_gp(ctxt, 0);
//flaw_line_below:
		break;
//flaw_line_below:
	default:
//flaw_line_below:
		break;
//flaw_line_below:
	}
//fix_flaw_line_below:
//	if ((msr_data & 0xfffc) == 0x0)
//fix_flaw_line_below:
//		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
//flaw_line_below:
	cs_sel = (u16)msr_data;
//flaw_line_below:
	cs_sel &= ~SELECTOR_RPL_MASK;
//fix_flaw_line_below:
//	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
//flaw_line_below:
	ss_sel &= ~SELECTOR_RPL_MASK;
//flaw_line_below:
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
//fix_flaw_line_below:
//	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
//flaw_line_below:
	ctxt->_eip = msr_data;
//fix_flaw_line_below:
//	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
//flaw_line_below:
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
//fix_flaw_line_below:
//	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
//fix_flaw_line_below:
//							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
",179914,"static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","The `em_sysenter` function emulates the `SYSENTER` instruction in an x86 architecture context, handling different operating modes such as real, protected 32-bit, and protected 64-bit. It first retrieves the Extended Feature Enable Register (EFER) and checks for compatibility issues, injecting a general protection fault if necessary. The function sets up the code and stack segment descriptors, ensuring proper privilege levels, and updates the instruction pointer and stack pointer based on the values from the Model-Specific Registers (MSRs) for `SYSENTER`. Finally, it returns a status indicating whether to continue emulation or handle an error.","The code emulates the execution of a specific instruction in an x86 architecture, managing various operating modes such as real and protected modes. It first checks for compatibility and potential errors, injecting faults when necessary. The function sets up segment descriptors for code and stack, ensuring correct privilege levels, and updates the instruction and stack pointers based on specific register values. It handles different scenarios for 32-bit and 64-bit modes, ultimately determining whether to continue the emulation process or signal an error condition based on the outcomes of these checks and setups.","The vulnerability in the original code arises from the lack of proper checks for the initialization of the SYSENTER Model-Specific Registers (MSRs), which could allow a guest operating system to execute a SYSENTER instruction using an uninitialized or incorrect 16-bit code segment. This could lead to privilege escalation or denial of service. The fixed code addresses this by ensuring that the code segment selector is validated before being used, specifically checking if the MSR value is zero and returning an appropriate fault if it is, thus preventing the execution of potentially harmful instructions and ensuring proper emulation behavior.","The vulnerability in the original code stems from inadequate validation of the initialization state of specific registers, which could allow a guest operating system to execute a SYSENTER instruction with an uninitialized or incorrect code segment. This oversight could lead to privilege escalation or crashes within the guest OS. The fixed code rectifies this issue by implementing checks to ensure that the relevant registers are properly initialized before use. If the checks reveal that the registers are not set correctly, the code now returns an appropriate fault, thereby preventing the execution of potentially harmful instructions and enhancing overall system stability.","The vulnerability was fixed by adding checks to ensure that the SYSENTER Model-Specific Registers (MSRs) are properly initialized before being used. In the updated code, the function retrieves the value of the SYSENTER code segment MSR and checks if it is zero. If the value is zero, it returns a general protection fault, preventing the use of an uninitialized code segment. Additionally, the code now ensures that the instruction and stack pointers are set correctly based on the state of the Extended Feature Enable Register, thereby safeguarding against privilege escalation and enhancing the stability of the emulation process.","The vulnerability was addressed by implementing additional validation checks to ensure that critical registers are properly initialized before they are utilized. The updated code now verifies the values of the relevant registers, specifically checking for zero values that indicate uninitialized states. If such values are detected, the code responds by returning a fault, thereby preventing the execution of potentially harmful instructions. This proactive approach ensures that only valid and initialized states are processed, effectively mitigating the risk of privilege escalation and enhancing the overall stability and security of the emulation environment."
16,179955,179955,,Local,Not required,Complete,CVE-2014-9529,https://www.cvedetails.com/cve/CVE-2014-9529/,CWE-362,Low,Complete,Complete,,2015-01-09,7.2,Race condition in the key_gc_unused_keys function in security/keys/gc.c in the Linux kernel through 3.18.2 allows local users to cause a denial of service (memory corruption or panic) or possibly have unspecified other impact via keyctl commands that trigger access to a key structure member during garbage collection of a key.,2018-01-04,DoS Mem. Corr. ,2,https://github.com/torvalds/linux/commit/a3a8784454692dd72e5d5d34dcdab17b4420e74c,a3a8784454692dd72e5d5d34dcdab17b4420e74c,"KEYS: close race between key lookup and freeing

When a key is being garbage collected, it's key->user would get put before
the ->destroy() callback is called, where the key is removed from it's
respective tracking structures.

This leaves a key hanging in a semi-invalid state which leaves a window open
for a different task to try an access key->user. An example is
find_keyring_by_name() which would dereference key->user for a key that is
in the process of being garbage collected (where key->user was freed but
->destroy() wasn't called yet - so it's still present in the linked list).

This would cause either a panic, or corrupt memory.

Fixes CVE-2014-9529.

Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
Signed-off-by: David Howells <dhowells@redhat.com>",1,security/keys/gc.c,"{""sha"": ""c7952375ac5325cfb4c403fa1020671b5f31a150"", ""filename"": ""security/keys/gc.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 2, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/a3a8784454692dd72e5d5d34dcdab17b4420e74c/security/keys/gc.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/a3a8784454692dd72e5d5d34dcdab17b4420e74c/security/keys/gc.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/security/keys/gc.c?ref=a3a8784454692dd72e5d5d34dcdab17b4420e74c"", ""patch"": ""@@ -148,12 +148,12 @@ static noinline void key_gc_unused_keys(struct list_head *keys)\n \t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n \t\t\tatomic_dec(&key->user->nikeys);\n \n-\t\tkey_user_put(key->user);\n-\n \t\t/* now throw away the key memory */\n \t\tif (key->type->destroy)\n \t\t\tkey->type->destroy(key);\n \n+\t\tkey_user_put(key->user);\n+\n \t\tkfree(key->description);\n \n #ifdef KEY_DEBUGGING""}","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
		key_user_put(key->user);

 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
		key_user_put(key->user);
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
",C,"		key_user_put(key->user);

","		key_user_put(key->user);
",,"@@ -148,12 +148,12 @@ static noinline void key_gc_unused_keys(struct list_head *keys)
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
-		key_user_put(key->user);
-
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
+		key_user_put(key->user);
+
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING",linux,a3a8784454692dd72e5d5d34dcdab17b4420e74c,693a30b8f19a964087a3762d09fb2e1cbad6b0d4,1,"static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
//flaw_line_below:
		key_user_put(key->user);
//flaw_line_below:

 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
//fix_flaw_line_below:
//		key_user_put(key->user);
//fix_flaw_line_below:
//
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
",179955,"static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
		key_user_put(key->user);
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
		key_user_put(key->user);

 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","The function `key_gc_unused_keys` is designed to clean up and free unused keys from a linked list, referred to as `keys`. It iterates through the list, removing each key and performing several operations: logging the key's serial number, checking its validity, and freeing its associated resources. If the key is tracked by a user, it updates the user's quota and key counts accordingly. The function also invokes a destroy method if defined for the key type, deallocates the key's description, and finally frees the key's memory using a cache. Debugging information is optionally set for tracking purposes.","The provided code defines a function that manages the cleanup of unused keys from a linked list. It iteratively processes each key, removing it from the list and performing several tasks, including logging its identifier and verifying its integrity. The function also updates the associated user's quota and key counts if the key is tracked. Additionally, it releases any resources linked to the key, such as its description, and invokes a destruction routine if applicable. Finally, it frees the memory allocated for the key, ensuring efficient resource management and preventing memory leaks.","The vulnerability, identified as CWE-362, arises from a race condition in the original code where a key structure could be accessed concurrently while being modified during garbage collection. Specifically, the function does not adequately synchronize access to the key's user data, allowing potential memory corruption or system panic when multiple threads interact with the same key. The fixed code addresses this issue by ensuring that access to the user's key tracking and quota is protected with a spin lock, which prevents concurrent modifications while the key is being processed, thus eliminating the race condition and enhancing overall stability.","The vulnerability, classified as CWE-362, involves a race condition that occurs when multiple threads access and modify a key structure simultaneously during garbage collection. This can lead to memory corruption or system instability, as one thread may alter the key's state while another is reading or modifying it, resulting in unpredictable behavior. The fix addresses this issue by implementing synchronization mechanisms that ensure exclusive access to the key's associated user data during modifications. This prevents concurrent access, thereby eliminating the race condition and enhancing the stability and security of the system during key management operations.","The vulnerability was fixed by introducing a spin lock around the critical section that modifies the user's key tracking and quota. Specifically, the code now uses `spin_lock(&key->user->lock)` before decrementing the user's key count and quota values, ensuring that these operations are atomic and preventing concurrent access. This synchronization mechanism ensures that while one thread is processing a key, no other thread can modify the same user's data, thus eliminating the race condition. The use of `spin_unlock(&key->user->lock)` after the updates ensures that the lock is released, allowing other threads to safely access the user data afterward.","The vulnerability was fixed by implementing a synchronization mechanism that ensures exclusive access to shared resources during critical operations. This was achieved by introducing a locking mechanism around the section of code that modifies user-related data, preventing concurrent access from multiple threads. By ensuring that only one thread can modify the user's key tracking and quota at a time, the fix eliminates the race condition that could lead to memory corruption or system instability. This approach enhances the overall stability and security of the system during the key management process, safeguarding against potential denial of service attacks."
17,180092,180092,,Local,Not required,,CVE-2016-7916,https://www.cvedetails.com/cve/CVE-2016-7916/,CWE-362,Medium,Complete,,,2016-11-16,4.7,Race condition in the environ_read function in fs/proc/base.c in the Linux kernel before 4.5.4 allows local users to obtain sensitive information from kernel memory by reading a /proc/*/environ file during a process-setup time interval in which environment-variable copying is incomplete.,2017-01-17,+Info ,2,https://github.com/torvalds/linux/commit/8148a73c9901a8794a50f950083c00ccf97d43b3,8148a73c9901a8794a50f950083c00ccf97d43b3,"proc: prevent accessing /proc/<PID>/environ until it's ready

If /proc/<PID>/environ gets read before the envp[] array is fully set up
in create_{aout,elf,elf_fdpic,flat}_tables(), we might end up trying to
read more bytes than are actually written, as env_start will already be
set but env_end will still be zero, making the range calculation
underflow, allowing to read beyond the end of what has been written.

Fix this as it is done for /proc/<PID>/cmdline by testing env_end for
zero.  It is, apparently, intentionally set last in create_*_tables().

This bug was found by the PaX size_overflow plugin that detected the
arithmetic underflow of 'this_len = env_end - (env_start + src)' when
env_end is still zero.

The expected consequence is that userland trying to access
/proc/<PID>/environ of a not yet fully set up process may get
inconsistent data as we're in the middle of copying in the environment
variables.

Fixes: https://forums.grsecurity.net/viewtopic.php?f=3&t=4363
Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=116461
Signed-off-by: Mathias Krause <minipli@googlemail.com>
Cc: Emese Revfy <re.emese@gmail.com>
Cc: Pax Team <pageexec@freemail.hu>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Cc: Mateusz Guzik <mguzik@redhat.com>
Cc: Alexey Dobriyan <adobriyan@gmail.com>
Cc: Cyrill Gorcunov <gorcunov@openvz.org>
Cc: Jarod Wilson <jarod@redhat.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",1,fs/proc/base.c,"{""sha"": ""92e37e224cd22d66d0b9675f1790e110d5e7b415"", ""filename"": ""fs/proc/base.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 1, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/8148a73c9901a8794a50f950083c00ccf97d43b3/fs/proc/base.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8148a73c9901a8794a50f950083c00ccf97d43b3/fs/proc/base.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/base.c?ref=8148a73c9901a8794a50f950083c00ccf97d43b3"", ""patch"": ""@@ -955,7 +955,8 @@ static ssize_t environ_read(struct file *file, char __user *buf,\n \tstruct mm_struct *mm = file->private_data;\n \tunsigned long env_start, env_end;\n \n-\tif (!mm)\n+\t/* Ensure the process spawned far enough to have an environment. */\n+\tif (!mm || !mm->env_end)\n \t\treturn 0;\n \n \tpage = (char *)__get_free_page(GFP_TEMPORARY);""}","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	if (!mm)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
",C,"	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
","	if (!mm)
",,"@@ -955,7 +955,8 @@ static ssize_t environ_read(struct file *file, char __user *buf,
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
-	if (!mm)
+	/* Ensure the process spawned far enough to have an environment. */
+	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);",linux,8148a73c9901a8794a50f950083c00ccf97d43b3,32a4e169039927bfb6ee9f0ccbbe3a8aaf13a4bc,1,"static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
//flaw_line_below:
	if (!mm)
//fix_flaw_line_below:
//	/* Ensure the process spawned far enough to have an environment. */
//fix_flaw_line_below:
//	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
",180092,"static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	if (!mm)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","The `environ_read` function is designed to read the environment variables of a process in a Linux kernel module context. It takes a file structure, a user buffer, a count of bytes to read, and a position pointer as parameters. The function first retrieves the memory management structure of the target process and allocates a temporary page for reading. It then acquires a read lock on the process's memory map, calculates the range of environment variables, and reads them into the allocated page. The data is subsequently copied to the user buffer, updating the position pointer accordingly, and finally, the allocated page is freed before returning the number of bytes read or an error code.","The provided code implements a function that reads the environment variables of a specified process in a Linux kernel module. It begins by checking the validity of the target process's memory structure and allocates a temporary memory page for data retrieval. The function then acquires a read lock on the process's memory map to safely access the environment variable range. It reads the data into the allocated page and copies it to a user-provided buffer, updating the read position accordingly. Finally, the function releases the allocated memory and returns the number of bytes successfully read or an error code if an issue occurs.","The vulnerability in the original code arises from a race condition where a user could read the environment variables of a process during its setup phase, potentially accessing incomplete or sensitive information. This occurs because the function does not verify if the environment variables are fully initialized before allowing access. The fix introduces a check to ensure that the memory structure is valid and that the environment end pointer is set, preventing access if the environment is not fully established. This change mitigates the risk of reading incomplete data by ensuring that the process has completed its setup before allowing any read operations.","The vulnerability in the original code stems from a race condition that allows users to read environment variables from a process before they are fully initialized, potentially exposing sensitive information. This occurs during the process setup phase when the copying of environment variables is not yet complete. The fix addresses this issue by adding a validation step to ensure that the environment variables are fully established before permitting any read operations. This prevents unauthorized access to incomplete data, thereby securing the information and mitigating the risk of sensitive data exposure during the process's initialization.","The vulnerability was fixed by adding a check for the validity of the memory management structure and ensuring that the environment end pointer is set before proceeding with the read operation. Specifically, the code now includes a condition that verifies if the memory structure is valid and if `mm->env_end` is non-zero. This prevents the function from executing further if the environment variables are not fully initialized, thereby eliminating the race condition. By enforcing this check, the code ensures that access to the environment variables only occurs when they are completely set up, thus protecting sensitive information.","The vulnerability was fixed by introducing a validation step that checks whether the environment variables of the process are fully initialized before allowing any read operations. This involves ensuring that the memory structure associated with the process is valid and that the environment is completely set up. By implementing this check, the code prevents access to potentially incomplete or sensitive information during the process setup phase. This change effectively mitigates the race condition, ensuring that users can only read environment variables when they are guaranteed to be fully established and secure."
18,180189,180189,,Local,Not required,Partial,CVE-2016-6156,https://www.cvedetails.com/cve/CVE-2016-6156/,CWE-362,Medium,,,,2016-08-06,1.9,"Race condition in the ec_device_ioctl_xcmd function in drivers/platform/chrome/cros_ec_dev.c in the Linux kernel before 4.7 allows local users to cause a denial of service (out-of-bounds array access) by changing a certain size value, aka a *double fetch* vulnerability.",2016-11-28,DoS ,7,https://github.com/torvalds/linux/commit/096cdc6f52225835ff503f987a0d68ef770bb78e,096cdc6f52225835ff503f987a0d68ef770bb78e,"platform/chrome: cros_ec_dev - double fetch bug in ioctl

We verify ""u_cmd.outsize"" and ""u_cmd.insize"" but we need to make sure
that those values have not changed between the two copy_from_user()
calls.  Otherwise it could lead to a buffer overflow.

Additionally, cros_ec_cmd_xfer() can set s_cmd->insize to a lower value.
We should use the new smaller value so we don't copy too much data to
the user.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Fixes: a841178445bb ('mfd: cros_ec: Use a zero-length array for command data')
Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
Reviewed-by: Kees Cook <keescook@chromium.org>
Tested-by: Gwendal Grignou <gwendal@chromium.org>
Cc: <stable@vger.kernel.org> # v4.2+
Signed-off-by: Olof Johansson <olof@lixom.net>",1,drivers/platform/chrome/cros_ec_dev.c,"{""sha"": ""8abd80dbcbed7974b4ace4265dd9dad1bca89edd"", ""filename"": ""drivers/platform/chrome/cros_ec_dev.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 1, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/096cdc6f52225835ff503f987a0d68ef770bb78e/drivers/platform/chrome/cros_ec_dev.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/096cdc6f52225835ff503f987a0d68ef770bb78e/drivers/platform/chrome/cros_ec_dev.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/platform/chrome/cros_ec_dev.c?ref=096cdc6f52225835ff503f987a0d68ef770bb78e"", ""patch"": ""@@ -151,13 +151,19 @@ static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n \t\tgoto exit;\n \t}\n \n+\tif (u_cmd.outsize != s_cmd->outsize ||\n+\t    u_cmd.insize != s_cmd->insize) {\n+\t\tret = -EINVAL;\n+\t\tgoto exit;\n+\t}\n+\n \ts_cmd->command += ec->cmd_offset;\n \tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n \t/* Only copy data to userland if data was received. */\n \tif (ret < 0)\n \t\tgoto exit;\n \n-\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))\n+\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n \t\tret = -EFAULT;\n exit:\n \tkfree(s_cmd);""}","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
",C,"	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
","	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
",,"@@ -151,13 +151,19 @@ static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
 		goto exit;
 	}
 
+	if (u_cmd.outsize != s_cmd->outsize ||
+	    u_cmd.insize != s_cmd->insize) {
+		ret = -EINVAL;
+		goto exit;
+	}
+
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
-	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
+	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);",linux,096cdc6f52225835ff503f987a0d68ef770bb78e,33688abb2802ff3a230bd2441f765477b94cc89e,1,"static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
//fix_flaw_line_below:
//	if (u_cmd.outsize != s_cmd->outsize ||
//fix_flaw_line_below:
//	    u_cmd.insize != s_cmd->insize) {
//fix_flaw_line_below:
//		ret = -EINVAL;
//fix_flaw_line_below:
//		goto exit;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
//flaw_line_below:
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
//fix_flaw_line_below:
//	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
",180189,"static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","The `ec_device_ioctl_xcmd` function facilitates communication between user space and a device managed by the Chrome OS Embedded Controller (EC). It begins by copying a command structure from user space and validating the output and input sizes against predefined limits. If valid, it allocates memory for a command structure, then copies the command data from user space. The command is adjusted with an offset before being sent to the EC device. If the command execution is successful, it copies any received data back to user space. Finally, it cleans up allocated memory and returns the result of the operation.","The provided code implements a function that facilitates communication between user space and a device managed by an embedded controller. It begins by retrieving a command structure from user space and checking the validity of the specified input and output sizes. If the sizes are acceptable, it allocates memory for the command structure and copies the command data. The command is then modified with an offset before being sent to the embedded controller. If the command execution is successful, any received data is copied back to user space, and the function cleans up allocated memory before returning the result of the operation.","The vulnerability in the original code arises from a race condition, specifically a double fetch issue, where the sizes of input and output data can be altered by a malicious user after the initial validation but before the actual command execution. This is evident in the lack of checks after copying the command data into the `s_cmd` structure, allowing an attacker to manipulate `u_cmd.outsize` and `u_cmd.insize`. The fixed code addresses this by adding a validation step that compares the sizes in `u_cmd` with those in `s_cmd` after copying the data, ensuring that any discrepancies lead to an immediate error response, thus preventing out-of-bounds access.","The vulnerability in the original code is a race condition that allows an attacker to manipulate size values after they have been initially validated, leading to potential out-of-bounds array access. This occurs because the code does not recheck the sizes after copying data from user space, enabling a malicious user to alter the sizes and cause a denial of service. The fixed code addresses this issue by introducing an additional validation step that compares the sizes after the data has been copied, ensuring that any discrepancies result in an error, thereby preventing the exploitation of the race condition.","The vulnerability was fixed by adding a validation check after copying the command data into the `s_cmd` structure. Specifically, the code now compares `u_cmd.outsize` and `u_cmd.insize` with `s_cmd->outsize` and `s_cmd->insize`, respectively. If there is a mismatch, the function immediately returns an error code, preventing any further processing that could lead to out-of-bounds access. This additional check effectively mitigates the race condition by ensuring that the sizes remain consistent and valid throughout the execution of the command, thus eliminating the potential for exploitation.","The vulnerability was fixed by introducing an additional validation step after the data was copied from user space. This step checks that the sizes of the output and input data match the expected values after the copy operation. If there is any discrepancy between the expected and actual sizes, the function immediately returns an error, preventing further processing. This ensures that any potential manipulation of size values by an attacker is caught before it can lead to out-of-bounds access, effectively mitigating the race condition and enhancing the overall security of the code."
19,180191,180191,,Local,Not required,,CVE-2016-6136,https://www.cvedetails.com/cve/CVE-2016-6136/,CWE-362,Medium,,Partial,,2016-08-06,1.9,"Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a *double fetch* vulnerability.",2018-01-04,Bypass ,93,https://github.com/torvalds/linux/commit/43761473c254b45883a64441dd0bc85a42f3645c,43761473c254b45883a64441dd0bc85a42f3645c,"audit: fix a double fetch in audit_log_single_execve_arg()

There is a double fetch problem in audit_log_single_execve_arg()
where we first check the execve(2) argumnets for any ""bad"" characters
which would require hex encoding and then re-fetch the arguments for
logging in the audit record[1].  Of course this leaves a window of
opportunity for an unsavory application to munge with the data.

This patch reworks things by only fetching the argument data once[2]
into a buffer where it is scanned and logged into the audit
records(s).  In addition to fixing the double fetch, this patch
improves on the original code in a few other ways: better handling
of large arguments which require encoding, stricter record length
checking, and some performance improvements (completely unverified,
but we got rid of some strlen() calls, that's got to be a good
thing).

As part of the development of this patch, I've also created a basic
regression test for the audit-testsuite, the test can be tracked on
GitHub at the following link:

 * https://github.com/linux-audit/audit-testsuite/issues/25

[1] If you pay careful attention, there is actually a triple fetch
problem due to a strnlen_user() call at the top of the function.

[2] This is a tiny white lie, we do make a call to strnlen_user()
prior to fetching the argument data.  I don't like it, but due to the
way the audit record is structured we really have no choice unless we
copy the entire argument at once (which would require a rather
wasteful allocation).  The good news is that with this patch the
kernel no longer relies on this strnlen_user() value for anything
beyond recording it in the log, we also update it with a trustworthy
value whenever possible.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Paul Moore <paul@paul-moore.com>",117,kernel/auditsc.c,"{""sha"": ""c65af21a12d6d2e7b56e99ed2fdd6d284a9ec43a"", ""filename"": ""kernel/auditsc.c"", ""status"": ""modified"", ""additions"": 164, ""deletions"": 168, ""changes"": 332, ""blob_url"": ""https://github.com/torvalds/linux/blob/43761473c254b45883a64441dd0bc85a42f3645c/kernel/auditsc.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/43761473c254b45883a64441dd0bc85a42f3645c/kernel/auditsc.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/kernel/auditsc.c?ref=43761473c254b45883a64441dd0bc85a42f3645c"", ""patch"": ""@@ -73,6 +73,7 @@\n #include <linux/compat.h>\n #include <linux/ctype.h>\n #include <linux/string.h>\n+#include <linux/uaccess.h>\n #include <uapi/linux/limits.h>\n \n #include \""audit.h\""\n@@ -82,7 +83,8 @@\n #define AUDITSC_SUCCESS 1\n #define AUDITSC_FAILURE 2\n \n-/* no execve audit message should be longer than this (userspace limits) */\n+/* no execve audit message should be longer than this (userspace limits),\n+ * see the note near the top of audit_log_execve_info() about this value */\n #define MAX_EXECVE_AUDIT_LEN 7500\n \n /* max length to print of cmdline/proctitle value during audit */\n@@ -992,184 +994,178 @@ static int audit_log_pid_context(struct audit_context *context, pid_t pid,\n \treturn rc;\n }\n \n-/*\n- * to_send and len_sent accounting are very loose estimates.  We aren't\n- * really worried about a hard cap to MAX_EXECVE_AUDIT_LEN so much as being\n- * within about 500 bytes (next page boundary)\n- *\n- * why snprintf?  an int is up to 12 digits long.  if we just assumed when\n- * logging that a[%d]= was going to be 16 characters long we would be wasting\n- * space in every audit message.  In one 7500 byte message we can log up to\n- * about 1000 min size arguments.  That comes down to about 50% waste of space\n- * if we didn't do the snprintf to find out how long arg_num_len was.\n- */\n-static int audit_log_single_execve_arg(struct audit_context *context,\n-\t\t\t\t\tstruct audit_buffer **ab,\n-\t\t\t\t\tint arg_num,\n-\t\t\t\t\tsize_t *len_sent,\n-\t\t\t\t\tconst char __user *p,\n-\t\t\t\t\tchar *buf)\n+static void audit_log_execve_info(struct audit_context *context,\n+\t\t\t\t  struct audit_buffer **ab)\n {\n-\tchar arg_num_len_buf[12];\n-\tconst char __user *tmp_p = p;\n-\t/* how many digits are in arg_num? 5 is the length of ' a=\""\""' */\n-\tsize_t arg_num_len = snprintf(arg_num_len_buf, 12, \""%d\"", arg_num) + 5;\n-\tsize_t len, len_left, to_send;\n-\tsize_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;\n-\tunsigned int i, has_cntl = 0, too_long = 0;\n-\tint ret;\n-\n-\t/* strnlen_user includes the null we don't want to send */\n-\tlen_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n-\n-\t/*\n-\t * We just created this mm, if we can't find the strings\n-\t * we just copied into it something is _very_ wrong. Similar\n-\t * for strings that are too long, we should not have created\n-\t * any.\n-\t */\n-\tif (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {\n-\t\tsend_sig(SIGKILL, current, 0);\n-\t\treturn -1;\n+\tlong len_max;\n+\tlong len_rem;\n+\tlong len_full;\n+\tlong len_buf;\n+\tlong len_abuf;\n+\tlong len_tmp;\n+\tbool require_data;\n+\tbool encode;\n+\tunsigned int iter;\n+\tunsigned int arg;\n+\tchar *buf_head;\n+\tchar *buf;\n+\tconst char __user *p = (const char __user *)current->mm->arg_start;\n+\n+\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\n+\t *       data we put in the audit record for this argument (see the\n+\t *       code below) ... at this point in time 96 is plenty */\n+\tchar abuf[96];\n+\n+\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\n+\t *       current value of 7500 is not as important as the fact that it\n+\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\n+\t *       room if we go over a little bit in the logging below */\n+\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\n+\tlen_max = MAX_EXECVE_AUDIT_LEN;\n+\n+\t/* scratch buffer to hold the userspace args */\n+\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n+\tif (!buf_head) {\n+\t\taudit_panic(\""out of memory for argv string\"");\n+\t\treturn;\n \t}\n+\tbuf = buf_head;\n \n-\t/* walk the whole argument looking for non-ascii chars */\n+\taudit_log_format(*ab, \""argc=%d\"", context->execve.argc);\n+\n+\tlen_rem = len_max;\n+\tlen_buf = 0;\n+\tlen_full = 0;\n+\trequire_data = true;\n+\tencode = false;\n+\titer = 0;\n+\targ = 0;\n \tdo {\n-\t\tif (len_left > MAX_EXECVE_AUDIT_LEN)\n-\t\t\tto_send = MAX_EXECVE_AUDIT_LEN;\n-\t\telse\n-\t\t\tto_send = len_left;\n-\t\tret = copy_from_user(buf, tmp_p, to_send);\n-\t\t/*\n-\t\t * There is no reason for this copy to be short. We just\n-\t\t * copied them here, and the mm hasn't been exposed to user-\n-\t\t * space yet.\n-\t\t */\n-\t\tif (ret) {\n-\t\t\tWARN_ON(1);\n-\t\t\tsend_sig(SIGKILL, current, 0);\n-\t\t\treturn -1;\n-\t\t}\n-\t\tbuf[to_send] = '\\0';\n-\t\thas_cntl = audit_string_contains_control(buf, to_send);\n-\t\tif (has_cntl) {\n-\t\t\t/*\n-\t\t\t * hex messages get logged as 2 bytes, so we can only\n-\t\t\t * send half as much in each message\n-\t\t\t */\n-\t\t\tmax_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;\n-\t\t\tbreak;\n-\t\t}\n-\t\tlen_left -= to_send;\n-\t\ttmp_p += to_send;\n-\t} while (len_left > 0);\n-\n-\tlen_left = len;\n-\n-\tif (len > max_execve_audit_len)\n-\t\ttoo_long = 1;\n-\n-\t/* rewalk the argument actually logging the message */\n-\tfor (i = 0; len_left > 0; i++) {\n-\t\tint room_left;\n-\n-\t\tif (len_left > max_execve_audit_len)\n-\t\t\tto_send = max_execve_audit_len;\n-\t\telse\n-\t\t\tto_send = len_left;\n-\n-\t\t/* do we have space left to send this argument in this ab? */\n-\t\troom_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;\n-\t\tif (has_cntl)\n-\t\t\troom_left -= (to_send * 2);\n-\t\telse\n-\t\t\troom_left -= to_send;\n-\t\tif (room_left < 0) {\n-\t\t\t*len_sent = 0;\n-\t\t\taudit_log_end(*ab);\n-\t\t\t*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);\n-\t\t\tif (!*ab)\n-\t\t\t\treturn 0;\n-\t\t}\n+\t\t/* NOTE: we don't ever want to trust this value for anything\n+\t\t *       serious, but the audit record format insists we\n+\t\t *       provide an argument length for really long arguments,\n+\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\n+\t\t *       to use strncpy_from_user() to obtain this value for\n+\t\t *       recording in the log, although we don't use it\n+\t\t *       anywhere here to avoid a double-fetch problem */\n+\t\tif (len_full == 0)\n+\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n+\n+\t\t/* read more data from userspace */\n+\t\tif (require_data) {\n+\t\t\t/* can we make more room in the buffer? */\n+\t\t\tif (buf != buf_head) {\n+\t\t\t\tmemmove(buf_head, buf, len_buf);\n+\t\t\t\tbuf = buf_head;\n+\t\t\t}\n+\n+\t\t\t/* fetch as much as we can of the argument */\n+\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\n+\t\t\t\t\t\t    len_max - len_buf);\n+\t\t\tif (len_tmp == -EFAULT) {\n+\t\t\t\t/* unable to copy from userspace */\n+\t\t\t\tsend_sig(SIGKILL, current, 0);\n+\t\t\t\tgoto out;\n+\t\t\t} else if (len_tmp == (len_max - len_buf)) {\n+\t\t\t\t/* buffer is not large enough */\n+\t\t\t\trequire_data = true;\n+\t\t\t\t/* NOTE: if we are going to span multiple\n+\t\t\t\t *       buffers force the encoding so we stand\n+\t\t\t\t *       a chance at a sane len_full value and\n+\t\t\t\t *       consistent record encoding */\n+\t\t\t\tencode = true;\n+\t\t\t\tlen_full = len_full * 2;\n+\t\t\t\tp += len_tmp;\n+\t\t\t} else {\n+\t\t\t\trequire_data = false;\n+\t\t\t\tif (!encode)\n+\t\t\t\t\tencode = audit_string_contains_control(\n+\t\t\t\t\t\t\t\tbuf, len_tmp);\n+\t\t\t\t/* try to use a trusted value for len_full */\n+\t\t\t\tif (len_full < len_max)\n+\t\t\t\t\tlen_full = (encode ?\n+\t\t\t\t\t\t    len_tmp * 2 : len_tmp);\n+\t\t\t\tp += len_tmp + 1;\n+\t\t\t}\n+\t\t\tlen_buf += len_tmp;\n+\t\t\tbuf_head[len_buf] = '\\0';\n \n-\t\t/*\n-\t\t * first record needs to say how long the original string was\n-\t\t * so we can be sure nothing was lost.\n-\t\t */\n-\t\tif ((i == 0) && (too_long))\n-\t\t\taudit_log_format(*ab, \"" a%d_len=%zu\"", arg_num,\n-\t\t\t\t\t has_cntl ? 2*len : len);\n-\n-\t\t/*\n-\t\t * normally arguments are small enough to fit and we already\n-\t\t * filled buf above when we checked for control characters\n-\t\t * so don't bother with another copy_from_user\n-\t\t */\n-\t\tif (len >= max_execve_audit_len)\n-\t\t\tret = copy_from_user(buf, p, to_send);\n-\t\telse\n-\t\t\tret = 0;\n-\t\tif (ret) {\n-\t\t\tWARN_ON(1);\n-\t\t\tsend_sig(SIGKILL, current, 0);\n-\t\t\treturn -1;\n+\t\t\t/* length of the buffer in the audit record? */\n+\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\n \t\t}\n-\t\tbuf[to_send] = '\\0';\n-\n-\t\t/* actually log it */\n-\t\taudit_log_format(*ab, \"" a%d\"", arg_num);\n-\t\tif (too_long)\n-\t\t\taudit_log_format(*ab, \""[%d]\"", i);\n-\t\taudit_log_format(*ab, \""=\"");\n-\t\tif (has_cntl)\n-\t\t\taudit_log_n_hex(*ab, buf, to_send);\n-\t\telse\n-\t\t\taudit_log_string(*ab, buf);\n-\n-\t\tp += to_send;\n-\t\tlen_left -= to_send;\n-\t\t*len_sent += arg_num_len;\n-\t\tif (has_cntl)\n-\t\t\t*len_sent += to_send * 2;\n-\t\telse\n-\t\t\t*len_sent += to_send;\n-\t}\n-\t/* include the null we didn't log */\n-\treturn len + 1;\n-}\n \n-static void audit_log_execve_info(struct audit_context *context,\n-\t\t\t\t  struct audit_buffer **ab)\n-{\n-\tint i, len;\n-\tsize_t len_sent = 0;\n-\tconst char __user *p;\n-\tchar *buf;\n+\t\t/* write as much as we can to the audit log */\n+\t\tif (len_buf > 0) {\n+\t\t\t/* NOTE: some magic numbers here - basically if we\n+\t\t\t *       can't fit a reasonable amount of data into the\n+\t\t\t *       existing audit buffer, flush it and start with\n+\t\t\t *       a new buffer */\n+\t\t\tif ((sizeof(abuf) + 8) > len_rem) {\n+\t\t\t\tlen_rem = len_max;\n+\t\t\t\taudit_log_end(*ab);\n+\t\t\t\t*ab = audit_log_start(context,\n+\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);\n+\t\t\t\tif (!*ab)\n+\t\t\t\t\tgoto out;\n+\t\t\t}\n \n-\tp = (const char __user *)current->mm->arg_start;\n+\t\t\t/* create the non-arg portion of the arg record */\n+\t\t\tlen_tmp = 0;\n+\t\t\tif (require_data || (iter > 0) ||\n+\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\n+\t\t\t\tif (iter == 0) {\n+\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t\t\"" a%d_len=%lu\"",\n+\t\t\t\t\t\t\targ, len_full);\n+\t\t\t\t}\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \"" a%d[%d]=\"", arg, iter++);\n+\t\t\t} else\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \"" a%d=\"", arg);\n+\t\t\tWARN_ON(len_tmp >= sizeof(abuf));\n+\t\t\tabuf[sizeof(abuf) - 1] = '\\0';\n+\n+\t\t\t/* log the arg in the audit record */\n+\t\t\taudit_log_format(*ab, \""%s\"", abuf);\n+\t\t\tlen_rem -= len_tmp;\n+\t\t\tlen_tmp = len_buf;\n+\t\t\tif (encode) {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */\n+\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp * 2;\n+\t\t\t\tlen_abuf -= len_tmp * 2;\n+\t\t\t} else {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */\n+\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp + 2;\n+\t\t\t\t/* don't subtract the \""2\"" because we still need\n+\t\t\t\t * to add quotes to the remaining string */\n+\t\t\t\tlen_abuf -= len_tmp;\n+\t\t\t}\n+\t\t\tlen_buf -= len_tmp;\n+\t\t\tbuf += len_tmp;\n+\t\t}\n \n-\taudit_log_format(*ab, \""argc=%d\"", context->execve.argc);\n+\t\t/* ready to move to the next argument? */\n+\t\tif ((len_buf == 0) && !require_data) {\n+\t\t\targ++;\n+\t\t\titer = 0;\n+\t\t\tlen_full = 0;\n+\t\t\trequire_data = true;\n+\t\t\tencode = false;\n+\t\t}\n+\t} while (arg < context->execve.argc);\n \n-\t/*\n-\t * we need some kernel buffer to hold the userspace args.  Just\n-\t * allocate one big one rather than allocating one of the right size\n-\t * for every single argument inside audit_log_single_execve_arg()\n-\t * should be <8k allocation so should be pretty safe.\n-\t */\n-\tbuf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n-\tif (!buf) {\n-\t\taudit_panic(\""out of memory for argv string\"");\n-\t\treturn;\n-\t}\n+\t/* NOTE: the caller handles the final audit_log_end() call */\n \n-\tfor (i = 0; i < context->execve.argc; i++) {\n-\t\tlen = audit_log_single_execve_arg(context, ab, i,\n-\t\t\t\t\t\t  &len_sent, p, buf);\n-\t\tif (len <= 0)\n-\t\t\tbreak;\n-\t\tp += len;\n-\t}\n-\tkfree(buf);\n+out:\n+\tkfree(buf_head);\n }\n \n static void show_special(struct audit_context *context, int *call_panic)""}","static int audit_log_single_execve_arg(struct audit_context *context,
static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
 {
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
 	}
	buf = buf_head;
 
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
 	do {
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
 
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
","static int audit_log_single_execve_arg(struct audit_context *context,
					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
 {
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
 	}
 
	/* walk the whole argument looking for non-ascii chars */
 	do {
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
 
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
 		}
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
",C,"static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
	buf = buf_head;
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
","					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
	/* walk the whole argument looking for non-ascii chars */
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
",,"@@ -73,6 +73,7 @@
 #include <linux/compat.h>
 #include <linux/ctype.h>
 #include <linux/string.h>
+#include <linux/uaccess.h>
 #include <uapi/linux/limits.h>
 
 #include ""audit.h""
@@ -82,7 +83,8 @@
 #define AUDITSC_SUCCESS 1
 #define AUDITSC_FAILURE 2
 
-/* no execve audit message should be longer than this (userspace limits) */
+/* no execve audit message should be longer than this (userspace limits),
+ * see the note near the top of audit_log_execve_info() about this value */
 #define MAX_EXECVE_AUDIT_LEN 7500
 
 /* max length to print of cmdline/proctitle value during audit */
@@ -992,184 +994,178 @@ static int audit_log_pid_context(struct audit_context *context, pid_t pid,
 	return rc;
 }
 
-/*
- * to_send and len_sent accounting are very loose estimates.  We aren't
- * really worried about a hard cap to MAX_EXECVE_AUDIT_LEN so much as being
- * within about 500 bytes (next page boundary)
- *
- * why snprintf?  an int is up to 12 digits long.  if we just assumed when
- * logging that a[%d]= was going to be 16 characters long we would be wasting
- * space in every audit message.  In one 7500 byte message we can log up to
- * about 1000 min size arguments.  That comes down to about 50% waste of space
- * if we didn't do the snprintf to find out how long arg_num_len was.
- */
-static int audit_log_single_execve_arg(struct audit_context *context,
-					struct audit_buffer **ab,
-					int arg_num,
-					size_t *len_sent,
-					const char __user *p,
-					char *buf)
+static void audit_log_execve_info(struct audit_context *context,
+				  struct audit_buffer **ab)
 {
-	char arg_num_len_buf[12];
-	const char __user *tmp_p = p;
-	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
-	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
-	size_t len, len_left, to_send;
-	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
-	unsigned int i, has_cntl = 0, too_long = 0;
-	int ret;
-
-	/* strnlen_user includes the null we don't want to send */
-	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
-
-	/*
-	 * We just created this mm, if we can't find the strings
-	 * we just copied into it something is _very_ wrong. Similar
-	 * for strings that are too long, we should not have created
-	 * any.
-	 */
-	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
-		send_sig(SIGKILL, current, 0);
-		return -1;
+	long len_max;
+	long len_rem;
+	long len_full;
+	long len_buf;
+	long len_abuf;
+	long len_tmp;
+	bool require_data;
+	bool encode;
+	unsigned int iter;
+	unsigned int arg;
+	char *buf_head;
+	char *buf;
+	const char __user *p = (const char __user *)current->mm->arg_start;
+
+	/* NOTE: this buffer needs to be large enough to hold all the non-arg
+	 *       data we put in the audit record for this argument (see the
+	 *       code below) ... at this point in time 96 is plenty */
+	char abuf[96];
+
+	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
+	 *       current value of 7500 is not as important as the fact that it
+	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
+	 *       room if we go over a little bit in the logging below */
+	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
+	len_max = MAX_EXECVE_AUDIT_LEN;
+
+	/* scratch buffer to hold the userspace args */
+	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
+	if (!buf_head) {
+		audit_panic(""out of memory for argv string"");
+		return;
 	}
+	buf = buf_head;
 
-	/* walk the whole argument looking for non-ascii chars */
+	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
+
+	len_rem = len_max;
+	len_buf = 0;
+	len_full = 0;
+	require_data = true;
+	encode = false;
+	iter = 0;
+	arg = 0;
 	do {
-		if (len_left > MAX_EXECVE_AUDIT_LEN)
-			to_send = MAX_EXECVE_AUDIT_LEN;
-		else
-			to_send = len_left;
-		ret = copy_from_user(buf, tmp_p, to_send);
-		/*
-		 * There is no reason for this copy to be short. We just
-		 * copied them here, and the mm hasn't been exposed to user-
-		 * space yet.
-		 */
-		if (ret) {
-			WARN_ON(1);
-			send_sig(SIGKILL, current, 0);
-			return -1;
-		}
-		buf[to_send] = '\0';
-		has_cntl = audit_string_contains_control(buf, to_send);
-		if (has_cntl) {
-			/*
-			 * hex messages get logged as 2 bytes, so we can only
-			 * send half as much in each message
-			 */
-			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
-			break;
-		}
-		len_left -= to_send;
-		tmp_p += to_send;
-	} while (len_left > 0);
-
-	len_left = len;
-
-	if (len > max_execve_audit_len)
-		too_long = 1;
-
-	/* rewalk the argument actually logging the message */
-	for (i = 0; len_left > 0; i++) {
-		int room_left;
-
-		if (len_left > max_execve_audit_len)
-			to_send = max_execve_audit_len;
-		else
-			to_send = len_left;
-
-		/* do we have space left to send this argument in this ab? */
-		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
-		if (has_cntl)
-			room_left -= (to_send * 2);
-		else
-			room_left -= to_send;
-		if (room_left < 0) {
-			*len_sent = 0;
-			audit_log_end(*ab);
-			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
-			if (!*ab)
-				return 0;
-		}
+		/* NOTE: we don't ever want to trust this value for anything
+		 *       serious, but the audit record format insists we
+		 *       provide an argument length for really long arguments,
+		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
+		 *       to use strncpy_from_user() to obtain this value for
+		 *       recording in the log, although we don't use it
+		 *       anywhere here to avoid a double-fetch problem */
+		if (len_full == 0)
+			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;
+
+		/* read more data from userspace */
+		if (require_data) {
+			/* can we make more room in the buffer? */
+			if (buf != buf_head) {
+				memmove(buf_head, buf, len_buf);
+				buf = buf_head;
+			}
+
+			/* fetch as much as we can of the argument */
+			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
+						    len_max - len_buf);
+			if (len_tmp == -EFAULT) {
+				/* unable to copy from userspace */
+				send_sig(SIGKILL, current, 0);
+				goto out;
+			} else if (len_tmp == (len_max - len_buf)) {
+				/* buffer is not large enough */
+				require_data = true;
+				/* NOTE: if we are going to span multiple
+				 *       buffers force the encoding so we stand
+				 *       a chance at a sane len_full value and
+				 *       consistent record encoding */
+				encode = true;
+				len_full = len_full * 2;
+				p += len_tmp;
+			} else {
+				require_data = false;
+				if (!encode)
+					encode = audit_string_contains_control(
+								buf, len_tmp);
+				/* try to use a trusted value for len_full */
+				if (len_full < len_max)
+					len_full = (encode ?
+						    len_tmp * 2 : len_tmp);
+				p += len_tmp + 1;
+			}
+			len_buf += len_tmp;
+			buf_head[len_buf] = '\0';
 
-		/*
-		 * first record needs to say how long the original string was
-		 * so we can be sure nothing was lost.
-		 */
-		if ((i == 0) && (too_long))
-			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
-					 has_cntl ? 2*len : len);
-
-		/*
-		 * normally arguments are small enough to fit and we already
-		 * filled buf above when we checked for control characters
-		 * so don't bother with another copy_from_user
-		 */
-		if (len >= max_execve_audit_len)
-			ret = copy_from_user(buf, p, to_send);
-		else
-			ret = 0;
-		if (ret) {
-			WARN_ON(1);
-			send_sig(SIGKILL, current, 0);
-			return -1;
+			/* length of the buffer in the audit record? */
+			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
-		buf[to_send] = '\0';
-
-		/* actually log it */
-		audit_log_format(*ab, "" a%d"", arg_num);
-		if (too_long)
-			audit_log_format(*ab, ""[%d]"", i);
-		audit_log_format(*ab, ""="");
-		if (has_cntl)
-			audit_log_n_hex(*ab, buf, to_send);
-		else
-			audit_log_string(*ab, buf);
-
-		p += to_send;
-		len_left -= to_send;
-		*len_sent += arg_num_len;
-		if (has_cntl)
-			*len_sent += to_send * 2;
-		else
-			*len_sent += to_send;
-	}
-	/* include the null we didn't log */
-	return len + 1;
-}
 
-static void audit_log_execve_info(struct audit_context *context,
-				  struct audit_buffer **ab)
-{
-	int i, len;
-	size_t len_sent = 0;
-	const char __user *p;
-	char *buf;
+		/* write as much as we can to the audit log */
+		if (len_buf > 0) {
+			/* NOTE: some magic numbers here - basically if we
+			 *       can't fit a reasonable amount of data into the
+			 *       existing audit buffer, flush it and start with
+			 *       a new buffer */
+			if ((sizeof(abuf) + 8) > len_rem) {
+				len_rem = len_max;
+				audit_log_end(*ab);
+				*ab = audit_log_start(context,
+						      GFP_KERNEL, AUDIT_EXECVE);
+				if (!*ab)
+					goto out;
+			}
 
-	p = (const char __user *)current->mm->arg_start;
+			/* create the non-arg portion of the arg record */
+			len_tmp = 0;
+			if (require_data || (iter > 0) ||
+			    ((len_abuf + sizeof(abuf)) > len_rem)) {
+				if (iter == 0) {
+					len_tmp += snprintf(&abuf[len_tmp],
+							sizeof(abuf) - len_tmp,
+							"" a%d_len=%lu"",
+							arg, len_full);
+				}
+				len_tmp += snprintf(&abuf[len_tmp],
+						    sizeof(abuf) - len_tmp,
+						    "" a%d[%d]="", arg, iter++);
+			} else
+				len_tmp += snprintf(&abuf[len_tmp],
+						    sizeof(abuf) - len_tmp,
+						    "" a%d="", arg);
+			WARN_ON(len_tmp >= sizeof(abuf));
+			abuf[sizeof(abuf) - 1] = '\0';
+
+			/* log the arg in the audit record */
+			audit_log_format(*ab, ""%s"", abuf);
+			len_rem -= len_tmp;
+			len_tmp = len_buf;
+			if (encode) {
+				if (len_abuf > len_rem)
+					len_tmp = len_rem / 2; /* encoding */
+				audit_log_n_hex(*ab, buf, len_tmp);
+				len_rem -= len_tmp * 2;
+				len_abuf -= len_tmp * 2;
+			} else {
+				if (len_abuf > len_rem)
+					len_tmp = len_rem - 2; /* quotes */
+				audit_log_n_string(*ab, buf, len_tmp);
+				len_rem -= len_tmp + 2;
+				/* don't subtract the ""2"" because we still need
+				 * to add quotes to the remaining string */
+				len_abuf -= len_tmp;
+			}
+			len_buf -= len_tmp;
+			buf += len_tmp;
+		}
 
-	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
+		/* ready to move to the next argument? */
+		if ((len_buf == 0) && !require_data) {
+			arg++;
+			iter = 0;
+			len_full = 0;
+			require_data = true;
+			encode = false;
+		}
+	} while (arg < context->execve.argc);
 
-	/*
-	 * we need some kernel buffer to hold the userspace args.  Just
-	 * allocate one big one rather than allocating one of the right size
-	 * for every single argument inside audit_log_single_execve_arg()
-	 * should be <8k allocation so should be pretty safe.
-	 */
-	buf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
-	if (!buf) {
-		audit_panic(""out of memory for argv string"");
-		return;
-	}
+	/* NOTE: the caller handles the final audit_log_end() call */
 
-	for (i = 0; i < context->execve.argc; i++) {
-		len = audit_log_single_execve_arg(context, ab, i,
-						  &len_sent, p, buf);
-		if (len <= 0)
-			break;
-		p += len;
-	}
-	kfree(buf);
+out:
+	kfree(buf_head);
 }
 
 static void show_special(struct audit_context *context, int *call_panic)",linux,43761473c254b45883a64441dd0bc85a42f3645c,0b7a0fdb29715e38641beb39db4d01695b22b5aa,1,"static int audit_log_single_execve_arg(struct audit_context *context,
//flaw_line_below:
					struct audit_buffer **ab,
//flaw_line_below:
					int arg_num,
//flaw_line_below:
					size_t *len_sent,
//flaw_line_below:
					const char __user *p,
//flaw_line_below:
					char *buf)
//fix_flaw_line_below:
//static void audit_log_execve_info(struct audit_context *context,
//fix_flaw_line_below:
//				  struct audit_buffer **ab)
 {
//flaw_line_below:
	char arg_num_len_buf[12];
//flaw_line_below:
	const char __user *tmp_p = p;
//flaw_line_below:
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
//flaw_line_below:
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
//flaw_line_below:
	size_t len, len_left, to_send;
//flaw_line_below:
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
//flaw_line_below:
	unsigned int i, has_cntl = 0, too_long = 0;
//flaw_line_below:
	int ret;
//flaw_line_below:

//flaw_line_below:
	/* strnlen_user includes the null we don't want to send */
//flaw_line_below:
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
//flaw_line_below:

//flaw_line_below:
	/*
//flaw_line_below:
	 * We just created this mm, if we can't find the strings
//flaw_line_below:
	 * we just copied into it something is _very_ wrong. Similar
//flaw_line_below:
	 * for strings that are too long, we should not have created
//flaw_line_below:
	 * any.
//flaw_line_below:
	 */
//flaw_line_below:
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
//flaw_line_below:
		send_sig(SIGKILL, current, 0);
//flaw_line_below:
		return -1;
//fix_flaw_line_below:
//	long len_max;
//fix_flaw_line_below:
//	long len_rem;
//fix_flaw_line_below:
//	long len_full;
//fix_flaw_line_below:
//	long len_buf;
//fix_flaw_line_below:
//	long len_abuf;
//fix_flaw_line_below:
//	long len_tmp;
//fix_flaw_line_below:
//	bool require_data;
//fix_flaw_line_below:
//	bool encode;
//fix_flaw_line_below:
//	unsigned int iter;
//fix_flaw_line_below:
//	unsigned int arg;
//fix_flaw_line_below:
//	char *buf_head;
//fix_flaw_line_below:
//	char *buf;
//fix_flaw_line_below:
//	const char __user *p = (const char __user *)current->mm->arg_start;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* NOTE: this buffer needs to be large enough to hold all the non-arg
//fix_flaw_line_below:
//	 *       data we put in the audit record for this argument (see the
//fix_flaw_line_below:
//	 *       code below) ... at this point in time 96 is plenty */
//fix_flaw_line_below:
//	char abuf[96];
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
//fix_flaw_line_below:
//	 *       current value of 7500 is not as important as the fact that it
//fix_flaw_line_below:
//	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
//fix_flaw_line_below:
//	 *       room if we go over a little bit in the logging below */
//fix_flaw_line_below:
//	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
//fix_flaw_line_below:
//	len_max = MAX_EXECVE_AUDIT_LEN;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* scratch buffer to hold the userspace args */
//fix_flaw_line_below:
//	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
//fix_flaw_line_below:
//	if (!buf_head) {
//fix_flaw_line_below:
//		audit_panic(""out of memory for argv string"");
//fix_flaw_line_below:
//		return;
 	}
//fix_flaw_line_below:
//	buf = buf_head;
 
//flaw_line_below:
	/* walk the whole argument looking for non-ascii chars */
//fix_flaw_line_below:
//	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	len_rem = len_max;
//fix_flaw_line_below:
//	len_buf = 0;
//fix_flaw_line_below:
//	len_full = 0;
//fix_flaw_line_below:
//	require_data = true;
//fix_flaw_line_below:
//	encode = false;
//fix_flaw_line_below:
//	iter = 0;
//fix_flaw_line_below:
//	arg = 0;
 	do {
//flaw_line_below:
		if (len_left > MAX_EXECVE_AUDIT_LEN)
//flaw_line_below:
			to_send = MAX_EXECVE_AUDIT_LEN;
//flaw_line_below:
		else
//flaw_line_below:
			to_send = len_left;
//flaw_line_below:
		ret = copy_from_user(buf, tmp_p, to_send);
//flaw_line_below:
		/*
//flaw_line_below:
		 * There is no reason for this copy to be short. We just
//flaw_line_below:
		 * copied them here, and the mm hasn't been exposed to user-
//flaw_line_below:
		 * space yet.
//flaw_line_below:
		 */
//flaw_line_below:
		if (ret) {
//flaw_line_below:
			WARN_ON(1);
//flaw_line_below:
			send_sig(SIGKILL, current, 0);
//flaw_line_below:
			return -1;
//flaw_line_below:
		}
//flaw_line_below:
		buf[to_send] = '\0';
//flaw_line_below:
		has_cntl = audit_string_contains_control(buf, to_send);
//flaw_line_below:
		if (has_cntl) {
//flaw_line_below:
			/*
//flaw_line_below:
			 * hex messages get logged as 2 bytes, so we can only
//flaw_line_below:
			 * send half as much in each message
//flaw_line_below:
			 */
//flaw_line_below:
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
//flaw_line_below:
			break;
//flaw_line_below:
		}
//flaw_line_below:
		len_left -= to_send;
//flaw_line_below:
		tmp_p += to_send;
//flaw_line_below:
	} while (len_left > 0);
//flaw_line_below:

//flaw_line_below:
	len_left = len;
//flaw_line_below:

//flaw_line_below:
	if (len > max_execve_audit_len)
//flaw_line_below:
		too_long = 1;
//flaw_line_below:

//flaw_line_below:
	/* rewalk the argument actually logging the message */
//flaw_line_below:
	for (i = 0; len_left > 0; i++) {
//flaw_line_below:
		int room_left;
//flaw_line_below:

//flaw_line_below:
		if (len_left > max_execve_audit_len)
//flaw_line_below:
			to_send = max_execve_audit_len;
//flaw_line_below:
		else
//flaw_line_below:
			to_send = len_left;
//flaw_line_below:

//flaw_line_below:
		/* do we have space left to send this argument in this ab? */
//flaw_line_below:
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			room_left -= (to_send * 2);
//flaw_line_below:
		else
//flaw_line_below:
			room_left -= to_send;
//flaw_line_below:
		if (room_left < 0) {
//flaw_line_below:
			*len_sent = 0;
//flaw_line_below:
			audit_log_end(*ab);
//flaw_line_below:
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
//flaw_line_below:
			if (!*ab)
//flaw_line_below:
				return 0;
//flaw_line_below:
		}
//fix_flaw_line_below:
//		/* NOTE: we don't ever want to trust this value for anything
//fix_flaw_line_below:
//		 *       serious, but the audit record format insists we
//fix_flaw_line_below:
//		 *       provide an argument length for really long arguments,
//fix_flaw_line_below:
//		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
//fix_flaw_line_below:
//		 *       to use strncpy_from_user() to obtain this value for
//fix_flaw_line_below:
//		 *       recording in the log, although we don't use it
//fix_flaw_line_below:
//		 *       anywhere here to avoid a double-fetch problem */
//fix_flaw_line_below:
//		if (len_full == 0)
//fix_flaw_line_below:
//			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/* read more data from userspace */
//fix_flaw_line_below:
//		if (require_data) {
//fix_flaw_line_below:
//			/* can we make more room in the buffer? */
//fix_flaw_line_below:
//			if (buf != buf_head) {
//fix_flaw_line_below:
//				memmove(buf_head, buf, len_buf);
//fix_flaw_line_below:
//				buf = buf_head;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//			/* fetch as much as we can of the argument */
//fix_flaw_line_below:
//			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
//fix_flaw_line_below:
//						    len_max - len_buf);
//fix_flaw_line_below:
//			if (len_tmp == -EFAULT) {
//fix_flaw_line_below:
//				/* unable to copy from userspace */
//fix_flaw_line_below:
//				send_sig(SIGKILL, current, 0);
//fix_flaw_line_below:
//				goto out;
//fix_flaw_line_below:
//			} else if (len_tmp == (len_max - len_buf)) {
//fix_flaw_line_below:
//				/* buffer is not large enough */
//fix_flaw_line_below:
//				require_data = true;
//fix_flaw_line_below:
//				/* NOTE: if we are going to span multiple
//fix_flaw_line_below:
//				 *       buffers force the encoding so we stand
//fix_flaw_line_below:
//				 *       a chance at a sane len_full value and
//fix_flaw_line_below:
//				 *       consistent record encoding */
//fix_flaw_line_below:
//				encode = true;
//fix_flaw_line_below:
//				len_full = len_full * 2;
//fix_flaw_line_below:
//				p += len_tmp;
//fix_flaw_line_below:
//			} else {
//fix_flaw_line_below:
//				require_data = false;
//fix_flaw_line_below:
//				if (!encode)
//fix_flaw_line_below:
//					encode = audit_string_contains_control(
//fix_flaw_line_below:
//								buf, len_tmp);
//fix_flaw_line_below:
//				/* try to use a trusted value for len_full */
//fix_flaw_line_below:
//				if (len_full < len_max)
//fix_flaw_line_below:
//					len_full = (encode ?
//fix_flaw_line_below:
//						    len_tmp * 2 : len_tmp);
//fix_flaw_line_below:
//				p += len_tmp + 1;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//			len_buf += len_tmp;
//fix_flaw_line_below:
//			buf_head[len_buf] = '\0';
 
//flaw_line_below:
		/*
//flaw_line_below:
		 * first record needs to say how long the original string was
//flaw_line_below:
		 * so we can be sure nothing was lost.
//flaw_line_below:
		 */
//flaw_line_below:
		if ((i == 0) && (too_long))
//flaw_line_below:
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
//flaw_line_below:
					 has_cntl ? 2*len : len);
//flaw_line_below:

//flaw_line_below:
		/*
//flaw_line_below:
		 * normally arguments are small enough to fit and we already
//flaw_line_below:
		 * filled buf above when we checked for control characters
//flaw_line_below:
		 * so don't bother with another copy_from_user
//flaw_line_below:
		 */
//flaw_line_below:
		if (len >= max_execve_audit_len)
//flaw_line_below:
			ret = copy_from_user(buf, p, to_send);
//flaw_line_below:
		else
//flaw_line_below:
			ret = 0;
//flaw_line_below:
		if (ret) {
//flaw_line_below:
			WARN_ON(1);
//flaw_line_below:
			send_sig(SIGKILL, current, 0);
//flaw_line_below:
			return -1;
//fix_flaw_line_below:
//			/* length of the buffer in the audit record? */
//fix_flaw_line_below:
//			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
//flaw_line_below:
		buf[to_send] = '\0';
//flaw_line_below:

//flaw_line_below:
		/* actually log it */
//flaw_line_below:
		audit_log_format(*ab, "" a%d"", arg_num);
//flaw_line_below:
		if (too_long)
//flaw_line_below:
			audit_log_format(*ab, ""[%d]"", i);
//flaw_line_below:
		audit_log_format(*ab, ""="");
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			audit_log_n_hex(*ab, buf, to_send);
//flaw_line_below:
		else
//flaw_line_below:
			audit_log_string(*ab, buf);
//flaw_line_below:

//flaw_line_below:
		p += to_send;
//flaw_line_below:
		len_left -= to_send;
//flaw_line_below:
		*len_sent += arg_num_len;
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			*len_sent += to_send * 2;
//flaw_line_below:
		else
//flaw_line_below:
			*len_sent += to_send;
//flaw_line_below:
	}
//flaw_line_below:
	/* include the null we didn't log */
//flaw_line_below:
	return len + 1;
//flaw_line_below:
}
",180191,"static int audit_log_single_execve_arg(struct audit_context *context,
					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
 {
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
 	}
 
	/* walk the whole argument looking for non-ascii chars */
 	do {
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
 
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
 		}
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
","static int audit_log_single_execve_arg(struct audit_context *context,
static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
 {
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
 	}
	buf = buf_head;
 
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
 	do {
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
 
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
","The `audit_log_single_execve_arg` function is designed to log a single argument from an `execve` system call for auditing purposes. It takes various parameters, including the audit context, a pointer to an audit buffer, the argument number, and a user-space pointer to the argument string. The function first checks the length of the argument and ensures it does not exceed predefined limits. It then copies the argument from user space, checking for control characters, and logs the argument in segments if necessary. The function also handles cases where the argument is too long or contains non-ASCII characters, ensuring proper formatting and safety during the logging process.","The provided code is responsible for logging a single argument from an `execve` system call for auditing purposes. It begins by determining the length of the argument and validating it against predefined limits. The code then copies the argument from user space while checking for control characters that may affect logging. If the argument is too long or contains special characters, it adjusts the logging process accordingly. The function formats the log entries to include the argument's details, ensuring that all relevant information is captured while maintaining safety and integrity during the logging operation.","The vulnerability in the original code arises from a race condition, specifically a double fetch issue, where the length of a user-supplied argument is read multiple times without proper synchronization. This allows an attacker to modify the argument between these reads, potentially bypassing character-set restrictions or disrupting system-call auditing. In the fixed code, the use of a single buffer to read the argument and the introduction of a mechanism to track whether more data is needed ensures that the argument is consistently handled. The code now employs a safer approach to manage user input, reducing the risk of exploitation by preventing inconsistent reads of the argument length.","The vulnerability in the original code is a race condition, specifically a double fetch issue, where the length of a user-supplied argument is read multiple times without proper synchronization. This allows an attacker to modify the argument between these reads, potentially bypassing character-set restrictions or disrupting system-call auditing. The fixed code addresses this vulnerability by consolidating the reading process into a single buffer and implementing a mechanism to track whether additional data is needed. This ensures that the argument is consistently handled, reducing the risk of exploitation by preventing inconsistent reads of the argument length.","The vulnerability was fixed by restructuring the argument handling process to eliminate the double fetch issue. In the updated code, a single buffer is allocated to hold the user-supplied argument, and the length is managed using a single read operation with `strncpy_from_user`. The introduction of flags like `require_data` and `encode` helps track whether more data needs to be fetched and whether the argument contains control characters. This approach ensures that the argument length is consistently determined and prevents an attacker from modifying the input between reads, thereby enhancing the security of the auditing process.","The vulnerability was fixed by consolidating the argument reading process into a single, controlled operation that prevents multiple reads of the same data. The updated code uses a dedicated buffer to store the user-supplied argument, ensuring that the length is determined in a consistent manner. By implementing flags to track whether additional data is needed and to identify the presence of control characters, the code effectively manages the input in a secure way. This approach eliminates the risk of an attacker modifying the argument between reads, thereby enhancing the integrity of the auditing process."
20,180192,180192,,Local,Not required,,CVE-2016-6130,https://www.cvedetails.com/cve/CVE-2016-6130/,CWE-362,Medium,Partial,,,2016-07-03,1.9,"Race condition in the sclp_ctl_ioctl_sccb function in drivers/s390/char/sclp_ctl.c in the Linux kernel before 4.6 allows local users to obtain sensitive information from kernel memory by changing a certain length value, aka a *double fetch* vulnerability.",2016-11-28,+Info ,7,https://github.com/torvalds/linux/commit/532c34b5fbf1687df63b3fcd5b2846312ac943c6,532c34b5fbf1687df63b3fcd5b2846312ac943c6,"s390/sclp_ctl: fix potential information leak with /dev/sclp

The sclp_ctl_ioctl_sccb function uses two copy_from_user calls to
retrieve the sclp request from user space. The first copy_from_user
fetches the length of the request which is stored in the first two
bytes of the request. The second copy_from_user gets the complete
sclp request, but this copies the length field a second time.
A malicious user may have changed the length in the meantime.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Reviewed-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>",5,drivers/s390/char/sclp_ctl.c,"{""sha"": ""ea607a4a1bddaf3e41165aebed1fd787b87d754e"", ""filename"": ""drivers/s390/char/sclp_ctl.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 5, ""changes"": 12, ""blob_url"": ""https://github.com/torvalds/linux/blob/532c34b5fbf1687df63b3fcd5b2846312ac943c6/drivers/s390/char/sclp_ctl.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/532c34b5fbf1687df63b3fcd5b2846312ac943c6/drivers/s390/char/sclp_ctl.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/s390/char/sclp_ctl.c?ref=532c34b5fbf1687df63b3fcd5b2846312ac943c6"", ""patch"": ""@@ -56,6 +56,7 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)\n {\n \tstruct sclp_ctl_sccb ctl_sccb;\n \tstruct sccb_header *sccb;\n+\tunsigned long copied;\n \tint rc;\n \n \tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n@@ -65,14 +66,15 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)\n \tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n \tif (!sccb)\n \t\treturn -ENOMEM;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {\n+\tcopied = PAGE_SIZE -\n+\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);\n+\tif (offsetof(struct sccb_header, length) +\n+\t    sizeof(sccb->length) > copied || sccb->length > copied) {\n \t\trc = -EFAULT;\n \t\tgoto out_free;\n \t}\n-\tif (sccb->length > PAGE_SIZE || sccb->length < 8)\n-\t\treturn -EINVAL;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {\n-\t\trc = -EFAULT;\n+\tif (sccb->length < 8) {\n+\t\trc = -EINVAL;\n \t\tgoto out_free;\n \t}\n \trc = sclp_sync_request(ctl_sccb.cmdw, sccb);""}","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length < 8) {
		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
",C,"	unsigned long copied;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
	if (sccb->length < 8) {
		rc = -EINVAL;
","	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
",,"@@ -56,6 +56,7 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
+	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
@@ -65,14 +66,15 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
-	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
+	copied = PAGE_SIZE -
+		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
+	if (offsetof(struct sccb_header, length) +
+	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
-	if (sccb->length > PAGE_SIZE || sccb->length < 8)
-		return -EINVAL;
-	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
-		rc = -EFAULT;
+	if (sccb->length < 8) {
+		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);",linux,532c34b5fbf1687df63b3fcd5b2846312ac943c6,723cacbd9dc79582e562c123a0bacf8bfc69e72a,1,"static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
//fix_flaw_line_below:
//	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
//flaw_line_below:
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
//fix_flaw_line_below:
//	copied = PAGE_SIZE -
//fix_flaw_line_below:
//		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
//fix_flaw_line_below:
//	if (offsetof(struct sccb_header, length) +
//fix_flaw_line_below:
//	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
//flaw_line_below:
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
//flaw_line_below:
		return -EINVAL;
//flaw_line_below:
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
//flaw_line_below:
		rc = -EFAULT;
//fix_flaw_line_below:
//	if (sccb->length < 8) {
//fix_flaw_line_below:
//		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
",180192,"static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length < 8) {
		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","The `sclp_ctl_ioctl_sccb` function handles an IOCTL request for a System Control Block (SCCB) in a kernel module. It begins by copying data from user space into a local structure and checks if the command is supported. If valid, it allocates memory for an SCCB and copies the initial data from user space. The function then validates the SCCB's length, ensuring it falls within acceptable limits. It synchronously processes the SCCB request and, upon success, copies the modified data back to user space. Finally, it frees the allocated memory and returns the result of the operation, indicating success or failure.","The function processes an IOCTL request related to a control block by first retrieving data from user space and verifying the command's support. It allocates memory for the control block and copies the necessary data from user space, ensuring the data length is within specified limits. After validating the input, it synchronously executes a request using the control block. If successful, it transfers the modified data back to user space. The function concludes by freeing the allocated memory and returning a status code that indicates whether the operation was successful or encountered an error.","The vulnerability in the original code arises from a race condition, specifically a double fetch issue, where the length of the control block is read multiple times without proper synchronization. An attacker could manipulate the length value in user space between these reads, potentially allowing access to sensitive kernel memory. In the fixed code, the length is validated immediately after copying data from user space, ensuring that the length is checked against the amount of data copied. This prevents the possibility of an attacker altering the length value between fetches, thus mitigating the race condition and enhancing security.","The vulnerability in the original code is a race condition that allows an attacker to exploit a double fetch scenario, where the length of a control block is read multiple times without adequate protection. This enables the attacker to modify the length value in user space between these reads, potentially leading to unauthorized access to sensitive kernel memory. The fixed code addresses this issue by validating the length immediately after copying data from user space, ensuring that the length is consistent and preventing any manipulation between fetches. This change effectively mitigates the race condition and enhances the security of the function.","The vulnerability was fixed by modifying how the length of the control block is handled after copying data from user space. In the updated code, the length is validated immediately after the `copy_from_user` operation, ensuring that the length value is checked against the amount of data copied. Specifically, the code now calculates the amount of data copied and verifies that the length field does not exceed this value, as well as ensuring it is not less than a minimum threshold. This approach prevents an attacker from altering the length value between fetches, effectively mitigating the race condition and enhancing security.","The vulnerability was fixed by implementing a more robust validation process for the length of the control block immediately after data is copied from user space. The updated code calculates the amount of data successfully copied and checks that the length specified does not exceed this amount, while also ensuring it meets a minimum requirement. This change prevents the possibility of an attacker manipulating the length value between reads, thereby eliminating the race condition. By validating the length in a single, secure step, the code enhances its protection against unauthorized access to sensitive memory."
21,180581,180581,,Local,Not required,Complete,CVE-2016-2544,https://www.cvedetails.com/cve/CVE-2016-2544/,CWE-362,Medium,,,,2016-04-27,4.7,Race condition in the queue_delete function in sound/core/seq/seq_queue.c in the Linux kernel before 4.4.1 allows local users to cause a denial of service (use-after-free and system crash) by making an ioctl call at a certain time.,2017-09-06,DoS ,2,https://github.com/torvalds/linux/commit/3567eb6af614dac436c4b16a8d426f9faed639b3,3567eb6af614dac436c4b16a8d426f9faed639b3,"ALSA: seq: Fix race at timer setup and close

ALSA sequencer code has an open race between the timer setup ioctl and
the close of the client.  This was triggered by syzkaller fuzzer, and
a use-after-free was caught there as a result.

This patch papers over it by adding a proper queue->timer_mutex lock
around the timer-related calls in the relevant code path.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Tested-by: Dmitry Vyukov <dvyukov@google.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Takashi Iwai <tiwai@suse.de>",0,sound/core/seq/seq_queue.c,"{""sha"": ""0bec02e89d5118b3dffe1e22e88e423baa037491"", ""filename"": ""sound/core/seq/seq_queue.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 0, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/3567eb6af614dac436c4b16a8d426f9faed639b3/sound/core/seq/seq_queue.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/3567eb6af614dac436c4b16a8d426f9faed639b3/sound/core/seq/seq_queue.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/sound/core/seq/seq_queue.c?ref=3567eb6af614dac436c4b16a8d426f9faed639b3"", ""patch"": ""@@ -142,8 +142,10 @@ static struct snd_seq_queue *queue_new(int owner, int locked)\n static void queue_delete(struct snd_seq_queue *q)\n {\n \t/* stop and release the timer */\n+\tmutex_lock(&q->timer_mutex);\n \tsnd_seq_timer_stop(q->timer);\n \tsnd_seq_timer_close(q);\n+\tmutex_unlock(&q->timer_mutex);\n \t/* wait until access free */\n \tsnd_use_lock_sync(&q->use_lock);\n \t/* release resources... */""}"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
",C,"	mutex_lock(&q->timer_mutex);
	mutex_unlock(&q->timer_mutex);
",,,"@@ -142,8 +142,10 @@ static struct snd_seq_queue *queue_new(int owner, int locked)
 static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
+	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
+	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */",linux,3567eb6af614dac436c4b16a8d426f9faed639b3,030e2c78d3a91dd0d27fef37e91950dde333eba1,1," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
//fix_flaw_line_below:
//	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
//fix_flaw_line_below:
//	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
",180581," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
","The `queue_delete` function is responsible for safely deallocating a sound sequence queue structure in a sound system. It first stops and closes the associated timer to ensure no ongoing operations are affected. The function then synchronizes access to the queue using a lock to prevent concurrent modifications. After ensuring safe access, it deletes priority queues for ticks and time, effectively releasing any resources they hold. Finally, it deletes the timer and frees the memory allocated for the queue structure itself, ensuring proper cleanup and preventing memory leaks in the system.","The provided code defines a function that handles the deletion of a sound sequence queue. It begins by stopping and closing an associated timer to prevent any ongoing processes from interfering. The function then ensures that access to the queue is synchronized, allowing safe modifications. After confirming that it is safe to proceed, it removes priority queues related to timing and ticks, effectively releasing their resources. Finally, the function deletes the timer and frees the memory allocated for the queue, ensuring a thorough cleanup and preventing potential memory leaks in the system.","The vulnerability in the original code arises from a race condition due to the lack of proper synchronization when accessing shared resources, specifically the timer. This could allow multiple threads to concurrently modify the timer while it is being stopped and closed, leading to use-after-free errors and potential system crashes. In the fixed code, a mutex lock (`timer_mutex`) is introduced around the timer operations, ensuring that only one thread can access the timer at a time. This change effectively prevents concurrent modifications, thereby eliminating the race condition and enhancing the stability and safety of the function.","The vulnerability in the original code stems from a race condition that occurs when multiple threads access and modify shared resources simultaneously, particularly during the deletion process of a sound sequence queue. This lack of synchronization can lead to use-after-free errors, where a resource is accessed after it has been deallocated, potentially causing system crashes. The fixed code addresses this issue by introducing a mutex lock around critical operations, ensuring that only one thread can interact with the shared resource at a time. This change effectively prevents concurrent access, thereby eliminating the race condition and enhancing system stability.","The vulnerability was fixed by introducing a mutex lock around the timer operations in the modified code. Specifically, a lock is acquired using `mutex_lock(&q->timer_mutex)` before stopping and closing the timer, ensuring that these operations are performed atomically. This prevents other threads from accessing the timer concurrently, which could lead to race conditions and use-after-free errors. After the timer operations are completed, the lock is released with `mutex_unlock(&q->timer_mutex)`. This synchronization mechanism effectively safeguards the critical section of the code, thereby eliminating the race condition and enhancing the overall stability of the function.","The vulnerability was fixed by implementing a synchronization mechanism that ensures exclusive access to critical operations involving shared resources. By introducing a locking mechanism around the operations that manipulate the timer, the code prevents multiple threads from concurrently accessing and modifying the same resource. This change effectively eliminates the race condition that could lead to use-after-free errors, as it guarantees that only one thread can perform the deletion process at a time. As a result, the system's stability is improved, reducing the risk of crashes and denial of service due to improper resource handling."
22,180625,180625,,Local,Not required,Complete,CVE-2016-0723,https://www.cvedetails.com/cve/CVE-2016-0723/,CWE-362,Low,Partial,,,2016-02-07,5.6,Race condition in the tty_ioctl function in drivers/tty/tty_io.c in the Linux kernel through 4.4.1 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free and system crash) by making a TIOCGETD ioctl call during processing of a TIOCSETD ioctl call.,2016-12-05,DoS +Info ,1,https://github.com/torvalds/linux/commit/5c17c861a357e9458001f021a7afa7aab9937439,5c17c861a357e9458001f021a7afa7aab9937439,"tty: Fix unsafe ldisc reference via ioctl(TIOCGETD)

ioctl(TIOCGETD) retrieves the line discipline id directly from the
ldisc because the line discipline id (c_line) in termios is untrustworthy;
userspace may have set termios via ioctl(TCSETS*) without actually
changing the line discipline via ioctl(TIOCSETD).

However, directly accessing the current ldisc via tty->ldisc is
unsafe; the ldisc ptr dereferenced may be stale if the line discipline
is changing via ioctl(TIOCSETD) or hangup.

Wait for the line discipline reference (just like read() or write())
to retrieve the ""current"" line discipline id.

Cc: <stable@vger.kernel.org>
Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",1,drivers/tty/tty_io.c,"{""sha"": ""5cec01c75691a6b7ee58ea65346c05572db1bab1"", ""filename"": ""drivers/tty/tty_io.c"", ""status"": ""modified"", ""additions"": 23, ""deletions"": 1, ""changes"": 24, ""blob_url"": ""https://github.com/torvalds/linux/blob/5c17c861a357e9458001f021a7afa7aab9937439/drivers/tty/tty_io.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/5c17c861a357e9458001f021a7afa7aab9937439/drivers/tty/tty_io.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/tty/tty_io.c?ref=5c17c861a357e9458001f021a7afa7aab9937439"", ""patch"": ""@@ -2658,6 +2658,28 @@ static int tiocsetd(struct tty_struct *tty, int __user *p)\n \treturn ret;\n }\n \n+/**\n+ *\ttiocgetd\t-\tget line discipline\n+ *\t@tty: tty device\n+ *\t@p: pointer to user data\n+ *\n+ *\tRetrieves the line discipline id directly from the ldisc.\n+ *\n+ *\tLocking: waits for ldisc reference (in case the line discipline\n+ *\t\tis changing or the tty is being hungup)\n+ */\n+\n+static int tiocgetd(struct tty_struct *tty, int __user *p)\n+{\n+\tstruct tty_ldisc *ld;\n+\tint ret;\n+\n+\tld = tty_ldisc_ref_wait(tty);\n+\tret = put_user(ld->ops->num, p);\n+\ttty_ldisc_deref(ld);\n+\treturn ret;\n+}\n+\n /**\n  *\tsend_break\t-\tperformed time break\n  *\t@tty: device to break on\n@@ -2884,7 +2906,7 @@ long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n \tcase TIOCGSID:\n \t\treturn tiocgsid(tty, real_tty, p);\n \tcase TIOCGETD:\n-\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n+\t\treturn tiocgetd(tty, p);\n \tcase TIOCSETD:\n \t\treturn tiocsetd(tty, p);\n \tcase TIOCVHANGUP:""}","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
",C,"		return tiocgetd(tty, p);
","		return put_user(tty->ldisc->ops->num, (int __user *)p);
",,"@@ -2658,6 +2658,28 @@ static int tiocsetd(struct tty_struct *tty, int __user *p)
 	return ret;
 }
 
+/**
+ *	tiocgetd	-	get line discipline
+ *	@tty: tty device
+ *	@p: pointer to user data
+ *
+ *	Retrieves the line discipline id directly from the ldisc.
+ *
+ *	Locking: waits for ldisc reference (in case the line discipline
+ *		is changing or the tty is being hungup)
+ */
+
+static int tiocgetd(struct tty_struct *tty, int __user *p)
+{
+	struct tty_ldisc *ld;
+	int ret;
+
+	ld = tty_ldisc_ref_wait(tty);
+	ret = put_user(ld->ops->num, p);
+	tty_ldisc_deref(ld);
+	return ret;
+}
+
 /**
  *	send_break	-	performed time break
  *	@tty: device to break on
@@ -2884,7 +2906,7 @@ long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
-		return put_user(tty->ldisc->ops->num, (int __user *)p);
+		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:",linux,5c17c861a357e9458001f021a7afa7aab9937439,7f22f6c935cda600660e623a411fe380015d28d9,1,"long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
//flaw_line_below:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
//fix_flaw_line_below:
//		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
",180625,"long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","The `tty_ioctl` function handles various terminal I/O control commands for a given file descriptor associated with a terminal (TTY). It first verifies the integrity of the TTY structure and retrieves the corresponding real TTY. The function processes different commands, such as setting terminal attributes, managing window sizes, and handling break signals. It also manages exclusive access to the TTY and performs operations like flushing buffers and getting or setting modem control lines. If the command is not recognized, it delegates the call to the TTY's specific operations or the line discipline's operations, returning appropriate error codes when necessary.","The provided code implements a function that processes various input/output control commands for terminal devices. It begins by validating the terminal structure and retrieving the associated terminal instance. The function then handles a range of commands, including those for setting terminal attributes, managing window sizes, and controlling signal breaks. It also manages exclusive access to the terminal and performs operations like flushing buffers and manipulating modem control lines. If a command is unrecognized, it defers to the specific operations of the terminal or its line discipline, returning appropriate error codes as needed.","The vulnerability in the original code arises from a race condition in the `tty_ioctl` function, where simultaneous calls to `TIOCSETD` and `TIOCGETD` could lead to a use-after-free scenario, allowing unauthorized access to sensitive kernel memory or causing a system crash. This occurs because the function does not adequately synchronize access to shared resources, allowing one thread to free a resource while another is still using it. The fixed code addresses this issue by ensuring that the `tty_check_change` function is called before processing commands that could alter the state of the terminal, thus preventing concurrent modifications and ensuring safe access to the terminal's resources.","The vulnerability in the original code stems from a race condition that occurs when two simultaneous ioctl calls modify shared resources, potentially leading to a use-after-free situation. This allows unauthorized access to sensitive kernel memory or can result in a system crash. The fixed code addresses this issue by implementing checks to ensure that state changes are properly synchronized before processing commands that could alter the terminal's state. By enforcing this synchronization, the code prevents concurrent modifications, thereby eliminating the risk of accessing freed resources and enhancing overall system stability and security.","The vulnerability was fixed by introducing a call to `tty_check_change` before processing commands that could alter the terminal's state, such as `TIOCSETD`. This function checks for any pending changes to the terminal, ensuring that the state is stable before proceeding. Additionally, the code now waits for any ongoing operations to complete using `tty_wait_until_sent`, which helps prevent simultaneous access to shared resources. By implementing these checks, the fixed code effectively synchronizes access to the terminal, mitigating the risk of race conditions and ensuring safe handling of ioctl calls.","The vulnerability was fixed by introducing synchronization mechanisms that ensure safe access to shared resources during ioctl command processing. The code now includes checks to verify the stability of the terminal's state before executing commands that could modify it, preventing concurrent modifications that could lead to race conditions. Additionally, the implementation ensures that any ongoing operations are completed before proceeding with further actions, effectively managing the timing of resource access. These changes collectively enhance the robustness of the code, eliminating the risk of unauthorized memory access and improving overall system stability."
23,180738,180738,,Local,Not required,Complete,CVE-2015-4170,https://www.cvedetails.com/cve/CVE-2015-4170/,CWE-362,Medium,,,,2016-05-02,4.7,Race condition in the ldsem_cmpxchg function in drivers/tty/tty_ldsem.c in the Linux kernel before 3.13-rc4-next-20131218 allows local users to cause a denial of service (ldsem_down_read and ldsem_down_write deadlock) by establishing a new tty thread during shutdown of a previous tty thread.,2017-11-11,DoS ,8,https://github.com/torvalds/linux/commit/cf872776fc84128bb779ce2b83a37c884c3203ae,cf872776fc84128bb779ce2b83a37c884c3203ae,"tty: Fix hang at ldsem_down_read()

When a controlling tty is being hung up and the hang up is
waiting for a just-signalled tty reader or writer to exit, and a new tty
reader/writer tries to acquire an ldisc reference concurrently with the
ldisc reference release from the signalled reader/writer, the hangup
can hang. The new reader/writer is sleeping in ldsem_down_read() and the
hangup is sleeping in ldsem_down_write() [1].

The new reader/writer fails to wakeup the waiting hangup because the
wrong lock count value is checked (the old lock count rather than the new
lock count) to see if the lock is unowned.

Change helper function to return the new lock count if the cmpxchg was
successful; document this behavior.

[1] edited dmesg log from reporter

SysRq : Show Blocked State
  task                        PC stack   pid father
systemd         D ffff88040c4f0000     0     1      0 0x00000000
 ffff88040c49fbe0 0000000000000046 ffff88040c4a0000 ffff88040c49ffd8
 00000000001d3980 00000000001d3980 ffff88040c4a0000 ffff88040593d840
 ffff88040c49fb40 ffffffff810a4cc0 0000000000000006 0000000000000023
Call Trace:
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817a6649>] schedule+0x24/0x5e
 [<ffffffff817a588b>] schedule_timeout+0x15b/0x1ec
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817aa691>] ? _raw_spin_unlock_irq+0x24/0x26
 [<ffffffff817aa10c>] down_read_failed+0xe3/0x1b9
 [<ffffffff817aa26d>] ldsem_down_read+0x8b/0xa5
 [<ffffffff8142b5ca>] ? tty_ldisc_ref_wait+0x1b/0x44
 [<ffffffff8142b5ca>] tty_ldisc_ref_wait+0x1b/0x44
 [<ffffffff81423f5b>] tty_write+0x7d/0x28a
 [<ffffffff814241f5>] redirected_tty_write+0x8d/0x98
 [<ffffffff81424168>] ? tty_write+0x28a/0x28a
 [<ffffffff8115d03f>] do_loop_readv_writev+0x56/0x79
 [<ffffffff8115e604>] do_readv_writev+0x1b0/0x1ff
 [<ffffffff8116ea0b>] ? do_vfs_ioctl+0x32a/0x489
 [<ffffffff81167d9d>] ? final_putname+0x1d/0x3a
 [<ffffffff8115e6c7>] vfs_writev+0x2e/0x49
 [<ffffffff8115e7d3>] SyS_writev+0x47/0xaa
 [<ffffffff817ab822>] system_call_fastpath+0x16/0x1b
bash            D ffffffff81c104c0     0  5469   5302 0x00000082
 ffff8800cf817ac0 0000000000000046 ffff8804086b22a0 ffff8800cf817fd8
 00000000001d3980 00000000001d3980 ffff8804086b22a0 ffff8800cf817a48
 000000000000b9a0 ffff8800cf817a78 ffffffff81004675 ffff8800cf817a44
Call Trace:
 [<ffffffff81004675>] ? dump_trace+0x165/0x29c
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff8100edda>] ? save_stack_trace+0x26/0x41
 [<ffffffff817a6649>] schedule+0x24/0x5e
 [<ffffffff817a588b>] schedule_timeout+0x15b/0x1ec
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817a9f03>] ? down_write_failed+0xa3/0x1c9
 [<ffffffff817aa691>] ? _raw_spin_unlock_irq+0x24/0x26
 [<ffffffff817a9f0b>] down_write_failed+0xab/0x1c9
 [<ffffffff817aa300>] ldsem_down_write+0x79/0xb1
 [<ffffffff817aada3>] ? tty_ldisc_lock_pair_timeout+0xa5/0xd9
 [<ffffffff817aada3>] tty_ldisc_lock_pair_timeout+0xa5/0xd9
 [<ffffffff8142bf33>] tty_ldisc_hangup+0xc4/0x218
 [<ffffffff81423ab3>] __tty_hangup+0x2e2/0x3ed
 [<ffffffff81424a76>] disassociate_ctty+0x63/0x226
 [<ffffffff81078aa7>] do_exit+0x79f/0xa11
 [<ffffffff81086bdb>] ? get_signal_to_deliver+0x206/0x62f
 [<ffffffff810b4bfb>] ? lock_release_holdtime.part.8+0xf/0x16e
 [<ffffffff81079b05>] do_group_exit+0x47/0xb5
 [<ffffffff81086c16>] get_signal_to_deliver+0x241/0x62f
 [<ffffffff810020a7>] do_signal+0x43/0x59d
 [<ffffffff810f2af7>] ? __audit_syscall_exit+0x21a/0x2a8
 [<ffffffff810b4bfb>] ? lock_release_holdtime.part.8+0xf/0x16e
 [<ffffffff81002655>] do_notify_resume+0x54/0x6c
 [<ffffffff817abaf8>] int_signal+0x12/0x17

Reported-by: Sami Farin <sami.farin@gmail.com>
Cc: <stable@vger.kernel.org> # 3.12.x
Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",3,drivers/tty/tty_ldsem.c,"{""sha"": ""d8a55e87877f06f3141602e4f08cdcb668c465b0"", ""filename"": ""drivers/tty/tty_ldsem.c"", ""status"": ""modified"", ""additions"": 13, ""deletions"": 3, ""changes"": 16, ""blob_url"": ""https://github.com/torvalds/linux/blob/cf872776fc84128bb779ce2b83a37c884c3203ae/drivers/tty/tty_ldsem.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cf872776fc84128bb779ce2b83a37c884c3203ae/drivers/tty/tty_ldsem.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/tty/tty_ldsem.c?ref=cf872776fc84128bb779ce2b83a37c884c3203ae"", ""patch"": ""@@ -86,11 +86,21 @@ static inline long ldsem_atomic_update(long delta, struct ld_semaphore *sem)\n \treturn atomic_long_add_return(delta, (atomic_long_t *)&sem->count);\n }\n \n+/*\n+ * ldsem_cmpxchg() updates @*old with the last-known sem->count value.\n+ * Returns 1 if count was successfully changed; @*old will have @new value.\n+ * Returns 0 if count was not changed; @*old will have most recent sem->count\n+ */\n static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n {\n-\tlong tmp = *old;\n-\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n-\treturn *old == tmp;\n+\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n+\tif (tmp == *old) {\n+\t\t*old = new;\n+\t\treturn 1;\n+\t} else {\n+\t\t*old = tmp;\n+\t\treturn 0;\n+\t}\n }\n \n /*""}"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
 }
"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
 }
",C,"	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
","	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
",,"@@ -86,11 +86,21 @@ static inline long ldsem_atomic_update(long delta, struct ld_semaphore *sem)
 	return atomic_long_add_return(delta, (atomic_long_t *)&sem->count);
 }
 
+/*
+ * ldsem_cmpxchg() updates @*old with the last-known sem->count value.
+ * Returns 1 if count was successfully changed; @*old will have @new value.
+ * Returns 0 if count was not changed; @*old will have most recent sem->count
+ */
 static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
-	long tmp = *old;
-	*old = atomic_long_cmpxchg(&sem->count, *old, new);
-	return *old == tmp;
+	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
+	if (tmp == *old) {
+		*old = new;
+		return 1;
+	} else {
+		*old = tmp;
+		return 0;
+	}
 }
 
 /*",linux,cf872776fc84128bb779ce2b83a37c884c3203ae,319e2e3f63c348a9b66db4667efa73178e18b17d,1," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
//flaw_line_below:
	long tmp = *old;
//flaw_line_below:
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
//flaw_line_below:
	return *old == tmp;
//fix_flaw_line_below:
//	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
//fix_flaw_line_below:
//	if (tmp == *old) {
//fix_flaw_line_below:
//		*old = new;
//fix_flaw_line_below:
//		return 1;
//fix_flaw_line_below:
//	} else {
//fix_flaw_line_below:
//		*old = tmp;
//fix_flaw_line_below:
//		return 0;
//fix_flaw_line_below:
//	}
 }
",180738," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
 }
"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
 }
","The `ldsem_cmpxchg` function performs an atomic compare-and-swap operation on a semaphore's count value. It takes a pointer to a long integer `old`, a new long integer `new`, and a pointer to a `ld_semaphore` structure `sem`. The function first stores the current value of `*old` in a temporary variable `tmp`. It then attempts to update the semaphore's count using `atomic_long_cmpxchg`, which replaces the count with `new` only if it matches `*old`. Finally, the function returns a boolean result indicating whether the original value of `*old` was equal to `tmp`, signaling if the swap was successful.","The provided code implements an atomic compare-and-swap operation for a semaphore's count value. It retrieves the current value of a specified variable and stores it temporarily. The code then attempts to update the semaphore's count by replacing it with a new value, but only if it matches the previously retrieved value. After the operation, it checks if the original value was unchanged, indicating whether the update was successful. The function ultimately returns a boolean result that reflects whether the swap operation occurred, ensuring thread-safe modifications to the semaphore's count in a concurrent environment.","The vulnerability in the original code arises from a race condition during the atomic compare-and-swap operation, where the semaphore's count could be modified by multiple threads simultaneously, leading to inconsistent states and potential deadlocks. In the fixed code, the atomic operation is performed first, storing the result in a temporary variable. The code then checks if this temporary value matches the original value; if it does, it updates the original pointer with the new value, ensuring that the operation is only completed if the state has not changed. This change prevents race conditions by ensuring that the update is only applied when it is safe to do so.","The vulnerability stems from a race condition that occurs when multiple threads attempt to modify a shared resource simultaneously, leading to inconsistent states and potential deadlocks. In the original implementation, the update could proceed without verifying whether the resource had changed, allowing one thread to overwrite another's changes. The fixed code addresses this issue by first performing an atomic operation to safely read and potentially update the resource. It then checks if the original value remains unchanged before applying the new value, ensuring that updates are only made when it is safe, thus preventing the race condition and associated deadlocks.","The vulnerability was fixed by modifying the logic within the `ldsem_cmpxchg` function to ensure safe updates to the semaphore's count. In the revised code, the atomic operation `atomic_long_cmpxchg` is called first, storing the result in a temporary variable. This allows the function to check if the temporary value matches the original value before proceeding. If they match, the original pointer is updated with the new value, ensuring that the update only occurs when it is safe. If they do not match, the original pointer is updated with the temporary value, maintaining consistency and preventing race conditions.","The vulnerability was fixed by restructuring the update logic to ensure that changes to the shared resource occur only when it is safe to do so. The revised approach first performs an atomic operation to read the current state and determine if it matches the expected value. If the values align, the update is applied, ensuring that no other thread has modified the resource in the interim. If the values do not match, the function updates the reference to reflect the current state, thereby maintaining consistency and preventing race conditions that could lead to deadlocks."
24,180825,180825,,Local,Not required,Complete,CVE-2017-17712,https://www.cvedetails.com/cve/CVE-2017-17712/,CWE-362,Medium,Complete,Complete,,2017-12-15,6.9,The raw_sendmsg() function in net/ipv4/raw.c in the Linux kernel through 4.14.6 has a race condition in inet->hdrincl that leads to uninitialized stack pointer usage; this allows a local user to execute code and gain privileges.,2018-04-03,Exec Code +Priv ,10,https://github.com/torvalds/linux/commit/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,"net: ipv4: fix for a race condition in raw_sendmsg

inet->hdrincl is racy, and could lead to uninitialized stack pointer
usage, so its value should be read only once.

Fixes: c008ba5bdc9f (""ipv4: Avoid reading user iov twice after raw_probe_proto_opt"")
Signed-off-by: Mohamed Ghannam <simo.ghannam@gmail.com>
Reviewed-by: Eric Dumazet <edumazet@google.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",5,net/ipv4/raw.c,"{""sha"": ""125c1eab3eaa6d894804c3aa8918aa7fcc736ca0"", ""filename"": ""net/ipv4/raw.c"", ""status"": ""modified"", ""additions"": 10, ""deletions"": 5, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483/net/ipv4/raw.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483/net/ipv4/raw.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/ipv4/raw.c?ref=8f659a03a0ba9289b9aeb9b4470e6fb263d6f483"", ""patch"": ""@@ -513,11 +513,16 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \tint err;\n \tstruct ip_options_data opt_copy;\n \tstruct raw_frag_vec rfv;\n+\tint hdrincl;\n \n \terr = -EMSGSIZE;\n \tif (len > 0xFFFF)\n \t\tgoto out;\n \n+\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n+\t * but READ_ONCE() doesn't work with bit fields\n+\t */\n+\thdrincl = inet->hdrincl;\n \t/*\n \t *\tCheck the flags.\n \t */\n@@ -593,7 +598,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\t/* Linux does not mangle headers on raw sockets,\n \t\t * so that IP options + IP_HDRINCL is non-sense.\n \t\t */\n-\t\tif (inet->hdrincl)\n+\t\tif (hdrincl)\n \t\t\tgoto done;\n \t\tif (ipc.opt->opt.srr) {\n \t\t\tif (!daddr)\n@@ -615,12 +620,12 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \n \tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n \t\t\t   RT_SCOPE_UNIVERSE,\n-\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n+\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n \t\t\t   inet_sk_flowi_flags(sk) |\n-\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n+\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n \t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n \n-\tif (!inet->hdrincl) {\n+\tif (!hdrincl) {\n \t\trfv.msg = msg;\n \t\trfv.hlen = 0;\n \n@@ -645,7 +650,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\tgoto do_confirm;\n back_from_confirm:\n \n-\tif (inet->hdrincl)\n+\tif (hdrincl)\n \t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n \t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n ""}","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (inet->hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!inet->hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (inet->hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
",C,"	int hdrincl;
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
		if (hdrincl)
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
	if (!hdrincl) {
	if (hdrincl)
","		if (inet->hdrincl)
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
	if (!inet->hdrincl) {
	if (inet->hdrincl)
",,"@@ -513,11 +513,16 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
+	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
+	/* hdrincl should be READ_ONCE(inet->hdrincl)
+	 * but READ_ONCE() doesn't work with bit fields
+	 */
+	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */
@@ -593,7 +598,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
-		if (inet->hdrincl)
+		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
@@ -615,12 +620,12 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
-			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
+			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
-			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
+			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
-	if (!inet->hdrincl) {
+	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
@@ -645,7 +650,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 		goto do_confirm;
 back_from_confirm:
 
-	if (inet->hdrincl)
+	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 ",linux,8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,93c647643b48f0131f02e45da3bd367d80443291,1,"static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
//fix_flaw_line_below:
//	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
//fix_flaw_line_below:
//	/* hdrincl should be READ_ONCE(inet->hdrincl)
//fix_flaw_line_below:
//	 * but READ_ONCE() doesn't work with bit fields
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
//flaw_line_below:
		if (inet->hdrincl)
//fix_flaw_line_below:
//		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
//flaw_line_below:
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
//fix_flaw_line_below:
//			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
//flaw_line_below:
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
//fix_flaw_line_below:
//			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
//flaw_line_below:
	if (!inet->hdrincl) {
//fix_flaw_line_below:
//	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
//flaw_line_below:
	if (inet->hdrincl)
//fix_flaw_line_below:
//	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
",180825,"static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (inet->hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!inet->hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (inet->hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","The `raw_sendmsg` function is responsible for sending raw IP packets over a socket in a Linux networking context. It begins by validating the message length and flags, ensuring the destination address is correctly specified. The function retrieves socket options and prepares the necessary routing information, including handling multicast addresses. It constructs a flow structure for routing and checks for broadcast permissions. Depending on whether the IP header is included, it either sends the packet directly or appends data to the socket's transmission queue. Finally, it manages memory cleanup and returns the number of bytes sent or an error code if applicable.","The provided code implements a function that facilitates the sending of raw IP packets through a socket in a networking environment. It begins by validating the input message's length and flags, ensuring that a valid destination address is provided. The function prepares routing information and handles specific cases for multicast addresses. It constructs necessary structures for routing and checks permissions for broadcast transmissions. Depending on whether the IP header is included, it either sends the packet directly or queues the data for transmission. Finally, it manages memory cleanup and returns the number of bytes sent or an error code if an issue arises.","The vulnerability in the original code arises from a race condition involving the `inet->hdrincl` variable, which can be modified by concurrent threads, leading to the use of an uninitialized stack pointer. This situation can allow an attacker to exploit the race condition to execute arbitrary code with elevated privileges. In the fixed code, the vulnerability is addressed by introducing a local variable `hdrincl` that captures the value of `inet->hdrincl` at a specific point in time, ensuring that the value remains consistent throughout the function's execution, thus preventing any race condition from affecting the packet processing logic.","The vulnerability in the original code stems from a race condition that occurs when a shared variable is accessed concurrently by multiple threads, potentially leading to the use of an uninitialized stack pointer. This situation can be exploited by an attacker to execute arbitrary code with elevated privileges. The fix implemented in the modified code involves capturing the value of the shared variable into a local variable at a specific point, ensuring that the value remains stable throughout the function's execution. This change effectively mitigates the race condition, preventing unauthorized access and enhancing the overall security of the code.","The vulnerability was fixed by introducing a local variable named `hdrincl` that captures the value of `inet->hdrincl` at the beginning of the function. This change ensures that the value of `hdrincl` remains consistent throughout the execution of the function, preventing any race condition that could arise from concurrent modifications to `inet->hdrincl`. By using this local variable in subsequent checks and operations, the code avoids the risk of using an uninitialized stack pointer, thereby eliminating the potential for privilege escalation and enhancing the overall security of the `raw_sendmsg` function.","The vulnerability was addressed by introducing a local variable that captures the value of a shared resource at the start of the function. This ensures that the value remains stable and consistent throughout the function's execution, effectively preventing any race conditions that could arise from concurrent access. By relying on this local copy for subsequent operations, the code eliminates the risk of using an uninitialized stack pointer, thereby mitigating the potential for privilege escalation and enhancing the overall security of the function. This change significantly reduces the likelihood of exploitation by ensuring thread safety."
25,180889,180889,,Local,Not required,Partial,CVE-2017-15649,https://www.cvedetails.com/cve/CVE-2017-15649/,CWE-362,Low,Partial,Partial,,2017-10-19,4.6,"net/packet/af_packet.c in the Linux kernel before 4.13.6 allows local users to gain privileges via crafted system calls that trigger mishandling of packet_fanout data structures, because of a race condition (involving fanout_add and packet_do_bind) that leads to a use-after-free, a different vulnerability than CVE-2017-6346.",2018-08-24,+Priv ,5,https://github.com/torvalds/linux/commit/4971613c1639d8e5f102c4e797c3bf8f83a5a69e,4971613c1639d8e5f102c4e797c3bf8f83a5a69e,"packet: in packet_do_bind, test fanout with bind_lock held

Once a socket has po->fanout set, it remains a member of the group
until it is destroyed. The prot_hook must be constant and identical
across sockets in the group.

If fanout_add races with packet_do_bind between the test of po->fanout
and taking the lock, the bind call may make type or dev inconsistent
with that of the fanout group.

Hold po->bind_lock when testing po->fanout to avoid this race.

I had to introduce artificial delay (local_bh_enable) to actually
observe the race.

Fixes: dc99f600698d (""packet: Add fanout support."")
Signed-off-by: Willem de Bruijn <willemb@google.com>
Reviewed-by: Eric Dumazet <edumazet@google.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",2,net/packet/af_packet.c,"{""sha"": ""a10c2836465cf499e2b723419da25bc3ad07d69f"", ""filename"": ""net/packet/af_packet.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 3, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/4971613c1639d8e5f102c4e797c3bf8f83a5a69e/net/packet/af_packet.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/4971613c1639d8e5f102c4e797c3bf8f83a5a69e/net/packet/af_packet.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/packet/af_packet.c?ref=4971613c1639d8e5f102c4e797c3bf8f83a5a69e"", ""patch"": ""@@ -3069,13 +3069,15 @@ static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n \tint ret = 0;\n \tbool unlisted = false;\n \n-\tif (po->fanout)\n-\t\treturn -EINVAL;\n-\n \tlock_sock(sk);\n \tspin_lock(&po->bind_lock);\n \trcu_read_lock();\n \n+\tif (po->fanout) {\n+\t\tret = -EINVAL;\n+\t\tgoto out_unlock;\n+\t}\n+\n \tif (name) {\n \t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n \t\tif (!dev) {""}","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
	if (po->fanout)
		return -EINVAL;
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
",C,"	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

","	if (po->fanout)
		return -EINVAL;
",,"@@ -3069,13 +3069,15 @@ static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
 	int ret = 0;
 	bool unlisted = false;
 
-	if (po->fanout)
-		return -EINVAL;
-
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
+	if (po->fanout) {
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {",linux,4971613c1639d8e5f102c4e797c3bf8f83a5a69e,1579f678fb4397f9e439d2e373d4ade036c673b4,1,"static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
//flaw_line_below:
	if (po->fanout)
//flaw_line_below:
		return -EINVAL;
//flaw_line_below:

 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
//fix_flaw_line_below:
//	if (po->fanout) {
//fix_flaw_line_below:
//		ret = -EINVAL;
//fix_flaw_line_below:
//		goto out_unlock;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
",180889,"static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
	if (po->fanout)
		return -EINVAL;
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","The `packet_do_bind` function is responsible for binding a socket to a specific network device and protocol in a Linux kernel networking context. It first checks if the socket is already associated with a fanout, returning an error if so. The function then attempts to retrieve the network device either by name or index, handling errors if the device is not found. It evaluates whether a rehook is necessary based on the current protocol and device. If a rehook is needed and the socket is running, it unregisters the current protocol hook. Finally, it either registers a new protocol hook if conditions are met or reports an error if the device is down, ensuring proper resource management throughout.","The function manages the binding of a socket to a specific network device and protocol within the kernel's networking subsystem. It first checks for existing configurations and retrieves the desired network device based on provided identifiers. If a change in protocol or device is necessary, it unregisters the current settings if the socket is active. The function then updates the socket's configuration, ensuring that resources are properly managed. If the new device is operational, it registers the new settings; otherwise, it reports an error if the device is not available, maintaining the integrity of the socket's state throughout the process.","The vulnerability in the original code arises from a race condition involving the `fanout_add` and `packet_do_bind` functions, which can lead to a use-after-free scenario. Specifically, if the socket is bound while another thread modifies the `fanout` data structure, it may result in accessing freed memory, allowing local users to escalate privileges. The fixed code addresses this issue by ensuring that the socket is locked with `lock_sock(sk)` and the binding process is protected by acquiring `spin_lock(&po->bind_lock)` before any operations on the `fanout` structure, thus preventing concurrent modifications and ensuring safe access to shared resources.","The vulnerability in the original code stems from a race condition that occurs when multiple threads interact with shared data structures, specifically during the binding process. This situation can lead to a use-after-free error, where a thread accesses memory that has already been released, potentially allowing local users to escalate their privileges through crafted system calls. The fixed code mitigates this issue by implementing proper locking mechanisms to ensure that the binding process is safely executed without interference from other threads, thereby preventing concurrent modifications and ensuring that shared resources are accessed in a controlled manner.","The vulnerability was fixed by introducing proper locking mechanisms to ensure safe access to shared resources during the binding process. Specifically, the code now uses `lock_sock(sk)` to lock the socket and `spin_lock(&po->bind_lock)` to protect the binding operations. This prevents concurrent modifications to the `fanout` data structure while the socket is being bound. Additionally, the use of `rcu_read_lock()` and `rcu_read_unlock()` ensures that the read operations on the device are safely managed, preventing race conditions that could lead to a use-after-free scenario, thus enhancing the overall stability and security of the code.","The vulnerability was addressed by implementing a series of locking mechanisms to ensure that access to shared resources is properly synchronized during the binding process. This involved acquiring locks before performing operations that could be affected by concurrent modifications, thereby preventing race conditions. Additionally, the code was modified to ensure that any read and write operations on critical data structures are safely managed, which helps to avoid scenarios where memory could be accessed after it has been freed. These changes collectively enhance the stability and security of the code by ensuring that operations are executed in a controlled manner."
26,181435,181435,,Local,Not required,Complete,CVE-2017-7533,https://www.cvedetails.com/cve/CVE-2017-7533/,CWE-362,Medium,Complete,Complete,,2017-08-05,6.9,Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions.,2018-01-04,DoS +Priv Mem. Corr. ,4,https://github.com/torvalds/linux/commit/49d31c2f389acfe83417083e1208422b4091cd9e,49d31c2f389acfe83417083e1208422b4091cd9e,"dentry name snapshots

take_dentry_name_snapshot() takes a safe snapshot of dentry name;
if the name is a short one, it gets copied into caller-supplied
structure, otherwise an extra reference to external name is grabbed
(those are never modified).  In either case the pointer to stable
string is stored into the same structure.

dentry must be held by the caller of take_dentry_name_snapshot(),
but may be freely dropped afterwards - the snapshot will stay
until destroyed by release_dentry_name_snapshot().

Intended use:
	struct name_snapshot s;

	take_dentry_name_snapshot(&s, dentry);
	...
	access s.name
	...
	release_dentry_name_snapshot(&s);

Replaces fsnotify_oldname_...(), gets used in fsnotify to obtain the name
to pass down with event.

Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>",4,fs/namei.c,"{""sha"": ""831f3a9a8f05282b8ba9aae9c6ed963334b9b8f3"", ""filename"": ""fs/dcache.c"", ""status"": ""modified"", ""additions"": 27, ""deletions"": 0, ""changes"": 27, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/dcache.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/dcache.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/dcache.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -277,6 +277,33 @@ static inline int dname_external(const struct dentry *dentry)\n \treturn dentry->d_name.name != dentry->d_iname;\n }\n \n+void take_dentry_name_snapshot(struct name_snapshot *name, struct dentry *dentry)\n+{\n+\tspin_lock(&dentry->d_lock);\n+\tif (unlikely(dname_external(dentry))) {\n+\t\tstruct external_name *p = external_name(dentry);\n+\t\tatomic_inc(&p->u.count);\n+\t\tspin_unlock(&dentry->d_lock);\n+\t\tname->name = p->name;\n+\t} else {\n+\t\tmemcpy(name->inline_name, dentry->d_iname, DNAME_INLINE_LEN);\n+\t\tspin_unlock(&dentry->d_lock);\n+\t\tname->name = name->inline_name;\n+\t}\n+}\n+EXPORT_SYMBOL(take_dentry_name_snapshot);\n+\n+void release_dentry_name_snapshot(struct name_snapshot *name)\n+{\n+\tif (unlikely(name->name != name->inline_name)) {\n+\t\tstruct external_name *p;\n+\t\tp = container_of(name->name, struct external_name, name[0]);\n+\t\tif (unlikely(atomic_dec_and_test(&p->u.count)))\n+\t\t\tkfree_rcu(p, u.head);\n+\t}\n+}\n+EXPORT_SYMBOL(release_dentry_name_snapshot);\n+\n static inline void __d_set_inode_and_type(struct dentry *dentry,\n \t\t\t\t\t  struct inode *inode,\n \t\t\t\t\t  unsigned type_flags)""}<_**next**_>{""sha"": ""acd3be2cc691bb577916aa552668d2c1d179ed69"", ""filename"": ""fs/debugfs/inode.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 5, ""changes"": 10, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/debugfs/inode.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/debugfs/inode.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/debugfs/inode.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -766,7 +766,7 @@ struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n {\n \tint error;\n \tstruct dentry *dentry = NULL, *trap;\n-\tconst char *old_name;\n+\tstruct name_snapshot old_name;\n \n \ttrap = lock_rename(new_dir, old_dir);\n \t/* Source or destination directories don't exist? */\n@@ -781,19 +781,19 @@ struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n \tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n \t\tgoto exit;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \n \terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n \t\t\t      dentry, 0);\n \tif (error) {\n-\t\tfsnotify_oldname_free(old_name);\n+\t\trelease_dentry_name_snapshot(&old_name);\n \t\tgoto exit;\n \t}\n \td_move(old_dentry, dentry);\n-\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,\n+\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,\n \t\td_is_dir(old_dentry),\n \t\tNULL, old_dentry);\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \tunlock_rename(new_dir, old_dir);\n \tdput(dentry);\n \treturn old_dentry;""}<_**next**_>{""sha"": ""c5588e837b1532bfc201a5b8d6bfcdedad9f107f"", ""filename"": ""fs/namei.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/namei.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/namei.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/namei.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -4362,11 +4362,11 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n {\n \tint error;\n \tbool is_dir = d_is_dir(old_dentry);\n-\tconst unsigned char *old_name;\n \tstruct inode *source = old_dentry->d_inode;\n \tstruct inode *target = new_dentry->d_inode;\n \tbool new_is_dir = false;\n \tunsigned max_links = new_dir->i_sb->s_max_links;\n+\tstruct name_snapshot old_name;\n \n \tif (source == target)\n \t\treturn 0;\n@@ -4413,7 +4413,7 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n \tif (error)\n \t\treturn error;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \tdget(new_dentry);\n \tif (!is_dir || (flags & RENAME_EXCHANGE))\n \t\tlock_two_nondirectories(source, target);\n@@ -4468,14 +4468,14 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n \t\tinode_unlock(target);\n \tdput(new_dentry);\n \tif (!error) {\n-\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n+\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,\n \t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n \t\tif (flags & RENAME_EXCHANGE) {\n \t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n \t\t\t\t      new_is_dir, NULL, new_dentry);\n \t\t}\n \t}\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \n \treturn error;\n }""}<_**next**_>{""sha"": ""0c4583b61717646e491a28d879b5762faeb8956e"", ""filename"": ""fs/notify/fsnotify.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 2, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/notify/fsnotify.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/notify/fsnotify.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/notify/fsnotify.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -161,16 +161,20 @@ int __fsnotify_parent(const struct path *path, struct dentry *dentry, __u32 mask\n \tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n \t\t__fsnotify_update_child_dentry_flags(p_inode);\n \telse if (p_inode->i_fsnotify_mask & mask) {\n+\t\tstruct name_snapshot name;\n+\n \t\t/* we are notifying a parent so come up with the new mask which\n \t\t * specifies these are events which came from a child. */\n \t\tmask |= FS_EVENT_ON_CHILD;\n \n+\t\ttake_dentry_name_snapshot(&name, dentry);\n \t\tif (path)\n \t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n \t\telse\n \t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n+\t\trelease_dentry_name_snapshot(&name);\n \t}\n \n \tdput(parent);""}<_**next**_>{""sha"": ""025727bf679745e507bb427f14198bf1e607ae8e"", ""filename"": ""include/linux/dcache.h"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 0, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/dcache.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/dcache.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/dcache.h?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -591,5 +591,11 @@ static inline struct inode *d_real_inode(const struct dentry *dentry)\n \treturn d_backing_inode(d_real((struct dentry *) dentry, NULL, 0));\n }\n \n+struct name_snapshot {\n+\tconst char *name;\n+\tchar inline_name[DNAME_INLINE_LEN];\n+};\n+void take_dentry_name_snapshot(struct name_snapshot *, struct dentry *);\n+void release_dentry_name_snapshot(struct name_snapshot *);\n \n #endif\t/* __LINUX_DCACHE_H */""}<_**next**_>{""sha"": ""b78aa7ac77ce1d6920aa196f4cd71d6d6a7aee7b"", ""filename"": ""include/linux/fsnotify.h"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 31, ""changes"": 31, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/fsnotify.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/fsnotify.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/fsnotify.h?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -293,35 +293,4 @@ static inline void fsnotify_change(struct dentry *dentry, unsigned int ia_valid)\n \t}\n }\n \n-#if defined(CONFIG_FSNOTIFY)\t/* notify helpers */\n-\n-/*\n- * fsnotify_oldname_init - save off the old filename before we change it\n- */\n-static inline const unsigned char *fsnotify_oldname_init(const unsigned char *name)\n-{\n-\treturn kstrdup(name, GFP_KERNEL);\n-}\n-\n-/*\n- * fsnotify_oldname_free - free the name we got from fsnotify_oldname_init\n- */\n-static inline void fsnotify_oldname_free(const unsigned char *old_name)\n-{\n-\tkfree(old_name);\n-}\n-\n-#else\t/* CONFIG_FSNOTIFY */\n-\n-static inline const char *fsnotify_oldname_init(const unsigned char *name)\n-{\n-\treturn NULL;\n-}\n-\n-static inline void fsnotify_oldname_free(const unsigned char *old_name)\n-{\n-}\n-\n-#endif\t/*  CONFIG_FSNOTIFY */\n-\n #endif\t/* _LINUX_FS_NOTIFY_H */""}","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	fsnotify_oldname_free(old_name);
 
 	return error;
 }
",C,"	struct name_snapshot old_name;
	take_dentry_name_snapshot(&old_name, old_dentry);
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
	release_dentry_name_snapshot(&old_name);
","	const unsigned char *old_name;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
	fsnotify_oldname_free(old_name);
",,"@@ -4362,11 +4362,11 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
-	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
+	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;
@@ -4413,7 +4413,7 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 	if (error)
 		return error;
 
-	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
+	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
@@ -4468,14 +4468,14 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
-		fsnotify_move(old_dir, new_dir, old_name, is_dir,
+		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
-	fsnotify_oldname_free(old_name);
+	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }",linux,49d31c2f389acfe83417083e1208422b4091cd9e,a8e2b6367794e6cee9eecba6d5ff425f338e0754,1,"int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
//flaw_line_below:
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
//fix_flaw_line_below:
//	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
//flaw_line_below:
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
//fix_flaw_line_below:
//	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
//flaw_line_below:
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
//fix_flaw_line_below:
//		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
//flaw_line_below:
	fsnotify_oldname_free(old_name);
//fix_flaw_line_below:
//	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
",181435,"int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	fsnotify_oldname_free(old_name);
 
 	return error;
 }
","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
","The `vfs_rename` function implements the renaming of files or directories within a virtual filesystem. It checks permissions for deleting the old entry and creating or replacing the new entry, handling cases for directory and file renaming, including potential exchanges. The function ensures that the source and target inodes are not the same and verifies link limits for directories. It manages locks on inodes to prevent concurrent modifications and invokes the filesystem-specific rename operation. Finally, it notifies the filesystem of the move operation and cleans up resources, returning an error code if any checks fail during the process.","The provided code defines a function that facilitates the renaming of files or directories within a virtual filesystem. It performs various checks to ensure that the operation is permissible, including verifying permissions for deletion and creation, as well as managing potential conflicts with existing entries. The function handles different scenarios, such as renaming versus exchanging entries, and ensures that appropriate locks are in place to prevent concurrent access issues. After executing the rename operation, it notifies the filesystem of the changes and cleans up any resources, returning an error code if any validation fails during the process.","The vulnerability in the original code arises from a race condition between the `fsnotify` implementation and the `vfs_rename` function, which could allow local users to exploit simultaneous execution of these functions, leading to privilege escalation or memory corruption. This occurs because the old name of the directory entry is not adequately protected during the rename operation. The fixed code addresses this issue by introducing a name snapshot mechanism, using `take_dentry_name_snapshot` to securely capture the old name before any modifications occur, and `release_dentry_name_snapshot` to ensure proper cleanup, thus preventing race conditions and enhancing overall security.","The vulnerability in the original code stems from a race condition that occurs when two functions are executed simultaneously, allowing local users to exploit this timing issue to gain elevated privileges or cause memory corruption. This happens because the old name of a directory entry can be modified while it is still in use, leading to inconsistent states. The fixed code mitigates this vulnerability by implementing a mechanism to securely capture the old name before any changes are made, ensuring that it remains consistent and protected throughout the rename operation, thus preventing potential exploitation and enhancing system stability.","The vulnerability was fixed by introducing a name snapshot mechanism that captures the old name of the directory entry before any modifications occur. In the updated code, the function `take_dentry_name_snapshot` is called to securely store the old name, ensuring it remains unchanged during the rename operation. This prevents race conditions that could arise from simultaneous execution of related functions. Additionally, the cleanup is handled by `release_dentry_name_snapshot`, which ensures that the resources associated with the old name are properly released after the operation, thereby maintaining consistency and preventing potential exploitation.","The vulnerability was fixed by implementing a mechanism to securely capture and store the old name of the directory entry before any modifications take place. This approach ensures that the old name remains consistent and protected throughout the rename operation, effectively preventing race conditions that could arise from simultaneous executions of related functions. Additionally, proper cleanup procedures were established to release any resources associated with the old name after the operation is completed. This combination of capturing the old name and ensuring its integrity significantly enhances the system's stability and security against potential exploitation."
27,181539,181539,,Remote,Not required,Complete,CVE-2017-6001,https://www.cvedetails.com/cve/CVE-2017-6001/,CWE-362,High,Complete,Complete,,2017-02-18,7.6,Race condition in kernel/events/core.c in the Linux kernel before 4.9.7 allows local users to gain privileges via a crafted application that makes concurrent perf_event_open system calls for moving a software group into a hardware context.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-6786.,2018-06-19,+Priv ,23,https://github.com/torvalds/linux/commit/321027c1fe77f892f4ea07846aeae08cefbbb290,321027c1fe77f892f4ea07846aeae08cefbbb290,"perf/core: Fix concurrent sys_perf_event_open() vs. 'move_group' race

Di Shen reported a race between two concurrent sys_perf_event_open()
calls where both try and move the same pre-existing software group
into a hardware context.

The problem is exactly that described in commit:

  f63a8daa5812 (""perf: Fix event->ctx locking"")

... where, while we wait for a ctx->mutex acquisition, the event->ctx
relation can have changed under us.

That very same commit failed to recognise sys_perf_event_context() as an
external access vector to the events and thereby didn't apply the
established locking rules correctly.

So while one sys_perf_event_open() call is stuck waiting on
mutex_lock_double(), the other (which owns said locks) moves the group
about. So by the time the former sys_perf_event_open() acquires the
locks, the context we've acquired is stale (and possibly dead).

Apply the established locking rules as per perf_event_ctx_lock_nested()
to the mutex_lock_double() for the 'move_group' case. This obviously means
we need to validate state after we acquire the locks.

Reported-by: Di Shen (Keen Lab)
Tested-by: John Dias <joaodias@google.com>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
Cc: Jiri Olsa <jolsa@redhat.com>
Cc: Kees Cook <keescook@chromium.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Min Chong <mchong@google.com>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Stephane Eranian <eranian@google.com>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Vince Weaver <vincent.weaver@maine.edu>
Fixes: f63a8daa5812 (""perf: Fix event->ctx locking"")
Link: http://lkml.kernel.org/r/20170106131444.GZ3174@twins.programming.kicks-ass.net
Signed-off-by: Ingo Molnar <mingo@kernel.org>",4,kernel/events/core.c,"{""sha"": ""cbc5937265da842bc714cbccf8091484caa5f536"", ""filename"": ""kernel/events/core.c"", ""status"": ""modified"", ""additions"": 54, ""deletions"": 4, ""changes"": 58, ""blob_url"": ""https://github.com/torvalds/linux/blob/321027c1fe77f892f4ea07846aeae08cefbbb290/kernel/events/core.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/321027c1fe77f892f4ea07846aeae08cefbbb290/kernel/events/core.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/kernel/events/core.c?ref=321027c1fe77f892f4ea07846aeae08cefbbb290"", ""patch"": ""@@ -9529,6 +9529,37 @@ static int perf_event_set_clock(struct perf_event *event, clockid_t clk_id)\n \treturn 0;\n }\n \n+/*\n+ * Variation on perf_event_ctx_lock_nested(), except we take two context\n+ * mutexes.\n+ */\n+static struct perf_event_context *\n+__perf_event_ctx_lock_double(struct perf_event *group_leader,\n+\t\t\t     struct perf_event_context *ctx)\n+{\n+\tstruct perf_event_context *gctx;\n+\n+again:\n+\trcu_read_lock();\n+\tgctx = READ_ONCE(group_leader->ctx);\n+\tif (!atomic_inc_not_zero(&gctx->refcount)) {\n+\t\trcu_read_unlock();\n+\t\tgoto again;\n+\t}\n+\trcu_read_unlock();\n+\n+\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\n+\tif (group_leader->ctx != gctx) {\n+\t\tmutex_unlock(&ctx->mutex);\n+\t\tmutex_unlock(&gctx->mutex);\n+\t\tput_ctx(gctx);\n+\t\tgoto again;\n+\t}\n+\n+\treturn gctx;\n+}\n+\n /**\n  * sys_perf_event_open - open a performance event, associate it to a task/cpu\n  *\n@@ -9772,12 +9803,31 @@ SYSCALL_DEFINE5(perf_event_open,\n \t}\n \n \tif (move_group) {\n-\t\tgctx = group_leader->ctx;\n-\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);\n+\n \t\tif (gctx->task == TASK_TOMBSTONE) {\n \t\t\terr = -ESRCH;\n \t\t\tgoto err_locked;\n \t\t}\n+\n+\t\t/*\n+\t\t * Check if we raced against another sys_perf_event_open() call\n+\t\t * moving the software group underneath us.\n+\t\t */\n+\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n+\t\t\t/*\n+\t\t\t * If someone moved the group out from under us, check\n+\t\t\t * if this new event wound up on the same ctx, if so\n+\t\t\t * its the regular !move_group case, otherwise fail.\n+\t\t\t */\n+\t\t\tif (gctx != ctx) {\n+\t\t\t\terr = -EINVAL;\n+\t\t\t\tgoto err_locked;\n+\t\t\t} else {\n+\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n+\t\t\t\tmove_group = 0;\n+\t\t\t}\n+\t\t}\n \t} else {\n \t\tmutex_lock(&ctx->mutex);\n \t}\n@@ -9879,7 +9929,7 @@ SYSCALL_DEFINE5(perf_event_open,\n \tperf_unpin_context(ctx);\n \n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n \n \tif (task) {\n@@ -9905,7 +9955,7 @@ SYSCALL_DEFINE5(perf_event_open,\n \n err_locked:\n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n /* err_file: */\n \tfput(event_file);""}","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = __perf_event_ctx_lock_double(group_leader, ctx);

 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}

		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
",C,"		gctx = __perf_event_ctx_lock_double(group_leader, ctx);


		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
		perf_event_ctx_unlock(group_leader, gctx);
		perf_event_ctx_unlock(group_leader, gctx);
","		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
		mutex_unlock(&gctx->mutex);
		mutex_unlock(&gctx->mutex);
",,"@@ -9529,6 +9529,37 @@ static int perf_event_set_clock(struct perf_event *event, clockid_t clk_id)
 	return 0;
 }
 
+/*
+ * Variation on perf_event_ctx_lock_nested(), except we take two context
+ * mutexes.
+ */
+static struct perf_event_context *
+__perf_event_ctx_lock_double(struct perf_event *group_leader,
+			     struct perf_event_context *ctx)
+{
+	struct perf_event_context *gctx;
+
+again:
+	rcu_read_lock();
+	gctx = READ_ONCE(group_leader->ctx);
+	if (!atomic_inc_not_zero(&gctx->refcount)) {
+		rcu_read_unlock();
+		goto again;
+	}
+	rcu_read_unlock();
+
+	mutex_lock_double(&gctx->mutex, &ctx->mutex);
+
+	if (group_leader->ctx != gctx) {
+		mutex_unlock(&ctx->mutex);
+		mutex_unlock(&gctx->mutex);
+		put_ctx(gctx);
+		goto again;
+	}
+
+	return gctx;
+}
+
 /**
  * sys_perf_event_open - open a performance event, associate it to a task/cpu
  *
@@ -9772,12 +9803,31 @@ SYSCALL_DEFINE5(perf_event_open,
 	}
 
 	if (move_group) {
-		gctx = group_leader->ctx;
-		mutex_lock_double(&gctx->mutex, &ctx->mutex);
+		gctx = __perf_event_ctx_lock_double(group_leader, ctx);
+
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
+
+		/*
+		 * Check if we raced against another sys_perf_event_open() call
+		 * moving the software group underneath us.
+		 */
+		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
+			/*
+			 * If someone moved the group out from under us, check
+			 * if this new event wound up on the same ctx, if so
+			 * its the regular !move_group case, otherwise fail.
+			 */
+			if (gctx != ctx) {
+				err = -EINVAL;
+				goto err_locked;
+			} else {
+				perf_event_ctx_unlock(group_leader, gctx);
+				move_group = 0;
+			}
+		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}
@@ -9879,7 +9929,7 @@ SYSCALL_DEFINE5(perf_event_open,
 	perf_unpin_context(ctx);
 
 	if (move_group)
-		mutex_unlock(&gctx->mutex);
+		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
@@ -9905,7 +9955,7 @@ SYSCALL_DEFINE5(perf_event_open,
 
 err_locked:
 	if (move_group)
-		mutex_unlock(&gctx->mutex);
+		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);",linux,321027c1fe77f892f4ea07846aeae08cefbbb290,63cae12bce9861cec309798d34701cf3da20bc71,1,"SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
//flaw_line_below:
		gctx = group_leader->ctx;
//flaw_line_below:
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
//fix_flaw_line_below:
//		gctx = __perf_event_ctx_lock_double(group_leader, ctx);
//fix_flaw_line_below:
//
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * Check if we raced against another sys_perf_event_open() call
//fix_flaw_line_below:
//		 * moving the software group underneath us.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
//fix_flaw_line_below:
//			/*
//fix_flaw_line_below:
//			 * If someone moved the group out from under us, check
//fix_flaw_line_below:
//			 * if this new event wound up on the same ctx, if so
//fix_flaw_line_below:
//			 * its the regular !move_group case, otherwise fail.
//fix_flaw_line_below:
//			 */
//fix_flaw_line_below:
//			if (gctx != ctx) {
//fix_flaw_line_below:
//				err = -EINVAL;
//fix_flaw_line_below:
//				goto err_locked;
//fix_flaw_line_below:
//			} else {
//fix_flaw_line_below:
//				perf_event_ctx_unlock(group_leader, gctx);
//fix_flaw_line_below:
//				move_group = 0;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
//flaw_line_below:
		mutex_unlock(&gctx->mutex);
//fix_flaw_line_below:
//		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
//flaw_line_below:
		mutex_unlock(&gctx->mutex);
//fix_flaw_line_below:
//		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
",181539,"SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = __perf_event_ctx_lock_double(group_leader, ctx);

 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}

		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","The provided code implements the `perf_event_open` system call, which is used to create and configure performance monitoring events in a Linux environment. It allows users to monitor various performance metrics of processes or CPUs by specifying attributes such as event type, sampling frequency, and target process. The function validates input parameters, manages permissions, and handles event grouping. It allocates a new performance event, associates it with a specific task or CPU, and ensures proper context management. Finally, it returns a file descriptor for the newly created event, enabling user-space applications to interact with it for performance analysis.","The code implements a system call that facilitates the creation and configuration of performance monitoring events in a Linux environment. It allows users to specify various attributes for monitoring, such as the type of event and sampling frequency, while ensuring that input parameters are validated and permissions are checked. The function manages the association of events with specific processes or CPUs, handles event grouping, and allocates resources accordingly. Ultimately, it returns a file descriptor that enables user-space applications to interact with the created performance event for analysis and monitoring purposes.","The vulnerability, identified as a race condition, arises when multiple concurrent calls to the system function attempt to move a software event group into a hardware context, potentially allowing unauthorized privilege escalation. In the vulnerable code, the lack of proper synchronization when accessing shared resources, such as the group leader's context, could lead to inconsistent states. The fixed code addresses this by introducing a locking mechanism with `__perf_event_ctx_lock_double`, ensuring that only one thread can modify the context at a time, thus preventing the race condition and ensuring that the integrity of the event group is maintained during concurrent operations.","The vulnerability is a race condition that occurs when multiple concurrent system calls attempt to move a software event group into a hardware context, potentially allowing unauthorized privilege escalation for local users. This situation arises from inadequate synchronization mechanisms, which can lead to inconsistent states when accessing shared resources. The fix addresses this issue by implementing a robust locking mechanism that ensures only one thread can modify the context at a time, thereby preventing simultaneous modifications and maintaining the integrity of the event group during concurrent operations, effectively mitigating the risk of privilege escalation.","The vulnerability was fixed by introducing a locking mechanism using `__perf_event_ctx_lock_double`, which ensures that only one thread can access and modify the context of the event group at a time. This prevents race conditions when multiple concurrent calls attempt to move a software group into a hardware context. Additionally, the code checks if the group leader's capabilities have changed during the operation, ensuring that any modifications are valid and consistent. By enforcing these synchronization measures, the fix effectively mitigates the risk of privilege escalation associated with concurrent system calls.","The vulnerability was fixed by implementing a robust locking mechanism that ensures exclusive access to shared resources during critical operations. This prevents multiple concurrent calls from modifying the event group's context simultaneously, thereby eliminating the race condition that could lead to privilege escalation. The fix also includes checks to verify the integrity of the event group before any modifications are made, ensuring that the state remains consistent. By enforcing these synchronization measures, the solution effectively mitigates the risk associated with concurrent system calls, enhancing the overall security of the system."
28,182360,182360,,Local,Not required,Complete,CVE-2018-12633,https://www.cvedetails.com/cve/CVE-2018-12633/,CWE-362,Medium,Complete,,,2018-06-21,6.3,"An issue was discovered in the Linux kernel through 4.17.2. vbg_misc_device_ioctl() in drivers/virt/vboxguest/vboxguest_linux.c reads the same user data twice with copy_from_user. The header part of the user data is double-fetched, and a malicious user thread can tamper with the critical variables (hdr.size_in and hdr.size_out) in the header between the two fetches because of a race condition, leading to severe kernel errors, such as buffer over-accesses. This bug can cause a local denial of service and information leakage.",2018-08-21,DoS +Info ,3,https://github.com/torvalds/linux/commit/bd23a7269834dc7c1f93e83535d16ebc44b75eba,bd23a7269834dc7c1f93e83535d16ebc44b75eba,"virt: vbox: Only copy_from_user the request-header once

In vbg_misc_device_ioctl(), the header of the ioctl argument is copied from
the userspace pointer 'arg' and saved to the kernel object 'hdr'. Then the
'version', 'size_in', and 'size_out' fields of 'hdr' are verified.

Before this commit, after the checks a buffer for the entire request would
be allocated and then all data including the verified header would be
copied from the userspace 'arg' pointer again.

Given that the 'arg' pointer resides in userspace, a malicious userspace
process can race to change the data pointed to by 'arg' between the two
copies. By doing so, the user can bypass the verifications on the ioctl
argument.

This commit fixes this by using the already checked copy of the header
to fill the header part of the allocated buffer and only copying the
remainder of the data from userspace.

Signed-off-by: Wenwen Wang <wang6495@umn.edu>
Reviewed-by: Hans de Goede <hdegoede@redhat.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",1,drivers/virt/vboxguest/vboxguest_linux.c,"{""sha"": ""6e2a9619192d2317f8f449fbb5f9c24d0699e3f8"", ""filename"": ""drivers/virt/vboxguest/vboxguest_linux.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 1, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/bd23a7269834dc7c1f93e83535d16ebc44b75eba/drivers/virt/vboxguest/vboxguest_linux.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/bd23a7269834dc7c1f93e83535d16ebc44b75eba/drivers/virt/vboxguest/vboxguest_linux.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/virt/vboxguest/vboxguest_linux.c?ref=bd23a7269834dc7c1f93e83535d16ebc44b75eba"", ""patch"": ""@@ -121,7 +121,9 @@ static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,\n \tif (!buf)\n \t\treturn -ENOMEM;\n \n-\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {\n+\t*((struct vbg_ioctl_hdr *)buf) = hdr;\n+\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),\n+\t\t\t   hdr.size_in - sizeof(hdr))) {\n \t\tret = -EFAULT;\n \t\tgoto out;\n \t}""}","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
",C,"	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
","	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
",,"@@ -121,7 +121,9 @@ static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
 	if (!buf)
 		return -ENOMEM;
 
-	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
+	*((struct vbg_ioctl_hdr *)buf) = hdr;
+	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
+			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}",linux,bd23a7269834dc7c1f93e83535d16ebc44b75eba,bdeeed098811b36d1f988521600a89a400830a4b,1,"static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
//flaw_line_below:
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
//fix_flaw_line_below:
//	*((struct vbg_ioctl_hdr *)buf) = hdr;
//fix_flaw_line_below:
//	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
//fix_flaw_line_below:
//			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
",182360,"static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","The `vbg_misc_device_ioctl` function handles IOCTL (Input/Output Control) requests for a virtual device in a kernel module. It begins by copying a header structure from user space to kernel space and validating its version and size. The function checks if the request is for a specific virtual machine device and allocates an appropriate buffer. It then copies input data from user space, processes the request through `vbg_core_ioctl`, and prepares the output data for user space. Finally, it handles memory cleanup and returns the result of the operation, ensuring proper error handling throughout the process.","The provided code implements a function that processes IOCTL requests for a virtual device within a kernel module. It begins by retrieving and validating a header from user space, ensuring it meets specific version and size requirements. The function allocates a buffer based on the request type and copies input data from user space. It then invokes a core processing function to handle the request and prepares the output data for user space. Finally, the function manages memory cleanup and returns the appropriate status, including error handling for various conditions encountered during execution.","The vulnerability in the original code arises from the use of `copy_from_user` to read user data twice, allowing a malicious user to modify critical header fields between the two reads. This race condition can lead to improper handling of buffer sizes, resulting in potential buffer overflows or underflows, which can cause kernel crashes or information leakage. The fixed code addresses this issue by reading the header once and storing it in a local variable before using it, ensuring that the size fields are consistent throughout the function. This change effectively mitigates the risk of exploitation by preventing any modifications to the header after its initial read.","The vulnerability in the original code stems from reading user data multiple times, which creates a race condition that allows a malicious user to alter critical header fields between the reads. This manipulation can lead to improper buffer size handling, resulting in potential buffer overflows or underflows, causing severe kernel errors and exposing sensitive information. The fixed code resolves this issue by ensuring that the header is read only once and stored in a local variable, thereby maintaining consistency in the size fields throughout the function and effectively preventing any exploitation of the race condition.","The vulnerability was fixed by modifying the code to read the header only once using `copy_from_user`, storing it directly into a local variable. This change ensures that the critical fields, such as `hdr.size_in` and `hdr.size_out`, are not subject to modification between multiple reads. The fixed code also assigns the header to the allocated buffer immediately after the first read, preventing any potential tampering before the data is processed. By eliminating the double-fetching of user data, the revised implementation effectively mitigates the race condition and enhances the overall security of the function.","The vulnerability was fixed by ensuring that the header data is read from user space only once, thereby eliminating the risk of race conditions that could allow malicious modifications between reads. The code now immediately stores the header information in a local variable, which is then used consistently throughout the function. This approach prevents any changes to critical size fields after the initial read, ensuring that the buffer sizes are accurately maintained. By addressing the double-fetching issue, the revised implementation enhances security and prevents potential buffer overflows or underflows that could lead to kernel errors."
29,182375,182375,,Remote,Not required,Complete,CVE-2018-12232,https://www.cvedetails.com/cve/CVE-2018-12232/,CWE-362,Medium,,,,2018-06-12,7.1,"In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",2018-10-31,,1,https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14,6d8c50dcb029872b298eea68cc6209c866fd3e14,"socket: close race condition between sock_close() and sockfs_setattr()

fchownat() doesn't even hold refcnt of fd until it figures out
fd is really needed (otherwise is ignored) and releases it after
it resolves the path. This means sock_close() could race with
sockfs_setattr(), which leads to a NULL pointer dereference
since typically we set sock->sk to NULL in ->release().

As pointed out by Al, this is unique to sockfs. So we can fix this
in socket layer by acquiring inode_lock in sock_close() and
checking against NULL in sockfs_setattr().

sock_release() is called in many places, only the sock_close()
path matters here. And fortunately, this should not affect normal
sock_close() as it is only called when the last fd refcnt is gone.
It only affects sock_close() with a parallel sockfs_setattr() in
progress, which is not common.

Fixes: 86741ec25462 (""net: core: Add a UID field to struct sock."")
Reported-by: shankarapailoor <shankarapailoor@gmail.com>
Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
Cc: Lorenzo Colitti <lorenzo@google.com>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/socket.c,"{""sha"": ""8a109012608a6132a65293c86cd175426b851cbe"", ""filename"": ""net/socket.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 3, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/socket.c?ref=6d8c50dcb029872b298eea68cc6209c866fd3e14"", ""patch"": ""@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n \tif (!err && (iattr->ia_valid & ATTR_UID)) {\n \t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n \n-\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\tif (sock->sk)\n+\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\telse\n+\t\t\terr = -ENOENT;\n \t}\n \n \treturn err;\n@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);\n  *\tan inode not a file.\n  */\n \n-void sock_release(struct socket *sock)\n+static void __sock_release(struct socket *sock, struct inode *inode)\n {\n \tif (sock->ops) {\n \t\tstruct module *owner = sock->ops->owner;\n \n+\t\tif (inode)\n+\t\t\tinode_lock(inode);\n \t\tsock->ops->release(sock);\n+\t\tif (inode)\n+\t\t\tinode_unlock(inode);\n \t\tsock->ops = NULL;\n \t\tmodule_put(owner);\n \t}\n@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)\n \t}\n \tsock->file = NULL;\n }\n+\n+void sock_release(struct socket *sock)\n+{\n+\t__sock_release(sock, NULL);\n+}\n EXPORT_SYMBOL(sock_release);\n \n void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)\n@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)\n \n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }\n ""}"," static int sock_close(struct inode *inode, struct file *filp)
 {
	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
"," static int sock_close(struct inode *inode, struct file *filp)
 {
	sock_release(SOCKET_I(inode));
 	return 0;
 }
",C,"	__sock_release(SOCKET_I(inode), inode);
","	sock_release(SOCKET_I(inode));
",,"@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
-		sock->sk->sk_uid = iattr->ia_uid;
+		if (sock->sk)
+			sock->sk->sk_uid = iattr->ia_uid;
+		else
+			err = -ENOENT;
 	}
 
 	return err;
@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);
  *	an inode not a file.
  */
 
-void sock_release(struct socket *sock)
+static void __sock_release(struct socket *sock, struct inode *inode)
 {
 	if (sock->ops) {
 		struct module *owner = sock->ops->owner;
 
+		if (inode)
+			inode_lock(inode);
 		sock->ops->release(sock);
+		if (inode)
+			inode_unlock(inode);
 		sock->ops = NULL;
 		module_put(owner);
 	}
@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)
 	}
 	sock->file = NULL;
 }
+
+void sock_release(struct socket *sock)
+{
+	__sock_release(sock, NULL);
+}
 EXPORT_SYMBOL(sock_release);
 
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)
@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)
 
 static int sock_close(struct inode *inode, struct file *filp)
 {
-	sock_release(SOCKET_I(inode));
+	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
 ",linux,6d8c50dcb029872b298eea68cc6209c866fd3e14,873aca2ee86e533665a73292c4e308ded1e9bafe,1," static int sock_close(struct inode *inode, struct file *filp)
 {
//flaw_line_below:
	sock_release(SOCKET_I(inode));
//fix_flaw_line_below:
//	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
",182375," static int sock_close(struct inode *inode, struct file *filp)
 {
	sock_release(SOCKET_I(inode));
 	return 0;
 }
"," static int sock_close(struct inode *inode, struct file *filp)
 {
	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
","The provided code defines a function `sock_close`, which is responsible for closing a socket in a Linux kernel module. It takes two parameters: a pointer to an `inode` structure and a pointer to a `file` structure. The function retrieves the socket associated with the given inode using the `SOCKET_I` macro and then calls `sock_release` to properly release the socket resources. This ensures that any allocated memory and associated data structures are cleaned up, preventing memory leaks. Finally, the function returns 0, indicating successful completion of the socket closure operation.","The code defines a function that handles the closure of a socket within a kernel module. It takes two parameters related to the file and its associated inode. The function retrieves the socket linked to the inode and invokes a method to release the resources allocated to that socket. This process ensures that all memory and data structures associated with the socket are properly cleaned up, thereby preventing potential memory leaks. Upon successful execution of the closure operation, the function returns a value indicating that the operation was completed without errors.","The vulnerability in the original code arises from a race condition between the functions handling socket closure and attribute setting, where the socket could be set to NULL while still in use. This occurs because the original `sock_release` function does not account for concurrent operations, allowing a situation where the reference count is not incremented during operations like `fchownat`. The fixed code addresses this issue by using `__sock_release`, which takes the inode as an additional parameter, ensuring that the reference count is properly managed and preventing the socket from being prematurely released, thus eliminating the risk of a NULL pointer dereference.","The vulnerability stems from a race condition that occurs when two operations target the same socket file descriptor simultaneously. Specifically, one operation does not properly manage the reference count of the socket, allowing another operation to potentially set the socket to NULL while it is still in use. This can lead to a situation where a subsequent attempt to access the socket results in a NULL pointer dereference, causing a system crash. The fix addresses this issue by ensuring that the reference count is correctly incremented during socket operations, thereby preventing premature release and ensuring safe access to the socket throughout its lifecycle.","The vulnerability was fixed by replacing the original `sock_release` function with `__sock_release`, which takes the inode as an additional parameter. This change ensures that the reference count for the socket is properly managed during concurrent operations. By passing the inode, the fixed function can maintain the integrity of the socket's state, preventing it from being set to NULL while still in use. This adjustment effectively mitigates the race condition that could lead to a NULL pointer dereference, thereby enhancing the stability and safety of socket operations within the kernel.","The vulnerability was addressed by implementing a mechanism that ensures proper management of the reference count associated with the socket during concurrent operations. By modifying the function responsible for releasing the socket, the fix allows for the safe handling of the socket's state, preventing it from being prematurely set to NULL while still in use. This adjustment effectively eliminates the race condition that could lead to a NULL pointer dereference, thereby enhancing the overall stability and reliability of socket operations within the system. The changes ensure that resources are correctly managed throughout the lifecycle of the socket."
30,182377,182377,,Remote,Not required,Complete,CVE-2018-12232,https://www.cvedetails.com/cve/CVE-2018-12232/,CWE-362,Medium,,,,2018-06-12,7.1,"In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",2018-10-31,,4,https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14,6d8c50dcb029872b298eea68cc6209c866fd3e14,"socket: close race condition between sock_close() and sockfs_setattr()

fchownat() doesn't even hold refcnt of fd until it figures out
fd is really needed (otherwise is ignored) and releases it after
it resolves the path. This means sock_close() could race with
sockfs_setattr(), which leads to a NULL pointer dereference
since typically we set sock->sk to NULL in ->release().

As pointed out by Al, this is unique to sockfs. So we can fix this
in socket layer by acquiring inode_lock in sock_close() and
checking against NULL in sockfs_setattr().

sock_release() is called in many places, only the sock_close()
path matters here. And fortunately, this should not affect normal
sock_close() as it is only called when the last fd refcnt is gone.
It only affects sock_close() with a parallel sockfs_setattr() in
progress, which is not common.

Fixes: 86741ec25462 (""net: core: Add a UID field to struct sock."")
Reported-by: shankarapailoor <shankarapailoor@gmail.com>
Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
Cc: Lorenzo Colitti <lorenzo@google.com>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/socket.c,"{""sha"": ""8a109012608a6132a65293c86cd175426b851cbe"", ""filename"": ""net/socket.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 3, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/socket.c?ref=6d8c50dcb029872b298eea68cc6209c866fd3e14"", ""patch"": ""@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n \tif (!err && (iattr->ia_valid & ATTR_UID)) {\n \t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n \n-\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\tif (sock->sk)\n+\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\telse\n+\t\t\terr = -ENOENT;\n \t}\n \n \treturn err;\n@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);\n  *\tan inode not a file.\n  */\n \n-void sock_release(struct socket *sock)\n+static void __sock_release(struct socket *sock, struct inode *inode)\n {\n \tif (sock->ops) {\n \t\tstruct module *owner = sock->ops->owner;\n \n+\t\tif (inode)\n+\t\t\tinode_lock(inode);\n \t\tsock->ops->release(sock);\n+\t\tif (inode)\n+\t\t\tinode_unlock(inode);\n \t\tsock->ops = NULL;\n \t\tmodule_put(owner);\n \t}\n@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)\n \t}\n \tsock->file = NULL;\n }\n+\n+void sock_release(struct socket *sock)\n+{\n+\t__sock_release(sock, NULL);\n+}\n EXPORT_SYMBOL(sock_release);\n \n void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)\n@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)\n \n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }\n ""}","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
 	}
 
 	return err;
}
","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		sock->sk->sk_uid = iattr->ia_uid;
 	}
 
 	return err;
}
",C,"		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
","		sock->sk->sk_uid = iattr->ia_uid;
",,"@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
-		sock->sk->sk_uid = iattr->ia_uid;
+		if (sock->sk)
+			sock->sk->sk_uid = iattr->ia_uid;
+		else
+			err = -ENOENT;
 	}
 
 	return err;
@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);
  *	an inode not a file.
  */
 
-void sock_release(struct socket *sock)
+static void __sock_release(struct socket *sock, struct inode *inode)
 {
 	if (sock->ops) {
 		struct module *owner = sock->ops->owner;
 
+		if (inode)
+			inode_lock(inode);
 		sock->ops->release(sock);
+		if (inode)
+			inode_unlock(inode);
 		sock->ops = NULL;
 		module_put(owner);
 	}
@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)
 	}
 	sock->file = NULL;
 }
+
+void sock_release(struct socket *sock)
+{
+	__sock_release(sock, NULL);
+}
 EXPORT_SYMBOL(sock_release);
 
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)
@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)
 
 static int sock_close(struct inode *inode, struct file *filp)
 {
-	sock_release(SOCKET_I(inode));
+	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
 ",linux,6d8c50dcb029872b298eea68cc6209c866fd3e14,873aca2ee86e533665a73292c4e308ded1e9bafe,1,"static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
//flaw_line_below:
		sock->sk->sk_uid = iattr->ia_uid;
//fix_flaw_line_below:
//		if (sock->sk)
//fix_flaw_line_below:
//			sock->sk->sk_uid = iattr->ia_uid;
//fix_flaw_line_below:
//		else
//fix_flaw_line_below:
//			err = -ENOENT;
 	}
 
 	return err;
}
",182377,"static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		sock->sk->sk_uid = iattr->ia_uid;
 	}
 
 	return err;
}
","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
 	}
 
 	return err;
}
","The `sockfs_setattr` function is designed to update the attributes of a socket file in a Linux filesystem. It first calls `simple_setattr` to handle general attribute changes based on the provided `iattr` structure. If the operation is successful and the user ID attribute (`ATTR_UID`) is specified for modification, it retrieves the socket associated with the given `dentry` and updates its user ID (`sk_uid`) to the new value from `iattr`. The function ultimately returns any error code from the attribute setting process, ensuring that both general and specific updates are managed appropriately.","The provided code defines a function that updates the attributes of a socket file within a filesystem. It begins by attempting to set general attributes based on the input parameters. If this initial operation is successful and a specific user ID attribute is included for modification, the function retrieves the associated socket and updates its user ID to the new value. The function concludes by returning any error code encountered during the attribute update process, ensuring that both general and specific attribute changes are handled correctly and efficiently.","The vulnerability in the original code arises from a race condition between the functions that modify socket attributes and those that close the socket, specifically when both operations target the same socket file descriptor. In the vulnerable code, if the socket is closed while the user ID is being updated, it could lead to a NULL pointer dereference when accessing the socket's structure. The fixed code addresses this issue by adding a check to ensure that the socket pointer is valid before attempting to update the user ID. If the socket is NULL, it sets an error code, preventing potential crashes and enhancing stability.","The vulnerability stems from a race condition that occurs when two operations, one modifying socket attributes and the other closing the socket, target the same socket file descriptor. This situation arises because the attribute modification does not ensure that the socket remains valid throughout its execution, allowing the closing operation to nullify the socket reference. Consequently, if the attribute modification attempts to access the socket after it has been closed, it can lead to a NULL pointer dereference, resulting in a system crash. The fix introduces a validation check to ensure the socket is still valid before proceeding with the attribute update, thereby preventing potential crashes.","The vulnerability was fixed by adding a validation check for the socket pointer before attempting to update the user ID. In the modified code, after retrieving the socket associated with the given directory entry, a condition checks if the socket pointer is not NULL. If the socket is valid, the user ID is updated; otherwise, an error code is set to indicate that the socket does not exist. This change prevents the code from dereferencing a NULL pointer, thereby eliminating the risk of a system crash due to a race condition between the attribute modification and socket closure operations.","The vulnerability was addressed by implementing a validation check to ensure that the socket is still valid before proceeding with the user ID update. This fix involves verifying that the socket reference is not NULL after it is retrieved. If the socket is valid, the user ID can be safely updated; if it is NULL, an error code is set to indicate the absence of the socket. This change effectively prevents the code from attempting to access a non-existent socket, thereby mitigating the risk of a NULL pointer dereference and enhancing the overall stability of the system."
31,182442,182442,,Local,Not required,Complete,CVE-2018-8897,https://www.cvedetails.com/cve/CVE-2018-8897/,CWE-362,Low,Complete,Complete,,2018-05-08,7.2,"A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",2019-10-02,,8,https://github.com/torvalds/linux/commit/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,"x86/entry/64: Don't use IST entry for #BP stack

There's nothing IST-worthy about #BP/int3.  We don't allow kprobes
in the small handful of places in the kernel that run at CPL0 with
an invalid stack, and 32-bit kernels have used normal interrupt
gates for #BP forever.

Furthermore, we don't allow kprobes in places that have usergs while
in kernel mode, so ""paranoid"" is also unnecessary.

Signed-off-by: Andy Lutomirski <luto@kernel.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Cc: stable@vger.kernel.org",6,arch/x86/kernel/traps.c,"{""sha"": ""9b114675fbc05e1f589e6838f4603de871c0565d"", ""filename"": ""arch/x86/entry/entry_64.S"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 1, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/entry/entry_64.S"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/entry/entry_64.S"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/entry/entry_64.S?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -1138,7 +1138,7 @@ apicinterrupt3 HYPERV_REENLIGHTENMENT_VECTOR \\\n #endif /* CONFIG_HYPERV */\n \n idtentry debug\t\t\tdo_debug\t\thas_error_code=0\tparanoid=1 shift_ist=DEBUG_STACK\n-idtentry int3\t\t\tdo_int3\t\t\thas_error_code=0\tparanoid=1 shift_ist=DEBUG_STACK\n+idtentry int3\t\t\tdo_int3\t\t\thas_error_code=0\n idtentry stack_segment\t\tdo_stack_segment\thas_error_code=1\n \n #ifdef CONFIG_XEN""}<_**next**_>{""sha"": ""50bee5fe114013622ee858e4a3b7b16d1ffa8a05"", ""filename"": ""arch/x86/kernel/idt.c"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 2, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/idt.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/idt.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kernel/idt.c?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -160,7 +160,6 @@ static const __initconst struct idt_data early_pf_idts[] = {\n  */\n static const __initconst struct idt_data dbg_idts[] = {\n \tINTG(X86_TRAP_DB,\tdebug),\n-\tINTG(X86_TRAP_BP,\tint3),\n };\n #endif\n \n@@ -183,7 +182,6 @@ gate_desc debug_idt_table[IDT_ENTRIES] __page_aligned_bss;\n static const __initconst struct idt_data ist_idts[] = {\n \tISTG(X86_TRAP_DB,\tdebug,\t\tDEBUG_STACK),\n \tISTG(X86_TRAP_NMI,\tnmi,\t\tNMI_STACK),\n-\tSISTG(X86_TRAP_BP,\tint3,\t\tDEBUG_STACK),\n \tISTG(X86_TRAP_DF,\tdouble_fault,\tDOUBLEFAULT_STACK),\n #ifdef CONFIG_X86_MCE\n \tISTG(X86_TRAP_MC,\t&machine_check,\tMCE_STACK),""}<_**next**_>{""sha"": ""03f3d7695daccae1af7265f2aab221da81573ce5"", ""filename"": ""arch/x86/kernel/traps.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 7, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/traps.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/traps.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kernel/traps.c?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -577,7 +577,6 @@ do_general_protection(struct pt_regs *regs, long error_code)\n }\n NOKPROBE_SYMBOL(do_general_protection);\n \n-/* May run on IST stack. */\n dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n {\n #ifdef CONFIG_DYNAMIC_FTRACE\n@@ -592,6 +591,13 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n \tif (poke_int3_handler(regs))\n \t\treturn;\n \n+\t/*\n+\t * Use ist_enter despite the fact that we don't use an IST stack.\n+\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel\n+\t * mode or even during context tracking state changes.\n+\t *\n+\t * This means that we can't schedule.  That's okay.\n+\t */\n \tist_enter(regs);\n \tRCU_LOCKDEP_WARN(!rcu_is_watching(), \""entry code didn't wake RCU\"");\n #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n@@ -609,15 +615,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n \t\t\tSIGTRAP) == NOTIFY_STOP)\n \t\tgoto exit;\n \n-\t/*\n-\t * Let others (NMI) know that the debug stack is in use\n-\t * as we may switch to the interrupt stack.\n-\t */\n-\tdebug_stack_usage_inc();\n \tcond_local_irq_enable(regs);\n \tdo_trap(X86_TRAP_BP, SIGTRAP, \""int3\"", regs, error_code, NULL);\n \tcond_local_irq_disable(regs);\n-\tdebug_stack_usage_dec();\n+\n exit:\n \tist_exit(regs);\n }""}"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);

 exit:
 	ist_exit(regs);
 }
"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
	debug_stack_usage_dec();
 exit:
 	ist_exit(regs);
 }
",C,"	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */

","	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
	debug_stack_usage_dec();
",,"@@ -577,7 +577,6 @@ do_general_protection(struct pt_regs *regs, long error_code)
 }
 NOKPROBE_SYMBOL(do_general_protection);
 
-/* May run on IST stack. */
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -592,6 +591,13 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	if (poke_int3_handler(regs))
 		return;
 
+	/*
+	 * Use ist_enter despite the fact that we don't use an IST stack.
+	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
+	 * mode or even during context tracking state changes.
+	 *
+	 * This means that we can't schedule.  That's okay.
+	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
@@ -609,15 +615,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
-	/*
-	 * Let others (NMI) know that the debug stack is in use
-	 * as we may switch to the interrupt stack.
-	 */
-	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
-	debug_stack_usage_dec();
+
 exit:
 	ist_exit(regs);
 }",linux,d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,06ace26f4e6fcf747e890a39193be811777a048a,1," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * Use ist_enter despite the fact that we don't use an IST stack.
//fix_flaw_line_below:
//	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
//fix_flaw_line_below:
//	 * mode or even during context tracking state changes.
//fix_flaw_line_below:
//	 *
//fix_flaw_line_below:
//	 * This means that we can't schedule.  That's okay.
//fix_flaw_line_below:
//	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
//flaw_line_below:
	/*
//flaw_line_below:
	 * Let others (NMI) know that the debug stack is in use
//flaw_line_below:
	 * as we may switch to the interrupt stack.
//flaw_line_below:
	 */
//flaw_line_below:
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
//flaw_line_below:
	debug_stack_usage_dec();
//fix_flaw_line_below:
//
 exit:
 	ist_exit(regs);
 }
",182442," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
	debug_stack_usage_dec();
 exit:
 	ist_exit(regs);
 }
"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);

 exit:
 	ist_exit(regs);
 }
","The provided code defines a function `do_int3` that handles the INT3 (breakpoint) exception in a kernel environment. It first checks if dynamic ftrace is active and handles it accordingly to prevent recursive crashes. If not, it proceeds to handle the INT3 exception through various mechanisms, including KGDB for debugging and Kprobes for dynamic instrumentation. The function ensures that the RCU (Read-Copy-Update) system is properly managed and increments the debug stack usage before enabling interrupts. Finally, it invokes the `do_trap` function to process the exception and cleans up by decrementing the debug stack usage before exiting.","The code defines a function that manages the handling of a specific type of exception related to breakpoints in a kernel environment. It begins by checking for certain conditions to prevent recursive issues during execution. The function then processes the exception through various debugging and instrumentation mechanisms, ensuring that system integrity is maintained. It manages system states related to concurrency and debugging, allowing for the safe handling of interrupts. Finally, it executes the necessary procedures to address the exception and performs cleanup operations before exiting, ensuring that the system remains stable throughout the process.","The vulnerability, identified as CWE-362, arises from improper handling of exceptions during context switches, particularly when certain instructions inhibit interrupts. In the vulnerable code, the function did not adequately manage the state of the system when handling breakpoint exceptions, potentially allowing for privilege escalation or crashes. The fix introduced checks to ensure that the system state is correctly managed before enabling interrupts, specifically by using `ist_enter` to safely enter the interrupt context and `cond_local_irq_enable` to control interrupt handling. This ensures that the system remains stable and secure during exception processing, preventing unexpected behavior.","The vulnerability, classified as CWE-362, stems from improper handling of exceptions during context switches, particularly when certain instructions inhibit interrupts. This mishandling can lead to unexpected behaviors, such as privilege escalation or system crashes, especially when exceptions are deferred by specific instructions. The fix implemented in the updated code ensures that the system state is properly managed before allowing interrupts to be enabled. By introducing mechanisms to safely enter the interrupt context and control the handling of exceptions, the revised code prevents the system from entering an unstable state, thereby mitigating the risk of unexpected behaviors.","The vulnerability was fixed by enhancing the management of system states during exception handling. The updated code introduces the use of `ist_enter` to safely enter the interrupt context, ensuring that the system is prepared for handling exceptions without risking instability. Additionally, the code employs `cond_local_irq_enable` to control when interrupts are enabled, preventing potential race conditions or unexpected behaviors during critical operations. By implementing these changes, the code ensures that the handling of breakpoint exceptions is conducted in a secure manner, effectively mitigating the risks associated with the previously mishandled state transitions.","The vulnerability was addressed by improving the management of system states during exception handling to ensure stability. The updated code introduces mechanisms that safely prepare the system for handling exceptions, preventing potential instability during critical operations. Additionally, it implements controls to regulate when interrupts can be enabled, thereby mitigating the risk of race conditions and unexpected behaviors. These changes ensure that the handling of exceptions occurs in a secure manner, effectively reducing the likelihood of privilege escalation or system crashes that could arise from mishandled state transitions."
32,182476,182476,,Remote,Not required,Partial,CVE-2018-7998,https://www.cvedetails.com/cve/CVE-2018-7998/,CWE-362,High,Partial,Partial,,2018-03-09,5.1,"In libvips before 8.6.3, a NULL function pointer dereference vulnerability was found in the vips_region_generate function in region.c, which allows remote attackers to cause a denial of service or possibly have unspecified other impact via a crafted image file. This occurs because of a race condition involving a failed delayed load and other worker threads.",2018-03-27,DoS ,17,https://github.com/jcupitt/libvips/commit/20d840e6da15c1574b3ed998bc92f91d1e36c2a5,20d840e6da15c1574b3ed998bc92f91d1e36c2a5,"fix a crash with delayed load

If a delayed load failed, it could leave the pipeline only half-set up.
Sebsequent threads could then segv.

Set a load-has-failed flag and test before generate.

See https://github.com/jcupitt/libvips/issues/893",5,libvips/foreign/foreign.c,"{""sha"": ""08aaab8c21980577a74498f3b19bc3a155cbec9f"", ""filename"": ""ChangeLog"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/ChangeLog"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/ChangeLog"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/ChangeLog?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -4,6 +4,7 @@\n   writing twice to memory\n - better rounding behaviour in convolution means we hit the vector path more\n   often\n+- fix a crash if a delayed load failed [gsharpsh00ter]\n \n 5/1/18 started 8.6.2\n - vips_sink_screen() keeps a ref to the input image ... stops a rare race""}<_**next**_>{""sha"": ""fb03fd746990f1e8f7ff129a4e4db1e7320d3bb2"", ""filename"": ""libvips/foreign/foreign.c"", ""status"": ""modified"", ""additions"": 19, ""deletions"": 6, ""changes"": 25, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/foreign/foreign.c"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/foreign/foreign.c"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/libvips/foreign/foreign.c?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -18,6 +18,8 @@\n  * \t- transform cmyk->rgb if there's an embedded profile\n  * 16/6/17\n  * \t- add page_height\n+ * 5/3/18\n+ * \t- block _start if one start fails, see #893\n  */\n \n /*\n@@ -796,6 +798,11 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )\n \tVipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );\n \tVipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );\n \n+\t/* If this start has failed before in another thread, we can fail now.\n+\t */\n+\tif( load->error )\n+\t\treturn( NULL );\n+\n \tif( !load->real ) {\n \t\tif( !(load->real = vips_foreign_load_temp( load )) )\n \t\t\treturn( NULL );\n@@ -819,19 +826,25 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )\n \t\tg_object_set_qdata( G_OBJECT( load->real ), \n \t\t\tvips__foreign_load_operation, load ); \n \n-\t\tif( class->load( load ) ||\n-\t\t\tvips_image_pio_input( load->real ) ) \n-\t\t\treturn( NULL );\n-\n-\t\t/* ->header() read the header into @out, load has read the\n+\t\t/* Load the image and check the result.\n+\t\t *\n+\t\t * ->header() read the header into @out, load has read the\n \t\t * image into @real. They must match exactly in size, bands,\n \t\t * format and coding for the copy to work.  \n \t\t *\n \t\t * Some versions of ImageMagick give different results between\n \t\t * Ping and Load for some formats, for example.\n+\t\t *\n+\t\t * If the load fails, we need to stop\n \t\t */\n-\t\tif( !vips_foreign_load_iscompat( load->real, out ) )\n+\t\tif( class->load( load ) ||\n+\t\t\tvips_image_pio_input( load->real ) || \n+\t\t\tvips_foreign_load_iscompat( load->real, out ) ) {\n+\t\t\tvips_operation_invalidate( VIPS_OPERATION( load ) ); \n+\t\t\tload->error = TRUE;\n+\n \t\t\treturn( NULL );\n+\t\t}\n \n \t\t/* We have to tell vips that out depends on real. We've set\n \t\t * the demand hint below, but not given an input there.""}<_**next**_>{""sha"": ""dcbf21c82c7b70bba5714a9be2b33ee59aa6d51b"", ""filename"": ""libvips/include/vips/foreign.h"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 0, ""changes"": 5, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/include/vips/foreign.h"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/include/vips/foreign.h"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/libvips/include/vips/foreign.h?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -158,6 +158,11 @@ typedef struct _VipsForeignLoad {\n \t * TRUE.\n \t */\n \tgboolean disc;\n+\n+\t/* Set if a start function fails. We want to prevent the other starts\n+\t * from also triggering the load.\n+\t */\n+\tgboolean error;\n } VipsForeignLoad;\n \n typedef struct _VipsForeignLoadClass {""}","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
		 *
		 * If the load fails, we need to stop
 		 */
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

 			return( NULL );
		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
 		 */
		if( !vips_foreign_load_iscompat( load->real, out ) )
 			return( NULL );
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
",C,"	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
		 *
		 * If the load fails, we need to stop
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

		}
","		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
		if( !vips_foreign_load_iscompat( load->real, out ) )
",,"@@ -18,6 +18,8 @@
  * 	- transform cmyk->rgb if there's an embedded profile
  * 16/6/17
  * 	- add page_height
+ * 5/3/18
+ * 	- block _start if one start fails, see #893
  */
 
 /*
@@ -796,6 +798,11 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
+	/* If this start has failed before in another thread, we can fail now.
+	 */
+	if( load->error )
+		return( NULL );
+
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );
@@ -819,19 +826,25 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
-		if( class->load( load ) ||
-			vips_image_pio_input( load->real ) ) 
-			return( NULL );
-
-		/* ->header() read the header into @out, load has read the
+		/* Load the image and check the result.
+		 *
+		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
+		 *
+		 * If the load fails, we need to stop
 		 */
-		if( !vips_foreign_load_iscompat( load->real, out ) )
+		if( class->load( load ) ||
+			vips_image_pio_input( load->real ) || 
+			vips_foreign_load_iscompat( load->real, out ) ) {
+			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
+			load->error = TRUE;
+
 			return( NULL );
+		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.",libvips,20d840e6da15c1574b3ed998bc92f91d1e36c2a5,eefb2dad980c12922ad15d987be7c4b42a44d5cf,1,"vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
//fix_flaw_line_below:
//	/* If this start has failed before in another thread, we can fail now.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	if( load->error )
//fix_flaw_line_below:
//		return( NULL );
//fix_flaw_line_below:
//
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
//flaw_line_below:
		if( class->load( load ) ||
//flaw_line_below:
			vips_image_pio_input( load->real ) ) 
//flaw_line_below:
			return( NULL );
//flaw_line_below:

//flaw_line_below:
		/* ->header() read the header into @out, load has read the
//fix_flaw_line_below:
//		/* Load the image and check the result.
//fix_flaw_line_below:
//		 *
//fix_flaw_line_below:
//		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
//fix_flaw_line_below:
//		 *
//fix_flaw_line_below:
//		 * If the load fails, we need to stop
 		 */
//flaw_line_below:
		if( !vips_foreign_load_iscompat( load->real, out ) )
//fix_flaw_line_below:
//		if( class->load( load ) ||
//fix_flaw_line_below:
//			vips_image_pio_input( load->real ) || 
//fix_flaw_line_below:
//			vips_foreign_load_iscompat( load->real, out ) ) {
//fix_flaw_line_below:
//			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
//fix_flaw_line_below:
//			load->error = TRUE;
//fix_flaw_line_below:
//
 			return( NULL );
//fix_flaw_line_below:
//		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
",182476,"vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
 		 */
		if( !vips_foreign_load_iscompat( load->real, out ) )
 			return( NULL );
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
		 *
		 * If the load fails, we need to stop
 		 */
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

 			return( NULL );
		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","The `vips_foreign_load_start` function initiates the loading of an image into a specified output (`out`) using a foreign load structure (`load`). It first checks if the actual image (`load->real`) has been loaded; if not, it attempts to load the image using `vips_foreign_load_temp`. The function sets up progress signals and associates the load object with the image for error handling. It then verifies compatibility between the loaded image and the output, ensuring they match in size, bands, format, and coding. Finally, it establishes a dependency between the output and the loaded image before returning a new region representing the loaded image.","The provided code defines a function that begins the process of loading an image into a specified output. It checks if the actual image data has been loaded; if not, it attempts to load the image temporarily. The function sets up progress tracking and associates the loading process with the image for error management. It ensures that the loaded image and the output are compatible in terms of dimensions and format. Finally, it establishes a dependency between the output and the loaded image, allowing for efficient processing, and returns a new region that represents the loaded image data.","The vulnerability in the original code stemmed from a race condition where multiple threads could attempt to load an image simultaneously, leading to a potential NULL function pointer dereference if one thread encountered an error while loading. This was addressed in the fixed code by introducing a check for an error state before proceeding with the loading process. Specifically, the addition of the condition `if( load->error )` ensures that if a previous loading attempt failed, the function will immediately return NULL, preventing further operations that could lead to dereferencing a NULL pointer and thus mitigating the risk of denial of service.","The vulnerability in the original code was due to a race condition that allowed multiple threads to attempt loading an image simultaneously, which could lead to a NULL function pointer dereference if one thread encountered an error during the loading process. This situation could be exploited by attackers to cause a denial of service. The fix implemented in the updated code introduced a check for an error state before proceeding with the loading operation. By ensuring that any previous loading failures are acknowledged and handled appropriately, the code prevents further execution that could lead to dereferencing a NULL pointer, thereby enhancing stability and security.","The vulnerability was fixed by adding a check for the error state at the beginning of the loading function. Specifically, the line `if( load->error )` was introduced to immediately return NULL if a previous loading attempt had failed. This prevents the function from proceeding with potentially unsafe operations that could lead to dereferencing a NULL pointer. Additionally, if the loading process encounters any issues, the code now sets `load->error` to TRUE and calls `vips_operation_invalidate`, ensuring that subsequent attempts to load the image are properly managed and do not lead to further errors or crashes.","The vulnerability was addressed by implementing an initial check for any previous errors before proceeding with the image loading process. This ensures that if a loading attempt had already failed, the function would immediately terminate without executing further operations that could lead to unsafe conditions. Additionally, the code was modified to set an error flag if any issues were encountered during the loading process, which helps manage subsequent attempts and prevents further execution that could result in dereferencing a NULL pointer. This approach effectively mitigates the risk of denial of service and enhances overall stability."
33,182551,182551,,Local,Not required,Partial,CVE-2017-18249,https://www.cvedetails.com/cve/CVE-2017-18249/,CWE-362,Medium,Partial,Partial,,2018-03-26,4.4,"The add_free_nid function in fs/f2fs/node.c in the Linux kernel before 4.12 does not properly track an allocated nid, which allows local users to cause a denial of service (race condition) or possibly have unspecified other impact via concurrent threads.",2019-04-02,DoS ,45,https://github.com/torvalds/linux/commit/30a61ddf8117c26ac5b295e1233eaa9629a94ca3,30a61ddf8117c26ac5b295e1233eaa9629a94ca3,"f2fs: fix race condition in between free nid allocator/initializer

In below concurrent case, allocated nid can be loaded into free nid cache
and be allocated again.

Thread A				Thread B
- f2fs_create
 - f2fs_new_inode
  - alloc_nid
   - __insert_nid_to_list(ALLOC_NID_LIST)
					- f2fs_balance_fs_bg
					 - build_free_nids
					  - __build_free_nids
					   - scan_nat_page
					    - add_free_nid
					     - __lookup_nat_cache
 - f2fs_add_link
  - init_inode_metadata
   - new_inode_page
    - new_node_page
     - set_node_addr
 - alloc_nid_done
  - __remove_nid_from_list(ALLOC_NID_LIST)
					     - __insert_nid_to_list(FREE_NID_LIST)

This patch makes nat cache lookup and free nid list operation being atomical
to avoid this race condition.

Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
Signed-off-by: Chao Yu <yuchao0@huawei.com>
Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>",17,fs/f2fs/node.c,"{""sha"": ""29dc996b573c7ecfbbb748a79ce8378784f635dc"", ""filename"": ""fs/f2fs/node.c"", ""status"": ""modified"", ""additions"": 45, ""deletions"": 18, ""changes"": 63, ""blob_url"": ""https://github.com/torvalds/linux/blob/30a61ddf8117c26ac5b295e1233eaa9629a94ca3/fs/f2fs/node.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/30a61ddf8117c26ac5b295e1233eaa9629a94ca3/fs/f2fs/node.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/f2fs/node.c?ref=30a61ddf8117c26ac5b295e1233eaa9629a94ca3"", ""patch"": ""@@ -1761,40 +1761,67 @@ static void __remove_nid_from_list(struct f2fs_sb_info *sbi,\n static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n {\n \tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n-\tstruct free_nid *i;\n+\tstruct free_nid *i, *e;\n \tstruct nat_entry *ne;\n-\tint err;\n+\tint err = -EINVAL;\n+\tbool ret = false;\n \n \t/* 0 nid should not be used */\n \tif (unlikely(nid == 0))\n \t\treturn false;\n \n-\tif (build) {\n-\t\t/* do not add allocated nids */\n-\t\tne = __lookup_nat_cache(nm_i, nid);\n-\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n-\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n-\t\t\treturn false;\n-\t}\n-\n \ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n \ti->nid = nid;\n \ti->state = NID_NEW;\n \n-\tif (radix_tree_preload(GFP_NOFS)) {\n-\t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n+\tif (radix_tree_preload(GFP_NOFS))\n+\t\tgoto err;\n \n \tspin_lock(&nm_i->nid_list_lock);\n+\n+\tif (build) {\n+\t\t/*\n+\t\t *   Thread A             Thread B\n+\t\t *  - f2fs_create\n+\t\t *   - f2fs_new_inode\n+\t\t *    - alloc_nid\n+\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)\n+\t\t *                     - f2fs_balance_fs_bg\n+\t\t *                      - build_free_nids\n+\t\t *                       - __build_free_nids\n+\t\t *                        - scan_nat_page\n+\t\t *                         - add_free_nid\n+\t\t *                          - __lookup_nat_cache\n+\t\t *  - f2fs_add_link\n+\t\t *   - init_inode_metadata\n+\t\t *    - new_inode_page\n+\t\t *     - new_node_page\n+\t\t *      - set_node_addr\n+\t\t *  - alloc_nid_done\n+\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)\n+\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)\n+\t\t */\n+\t\tne = __lookup_nat_cache(nm_i, nid);\n+\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n+\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n+\t\t\tgoto err_out;\n+\n+\t\te = __lookup_free_nid_list(nm_i, nid);\n+\t\tif (e) {\n+\t\t\tif (e->state == NID_NEW)\n+\t\t\t\tret = true;\n+\t\t\tgoto err_out;\n+\t\t}\n+\t}\n+\tret = true;\n \terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\n+err_out:\n \tspin_unlock(&nm_i->nid_list_lock);\n \tradix_tree_preload_end();\n-\tif (err) {\n+err:\n+\tif (err)\n \t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n-\treturn true;\n+\treturn ret;\n }\n \n static void remove_free_nid(struct f2fs_sb_info *sbi, nid_t nid)""}"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i, *e;
 	struct nat_entry *ne;
	int err = -EINVAL;
	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS))
		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
err:
	if (err)
 		kmem_cache_free(free_nid_slab, i);
	return ret;
 }
"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i;
 	struct nat_entry *ne;
	int err;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
 
 	spin_lock(&nm_i->nid_list_lock);
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
	if (err) {
 		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	return true;
 }
",C,"	struct free_nid *i, *e;
	int err = -EINVAL;
	bool ret = false;
	if (radix_tree_preload(GFP_NOFS))
		goto err;

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
err_out:
err:
	if (err)
	return ret;
","	struct free_nid *i;
	int err;
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	if (err) {
		return true;
	}
	return true;
",,"@@ -1761,40 +1761,67 @@ static void __remove_nid_from_list(struct f2fs_sb_info *sbi,
 static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
-	struct free_nid *i;
+	struct free_nid *i, *e;
 	struct nat_entry *ne;
-	int err;
+	int err = -EINVAL;
+	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
-	if (build) {
-		/* do not add allocated nids */
-		ne = __lookup_nat_cache(nm_i, nid);
-		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
-				nat_get_blkaddr(ne) != NULL_ADDR))
-			return false;
-	}
-
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
-	if (radix_tree_preload(GFP_NOFS)) {
-		kmem_cache_free(free_nid_slab, i);
-		return true;
-	}
+	if (radix_tree_preload(GFP_NOFS))
+		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);
+
+	if (build) {
+		/*
+		 *   Thread A             Thread B
+		 *  - f2fs_create
+		 *   - f2fs_new_inode
+		 *    - alloc_nid
+		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
+		 *                     - f2fs_balance_fs_bg
+		 *                      - build_free_nids
+		 *                       - __build_free_nids
+		 *                        - scan_nat_page
+		 *                         - add_free_nid
+		 *                          - __lookup_nat_cache
+		 *  - f2fs_add_link
+		 *   - init_inode_metadata
+		 *    - new_inode_page
+		 *     - new_node_page
+		 *      - set_node_addr
+		 *  - alloc_nid_done
+		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
+		 *                         - __insert_nid_to_list(FREE_NID_LIST)
+		 */
+		ne = __lookup_nat_cache(nm_i, nid);
+		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
+				nat_get_blkaddr(ne) != NULL_ADDR))
+			goto err_out;
+
+		e = __lookup_free_nid_list(nm_i, nid);
+		if (e) {
+			if (e->state == NID_NEW)
+				ret = true;
+			goto err_out;
+		}
+	}
+	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
+err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
-	if (err) {
+err:
+	if (err)
 		kmem_cache_free(free_nid_slab, i);
-		return true;
-	}
-	return true;
+	return ret;
 }
 
 static void remove_free_nid(struct f2fs_sb_info *sbi, nid_t nid)",linux,30a61ddf8117c26ac5b295e1233eaa9629a94ca3,5f4c3dec225e03273e1f74816e676cccc1a54329,1," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
//flaw_line_below:
	struct free_nid *i;
//fix_flaw_line_below:
//	struct free_nid *i, *e;
 	struct nat_entry *ne;
//flaw_line_below:
	int err;
//fix_flaw_line_below:
//	int err = -EINVAL;
//fix_flaw_line_below:
//	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
//flaw_line_below:
	if (build) {
//flaw_line_below:
		/* do not add allocated nids */
//flaw_line_below:
		ne = __lookup_nat_cache(nm_i, nid);
//flaw_line_below:
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
//flaw_line_below:
				nat_get_blkaddr(ne) != NULL_ADDR))
//flaw_line_below:
			return false;
//flaw_line_below:
	}
//flaw_line_below:

 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
//flaw_line_below:
	if (radix_tree_preload(GFP_NOFS)) {
//flaw_line_below:
		kmem_cache_free(free_nid_slab, i);
//flaw_line_below:
		return true;
//flaw_line_below:
	}
//fix_flaw_line_below:
//	if (radix_tree_preload(GFP_NOFS))
//fix_flaw_line_below:
//		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	if (build) {
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 *   Thread A             Thread B
//fix_flaw_line_below:
//		 *  - f2fs_create
//fix_flaw_line_below:
//		 *   - f2fs_new_inode
//fix_flaw_line_below:
//		 *    - alloc_nid
//fix_flaw_line_below:
//		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
//fix_flaw_line_below:
//		 *                     - f2fs_balance_fs_bg
//fix_flaw_line_below:
//		 *                      - build_free_nids
//fix_flaw_line_below:
//		 *                       - __build_free_nids
//fix_flaw_line_below:
//		 *                        - scan_nat_page
//fix_flaw_line_below:
//		 *                         - add_free_nid
//fix_flaw_line_below:
//		 *                          - __lookup_nat_cache
//fix_flaw_line_below:
//		 *  - f2fs_add_link
//fix_flaw_line_below:
//		 *   - init_inode_metadata
//fix_flaw_line_below:
//		 *    - new_inode_page
//fix_flaw_line_below:
//		 *     - new_node_page
//fix_flaw_line_below:
//		 *      - set_node_addr
//fix_flaw_line_below:
//		 *  - alloc_nid_done
//fix_flaw_line_below:
//		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
//fix_flaw_line_below:
//		 *                         - __insert_nid_to_list(FREE_NID_LIST)
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		ne = __lookup_nat_cache(nm_i, nid);
//fix_flaw_line_below:
//		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
//fix_flaw_line_below:
//				nat_get_blkaddr(ne) != NULL_ADDR))
//fix_flaw_line_below:
//			goto err_out;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		e = __lookup_free_nid_list(nm_i, nid);
//fix_flaw_line_below:
//		if (e) {
//fix_flaw_line_below:
//			if (e->state == NID_NEW)
//fix_flaw_line_below:
//				ret = true;
//fix_flaw_line_below:
//			goto err_out;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
//fix_flaw_line_below:
//err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
//flaw_line_below:
	if (err) {
//fix_flaw_line_below:
//err:
//fix_flaw_line_below:
//	if (err)
 		kmem_cache_free(free_nid_slab, i);
//flaw_line_below:
		return true;
//flaw_line_below:
	}
//flaw_line_below:
	return true;
//fix_flaw_line_below:
//	return ret;
 }
",182551," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i;
 	struct nat_entry *ne;
	int err;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
 
 	spin_lock(&nm_i->nid_list_lock);
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
	if (err) {
 		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	return true;
 }
"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i, *e;
 	struct nat_entry *ne;
	int err = -EINVAL;
	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS))
		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
err:
	if (err)
 		kmem_cache_free(free_nid_slab, i);
	return ret;
 }
","The `add_free_nid` function is designed to manage the addition of free node IDs (NIDs) in a file system, specifically for the F2FS (Flash-Friendly File System). It first checks if the provided NID is valid (not zero) and, if the `build` flag is set, ensures that the NID is not already allocated by looking it up in the NAT (Node Address Table) cache. If the NID is valid and not allocated, the function allocates memory for a new `free_nid` structure, initializes it, and attempts to preload a radix tree. It then locks the NID list, inserts the new NID into the free list, and unlocks the list, ensuring thread safety. If any errors occur during insertion, the allocated memory is freed.","The function is responsible for adding a free node identifier to a file system's management structure. It first verifies that the identifier is valid and, if a specific condition is met, checks that the identifier is not already in use. If the identifier is available, it allocates memory for a new structure representing the free identifier and initializes its state. The function then attempts to prepare a data structure for efficient access and, while ensuring thread safety, inserts the new identifier into a list of free identifiers. If any issues arise during this process, the allocated memory is released.","The vulnerability in the original code arises from a race condition where multiple threads could concurrently access and modify the list of free node identifiers, leading to inconsistent states and potential denial of service. Specifically, the lack of checks for existing identifiers before insertion allowed one thread to add an identifier while another might be removing it, causing unpredictable behavior. The fixed code addresses this by introducing a check for existing identifiers in the free list before proceeding with the insertion. Additionally, it ensures proper locking around critical sections, thus preventing concurrent modifications and ensuring thread safety.","The vulnerability in the original code stems from a race condition that occurs when multiple threads access and modify the list of free node identifiers simultaneously, leading to inconsistent states and potential denial of service. This lack of proper synchronization allowed one thread to add or remove identifiers while another was performing operations on the same list, resulting in unpredictable behavior. The fixed code addresses this issue by implementing checks for existing identifiers before insertion and ensuring that critical sections are properly locked, thereby preventing concurrent modifications and enhancing overall thread safety in the management of node identifiers.","The vulnerability was fixed by adding checks for existing identifiers in the free list before proceeding with the insertion. In the updated code, after allocating memory for the new identifier, the function checks if the identifier is already present in the free list using a lookup function. If an existing identifier is found, the function verifies its state to prevent duplicate entries. Additionally, the locking mechanism around critical sections was improved, ensuring that the list is properly locked during modifications with `spin_lock` and `spin_unlock`, thus preventing race conditions and ensuring thread safety during concurrent access.","The vulnerability was addressed by implementing checks to ensure that an identifier is not already present in the list before attempting to add it, thereby preventing duplicate entries. The updated code also enhances synchronization by ensuring that critical sections are properly locked during modifications, which prevents concurrent threads from interfering with each other. This locking mechanism ensures that only one thread can modify the list at a time, thus eliminating the race condition that could lead to inconsistent states or denial of service. Overall, these changes improve the robustness and thread safety of the identifier management process."
34,182583,182583,,Local,Not required,Partial,CVE-2017-18203,https://www.cvedetails.com/cve/CVE-2017-18203/,CWE-362,Medium,,,,2018-02-27,1.9,The dm_get_from_kobject function in drivers/md/dm.c in the Linux kernel before 4.14.3 allow local users to cause a denial of service (BUG) by leveraging a race condition with __dm_destroy during creation and removal of DM devices.,2018-06-19,DoS ,8,https://github.com/torvalds/linux/commit/b9a41d21dceadf8104812626ef85dc56ee8a60ed,b9a41d21dceadf8104812626ef85dc56ee8a60ed,"dm: fix race between dm_get_from_kobject() and __dm_destroy()

The following BUG_ON was hit when testing repeat creation and removal of
DM devices:

    kernel BUG at drivers/md/dm.c:2919!
    CPU: 7 PID: 750 Comm: systemd-udevd Not tainted 4.1.44
    Call Trace:
     [<ffffffff81649e8b>] dm_get_from_kobject+0x34/0x3a
     [<ffffffff81650ef1>] dm_attr_show+0x2b/0x5e
     [<ffffffff817b46d1>] ? mutex_lock+0x26/0x44
     [<ffffffff811df7f5>] sysfs_kf_seq_show+0x83/0xcf
     [<ffffffff811de257>] kernfs_seq_show+0x23/0x25
     [<ffffffff81199118>] seq_read+0x16f/0x325
     [<ffffffff811de994>] kernfs_fop_read+0x3a/0x13f
     [<ffffffff8117b625>] __vfs_read+0x26/0x9d
     [<ffffffff8130eb59>] ? security_file_permission+0x3c/0x44
     [<ffffffff8117bdb8>] ? rw_verify_area+0x83/0xd9
     [<ffffffff8117be9d>] vfs_read+0x8f/0xcf
     [<ffffffff81193e34>] ? __fdget_pos+0x12/0x41
     [<ffffffff8117c686>] SyS_read+0x4b/0x76
     [<ffffffff817b606e>] system_call_fastpath+0x12/0x71

The bug can be easily triggered, if an extra delay (e.g. 10ms) is added
between the test of DMF_FREEING & DMF_DELETING and dm_get() in
dm_get_from_kobject().

To fix it, we need to ensure the test of DMF_FREEING & DMF_DELETING and
dm_get() are done in an atomic way, so _minor_lock is used.

The other callers of dm_get() have also been checked to be OK: some
callers invoke dm_get() under _minor_lock, some callers invoke it under
_hash_lock, and dm_start_request() invoke it after increasing
md->open_count.

Cc: stable@vger.kernel.org
Signed-off-by: Hou Tao <houtao1@huawei.com>
Signed-off-by: Mike Snitzer <snitzer@redhat.com>",3,drivers/md/dm.c,"{""sha"": ""dcfa1a8c93909cad5bc07c257b0b70af8f3a722e"", ""filename"": ""drivers/md/dm.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 4, ""changes"": 12, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a41d21dceadf8104812626ef85dc56ee8a60ed/drivers/md/dm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a41d21dceadf8104812626ef85dc56ee8a60ed/drivers/md/dm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/md/dm.c?ref=b9a41d21dceadf8104812626ef85dc56ee8a60ed"", ""patch"": ""@@ -2711,11 +2711,15 @@ struct mapped_device *dm_get_from_kobject(struct kobject *kobj)\n \n \tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n \n-\tif (test_bit(DMF_FREEING, &md->flags) ||\n-\t    dm_deleting_md(md))\n-\t\treturn NULL;\n-\n+\tspin_lock(&_minor_lock);\n+\tif (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {\n+\t\tmd = NULL;\n+\t\tgoto out;\n+\t}\n \tdm_get(md);\n+out:\n+\tspin_unlock(&_minor_lock);\n+\n \treturn md;\n }\n ""}","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
 	dm_get(md);
out:
	spin_unlock(&_minor_lock);

 	return md;
 }
","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
 	dm_get(md);
 	return md;
 }
",C,"	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
out:
	spin_unlock(&_minor_lock);

","	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
",,"@@ -2711,11 +2711,15 @@ struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
-	if (test_bit(DMF_FREEING, &md->flags) ||
-	    dm_deleting_md(md))
-		return NULL;
-
+	spin_lock(&_minor_lock);
+	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
+		md = NULL;
+		goto out;
+	}
 	dm_get(md);
+out:
+	spin_unlock(&_minor_lock);
+
 	return md;
 }
 ",linux,b9a41d21dceadf8104812626ef85dc56ee8a60ed,856eb0916d181da6d043cc33e03f54d5c5bbe54a,1,"struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
//flaw_line_below:
	if (test_bit(DMF_FREEING, &md->flags) ||
//flaw_line_below:
	    dm_deleting_md(md))
//flaw_line_below:
		return NULL;
//flaw_line_below:

//fix_flaw_line_below:
//	spin_lock(&_minor_lock);
//fix_flaw_line_below:
//	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
//fix_flaw_line_below:
//		md = NULL;
//fix_flaw_line_below:
//		goto out;
//fix_flaw_line_below:
//	}
 	dm_get(md);
//fix_flaw_line_below:
//out:
//fix_flaw_line_below:
//	spin_unlock(&_minor_lock);
//fix_flaw_line_below:
//
 	return md;
 }
",182583,"struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
 	dm_get(md);
 	return md;
 }
","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
 	dm_get(md);
out:
	spin_unlock(&_minor_lock);

 	return md;
 }
","The function `dm_get_from_kobject` retrieves a pointer to a `mapped_device` structure from a given kernel object (`kobj`). It uses the `container_of` macro to obtain the `mapped_device` instance associated with the provided `kobj`. Before returning the pointer, it checks if the device is in the process of being freed or deleted by evaluating specific flags. If either condition is true, the function returns `NULL`, indicating that the device is no longer valid. If the device is valid, it increments the reference count of the `mapped_device` and returns the pointer to it.","The provided code defines a function that retrieves a specific structure associated with a kernel object. It first determines the structure's address based on the kernel object provided. Before proceeding, the function checks if the structure is marked for freeing or deletion, returning a null value if either condition is met, indicating that the structure is no longer valid. If the structure is valid, the function increments its reference count to ensure it remains accessible and then returns a pointer to the structure, allowing further operations to be performed on it safely.","The vulnerability in the original code arises from a race condition where a kernel object could be freed or deleted after the validity check but before the reference count is incremented, potentially leading to a use-after-free scenario. In the fixed code, a spin lock is introduced around the validity checks and the reference count increment. By locking with `_minor_lock`, the code ensures that the state of the `mapped_device` structure remains consistent during these operations, preventing concurrent modifications that could lead to the race condition and subsequent denial of service. This change effectively mitigates the vulnerability by synchronizing access to the shared resource.","The vulnerability in the original code stems from a race condition that occurs when a kernel object is checked for validity and subsequently modified or freed by another process before the reference count is incremented. This situation can lead to a use-after-free error, potentially causing a denial of service. The fixed code addresses this issue by introducing a locking mechanism that ensures exclusive access to the critical section where the validity check and reference count increment occur. This synchronization prevents concurrent modifications, thereby eliminating the race condition and enhancing the stability and security of the code.","The vulnerability was fixed by introducing a spin lock around the critical section of the code that checks the state of the `mapped_device` structure. By acquiring the lock with `spin_lock(&_minor_lock)`, the code ensures that no other process can modify the state of the device while it is being checked for the `DMF_FREEING` flag or being deleted. If either condition is met, the pointer to the `mapped_device` is set to `NULL`, and the function exits safely. Finally, the lock is released with `spin_unlock(&_minor_lock)`, ensuring that the reference count is only incremented when it is safe to do so, thus preventing the race condition.","The vulnerability was fixed by implementing a locking mechanism that ensures exclusive access to the critical section of the code where the validity of the device is checked and the reference count is incremented. By acquiring the lock before performing these operations, the code prevents other processes from modifying the device's state concurrently. This synchronization ensures that the validity check and the subsequent reference count increment occur without interruption, effectively eliminating the race condition that could lead to a use-after-free error. The lock is released after these operations, maintaining the integrity and stability of the code."
35,182607,182607,,Local,Not required,Complete,CVE-2017-2616,https://www.cvedetails.com/cve/CVE-2017-2616/,CWE-362,Medium,,,,2018-07-27,4.7,A race condition was found in util-linux before 2.32.1 in the way su handled the management of child processes. A local authenticated attacker could use this flaw to kill other processes with root privileges under specific conditions.,2019-10-09,,10,https://github.com/karelzak/util-linux/commit/dffab154d29a288aa171ff50263ecc8f2e14a891,dffab154d29a288aa171ff50263ecc8f2e14a891,"su: properly clear child PID

Reported-by: Tobias Stöckmann <tobias@stoeckmann.org>
Signed-off-by: Karel Zak <kzak@redhat.com>",4,login-utils/su-common.c,"{""sha"": ""696adc8888ad901efbb98f7878c31e4a21c4cf27"", ""filename"": ""login-utils/su-common.c"", ""status"": ""modified"", ""additions"": 10, ""deletions"": 4, ""changes"": 14, ""blob_url"": ""https://github.com/karelzak/util-linux/blob/dffab154d29a288aa171ff50263ecc8f2e14a891/login-utils/su-common.c"", ""raw_url"": ""https://github.com/karelzak/util-linux/raw/dffab154d29a288aa171ff50263ecc8f2e14a891/login-utils/su-common.c"", ""contents_url"": ""https://api.github.com/repos/karelzak/util-linux/contents/login-utils/su-common.c?ref=dffab154d29a288aa171ff50263ecc8f2e14a891"", ""patch"": ""@@ -368,6 +368,9 @@ create_watching_parent (void)\n             }\n           else\n             status = WEXITSTATUS (status);\n+\n+\t  /* child is gone, don't use the PID anymore */\n+\t  child = (pid_t) -1;\n         }\n       else if (caught_signal)\n         status = caught_signal + 128;\n@@ -377,7 +380,7 @@ create_watching_parent (void)\n   else\n     status = 1;\n \n-  if (caught_signal)\n+  if (caught_signal && child != (pid_t)-1)\n     {\n       fprintf (stderr, _(\""\\nSession terminated, killing shell...\""));\n       kill (child, SIGTERM);\n@@ -387,9 +390,12 @@ create_watching_parent (void)\n \n   if (caught_signal)\n     {\n-      sleep (2);\n-      kill (child, SIGKILL);\n-      fprintf (stderr, _(\"" ...killed.\\n\""));\n+      if (child != (pid_t)-1)\n+\t{\n+\t  sleep (2);\n+\t  kill (child, SIGKILL);\n+\t  fprintf (stderr, _(\"" ...killed.\\n\""));\n+\t}\n \n       /* Let's terminate itself with the received signal.\n        *""}","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);

	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
",C,"
	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
  if (caught_signal && child != (pid_t)-1)
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
","  if (caught_signal)
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
",,"@@ -368,6 +368,9 @@ create_watching_parent (void)
             }
           else
             status = WEXITSTATUS (status);
+
+	  /* child is gone, don't use the PID anymore */
+	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
@@ -377,7 +380,7 @@ create_watching_parent (void)
   else
     status = 1;
 
-  if (caught_signal)
+  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
@@ -387,9 +390,12 @@ create_watching_parent (void)
 
   if (caught_signal)
     {
-      sleep (2);
-      kill (child, SIGKILL);
-      fprintf (stderr, _("" ...killed.\n""));
+      if (child != (pid_t)-1)
+	{
+	  sleep (2);
+	  kill (child, SIGKILL);
+	  fprintf (stderr, _("" ...killed.\n""));
+	}
 
       /* Let's terminate itself with the received signal.
        *",util-linux,dffab154d29a288aa171ff50263ecc8f2e14a891,904ffe1fd2ba6c2092e00b4b1b95fcc30be77b45,1,"create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	  /* child is gone, don't use the PID anymore */
//fix_flaw_line_below:
//	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
//flaw_line_below:
  if (caught_signal)
//fix_flaw_line_below:
//  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
//flaw_line_below:
      sleep (2);
//flaw_line_below:
      kill (child, SIGKILL);
//flaw_line_below:
      fprintf (stderr, _("" ...killed.\n""));
//fix_flaw_line_below:
//      if (child != (pid_t)-1)
//fix_flaw_line_below:
//	{
//fix_flaw_line_below:
//	  sleep (2);
//fix_flaw_line_below:
//	  kill (child, SIGKILL);
//fix_flaw_line_below:
//	  fprintf (stderr, _("" ...killed.\n""));
//fix_flaw_line_below:
//	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
",182607,"create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);

	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","The `create_watching_parent` function manages a child process that runs a shell while monitoring its execution in a parent process. Initially, it opens a PAM session and handles potential errors. The function then forks a child process; if successful, the child executes the shell, while the parent changes its working directory to the root and sets up signal handling for various termination signals. The parent continuously waits for the child process to change state, handling signals appropriately. If a signal is caught, it terminates the child and cleans up the PAM session before exiting with the appropriate status code.","The code defines a function that creates a child process to run a shell while the parent process monitors its execution. It begins by establishing a session and handling any errors that arise. After forking, the child process executes the shell, while the parent changes its working directory and sets up signal handling for termination signals. The parent continuously waits for the child to change state, managing signals as they occur. If a signal is caught, the parent terminates the child process, cleans up the session, and exits with the appropriate status code, ensuring proper resource management.","The vulnerability in the original code stemmed from a race condition in managing child processes, allowing an attacker to exploit timing discrepancies to terminate processes with elevated privileges. Specifically, the issue arose from the handling of the child process's PID, which could be reused or manipulated before the parent process properly monitored its state. In the fixed code, the child PID is set to an invalid value after it is no longer needed, ensuring that any subsequent operations do not mistakenly reference a valid PID. This change mitigates the risk of an attacker leveraging the race condition to affect process management.","The vulnerability in the original code was due to a race condition in managing child processes, which allowed an authenticated attacker to exploit timing issues to terminate processes with elevated privileges. This occurred because the parent process did not adequately synchronize its actions with the state of the child process, potentially leading to unintended interactions. In the revised code, the vulnerability was addressed by invalidating the child process identifier after it was no longer needed, ensuring that any subsequent operations could not mistakenly reference a valid identifier, thereby preventing the exploitation of the race condition.","The vulnerability was fixed by setting the child process identifier to an invalid value after it was no longer needed. In the revised code, after checking the status of the child process with `waitpid`, the child PID is explicitly set to `(pid_t) -1`. This change ensures that any subsequent checks or operations involving the child PID do not mistakenly reference a valid process, thus preventing potential exploitation of the race condition. Additionally, the code includes checks for the `caught_signal` variable to manage signal handling more effectively, further enhancing the robustness of the process management.","The vulnerability was fixed by ensuring that the process identifier for the child process is invalidated after it is no longer needed. This prevents any subsequent operations from mistakenly referencing a valid identifier, thereby mitigating the risk of exploitation through race conditions. Additionally, the revised code includes enhanced signal handling to manage termination signals more effectively, ensuring that the parent process does not inadvertently interact with the child process in an unsafe manner. These changes collectively strengthen the synchronization between the parent and child processes, reducing the potential for unauthorized actions by an attacker."
36,182851,182851,,Remote,Not required,Complete,CVE-2019-11815,https://www.cvedetails.com/cve/CVE-2019-11815/,CWE-362,Medium,Complete,Complete,,2019-05-08,9.3,"An issue was discovered in rds_tcp_kill_sock in net/rds/tcp.c in the Linux kernel before 5.0.8. There is a race condition leading to a use-after-free, related to net namespace cleanup.",2019-06-07,,1,https://github.com/torvalds/linux/commit/cb66ddd156203daefb8d71158036b27b0e2caf63,cb66ddd156203daefb8d71158036b27b0e2caf63,"net: rds: force to destroy connection if t_sock is NULL in rds_tcp_kill_sock().

When it is to cleanup net namespace, rds_tcp_exit_net() will call
rds_tcp_kill_sock(), if t_sock is NULL, it will not call
rds_conn_destroy(), rds_conn_path_destroy() and rds_tcp_conn_free() to free
connection, and the worker cp_conn_w is not stopped, afterwards the net is freed in
net_drop_ns(); While cp_conn_w rds_connect_worker() will call rds_tcp_conn_path_connect()
and reference 'net' which has already been freed.

In rds_tcp_conn_path_connect(), rds_tcp_set_callbacks() will set t_sock = sock before
sock->ops->connect, but if connect() is failed, it will call
rds_tcp_restore_callbacks() and set t_sock = NULL, if connect is always
failed, rds_connect_worker() will try to reconnect all the time, so
rds_tcp_kill_sock() will never to cancel worker cp_conn_w and free the
connections.

Therefore, the condition !tc->t_sock is not needed if it is going to do
cleanup_net->rds_tcp_exit_net->rds_tcp_kill_sock, because tc->t_sock is always
NULL, and there is on other path to cancel cp_conn_w and free
connection. So this patch is to fix this.

rds_tcp_kill_sock():
...
if (net != c_net || !tc->t_sock)
...
Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

==================================================================
BUG: KASAN: use-after-free in inet_create+0xbcc/0xd28
net/ipv4/af_inet.c:340
Read of size 4 at addr ffff8003496a4684 by task kworker/u8:4/3721

CPU: 3 PID: 3721 Comm: kworker/u8:4 Not tainted 5.1.0 #11
Hardware name: linux,dummy-virt (DT)
Workqueue: krdsd rds_connect_worker
Call trace:
 dump_backtrace+0x0/0x3c0 arch/arm64/kernel/time.c:53
 show_stack+0x28/0x38 arch/arm64/kernel/traps.c:152
 __dump_stack lib/dump_stack.c:77 [inline]
 dump_stack+0x120/0x188 lib/dump_stack.c:113
 print_address_description+0x68/0x278 mm/kasan/report.c:253
 kasan_report_error mm/kasan/report.c:351 [inline]
 kasan_report+0x21c/0x348 mm/kasan/report.c:409
 __asan_report_load4_noabort+0x30/0x40 mm/kasan/report.c:429
 inet_create+0xbcc/0xd28 net/ipv4/af_inet.c:340
 __sock_create+0x4f8/0x770 net/socket.c:1276
 sock_create_kern+0x50/0x68 net/socket.c:1322
 rds_tcp_conn_path_connect+0x2b4/0x690 net/rds/tcp_connect.c:114
 rds_connect_worker+0x108/0x1d0 net/rds/threads.c:175
 process_one_work+0x6e8/0x1700 kernel/workqueue.c:2153
 worker_thread+0x3b0/0xdd0 kernel/workqueue.c:2296
 kthread+0x2f0/0x378 kernel/kthread.c:255
 ret_from_fork+0x10/0x18 arch/arm64/kernel/entry.S:1117

Allocated by task 687:
 save_stack mm/kasan/kasan.c:448 [inline]
 set_track mm/kasan/kasan.c:460 [inline]
 kasan_kmalloc+0xd4/0x180 mm/kasan/kasan.c:553
 kasan_slab_alloc+0x14/0x20 mm/kasan/kasan.c:490
 slab_post_alloc_hook mm/slab.h:444 [inline]
 slab_alloc_node mm/slub.c:2705 [inline]
 slab_alloc mm/slub.c:2713 [inline]
 kmem_cache_alloc+0x14c/0x388 mm/slub.c:2718
 kmem_cache_zalloc include/linux/slab.h:697 [inline]
 net_alloc net/core/net_namespace.c:384 [inline]
 copy_net_ns+0xc4/0x2d0 net/core/net_namespace.c:424
 create_new_namespaces+0x300/0x658 kernel/nsproxy.c:107
 unshare_nsproxy_namespaces+0xa0/0x198 kernel/nsproxy.c:206
 ksys_unshare+0x340/0x628 kernel/fork.c:2577
 __do_sys_unshare kernel/fork.c:2645 [inline]
 __se_sys_unshare kernel/fork.c:2643 [inline]
 __arm64_sys_unshare+0x38/0x58 kernel/fork.c:2643
 __invoke_syscall arch/arm64/kernel/syscall.c:35 [inline]
 invoke_syscall arch/arm64/kernel/syscall.c:47 [inline]
 el0_svc_common+0x168/0x390 arch/arm64/kernel/syscall.c:83
 el0_svc_handler+0x60/0xd0 arch/arm64/kernel/syscall.c:129
 el0_svc+0x8/0xc arch/arm64/kernel/entry.S:960

Freed by task 264:
 save_stack mm/kasan/kasan.c:448 [inline]
 set_track mm/kasan/kasan.c:460 [inline]
 __kasan_slab_free+0x114/0x220 mm/kasan/kasan.c:521
 kasan_slab_free+0x10/0x18 mm/kasan/kasan.c:528
 slab_free_hook mm/slub.c:1370 [inline]
 slab_free_freelist_hook mm/slub.c:1397 [inline]
 slab_free mm/slub.c:2952 [inline]
 kmem_cache_free+0xb8/0x3a8 mm/slub.c:2968
 net_free net/core/net_namespace.c:400 [inline]
 net_drop_ns.part.6+0x78/0x90 net/core/net_namespace.c:407
 net_drop_ns net/core/net_namespace.c:406 [inline]
 cleanup_net+0x53c/0x6d8 net/core/net_namespace.c:569
 process_one_work+0x6e8/0x1700 kernel/workqueue.c:2153
 worker_thread+0x3b0/0xdd0 kernel/workqueue.c:2296
 kthread+0x2f0/0x378 kernel/kthread.c:255
 ret_from_fork+0x10/0x18 arch/arm64/kernel/entry.S:1117

The buggy address belongs to the object at ffff8003496a3f80
 which belongs to the cache net_namespace of size 7872
The buggy address is located 1796 bytes inside of
 7872-byte region [ffff8003496a3f80, ffff8003496a5e40)
The buggy address belongs to the page:
page:ffff7e000d25a800 count:1 mapcount:0 mapping:ffff80036ce4b000
index:0x0 compound_mapcount: 0
flags: 0xffffe0000008100(slab|head)
raw: 0ffffe0000008100 dead000000000100 dead000000000200 ffff80036ce4b000
raw: 0000000000000000 0000000080040004 00000001ffffffff 0000000000000000
page dumped because: kasan: bad access detected

Memory state around the buggy address:
 ffff8003496a4580: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
 ffff8003496a4600: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
>ffff8003496a4680: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
                   ^
 ffff8003496a4700: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
 ffff8003496a4780: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
==================================================================

Fixes: 467fa15356ac(""RDS-TCP: Support multiple RDS-TCP listen endpoints, one per netns."")
Reported-by: Hulk Robot <hulkci@huawei.com>
Signed-off-by: Mao Wenan <maowenan@huawei.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/rds/tcp.c,"{""sha"": ""faf726e00e27c75b11721dbc55518ca60bdf00a6"", ""filename"": ""net/rds/tcp.c"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 1, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/cb66ddd156203daefb8d71158036b27b0e2caf63/net/rds/tcp.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cb66ddd156203daefb8d71158036b27b0e2caf63/net/rds/tcp.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/tcp.c?ref=cb66ddd156203daefb8d71158036b27b0e2caf63"", ""patch"": ""@@ -608,7 +608,7 @@ static void rds_tcp_kill_sock(struct net *net)\n \tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n \t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n \n-\t\tif (net != c_net || !tc->t_sock)\n+\t\tif (net != c_net)\n \t\t\tcontinue;\n \t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n \t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);""}","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net || !tc->t_sock)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
",C,"		if (net != c_net)
","		if (net != c_net || !tc->t_sock)
",,"@@ -608,7 +608,7 @@ static void rds_tcp_kill_sock(struct net *net)
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
-		if (net != c_net || !tc->t_sock)
+		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);",linux,cb66ddd156203daefb8d71158036b27b0e2caf63,f28cd2af22a0c134e4aa1c64a70f70d815d473fb,1,"static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
//flaw_line_below:
		if (net != c_net || !tc->t_sock)
//fix_flaw_line_below:
//		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
",182851,"static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net || !tc->t_sock)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","The provided code defines a function `rds_tcp_kill_sock` that is responsible for terminating TCP connections associated with a specific network namespace. It first retrieves the listening socket and stops it, ensuring no new connections can be accepted. The function then acquires a lock to safely iterate through a list of active TCP connections, checking if each connection belongs to the specified network. If a connection is found, it is either moved to a temporary list for destruction or marked as detached if it already exists in the temporary list. Finally, the function cleans up by destroying the connections in the temporary list.","The code defines a function that manages the termination of TCP connections within a specific network context. It begins by stopping the listening socket to prevent new connections. The function then locks access to a list of active connections and iterates through them, checking if they belong to the specified network. For each relevant connection, it either moves it to a temporary list for later destruction or marks it as detached if it is already present. Finally, the function cleans up by destroying all connections stored in the temporary list, ensuring proper resource management.","The vulnerability in the original code arises from a race condition during the cleanup of network namespaces, which can lead to a use-after-free error. Specifically, while iterating through the list of active TCP connections, if a connection is freed while still being accessed, it can result in dereferencing invalid memory. In the fixed code, the check for the network context is simplified by removing the condition that checks for the existence of the socket, thus ensuring that connections are only processed if they belong to the specified network. This change helps prevent accessing freed connections, thereby mitigating the race condition.","The vulnerability in the original code stems from a race condition during the cleanup of network namespaces, which can lead to a use-after-free scenario. This occurs when a connection is freed while still being accessed, potentially allowing the program to dereference invalid memory. In the revised code, the vulnerability is addressed by simplifying the conditions under which connections are processed, ensuring that only valid connections belonging to the specified network are handled. This change reduces the likelihood of accessing freed memory, effectively mitigating the race condition and enhancing the overall stability of the code.","The vulnerability was fixed by modifying the condition that checks the network context during the iteration over active TCP connections. In the original code, the check included both the network context and the existence of the socket, which could lead to accessing freed connections. In the revised code, the condition was simplified to only check if the connection belongs to the specified network, removing the reliance on the socket's existence. This change ensures that only valid connections are processed, thereby preventing the potential use-after-free issue and enhancing the safety of the connection cleanup process.","The vulnerability was fixed by streamlining the conditions under which connections are processed during the cleanup operation. The revised code eliminates the check for the existence of the socket, focusing solely on whether the connection belongs to the specified network context. This adjustment reduces the risk of accessing freed memory by ensuring that only valid connections are handled. By simplifying the logic, the code effectively mitigates the race condition that could lead to a use-after-free scenario, thereby enhancing the overall safety and stability of the connection management process."
37,182856,182856,,Local,Not required,Complete,CVE-2019-11599,https://www.cvedetails.com/cve/CVE-2019-11599/,CWE-362,Medium,Complete,Complete,,2019-04-29,6.9,"The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",2019-05-28,DoS +Info ,3,https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a,04f5866e41fb70690e28397487d8bd8eea7d712a,"coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping

The core dumping code has always run without holding the mmap_sem for
writing, despite that is the only way to ensure that the entire vma
layout will not change from under it.  Only using some signal
serialization on the processes belonging to the mm is not nearly enough.
This was pointed out earlier.  For example in Hugh's post from Jul 2017:

  https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils

  ""Not strictly relevant here, but a related note: I was very surprised
   to discover, only quite recently, how handle_mm_fault() may be called
   without down_read(mmap_sem) - when core dumping. That seems a
   misguided optimization to me, which would also be nice to correct""

In particular because the growsdown and growsup can move the
vm_start/vm_end the various loops the core dump does around the vma will
not be consistent if page faults can happen concurrently.

Pretty much all users calling mmget_not_zero()/get_task_mm() and then
taking the mmap_sem had the potential to introduce unexpected side
effects in the core dumping code.

Adding mmap_sem for writing around the ->core_dump invocation is a
viable long term fix, but it requires removing all copy user and page
faults and to replace them with get_dump_page() for all binary formats
which is not suitable as a short term fix.

For the time being this solution manually covers the places that can
confuse the core dump either by altering the vma layout or the vma flags
while it runs.  Once ->core_dump runs under mmap_sem for writing the
function mmget_still_valid() can be dropped.

Allowing mmap_sem protected sections to run in parallel with the
coredump provides some minor parallelism advantage to the swapoff code
(which seems to be safe enough by never mangling any vma field and can
keep doing swapins in parallel to the core dumping) and to some other
corner case.

In order to facilitate the backporting I added ""Fixes: 86039bd3b4e6""
however the side effect of this same race condition in /proc/pid/mem
should be reproducible since before 2.6.12-rc2 so I couldn't add any
other ""Fixes:"" because there's no hash beyond the git genesis commit.

Because find_extend_vma() is the only location outside of the process
context that could modify the ""mm"" structures under mmap_sem for
reading, by adding the mmget_still_valid() check to it, all other cases
that take the mmap_sem for reading don't need the new check after
mmget_not_zero()/get_task_mm().  The expand_stack() in page fault
context also doesn't need the new check, because all tasks under core
dumping are frozen.

Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com
Fixes: 86039bd3b4e6 (""userfaultfd: add new syscall to provide memory externalization"")
Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Reported-by: Jann Horn <jannh@google.com>
Suggested-by: Oleg Nesterov <oleg@redhat.com>
Acked-by: Peter Xu <peterx@redhat.com>
Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
Reviewed-by: Oleg Nesterov <oleg@redhat.com>
Reviewed-by: Jann Horn <jannh@google.com>
Acked-by: Jason Gunthorpe <jgg@mellanox.com>
Acked-by: Michal Hocko <mhocko@suse.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,drivers/infiniband/core/uverbs_main.c,"{""sha"": ""f2e7ffe6fc546612f62da9cde853b9c1bf37d8bb"", ""filename"": ""drivers/infiniband/core/uverbs_main.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 0, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/infiniband/core/uverbs_main.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t * will only be one mm, so no big deal.\n \t\t */\n \t\tdown_write(&mm->mmap_sem);\n+\t\tif (!mmget_still_valid(mm))\n+\t\t\tgoto skip_mm;\n \t\tmutex_lock(&ufile->umap_lock);\n \t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n \t\t\t\t\t  list) {\n@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n \t\t}\n \t\tmutex_unlock(&ufile->umap_lock);\n+\tskip_mm:\n \t\tup_write(&mm->mmap_sem);\n \t\tmmput(mm);\n \t}""}<_**next**_>{""sha"": ""95ca1fe7283cff265247c6f3a84e5fa573299fca"", ""filename"": ""fs/proc/task_mmu.c"", ""status"": ""modified"", ""additions"": 18, ""deletions"": 0, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/task_mmu.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,\n \t\t\t\t\tcount = -EINTR;\n \t\t\t\t\tgoto out_mm;\n \t\t\t\t}\n+\t\t\t\t/*\n+\t\t\t\t * Avoid to modify vma->vm_flags\n+\t\t\t\t * without locked ops while the\n+\t\t\t\t * coredump reads the vm_flags.\n+\t\t\t\t */\n+\t\t\t\tif (!mmget_still_valid(mm)) {\n+\t\t\t\t\t/*\n+\t\t\t\t\t * Silently return \""count\""\n+\t\t\t\t\t * like if get_task_mm()\n+\t\t\t\t\t * failed. FIXME: should this\n+\t\t\t\t\t * function have returned\n+\t\t\t\t\t * -ESRCH if get_task_mm()\n+\t\t\t\t\t * failed like if\n+\t\t\t\t\t * get_proc_task() fails?\n+\t\t\t\t\t */\n+\t\t\t\t\tup_write(&mm->mmap_sem);\n+\t\t\t\t\tgoto out_mm;\n+\t\t\t\t}\n \t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n \t\t\t\t\tvma_set_page_prot(vma);""}<_**next**_>{""sha"": ""f5de1e726356a51c27ff529f98d99032650eb839"", ""filename"": ""fs/userfaultfd.c"", ""status"": ""modified"", ""additions"": 9, ""deletions"": 0, ""changes"": 9, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/userfaultfd.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -629,6 +629,8 @@ static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n \n \t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n \t\tdown_write(&mm->mmap_sem);\n+\t\t/* no task can run (and in turn coredump) yet */\n+\t\tVM_WARN_ON(!mmget_still_valid(mm));\n \t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n \t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n \t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n@@ -883,6 +885,8 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t * taking the mmap_sem for writing.\n \t */\n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto skip_mm;\n \tprev = NULL;\n \tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\tcond_resched();\n@@ -905,6 +909,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t\tvma->vm_flags = new_flags;\n \t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n \t}\n+skip_mm:\n \tup_write(&mm->mmap_sem);\n \tmmput(mm);\n wakeup:\n@@ -1333,6 +1338,8 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;\n@@ -1520,6 +1527,8 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;""}<_**next**_>{""sha"": ""a3fda9f024c3c1988b6ff60954d7f7e74a9c1ecf"", ""filename"": ""include/linux/sched/mm.h"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/sched/mm.h?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -49,6 +49,27 @@ static inline void mmdrop(struct mm_struct *mm)\n \t\t__mmdrop(mm);\n }\n \n+/*\n+ * This has to be called after a get_task_mm()/mmget_not_zero()\n+ * followed by taking the mmap_sem for writing before modifying the\n+ * vmas or anything the coredump pretends not to change from under it.\n+ *\n+ * NOTE: find_extend_vma() called from GUP context is the only place\n+ * that can modify the \""mm\"" (notably the vm_start/end) under mmap_sem\n+ * for reading and outside the context of the process, so it is also\n+ * the only case that holds the mmap_sem for reading that must call\n+ * this function. Generally if the mmap_sem is hold for reading\n+ * there's no need of this check after get_task_mm()/mmget_not_zero().\n+ *\n+ * This function can be obsoleted and the check can be removed, after\n+ * the coredump code will hold the mmap_sem for writing before\n+ * invoking the ->core_dump methods.\n+ */\n+static inline bool mmget_still_valid(struct mm_struct *mm)\n+{\n+\treturn likely(!mm->core_state);\n+}\n+\n /**\n  * mmget() - Pin the address space associated with a &struct mm_struct.\n  * @mm: The address space to pin.""}<_**next**_>{""sha"": ""bd7b9f293b391f22b85810e48bc7c0679b217f05"", ""filename"": ""mm/mmap.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 1, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/mmap.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -45,6 +45,7 @@\n #include <linux/moduleparam.h>\n #include <linux/pkeys.h>\n #include <linux/oom.h>\n+#include <linux/sched/mm.h>\n \n #include <linux/uaccess.h>\n #include <asm/cacheflush.h>\n@@ -2525,7 +2526,8 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \tvma = find_vma_prev(mm, addr, &prev);\n \tif (vma && (vma->vm_start <= addr))\n \t\treturn vma;\n-\tif (!prev || expand_stack(prev, addr))\n+\t/* don't alter vm_end if the coredump is running */\n+\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n \t\treturn NULL;\n \tif (prev->vm_flags & VM_LOCKED)\n \t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n@@ -2551,6 +2553,9 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \t\treturn vma;\n \tif (!(vma->vm_flags & VM_GROWSDOWN))\n \t\treturn NULL;\n+\t/* don't alter vm_start if the coredump is running */\n+\tif (!mmget_still_valid(mm))\n+\t\treturn NULL;\n \tstart = vma->vm_start;\n \tif (expand_stack(vma, addr))\n \t\treturn NULL;""}","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
		if (!mmget_still_valid(mm))
			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
",C,"		if (!mmget_still_valid(mm))
			goto skip_mm;
	skip_mm:
",,,"@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
+		if (!mmget_still_valid(mm))
+			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
+	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}",linux,04f5866e41fb70690e28397487d8bd8eea7d712a,dce5b0bdeec61bdbee56121ceb1d014151d5cab1,1,"void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
//fix_flaw_line_below:
//		if (!mmget_still_valid(mm))
//fix_flaw_line_below:
//			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
//fix_flaw_line_below:
//	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
",182856,"void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
		if (!mmget_still_valid(mm))
			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","The function `uverbs_user_mmap_disassociate` is designed to disassociate user memory mappings from an RDMA (Remote Direct Memory Access) file structure in a concurrent environment. It first acquires a lock to ensure thread safety while iterating through a list of user mappings associated with the file. For each mapping, it checks if the corresponding memory area is still valid; if so, it locks the memory structure, removes the mapping from the list, and clears specific flags while also unmapping the virtual memory area. This process continues until all mappings are processed, ensuring proper resource management and preventing memory leaks.","The provided code implements a function that disassociates user memory mappings from a specific file structure in a multi-threaded environment. It begins by acquiring a lock to ensure safe access to a list of mappings. The function iterates through this list, checking the validity of each memory area. For valid areas, it locks the memory structure, removes the mapping from the list, and clears certain flags while unmapping the associated virtual memory. This process continues until all mappings are processed, effectively managing resources and preventing potential memory leaks in the system.","The vulnerability in the original code arises from a race condition due to inadequate locking mechanisms while accessing and modifying virtual memory areas (VMAs). Specifically, the function did not ensure that the VMA layout or flags remained stable during operations, allowing concurrent modifications that could lead to sensitive information disclosure or denial of service. The fixed code addresses this by introducing a check with `mmget_still_valid(mm)` after acquiring the `mmap_sem` lock, ensuring that the memory structure remains valid before proceeding with the cleanup. This change effectively prevents race conditions by ensuring that the memory area is not altered by other threads during critical operations.","The vulnerability in the original code stems from a race condition that occurs due to the lack of proper locking mechanisms while accessing and modifying virtual memory areas. This oversight allows concurrent changes to the layout or flags of these memory areas, potentially leading to sensitive information exposure or denial of service. The fixed code addresses this issue by implementing a validation check after acquiring the necessary locks, ensuring that the memory structure remains stable and valid before proceeding with any operations. This enhancement effectively mitigates the risk of race conditions and protects against unintended consequences from concurrent modifications.","The vulnerability was fixed by introducing a validation check using `mmget_still_valid(mm)` after acquiring the `mmap_sem` lock. This ensures that the memory structure remains valid before proceeding with operations on the virtual memory areas. If the memory structure is no longer valid, the code skips the cleanup process, preventing any potential race conditions that could arise from concurrent modifications. Additionally, the use of `down_write` and `up_write` around the critical section ensures that the memory area is protected during the cleanup, thereby maintaining the integrity of the virtual memory layout and flags throughout the operation.","The vulnerability was addressed by implementing a validation check to ensure that the memory structure remains stable and valid before any operations are performed on it. This check occurs after acquiring the necessary lock, allowing the code to skip cleanup if the memory structure has been altered by other threads. Additionally, the introduction of proper locking mechanisms around critical sections ensures that the memory layout and flags are protected from concurrent modifications during the cleanup process. These changes effectively mitigate the risk of race conditions and enhance the overall stability and security of the memory management operations."
38,182857,182857,,Local,Not required,Complete,CVE-2019-11599,https://www.cvedetails.com/cve/CVE-2019-11599/,CWE-362,Medium,Complete,Complete,,2019-04-29,6.9,"The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",2019-05-28,DoS +Info ,18,https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a,04f5866e41fb70690e28397487d8bd8eea7d712a,"coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping

The core dumping code has always run without holding the mmap_sem for
writing, despite that is the only way to ensure that the entire vma
layout will not change from under it.  Only using some signal
serialization on the processes belonging to the mm is not nearly enough.
This was pointed out earlier.  For example in Hugh's post from Jul 2017:

  https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils

  ""Not strictly relevant here, but a related note: I was very surprised
   to discover, only quite recently, how handle_mm_fault() may be called
   without down_read(mmap_sem) - when core dumping. That seems a
   misguided optimization to me, which would also be nice to correct""

In particular because the growsdown and growsup can move the
vm_start/vm_end the various loops the core dump does around the vma will
not be consistent if page faults can happen concurrently.

Pretty much all users calling mmget_not_zero()/get_task_mm() and then
taking the mmap_sem had the potential to introduce unexpected side
effects in the core dumping code.

Adding mmap_sem for writing around the ->core_dump invocation is a
viable long term fix, but it requires removing all copy user and page
faults and to replace them with get_dump_page() for all binary formats
which is not suitable as a short term fix.

For the time being this solution manually covers the places that can
confuse the core dump either by altering the vma layout or the vma flags
while it runs.  Once ->core_dump runs under mmap_sem for writing the
function mmget_still_valid() can be dropped.

Allowing mmap_sem protected sections to run in parallel with the
coredump provides some minor parallelism advantage to the swapoff code
(which seems to be safe enough by never mangling any vma field and can
keep doing swapins in parallel to the core dumping) and to some other
corner case.

In order to facilitate the backporting I added ""Fixes: 86039bd3b4e6""
however the side effect of this same race condition in /proc/pid/mem
should be reproducible since before 2.6.12-rc2 so I couldn't add any
other ""Fixes:"" because there's no hash beyond the git genesis commit.

Because find_extend_vma() is the only location outside of the process
context that could modify the ""mm"" structures under mmap_sem for
reading, by adding the mmget_still_valid() check to it, all other cases
that take the mmap_sem for reading don't need the new check after
mmget_not_zero()/get_task_mm().  The expand_stack() in page fault
context also doesn't need the new check, because all tasks under core
dumping are frozen.

Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com
Fixes: 86039bd3b4e6 (""userfaultfd: add new syscall to provide memory externalization"")
Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Reported-by: Jann Horn <jannh@google.com>
Suggested-by: Oleg Nesterov <oleg@redhat.com>
Acked-by: Peter Xu <peterx@redhat.com>
Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
Reviewed-by: Oleg Nesterov <oleg@redhat.com>
Reviewed-by: Jann Horn <jannh@google.com>
Acked-by: Jason Gunthorpe <jgg@mellanox.com>
Acked-by: Michal Hocko <mhocko@suse.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,fs/proc/task_mmu.c,"{""sha"": ""f2e7ffe6fc546612f62da9cde853b9c1bf37d8bb"", ""filename"": ""drivers/infiniband/core/uverbs_main.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 0, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/infiniband/core/uverbs_main.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t * will only be one mm, so no big deal.\n \t\t */\n \t\tdown_write(&mm->mmap_sem);\n+\t\tif (!mmget_still_valid(mm))\n+\t\t\tgoto skip_mm;\n \t\tmutex_lock(&ufile->umap_lock);\n \t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n \t\t\t\t\t  list) {\n@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n \t\t}\n \t\tmutex_unlock(&ufile->umap_lock);\n+\tskip_mm:\n \t\tup_write(&mm->mmap_sem);\n \t\tmmput(mm);\n \t}""}<_**next**_>{""sha"": ""95ca1fe7283cff265247c6f3a84e5fa573299fca"", ""filename"": ""fs/proc/task_mmu.c"", ""status"": ""modified"", ""additions"": 18, ""deletions"": 0, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/task_mmu.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,\n \t\t\t\t\tcount = -EINTR;\n \t\t\t\t\tgoto out_mm;\n \t\t\t\t}\n+\t\t\t\t/*\n+\t\t\t\t * Avoid to modify vma->vm_flags\n+\t\t\t\t * without locked ops while the\n+\t\t\t\t * coredump reads the vm_flags.\n+\t\t\t\t */\n+\t\t\t\tif (!mmget_still_valid(mm)) {\n+\t\t\t\t\t/*\n+\t\t\t\t\t * Silently return \""count\""\n+\t\t\t\t\t * like if get_task_mm()\n+\t\t\t\t\t * failed. FIXME: should this\n+\t\t\t\t\t * function have returned\n+\t\t\t\t\t * -ESRCH if get_task_mm()\n+\t\t\t\t\t * failed like if\n+\t\t\t\t\t * get_proc_task() fails?\n+\t\t\t\t\t */\n+\t\t\t\t\tup_write(&mm->mmap_sem);\n+\t\t\t\t\tgoto out_mm;\n+\t\t\t\t}\n \t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n \t\t\t\t\tvma_set_page_prot(vma);""}<_**next**_>{""sha"": ""f5de1e726356a51c27ff529f98d99032650eb839"", ""filename"": ""fs/userfaultfd.c"", ""status"": ""modified"", ""additions"": 9, ""deletions"": 0, ""changes"": 9, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/userfaultfd.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -629,6 +629,8 @@ static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n \n \t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n \t\tdown_write(&mm->mmap_sem);\n+\t\t/* no task can run (and in turn coredump) yet */\n+\t\tVM_WARN_ON(!mmget_still_valid(mm));\n \t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n \t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n \t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n@@ -883,6 +885,8 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t * taking the mmap_sem for writing.\n \t */\n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto skip_mm;\n \tprev = NULL;\n \tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\tcond_resched();\n@@ -905,6 +909,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t\tvma->vm_flags = new_flags;\n \t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n \t}\n+skip_mm:\n \tup_write(&mm->mmap_sem);\n \tmmput(mm);\n wakeup:\n@@ -1333,6 +1338,8 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;\n@@ -1520,6 +1527,8 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;""}<_**next**_>{""sha"": ""a3fda9f024c3c1988b6ff60954d7f7e74a9c1ecf"", ""filename"": ""include/linux/sched/mm.h"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/sched/mm.h?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -49,6 +49,27 @@ static inline void mmdrop(struct mm_struct *mm)\n \t\t__mmdrop(mm);\n }\n \n+/*\n+ * This has to be called after a get_task_mm()/mmget_not_zero()\n+ * followed by taking the mmap_sem for writing before modifying the\n+ * vmas or anything the coredump pretends not to change from under it.\n+ *\n+ * NOTE: find_extend_vma() called from GUP context is the only place\n+ * that can modify the \""mm\"" (notably the vm_start/end) under mmap_sem\n+ * for reading and outside the context of the process, so it is also\n+ * the only case that holds the mmap_sem for reading that must call\n+ * this function. Generally if the mmap_sem is hold for reading\n+ * there's no need of this check after get_task_mm()/mmget_not_zero().\n+ *\n+ * This function can be obsoleted and the check can be removed, after\n+ * the coredump code will hold the mmap_sem for writing before\n+ * invoking the ->core_dump methods.\n+ */\n+static inline bool mmget_still_valid(struct mm_struct *mm)\n+{\n+\treturn likely(!mm->core_state);\n+}\n+\n /**\n  * mmget() - Pin the address space associated with a &struct mm_struct.\n  * @mm: The address space to pin.""}<_**next**_>{""sha"": ""bd7b9f293b391f22b85810e48bc7c0679b217f05"", ""filename"": ""mm/mmap.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 1, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/mmap.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -45,6 +45,7 @@\n #include <linux/moduleparam.h>\n #include <linux/pkeys.h>\n #include <linux/oom.h>\n+#include <linux/sched/mm.h>\n \n #include <linux/uaccess.h>\n #include <asm/cacheflush.h>\n@@ -2525,7 +2526,8 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \tvma = find_vma_prev(mm, addr, &prev);\n \tif (vma && (vma->vm_start <= addr))\n \t\treturn vma;\n-\tif (!prev || expand_stack(prev, addr))\n+\t/* don't alter vm_end if the coredump is running */\n+\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n \t\treturn NULL;\n \tif (prev->vm_flags & VM_LOCKED)\n \t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n@@ -2551,6 +2553,9 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \t\treturn vma;\n \tif (!(vma->vm_flags & VM_GROWSDOWN))\n \t\treturn NULL;\n+\t/* don't alter vm_start if the coredump is running */\n+\tif (!mmget_still_valid(mm))\n+\t\treturn NULL;\n \tstart = vma->vm_start;\n \tif (expand_stack(vma, addr))\n \t\treturn NULL;""}","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
",C,"				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
",,,"@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 					count = -EINTR;
 					goto out_mm;
 				}
+				/*
+				 * Avoid to modify vma->vm_flags
+				 * without locked ops while the
+				 * coredump reads the vm_flags.
+				 */
+				if (!mmget_still_valid(mm)) {
+					/*
+					 * Silently return ""count""
+					 * like if get_task_mm()
+					 * failed. FIXME: should this
+					 * function have returned
+					 * -ESRCH if get_task_mm()
+					 * failed like if
+					 * get_proc_task() fails?
+					 */
+					up_write(&mm->mmap_sem);
+					goto out_mm;
+				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);",linux,04f5866e41fb70690e28397487d8bd8eea7d712a,dce5b0bdeec61bdbee56121ceb1d014151d5cab1,1,"static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
//fix_flaw_line_below:
//				/*
//fix_flaw_line_below:
//				 * Avoid to modify vma->vm_flags
//fix_flaw_line_below:
//				 * without locked ops while the
//fix_flaw_line_below:
//				 * coredump reads the vm_flags.
//fix_flaw_line_below:
//				 */
//fix_flaw_line_below:
//				if (!mmget_still_valid(mm)) {
//fix_flaw_line_below:
//					/*
//fix_flaw_line_below:
//					 * Silently return ""count""
//fix_flaw_line_below:
//					 * like if get_task_mm()
//fix_flaw_line_below:
//					 * failed. FIXME: should this
//fix_flaw_line_below:
//					 * function have returned
//fix_flaw_line_below:
//					 * -ESRCH if get_task_mm()
//fix_flaw_line_below:
//					 * failed like if
//fix_flaw_line_below:
//					 * get_proc_task() fails?
//fix_flaw_line_below:
//					 */
//fix_flaw_line_below:
//					up_write(&mm->mmap_sem);
//fix_flaw_line_below:
//					goto out_mm;
//fix_flaw_line_below:
//				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
",182857,"static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","The `clear_refs_write` function is designed to handle write operations to a specific file in the `/proc` filesystem, allowing users to clear reference types in a process's memory management. It begins by reading user input to determine the type of reference to clear, validating the input against predefined types. If the input is valid, it retrieves the associated task and its memory management structure. Depending on the reference type, it may reset the peak resident set size or clear soft dirty flags from virtual memory areas. The function utilizes locking mechanisms to ensure safe access to memory structures and performs necessary notifications to update memory management states before releasing resources and returning the count of bytes processed.","The provided code implements a function that processes write requests to a specific file in the system's process directory, enabling the clearing of memory reference types for a specified process. It begins by reading and validating user input to determine the reference type. Upon successful validation, it retrieves the target process's memory management structure and, based on the reference type, either resets memory usage metrics or clears specific memory flags. The function ensures safe access to memory structures through appropriate locking mechanisms and notifies the system of changes, ultimately returning the number of bytes processed from the user input.","The vulnerability in the original code arises from the lack of proper locking mechanisms when modifying virtual memory area (VMA) flags, which can lead to race conditions during concurrent access. Specifically, the code does not ensure that VMA flags are not altered while being read by other processes, potentially allowing unauthorized access to sensitive information or causing system instability. The fixed code addresses this issue by introducing checks with `mmget_still_valid(mm)` to ensure the memory management structure remains valid during operations. Additionally, it employs appropriate locking with `down_read` and `up_read` to safely manage access to VMA flags, thus preventing race conditions.","The vulnerability in the original code stems from the absence of proper synchronization mechanisms when accessing and modifying memory management structures, specifically regarding virtual memory area flags. This lack of locking allows concurrent modifications, leading to potential race conditions that could enable local users to access sensitive information or disrupt system stability. The fixed code addresses this issue by implementing checks to ensure the validity of memory management structures during operations and by introducing appropriate locking to manage access to these structures safely. This prevents unauthorized changes while the memory management operations are in progress, thereby mitigating the risk of exploitation.","The vulnerability was fixed by introducing checks and locking mechanisms to ensure safe access to memory management structures. Specifically, the code now uses `mmget_still_valid(mm)` to verify that the memory management structure remains valid before modifying virtual memory area flags. Additionally, the use of `down_read` and `up_read` ensures that read operations on the memory management structure are properly synchronized, preventing concurrent modifications during critical sections. This approach effectively mitigates the risk of race conditions, ensuring that VMA flags are not altered while being accessed, thus enhancing the overall stability and security of the system.","The vulnerability was addressed by implementing synchronization mechanisms to ensure safe access to memory management structures during operations. The code now includes checks to confirm the validity of these structures before any modifications are made, preventing unauthorized access or changes. Additionally, proper locking is employed to manage read and write operations, ensuring that modifications to memory flags cannot occur simultaneously with read actions. This approach effectively eliminates the potential for race conditions, thereby enhancing the stability and security of the system by safeguarding against concurrent access issues."
39,182913,182913,,Remote,Not required,Partial,CVE-2019-6974,https://www.cvedetails.com/cve/CVE-2019-6974/,CWE-362,Medium,Partial,Partial,,2019-02-15,6.8,"In the Linux kernel before 4.20.8, kvm_ioctl_create_device in virt/kvm/kvm_main.c mishandles reference counting because of a race condition, leading to a use-after-free.",2019-09-20,,2,https://github.com/torvalds/linux/commit/cfa39381173d5f969daf43582c95ad679189cbc9,cfa39381173d5f969daf43582c95ad679189cbc9,"kvm: fix kvm_ioctl_create_device() reference counting (CVE-2019-6974)

kvm_ioctl_create_device() does the following:

1. creates a device that holds a reference to the VM object (with a borrowed
   reference, the VM's refcount has not been bumped yet)
2. initializes the device
3. transfers the reference to the device to the caller's file descriptor table
4. calls kvm_get_kvm() to turn the borrowed reference to the VM into a real
   reference

The ownership transfer in step 3 must not happen before the reference to the VM
becomes a proper, non-borrowed reference, which only happens in step 4.
After step 3, an attacker can close the file descriptor and drop the borrowed
reference, which can cause the refcount of the kvm object to drop to zero.

This means that we need to grab a reference for the device before
anon_inode_getfd(), otherwise the VM can disappear from under us.

Fixes: 852b6d57dc7f (""kvm: add device control API"")
Cc: stable@kernel.org
Signed-off-by: Jann Horn <jannh@google.com>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>",1,virt/kvm/kvm_main.c,"{""sha"": ""585845203db89e5d2bc90722fd75bfcf595671d9"", ""filename"": ""virt/kvm/kvm_main.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 1, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/cfa39381173d5f969daf43582c95ad679189cbc9/virt/kvm/kvm_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cfa39381173d5f969daf43582c95ad679189cbc9/virt/kvm/kvm_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/virt/kvm/kvm_main.c?ref=cfa39381173d5f969daf43582c95ad679189cbc9"", ""patch"": ""@@ -3000,16 +3000,17 @@ static int kvm_ioctl_create_device(struct kvm *kvm,\n \tif (ops->init)\n \t\tops->init(dev);\n \n+\tkvm_get_kvm(kvm);\n \tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n \tif (ret < 0) {\n+\t\tkvm_put_kvm(kvm);\n \t\tmutex_lock(&kvm->lock);\n \t\tlist_del(&dev->vm_node);\n \t\tmutex_unlock(&kvm->lock);\n \t\tops->destroy(dev);\n \t\treturn ret;\n \t}\n \n-\tkvm_get_kvm(kvm);\n \tcd->fd = ret;\n \treturn 0;\n }""}","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
 	cd->fd = ret;
 	return 0;
 }
","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
",C,"	kvm_get_kvm(kvm);
		kvm_put_kvm(kvm);
","	kvm_get_kvm(kvm);
",,"@@ -3000,16 +3000,17 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 	if (ops->init)
 		ops->init(dev);
 
+	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
+		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
-	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }",linux,cfa39381173d5f969daf43582c95ad679189cbc9,8834f5600cf3c8db365e18a3d5cac2c2780c81e5,1,"static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
//fix_flaw_line_below:
//	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
//fix_flaw_line_below:
//		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
//flaw_line_below:
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
",182913,"static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
 	cd->fd = ret;
 	return 0;
 }
","The `kvm_ioctl_create_device` function is responsible for creating a virtual device within a Kernel-based Virtual Machine (KVM) environment. It first checks if the requested device type is valid and retrieves the corresponding device operations. If the function is called in test mode, it returns immediately without creating a device. Otherwise, it allocates memory for the device, locks the KVM structure, and invokes the device's creation operation. If successful, it adds the device to the KVM's device list, initializes it if applicable, and retrieves a file descriptor for the device. Finally, it returns the file descriptor or an error code if any step fails.","The provided code defines a function that facilitates the creation of a virtual device within a virtual machine environment. It begins by validating the requested device type and checking if the operation is in test mode, in which case it exits early. If not in test mode, it allocates memory for the device and locks the virtual machine structure to ensure thread safety. The function then attempts to create the device using specified operations, adding it to the device list upon success. It also initializes the device if necessary and retrieves a file descriptor for further interactions, returning this descriptor or an error code as needed.","The vulnerability in the original code arises from improper reference counting and a race condition, which can lead to a use-after-free scenario. Specifically, if the device creation fails after the device has been added to the device list, the reference count is not correctly managed, allowing the device to be freed while still being referenced. In the fixed code, the addition of the device to the list is now protected by a mutex lock, and the reference count is incremented with `kvm_get_kvm(kvm)` before the file descriptor is obtained. Additionally, if obtaining the file descriptor fails, the reference is decremented with `kvm_put_kvm(kvm)`, ensuring proper cleanup and preventing use-after-free issues.","The vulnerability in the original code stems from improper handling of reference counting, which creates a race condition that can result in a use-after-free error. Specifically, if the device is added to a list and then fails during subsequent operations, the reference count is not decremented appropriately, allowing the device to be accessed after it has been freed. The fixed code addresses this issue by ensuring that the reference count is correctly managed throughout the device creation process. It introduces additional checks and balances to maintain the integrity of the reference count, preventing premature freeing of the device and ensuring safe access.","The vulnerability was fixed by enhancing the management of reference counting and ensuring proper synchronization. In the updated code, after the device is created and before obtaining the file descriptor, the reference count is incremented using `kvm_get_kvm(kvm)`. If obtaining the file descriptor fails, the code now correctly decrements the reference count with `kvm_put_kvm(kvm)` before proceeding to remove the device from the list and destroy it. Additionally, the mutex lock is consistently used around critical sections to prevent race conditions, ensuring that the device remains valid throughout its lifecycle until all references are properly handled.","The vulnerability was fixed by improving the management of reference counting and ensuring proper synchronization during the device creation process. The updated code ensures that the reference count is incremented before any critical operations that could lead to failure, thereby maintaining a valid reference to the device. If an operation fails, the code now correctly decrements the reference count to prevent accessing a freed device. Additionally, the use of locks around critical sections prevents race conditions, ensuring that the device remains valid and properly managed throughout its lifecycle, thus eliminating the risk of use-after-free errors."
40,183036,183036,,Local,Not required,Complete,CVE-2016-10741,https://www.cvedetails.com/cve/CVE-2016-10741/,CWE-362,Medium,,,,2019-02-01,4.7,"In the Linux kernel before 4.9.3, fs/xfs/xfs_aops.c allows local users to cause a denial of service (system crash) because there is a race condition between direct and memory-mapped I/O (associated with a hole) that is handled with BUG_ON instead of an I/O failure.",2019-04-18,DoS ,20,https://github.com/torvalds/linux/commit/04197b341f23b908193308b8d63d17ff23232598,04197b341f23b908193308b8d63d17ff23232598,"xfs: don't BUG() on mixed direct and mapped I/O

We've had reports of generic/095 causing XFS to BUG() in
__xfs_get_blocks() due to the existence of delalloc blocks on a
direct I/O read. generic/095 issues a mix of various types of I/O,
including direct and memory mapped I/O to a single file. This is
clearly not supported behavior and is known to lead to such
problems. E.g., the lack of exclusion between the direct I/O and
write fault paths means that a write fault can allocate delalloc
blocks in a region of a file that was previously a hole after the
direct read has attempted to flush/inval the file range, but before
it actually reads the block mapping. In turn, the direct read
discovers a delalloc extent and cannot proceed.

While the appropriate solution here is to not mix direct and memory
mapped I/O to the same regions of the same file, the current
BUG_ON() behavior is probably overkill as it can crash the entire
system.  Instead, localize the failure to the I/O in question by
returning an error for a direct I/O that cannot be handled safely
due to delalloc blocks. Be careful to allow the case of a direct
write to post-eof delalloc blocks. This can occur due to speculative
preallocation and is safe as post-eof blocks are not accompanied by
dirty pages in pagecache (conversely, preallocation within eof must
have been zeroed, and thus dirtied, before the inode size could have
been increased beyond said blocks).

Finally, provide an additional warning if a direct I/O write occurs
while the file is memory mapped. This may not catch all problematic
scenarios, but provides a hint that some known-to-be-problematic I/O
methods are in use.

Signed-off-by: Brian Foster <bfoster@redhat.com>
Reviewed-by: Dave Chinner <dchinner@redhat.com>
Signed-off-by: Dave Chinner <david@fromorbit.com>",1,fs/xfs/xfs_aops.c,"{""sha"": ""2693ba84ec2541072396d138fbf970bca90a597c"", ""filename"": ""fs/xfs/xfs_aops.c"", ""status"": ""modified"", ""additions"": 20, ""deletions"": 2, ""changes"": 22, ""blob_url"": ""https://github.com/torvalds/linux/blob/04197b341f23b908193308b8d63d17ff23232598/fs/xfs/xfs_aops.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04197b341f23b908193308b8d63d17ff23232598/fs/xfs/xfs_aops.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/xfs/xfs_aops.c?ref=04197b341f23b908193308b8d63d17ff23232598"", ""patch"": ""@@ -1361,6 +1361,26 @@ __xfs_get_blocks(\n \tif (error)\n \t\tgoto out_unlock;\n \n+\t/*\n+\t * The only time we can ever safely find delalloc blocks on direct I/O\n+\t * is a dio write to post-eof speculative preallocation. All other\n+\t * scenarios are indicative of a problem or misuse (such as mixing\n+\t * direct and mapped I/O).\n+\t *\n+\t * The file may be unmapped by the time we get here so we cannot\n+\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this\n+\t * is a read or a write within eof. Otherwise, carry on but warn as a\n+\t * precuation if the file happens to be mapped.\n+\t */\n+\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {\n+\t\tif (!create || offset < i_size_read(VFS_I(ip))) {\n+\t\t\tWARN_ON_ONCE(1);\n+\t\t\terror = -EIO;\n+\t\t\tgoto out_unlock;\n+\t\t}\n+\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));\n+\t}\n+\n \t/* for DAX, we convert unwritten extents directly */\n \tif (create &&\n \t    (!nimaps ||\n@@ -1450,8 +1470,6 @@ __xfs_get_blocks(\n \t     (new || ISUNWRITTEN(&imap))))\n \t\tset_buffer_new(bh_result);\n \n-\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);\n-\n \treturn 0;\n \n out_unlock:""}","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
",C,"	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

","	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
",,"@@ -1361,6 +1361,26 @@ __xfs_get_blocks(
 	if (error)
 		goto out_unlock;
 
+	/*
+	 * The only time we can ever safely find delalloc blocks on direct I/O
+	 * is a dio write to post-eof speculative preallocation. All other
+	 * scenarios are indicative of a problem or misuse (such as mixing
+	 * direct and mapped I/O).
+	 *
+	 * The file may be unmapped by the time we get here so we cannot
+	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
+	 * is a read or a write within eof. Otherwise, carry on but warn as a
+	 * precuation if the file happens to be mapped.
+	 */
+	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
+		if (!create || offset < i_size_read(VFS_I(ip))) {
+			WARN_ON_ONCE(1);
+			error = -EIO;
+			goto out_unlock;
+		}
+		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
+	}
+
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
@@ -1450,8 +1470,6 @@ __xfs_get_blocks(
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
-	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
-
 	return 0;
 
 out_unlock:",linux,04197b341f23b908193308b8d63d17ff23232598,399372349a7f9b2d7e56e4fa4467c69822d07024,1,"__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * The only time we can ever safely find delalloc blocks on direct I/O
//fix_flaw_line_below:
//	 * is a dio write to post-eof speculative preallocation. All other
//fix_flaw_line_below:
//	 * scenarios are indicative of a problem or misuse (such as mixing
//fix_flaw_line_below:
//	 * direct and mapped I/O).
//fix_flaw_line_below:
//	 *
//fix_flaw_line_below:
//	 * The file may be unmapped by the time we get here so we cannot
//fix_flaw_line_below:
//	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
//fix_flaw_line_below:
//	 * is a read or a write within eof. Otherwise, carry on but warn as a
//fix_flaw_line_below:
//	 * precuation if the file happens to be mapped.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
//fix_flaw_line_below:
//		if (!create || offset < i_size_read(VFS_I(ip))) {
//fix_flaw_line_below:
//			WARN_ON_ONCE(1);
//fix_flaw_line_below:
//			error = -EIO;
//fix_flaw_line_below:
//			goto out_unlock;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
//flaw_line_below:
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
//flaw_line_below:

 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
",183036,"__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","The `__xfs_get_blocks` function is responsible for managing block allocation and mapping in the XFS filesystem, particularly for reading and writing data to files. It takes parameters such as an inode, block index, a buffer head for results, and flags indicating whether to create new blocks or handle direct I/O. The function first checks for forced shutdowns and calculates the appropriate offsets and sizes. It attempts to read the block mapping, handling copy-on-write scenarios if necessary. Depending on the conditions, it may allocate new blocks, trim mappings, and set buffer states, ensuring proper handling of unwritten extents and direct I/O requirements.","The provided code manages block allocation and mapping within a filesystem, specifically for reading and writing data to files. It checks for system shutdowns and calculates necessary offsets and sizes for data operations. The function attempts to read existing block mappings and handles scenarios where new blocks may need to be created, including special cases for direct input/output operations. It ensures proper management of unwritten data and adjusts buffer states accordingly. The code also accounts for specific file types and conditions, ensuring efficient data handling while maintaining filesystem integrity.","The vulnerability in the original code arises from a race condition between direct and memory-mapped I/O operations, which could lead to a system crash when accessing a file with a hole. Specifically, the use of `BUG_ON(create && !direct)` could trigger an assertion failure, causing a denial of service. In the fixed code, this issue is addressed by introducing checks that ensure proper handling of direct I/O scenarios, particularly when encountering delayed allocation blocks. The code now warns and returns an error instead of crashing, thus preventing the race condition from leading to a system failure.","The vulnerability in the original code stems from a race condition between direct and memory-mapped I/O operations, which could lead to a system crash when accessing files with unallocated space. This situation was inadequately managed, resulting in an assertion failure that caused a denial of service. The fixed code addresses this issue by implementing checks that properly handle scenarios involving delayed allocation blocks, ensuring that the system does not crash. Instead of triggering an assertion failure, the updated code now returns an error when encountering problematic conditions, thereby enhancing stability and preventing system crashes.","The vulnerability was fixed by modifying the handling of direct I/O operations when encountering delayed allocation blocks. In the updated code, a check was added to determine if the block is in a delayed state, using the condition `if (direct && imap.br_startblock == DELAYSTARTBLOCK)`. If this condition is met and the operation is not a creation or is within the file's existing size, the code now issues a warning and sets an error state instead of crashing. This change prevents the assertion failure caused by `BUG_ON(create && !direct)`, ensuring safer handling of race conditions between I/O types.","The vulnerability was fixed by implementing additional checks to manage the interaction between direct and memory-mapped I/O operations more safely. The updated code introduces conditions that assess the state of the data being accessed, particularly when dealing with unallocated or delayed blocks. Instead of allowing the system to crash due to an assertion failure, the code now generates warnings and returns error states when problematic scenarios are detected. This approach enhances the robustness of the system by preventing crashes and ensuring that I/O operations are handled gracefully, even in the presence of race conditions."
41,184595,184595,,Remote,Not required,Complete,CVE-2011-3961,https://www.cvedetails.com/cve/CVE-2011-3961/,CWE-362,Medium,Complete,Complete,,2012-02-08,9.3,Race condition in Google Chrome before 17.0.963.46 allows remote attackers to execute arbitrary code via vectors that trigger a crash of a utility process.,2017-09-18,Exec Code ,4,https://github.com/chromium/chromium/commit/b712795852f9d6073e062680e280634290c4ba5d,b712795852f9d6073e062680e280634290c4ba5d,"Fix uninitialized variables in HarfBuzzShaperBase
https://bugs.webkit.org/show_bug.cgi?id=79546

Reviewed by Dirk Pranke.

These were introduced in r108733.

* platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp:
(WebCore::HarfBuzzShaperBase::HarfBuzzShaperBase):

git-svn-id: svn://svn.chromium.org/blink/trunk@108871 bbb929c8-8fbe-4397-9dbb-9b2b20218538",0,third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp,"{""sha"": ""47cc92f4eecf9c468b793df4067b1908ccbcfe30"", ""filename"": ""third_party/WebKit/Source/WebCore/ChangeLog"", ""status"": ""modified"", ""additions"": 12, ""deletions"": 0, ""changes"": 12, ""blob_url"": ""https://github.com/chromium/chromium/blob/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/ChangeLog"", ""raw_url"": ""https://github.com/chromium/chromium/raw/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/ChangeLog"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/WebCore/ChangeLog?ref=b712795852f9d6073e062680e280634290c4ba5d"", ""patch"": ""@@ -1,3 +1,15 @@\n+2012-02-24  Adrienne Walker  <enne@google.com>\n+\n+        Fix uninitialized variables in HarfBuzzShaperBase\n+        https://bugs.webkit.org/show_bug.cgi?id=79546\n+\n+        Reviewed by Dirk Pranke.\n+\n+        These were introduced in r108733.\n+\n+        * platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp:\n+        (WebCore::HarfBuzzShaperBase::HarfBuzzShaperBase):\n+\n 2012-02-24  Noel Gordon  <noel.gordon@gmail.com>\n \n         [chromium] JPEG RGB image with Adode Marker fails to turbo swizzle decode""}<_**next**_>{""sha"": ""b8daff9e3a07c5118ba92b6d6a8096b93a2beccc"", ""filename"": ""third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 0, ""changes"": 4, ""blob_url"": ""https://github.com/chromium/chromium/blob/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""raw_url"": ""https://github.com/chromium/chromium/raw/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp?ref=b712795852f9d6073e062680e280634290c4ba5d"", ""patch"": ""@@ -42,8 +42,12 @@ namespace WebCore {\n \n HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)\n     : m_font(font)\n+    , m_normalizedBufferLength(0)\n     , m_run(run)\n     , m_wordSpacingAdjustment(font->wordSpacing())\n+    , m_padding(0)\n+    , m_padPerWordBreak(0)\n+    , m_padError(0)\n     , m_letterSpacing(font->letterSpacing())\n {\n }""}"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
     , m_letterSpacing(font->letterSpacing())
 {
 }
",C,"    , m_normalizedBufferLength(0)
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
",,,"@@ -42,8 +42,12 @@ namespace WebCore {
 
 HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
+    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
+    , m_padding(0)
+    , m_padPerWordBreak(0)
+    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }",Chrome,b712795852f9d6073e062680e280634290c4ba5d,617f382e9c9abab47e325c5d2e319524a5440faa,1," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
//fix_flaw_line_below:
//    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
//fix_flaw_line_below:
//    , m_padding(0)
//fix_flaw_line_below:
//    , m_padPerWordBreak(0)
//fix_flaw_line_below:
//    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
",184595," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
     , m_letterSpacing(font->letterSpacing())
 {
 }
"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
","The `HarfBuzzShaperBase` constructor initializes an instance of the `HarfBuzzShaperBase` class, which is designed to handle text shaping using the HarfBuzz library. It takes two parameters: a pointer to a `Font` object and a `TextRun` object. The constructor initializes member variables with the provided font and text run, while also retrieving and storing the font's word spacing and letter spacing adjustments. This setup is essential for accurately rendering text with the specified typographic properties, ensuring that the text is shaped correctly according to the font's characteristics and the given text run.","The provided code defines a constructor for a class that facilitates text shaping using a specific font and a sequence of text. It initializes the class by accepting a font object and a text run, which represent the visual characteristics and content of the text, respectively. During the initialization, it also retrieves and stores adjustments for spacing between words and letters as defined by the font. This setup is crucial for ensuring that the text is rendered accurately and aesthetically, taking into account the typographic properties associated with the chosen font and the specific text being processed.","The vulnerability in the original code arises from a race condition, where concurrent access to shared resources could lead to inconsistent states, particularly involving the `m_font` and `m_run` variables. This could allow an attacker to exploit timing issues, potentially leading to arbitrary code execution. In the fixed code, additional member variables such as `m_normalizedBufferLength`, `m_padding`, `m_padPerWordBreak`, and `m_padError` were introduced and initialized, which likely serve to manage and synchronize access to shared resources more effectively. This change helps mitigate the race condition by ensuring that the state of the object is more predictable and less susceptible to concurrent modification issues.","The vulnerability in the original code stems from a race condition, where multiple threads could access and modify shared resources simultaneously, leading to unpredictable behavior and potential exploitation by attackers. This could allow for arbitrary code execution if the timing of operations was manipulated. In the revised code, additional member variables were introduced and initialized, which likely serve to manage internal state more effectively and reduce the risk of concurrent access issues. By enhancing the structure and initialization of the object, the changes help ensure that the state remains consistent and less vulnerable to race conditions, thereby improving overall security.","The vulnerability was fixed by introducing and initializing additional member variables in the constructor, specifically `m_normalizedBufferLength`, `m_padding`, `m_padPerWordBreak`, and `m_padError`. These variables help manage the internal state of the object more effectively, providing a clearer structure for handling text shaping. By ensuring that these variables are set to known values upon initialization, the code reduces the likelihood of race conditions that could arise from concurrent access to shared resources. This enhancement improves synchronization and stability, thereby mitigating the risk of arbitrary code execution associated with the original race condition.","The vulnerability was addressed by enhancing the initialization process within the constructor, which involved adding new member variables to better manage the internal state of the object. This change ensures that all relevant properties are set to known values at the time of object creation, thereby reducing the potential for inconsistent states that could arise from concurrent access. By improving the structure and organization of the code, the fix mitigates the risk of race conditions, ultimately enhancing the stability and security of the application against potential exploitation."
42,184867,184867,,Remote,Not required,Partial,CVE-2013-2906,https://www.cvedetails.com/cve/CVE-2013-2906/,CWE-362,Medium,Partial,Partial,,2013-10-02,6.8,"Multiple race conditions in the Web Audio implementation in Blink, as used in Google Chrome before 30.0.1599.66, allow remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to threading in core/html/HTMLMediaElement.cpp, core/platform/audio/AudioDSPKernelProcessor.cpp, core/platform/audio/HRTFElevation.cpp, and modules/webaudio/ConvolverNode.cpp.",2017-09-18,DoS ,4,https://github.com/chromium/chromium/commit/c2364e0ce42878a2177c6f4cf7adb3c715b777c1,c2364e0ce42878a2177c6f4cf7adb3c715b777c1,"[OriginChip] Re-enable the chip as necessary when switching tabs.

BUG=369500

Review URL: https://codereview.chromium.org/292493003

git-svn-id: svn://svn.chromium.org/chrome/trunk/src@271161 0039d316-1c4b-4281-b951-d872f2087c98",1,chrome/browser/ui/omnibox/omnibox_edit_model.cc,"{""sha"": ""587dd72386e3a10288ba621f07704dd623f37872"", ""filename"": ""chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 1, ""changes"": 5, ""blob_url"": ""https://github.com/chromium/chromium/blob/c2364e0ce42878a2177c6f4cf7adb3c715b777c1/chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/c2364e0ce42878a2177c6f4cf7adb3c715b777c1/chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/chrome/browser/ui/omnibox/omnibox_edit_model.cc?ref=c2364e0ce42878a2177c6f4cf7adb3c715b777c1"", ""patch"": ""@@ -247,8 +247,11 @@ const OmniboxEditModel::State OmniboxEditModel::GetStateForTabSwitch() {\n void OmniboxEditModel::RestoreState(const State* state) {\n   // We need to update the permanent text correctly and revert the view\n   // regardless of whether there is saved state.\n+  bool url_replacement_enabled = !state || state->url_replacement_enabled;\n   controller_->GetToolbarModel()->set_url_replacement_enabled(\n-      !state || state->url_replacement_enabled);\n+      url_replacement_enabled);\n+  controller_->GetToolbarModel()->set_origin_chip_enabled(\n+      url_replacement_enabled);\n   permanent_text_ = controller_->GetToolbarModel()->GetText();\n   // Don't muck with the search term replacement state, as we've just set it\n   // correctly.""}"," void OmniboxEditModel::RestoreState(const State* state) {
  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
"," void OmniboxEditModel::RestoreState(const State* state) {
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      !state || state->url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
",C,"  bool url_replacement_enabled = !state || state->url_replacement_enabled;
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
","      !state || state->url_replacement_enabled);
",,"@@ -247,8 +247,11 @@ const OmniboxEditModel::State OmniboxEditModel::GetStateForTabSwitch() {
 void OmniboxEditModel::RestoreState(const State* state) {
   // We need to update the permanent text correctly and revert the view
   // regardless of whether there is saved state.
+  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
-      !state || state->url_replacement_enabled);
+      url_replacement_enabled);
+  controller_->GetToolbarModel()->set_origin_chip_enabled(
+      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
   // Don't muck with the search term replacement state, as we've just set it
   // correctly.",Chrome,c2364e0ce42878a2177c6f4cf7adb3c715b777c1,9016f5323611c8bb8d24f40cddeb38f75990fdc0,1," void OmniboxEditModel::RestoreState(const State* state) {
   // We need to update the permanent text correctly and revert the view
   // regardless of whether there is saved state.
//fix_flaw_line_below:
//  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
//flaw_line_below:
      !state || state->url_replacement_enabled);
//fix_flaw_line_below:
//      url_replacement_enabled);
//fix_flaw_line_below:
//  controller_->GetToolbarModel()->set_origin_chip_enabled(
//fix_flaw_line_below:
//      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
   // Don't muck with the search term replacement state, as we've just set it
   // correctly.
  view_->RevertWithoutResettingSearchTermReplacement();
  // Restore the autocomplete controller's input, or clear it if this is a new
  // tab.
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  // Restore any user editing.
  if (state->user_input_in_progress) {
    // NOTE: Be sure and set keyword-related state BEFORE invoking
    // DisplayTextFromUserText(), as its result depends upon this state.
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
",184867," void OmniboxEditModel::RestoreState(const State* state) {
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      !state || state->url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
"," void OmniboxEditModel::RestoreState(const State* state) {
  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
","The `RestoreState` function in the `OmniboxEditModel` class is designed to restore the state of the omnibox based on a provided `State` object. It first checks if the state is valid to determine whether to enable URL replacement. The function then retrieves and sets the current text from the toolbar model, reverts the view without resetting any search term replacements, and initializes the input with autocomplete data. If the state is valid, it updates the focus state and source, and if user input is in progress, it sets the keyword, hint status, and updates the view with the user's text and associated gray text for autocompletion.","The function is responsible for restoring the state of a user interface component based on a given state object. It determines whether to enable a specific feature based on the validity of the state and retrieves the current text from a model. The function then reverts the view to a previous state without altering any search terms and initializes input data accordingly. If the state is valid, it updates the focus and source of user interaction. Additionally, if there is ongoing user input, it sets relevant parameters and updates the display with the user's text and associated suggestions for better usability.","The vulnerability, identified as CWE-362, arises from race conditions in the original code, where the state of URL replacement could be modified concurrently by multiple threads, leading to unpredictable behavior. Specifically, the lack of synchronization when accessing the `url_replacement_enabled` state could allow one thread to change this value while another thread is reading it, potentially causing a denial of service or other unintended effects. The fixed code addresses this issue by introducing a local boolean variable, `url_replacement_enabled`, to store the state before it is used to set both the URL replacement and origin chip states, ensuring consistent access and reducing the risk of race conditions.","The vulnerability, classified as CWE-362, stems from race conditions that occur when multiple threads access and modify shared state variables without proper synchronization, leading to unpredictable behavior and potential denial of service. In the original code, the state related to URL replacement could be altered by one thread while another thread was reading it, resulting in inconsistent behavior. The fix introduces a local variable to store the state before it is used, ensuring that all subsequent operations reference a consistent value. This change mitigates the risk of concurrent modifications, thereby enhancing the stability and security of the code.","The vulnerability was fixed by introducing a local boolean variable, `url_replacement_enabled`, to store the state of URL replacement before it is used in subsequent operations. This change ensures that both the `set_url_replacement_enabled` and `set_origin_chip_enabled` functions access a consistent value, preventing race conditions that could arise from concurrent modifications. By isolating the state retrieval into a single variable, the code eliminates the risk of one thread altering the state while another is reading it, thereby enhancing the overall stability and security of the application.","The vulnerability was fixed by introducing a local variable to store the state of a critical feature before it is used in subsequent operations. This approach ensures that all related functions access a consistent value, effectively preventing race conditions that could occur due to concurrent modifications by multiple threads. By isolating the state retrieval into a single variable, the code mitigates the risk of unpredictable behavior, enhancing the stability and security of the application. This change allows for safer access to shared resources, ensuring that the application behaves as intended even under multi-threaded conditions."
43,184868,184868,,Remote,Not required,Partial,CVE-2013-2906,https://www.cvedetails.com/cve/CVE-2013-2906/,CWE-362,Medium,Partial,Partial,,2013-10-02,6.8,"Multiple race conditions in the Web Audio implementation in Blink, as used in Google Chrome before 30.0.1599.66, allow remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to threading in core/html/HTMLMediaElement.cpp, core/platform/audio/AudioDSPKernelProcessor.cpp, core/platform/audio/HRTFElevation.cpp, and modules/webaudio/ConvolverNode.cpp.",2017-09-18,DoS ,5,https://github.com/chromium/chromium/commit/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,"Suspend shared timers while blockingly closing databases

BUG=388771
R=michaeln@chromium.org

Review URL: https://codereview.chromium.org/409863002

git-svn-id: svn://svn.chromium.org/chrome/trunk/src@284785 0039d316-1c4b-4281-b951-d872f2087c98",0,content/renderer/render_thread_impl.cc,"{""sha"": ""5116fac7442aeafd9f36622a135f8b2669235f42"", ""filename"": ""content/renderer/render_thread_impl.cc"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 0, ""changes"": 5, ""blob_url"": ""https://github.com/chromium/chromium/blob/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2/content/renderer/render_thread_impl.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2/content/renderer/render_thread_impl.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/renderer/render_thread_impl.cc?ref=c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2"", ""patch"": ""@@ -574,8 +574,13 @@ void RenderThreadImpl::Shutdown() {\n \n   // Wait for all databases to be closed.\n   if (webkit_platform_support_) {\n+    // WaitForAllDatabasesToClose might run a nested message loop. To avoid\n+    // processing timer events while we're already in the process of shutting\n+    // down blink, put a ScopePageLoadDeferrer on the stack.\n+    WebView::willEnterModalLoop();\n     webkit_platform_support_->web_database_observer_impl()->\n         WaitForAllDatabasesToClose();\n+    WebView::didExitModalLoop();\n   }\n \n   // Shutdown in reverse of the initialization order.""}","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
    WebView::didExitModalLoop();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
",C,"    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
    WebView::didExitModalLoop();
",,,"@@ -574,8 +574,13 @@ void RenderThreadImpl::Shutdown() {
 
   // Wait for all databases to be closed.
   if (webkit_platform_support_) {
+    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
+    // processing timer events while we're already in the process of shutting
+    // down blink, put a ScopePageLoadDeferrer on the stack.
+    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
+    WebView::didExitModalLoop();
   }
 
   // Shutdown in reverse of the initialization order.",Chrome,c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,6e44c30f2dc7371711503b0a942da01180206c47,1,"void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   // Wait for all databases to be closed.
   if (webkit_platform_support_) {
//fix_flaw_line_below:
//    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
//fix_flaw_line_below:
//    // processing timer events while we're already in the process of shutting
//fix_flaw_line_below:
//    // down blink, put a ScopePageLoadDeferrer on the stack.
//fix_flaw_line_below:
//    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
//fix_flaw_line_below:
//    WebView::didExitModalLoop();
   }
 
   // Shutdown in reverse of the initialization order.
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  // Shutdown the file thread if it's running.
  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  // RemoveEmbeddedWorkerRoute may be called while deleting
  // EmbeddedWorkerDispatcher. So it must be deleted before deleting
  // RenderThreadImpl.
  embedded_worker_dispatcher_.reset();

  // Ramp down IDB before we ramp down WebKit (and V8), since IDB classes might
  // hold pointers to V8 objects (e.g., via pending requests).
  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

  // TODO(port)
#if defined(OS_WIN)
  // Clean up plugin channels before this thread goes away.
  NPChannelBase::CleanupChannels();
#endif
}
",184868,"void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
    WebView::didExitModalLoop();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","The `RenderThreadImpl::Shutdown` function is responsible for gracefully shutting down the rendering process in a multi-threaded environment. It begins by notifying all registered observers of the shutdown event, then proceeds to clean up various components, including removing task observers, filters, and resetting pointers to ensure proper resource management. The function also handles specific platform-related tasks, such as waiting for databases to close and cleaning up WebRTC handlers if enabled. Additionally, it stops associated threads, including file and media threads, and performs final cleanup operations, ensuring that all resources are released and no memory leaks occur.","The function is designed to safely terminate a rendering process by executing a series of cleanup operations. It begins by notifying all observers of the impending shutdown, followed by the removal of various task observers and filters to prevent further processing. The function ensures that all associated threads are stopped and resources are released, including database connections and media handlers. Additionally, it performs platform-specific cleanup tasks and resets pointers to avoid memory leaks. Overall, the function ensures a thorough and orderly shutdown of the rendering environment, maintaining system stability and resource integrity.","The vulnerability, identified as CWE-362, arises from race conditions in the rendering process, particularly during the shutdown sequence where multiple threads could access shared resources simultaneously. In the vulnerable code, the `WaitForAllDatabasesToClose` function could potentially run a nested message loop, allowing timer events to be processed while the shutdown was in progress, leading to unpredictable behavior. The fix introduces a mechanism to defer page load events by using `WebView::willEnterModalLoop()` and `WebView::didExitModalLoop()`, ensuring that no timer events are processed during the critical shutdown phase, thus preventing race conditions and enhancing stability.","The vulnerability, classified as CWE-362, involves race conditions that occur during the shutdown process of a rendering system, where multiple threads may concurrently access shared resources. This can lead to unpredictable behavior, including potential denial of service, as the system may process events while critical shutdown operations are underway. The fix addresses this issue by implementing a mechanism to defer event processing during the shutdown sequence, ensuring that no conflicting operations occur while resources are being released. This change effectively prevents race conditions, enhancing the stability and security of the rendering process.","The vulnerability was fixed by introducing a mechanism to defer event processing during the shutdown sequence. Specifically, the code now utilizes `WebView::willEnterModalLoop()` before calling `WaitForAllDatabasesToClose()` and `WebView::didExitModalLoop()` afterward. This ensures that while the system is waiting for all databases to close, no timer events or other processing can occur, effectively preventing race conditions. By controlling the flow of event handling during this critical phase, the fix enhances the stability of the shutdown process and mitigates the risk of denial of service or other unintended impacts.","The vulnerability was addressed by implementing a mechanism to temporarily suspend event processing during the shutdown sequence. This was achieved by introducing a deferral approach that prevents any conflicting operations from occurring while critical resources are being released. By ensuring that no events are processed during the wait for essential tasks to complete, the fix effectively eliminates the potential for race conditions. This change enhances the overall stability of the shutdown process, thereby reducing the risk of denial of service and other unintended consequences associated with concurrent access to shared resources."
44,185106,185106,,Remote,Not required,Partial,CVE-2013-0893,https://www.cvedetails.com/cve/CVE-2013-0893/,CWE-362,Medium,Partial,Partial,,2013-02-23,6.8,"Race condition in Google Chrome before 25.0.1364.97 on Windows and Linux, and before 25.0.1364.99 on Mac OS X, allows remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to media.",2018-10-30,DoS ,6,https://github.com/chromium/chromium/commit/ed6f4545a2a345697e07908c887333f5bdcc97a3,ed6f4545a2a345697e07908c887333f5bdcc97a3,"Apply 'x-content-type-options' check to dynamically inserted script.

BUG=348581

Review URL: https://codereview.chromium.org/185593011

git-svn-id: svn://svn.chromium.org/blink/trunk@168570 bbb929c8-8fbe-4397-9dbb-9b2b20218538",3,third_party/WebKit/Source/core/dom/ScriptLoader.cpp,"{""sha"": ""dafdb9606e69e9b0318994a254c8b73622ff304d"", ""filename"": ""third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""status"": ""added"", ""additions"": 11, ""deletions"": 0, ""changes"": 11, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -0,0 +1,11 @@\n+CONSOLE ERROR: Refused to execute script from 'http://127.0.0.1:8000/security/contentTypeOptions/resources/script-with-header.pl?mime=application/json' because its MIME type ('application/json') is not executable, and strict MIME type checking is enabled.\n+Check that script sent with an 'X-Content-Type-Options: nosniff' header is correctly blocked if the MIME type isn't scripty.\n+\n+On success, you will see a series of \""PASS\"" messages, followed by \""TEST COMPLETE\"".\n+\n+\n+PASS window.scriptsSuccessfullyLoaded is 0\n+PASS successfullyParsed is true\n+\n+TEST COMPLETE\n+""}<_**next**_>{""sha"": ""6d0c62d8e7c1e4cf8369e76e93c00a08b7fa755b"", ""filename"": ""third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""status"": ""added"", ""additions"": 23, ""deletions"": 0, ""changes"": 23, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -0,0 +1,23 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+    <title>'X-Content-Type-Options: nosniff;' blocks scripts!</title>\n+<body>\n+    <script src=\""/js-test-resources/js-test.js\""></script>\n+    <script>\n+        description('Check that script sent with an \\'X-Content-Type-Options: nosniff\\' header is correctly blocked if the MIME type isn\\'t scripty.');\n+        window.jsTestIsAsync = true;\n+\n+        window.scriptsSuccessfullyLoaded = 0;\n+\n+        var s = document.createElement('script');\n+        s.src = './resources/script-with-header.pl?mime=application/json';\n+        document.querySelector('head').appendChild(s);\n+\n+        window.onload = function () {\n+            shouldBe('window.scriptsSuccessfullyLoaded', '0');\n+            finishJSTest();\n+        };\n+    </script>\n+</body>\n+</html>""}<_**next**_>{""sha"": ""0ee2e25f4d5c4590e7c67948543e1b56ead8b167"", ""filename"": ""third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 3, ""changes"": 9, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/core/dom/ScriptLoader.cpp?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -308,9 +308,12 @@ void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)\n     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))\n         return;\n \n-    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {\n-        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, \""Refused to execute script from '\"" + m_resource->url().elidedString() + \""' because its MIME type ('\"" + m_resource->mimeType() + \""') is not executable, and strict MIME type checking is enabled.\"");\n-        return;\n+    if (m_isExternalScript) {\n+        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();\n+        if (resource && !resource->mimeTypeAllowedByNosniff()) {\n+            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, \""Refused to execute script from '\"" + resource->url().elidedString() + \""' because its MIME type ('\"" + resource->mimeType() + \""') is not executable, and strict MIME type checking is enabled.\"");\n+            return;\n+        }\n     }\n \n     if (frame) {""}","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
",C,"    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
","    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
",,"@@ -308,9 +308,12 @@ void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
-    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
-        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
-        return;
+    if (m_isExternalScript) {
+        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
+        if (resource && !resource->mimeTypeAllowedByNosniff()) {
+            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
+            return;
+        }
     }
 
     if (frame) {",Chrome,ed6f4545a2a345697e07908c887333f5bdcc97a3,2c29c343bc7bff16fb552abf162d17d26a9d9563,1,"void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
//flaw_line_below:
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
//flaw_line_below:
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
//flaw_line_below:
        return;
//fix_flaw_line_below:
//    if (m_isExternalScript) {
//fix_flaw_line_below:
//        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
//fix_flaw_line_below:
//        if (resource && !resource->mimeTypeAllowedByNosniff()) {
//fix_flaw_line_below:
//            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
//fix_flaw_line_below:
//            return;
//fix_flaw_line_below:
//        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        // http://www.whatwg.org/specs/web-apps/current-work/#execute-the-script-block step 2.3
        // with additional support for HTML imports.
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        // Create a script from the script element node, using the script
        // block's source and the script block's type.
        // Note: This is where the script is compiled and actually executed.
        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
",185106,"void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","The `executeScript` function in the `ScriptLoader` class is responsible for executing a given script represented by `sourceCode`. It first checks if the script execution has already started and if the source code is not empty. The function retrieves the document context and frame, then evaluates security policies, including content security policy (CSP) checks for inline and external scripts. If the script passes these checks, it executes the script in the main world of the frame, handling cross-origin resource sharing (CORS) appropriately. Additionally, it manages the current script context for HTML script elements, ensuring proper execution flow and security compliance.","The provided code defines a function that executes a script if certain conditions are met. It first verifies that the script execution process has started and that the script source is not empty. The function checks the document's security policies to determine if the script can be executed, considering both inline and external scripts. If the script passes these security checks, it is executed within the main context of the document's frame, while also managing cross-origin resource sharing. The function ensures that the current script context is correctly maintained throughout the execution process.","The vulnerability in the original code arises from a race condition where the script execution could be manipulated by an attacker, potentially leading to a denial of service or other impacts. This occurs because the code does not adequately synchronize access to shared resources, allowing for unexpected behavior during script execution. In the fixed code, the introduction of a check for the MIME type of the resource, along with the use of `IgnoreDestructiveWriteCountIncrementer`, ensures that the context is properly managed and that only scripts with valid MIME types are executed. This mitigates the risk of race conditions by enforcing stricter checks before executing scripts.","The vulnerability in the original code stems from a race condition that allows attackers to exploit timing issues during script execution, potentially leading to denial of service or other unintended consequences. This occurs when multiple threads or processes can access shared resources without proper synchronization, resulting in unpredictable behavior. The fixed code addresses this vulnerability by implementing stricter checks on the execution context and ensuring that only scripts with valid attributes are processed. Additionally, it introduces mechanisms to manage the execution flow more securely, thereby reducing the risk of race conditions and enhancing overall stability.","The vulnerability was fixed by enhancing the security checks related to script execution. In the updated code, the check for the MIME type of the resource was added, ensuring that only scripts with allowed MIME types are executed, which is done through the `mimeTypeAllowedByNosniff()` function. Additionally, the use of `IgnoreDestructiveWriteCountIncrementer` helps manage the execution context more effectively, preventing concurrent modifications that could lead to race conditions. By enforcing these checks and managing the script context with `pushCurrentScript` and `popCurrentScript`, the code ensures a more secure and stable execution environment.","The vulnerability was fixed by implementing stricter security checks before executing scripts, which helps prevent race conditions. The updated code ensures that only scripts with valid attributes and MIME types are processed, thereby reducing the risk of executing potentially harmful scripts. Additionally, mechanisms were introduced to manage the execution context more effectively, ensuring that concurrent modifications do not occur during script execution. This combination of enhanced validation and improved context management creates a more secure environment, mitigating the potential for exploitation and ensuring stable script execution."
45,186239,186239,,Remote,Not required,,CVE-2016-1670,https://www.cvedetails.com/cve/CVE-2016-1670/,CWE-362,High,,Partial,,2016-05-14,2.6,Race condition in the ResourceDispatcherHostImpl::BeginRequest function in content/browser/loader/resource_dispatcher_host_impl.cc in Google Chrome before 50.0.2661.102 allows remote attackers to make arbitrary HTTP requests by leveraging access to a renderer process and reusing a request ID.,2018-10-30,,7,https://github.com/chromium/chromium/commit/1af4fada49c4f3890f16daac31d38379a9d782b2,1af4fada49c4f3890f16daac31d38379a9d782b2,"Block a compromised renderer from reusing request ids.

BUG=578882

Review URL: https://codereview.chromium.org/1608573002

Cr-Commit-Position: refs/heads/master@{#372547}",0,content/browser/loader/resource_dispatcher_host_impl.cc,"{""sha"": ""8f0883073c4c5df0d87ebaef4685c987255b7f9a"", ""filename"": ""content/browser/bad_message.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/bad_message.h"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/bad_message.h"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/bad_message.h?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -129,6 +129,7 @@ enum BadMessageReason {\n   BDH_DEVICE_NOT_ALLOWED_FOR_ORIGIN = 105,\n   ACI_WRONG_STORAGE_PARTITION = 106,\n   RDHI_WRONG_STORAGE_PARTITION = 107,\n+  RDH_INVALID_REQUEST_ID = 108,\n \n   // Please add new elements here. The naming convention is abbreviated class\n   // name (e.g. RenderFrameHost becomes RFH) plus a unique description of the""}<_**next**_>{""sha"": ""1db3d98e157c29ae3455281fdbd72dbf65d12d4a"", ""filename"": ""content/browser/loader/resource_dispatcher_host_impl.cc"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/loader/resource_dispatcher_host_impl.cc?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -1187,6 +1187,20 @@ void ResourceDispatcherHostImpl::OnSyncLoad(\n                sync_result->routing_id());\n }\n \n+bool ResourceDispatcherHostImpl::IsRequestIDInUse(\n+    const GlobalRequestID& id) const {\n+  if (pending_loaders_.find(id) != pending_loaders_.end())\n+    return true;\n+  for (const auto& blocked_loaders : blocked_loaders_map_) {\n+    for (const auto& loader : *blocked_loaders.second.get()) {\n+      ResourceRequestInfoImpl* info = loader->GetRequestInfo();\n+      if (info->GetGlobalRequestID() == id)\n+        return true;\n+    }\n+  }\n+  return false;\n+}\n+\n void ResourceDispatcherHostImpl::UpdateRequestForTransfer(\n     int child_id,\n     int route_id,\n@@ -1281,6 +1295,13 @@ void ResourceDispatcherHostImpl::BeginRequest(\n   int process_type = filter_->process_type();\n   int child_id = filter_->child_id();\n \n+  // Reject request id that's currently in use.\n+  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {\n+    bad_message::ReceivedBadMessage(filter_,\n+                                    bad_message::RDH_INVALID_REQUEST_ID);\n+    return;\n+  }\n+\n   // PlzNavigate: reject invalid renderer main resource request.\n   if (IsBrowserSideNavigationEnabled() &&\n       IsResourceTypeFrame(request_data.resource_type) &&""}<_**next**_>{""sha"": ""29a82b248a109c7890fec7e0c2f5b6d2ff717d4c"", ""filename"": ""content/browser/loader/resource_dispatcher_host_impl.h"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 0, ""changes"": 2, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.h"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.h"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/loader/resource_dispatcher_host_impl.h?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -422,6 +422,8 @@ class CONTENT_EXPORT ResourceDispatcherHostImpl\n                   const ResourceHostMsg_Request& request_data,\n                   IPC::Message* sync_result);\n \n+  bool IsRequestIDInUse(const GlobalRequestID& id) const;\n+\n   // Update the ResourceRequestInfo and internal maps when a request is\n   // transferred from one process to another.\n   void UpdateRequestForTransfer(int child_id,""}<_**next**_>{""sha"": ""6714e9a086a737607f6947d6fdf8d1608742ee9e"", ""filename"": ""content/browser/security_exploit_browsertest.cc"", ""status"": ""modified"", ""additions"": 75, ""deletions"": 8, ""changes"": 83, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/security_exploit_browsertest.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/security_exploit_browsertest.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/security_exploit_browsertest.cc?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -23,6 +23,7 @@\n #include \""content/public/browser/content_browser_client.h\""\n #include \""content/public/browser/interstitial_page.h\""\n #include \""content/public/browser/interstitial_page_delegate.h\""\n+#include \""content/public/browser/resource_dispatcher_host.h\""\n #include \""content/public/browser/storage_partition.h\""\n #include \""content/public/common/appcache_info.h\""\n #include \""content/public/common/browser_side_navigation_policy.h\""\n@@ -37,13 +38,19 @@\n #include \""ipc/ipc_security_test_util.h\""\n #include \""net/dns/mock_host_resolver.h\""\n #include \""net/test/embedded_test_server/embedded_test_server.h\""\n+#include \""net/test/url_request/url_request_slow_download_job.h\""\n \n using IPC::IpcSecurityTestUtil;\n \n namespace content {\n \n namespace {\n \n+// This request id is used by tests that craft a\n+// ResourceHostMsg_RequestResource. The id is sufficiently large that it doesn't\n+// collide with ids used by previous navigation requests.\n+const int kRequestIdNotPreviouslyUsed = 10000;\n+\n // This is a helper function for the tests which attempt to create a\n // duplicate RenderViewHost or RenderWidgetHost. It tries to create two objects\n // with the same process and routing ids, which causes a collision.\n@@ -98,13 +105,11 @@ RenderViewHostImpl* PrepareToDuplicateHosts(Shell* shell,\n   return next_rfh->render_view_host();\n }\n \n-ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n+ResourceHostMsg_Request CreateXHRRequest(const char* url) {\n   ResourceHostMsg_Request request;\n   request.method = \""GET\"";\n-  request.url = GURL(\""http://bar.com/simple_page.html\"");\n-  request.first_party_for_cookies = GURL(origin);\n+  request.url = GURL(url);\n   request.referrer_policy = blink::WebReferrerPolicyDefault;\n-  request.headers = base::StringPrintf(\""Origin: %s\\r\\n\"", origin);\n   request.load_flags = 0;\n   request.origin_pid = 0;\n   request.resource_type = RESOURCE_TYPE_XHR;\n@@ -120,6 +125,49 @@ ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n   return request;\n }\n \n+ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n+  ResourceHostMsg_Request request =\n+      CreateXHRRequest(\""http://bar.com/simple_page.html\"");\n+  request.first_party_for_cookies = GURL(origin);\n+  request.headers = base::StringPrintf(\""Origin: %s\\r\\n\"", origin);\n+  return request;\n+}\n+\n+void TryCreateDuplicateRequestIds(Shell* shell, bool block_loaders) {\n+  NavigateToURL(shell, GURL(\""http://foo.com/simple_page.html\""));\n+  RenderFrameHost* rfh = shell->web_contents()->GetMainFrame();\n+\n+  if (block_loaders) {\n+    // Test the case where loaders are placed into blocked_loaders_map_.\n+    int child_id = rfh->GetProcess()->GetID();\n+    BrowserThread::PostTask(\n+        BrowserThread::IO, FROM_HERE,\n+        base::Bind(&ResourceDispatcherHost::BlockRequestsForRoute,\n+                   base::Unretained(ResourceDispatcherHost::Get()), child_id,\n+                   rfh->GetRoutingID()));\n+  }\n+\n+  // URLRequestSlowDownloadJob waits for another request to kFinishDownloadUrl\n+  // to finish all pending requests. It is never sent, so the following URL\n+  // blocks indefinitely, which is good because the request stays alive and the\n+  // test can try to reuse the request id without a race.\n+  const char* blocking_url = net::URLRequestSlowDownloadJob::kUnknownSizeUrl;\n+  ResourceHostMsg_Request request(CreateXHRRequest(blocking_url));\n+\n+  // Use the same request id twice.\n+  RenderProcessHostWatcher process_killed(\n+      rfh->GetProcess(), RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n+  IPC::IpcSecurityTestUtil::PwnMessageReceived(\n+      rfh->GetProcess()->GetChannel(),\n+      ResourceHostMsg_RequestResource(rfh->GetRoutingID(),\n+                                      kRequestIdNotPreviouslyUsed, request));\n+  IPC::IpcSecurityTestUtil::PwnMessageReceived(\n+      rfh->GetProcess()->GetChannel(),\n+      ResourceHostMsg_RequestResource(rfh->GetRoutingID(),\n+                                      kRequestIdNotPreviouslyUsed, request));\n+  process_killed.Wait();\n+}\n+\n }  // namespace\n \n \n@@ -145,6 +193,12 @@ class SecurityExploitBrowserTest : public ContentBrowserTest {\n             \"",EXCLUDE localhost\"");\n   }\n \n+  void SetUpOnMainThread() override {\n+    BrowserThread::PostTask(\n+        BrowserThread::IO, FROM_HERE,\n+        base::Bind(&net::URLRequestSlowDownloadJob::AddUrlHandler));\n+  }\n+\n  protected:\n   // Tests that a given file path sent in a ViewHostMsg_RunFileChooser will\n   // cause renderer to be killed.\n@@ -384,7 +438,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         chrome_origin_msg));\n     web_process_killed.Wait();\n   }\n@@ -404,7 +459,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         embedder_isolated_origin_msg));\n     web_process_killed.Wait();\n     SetBrowserClientForTesting(old_client);\n@@ -418,7 +474,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         invalid_origin_msg));\n     web_process_killed.Wait();\n   }\n@@ -431,10 +488,20 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         invalid_scheme_origin_msg));\n     web_process_killed.Wait();\n   }\n }\n \n+// Renderer process should not be able to create multiple requests with the same\n+// id.\n+IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidRequestId) {\n+  // Existing loader in pending_loaders_.\n+  TryCreateDuplicateRequestIds(shell(), false);\n+  // Existing loader in blocked_loaders_map_.\n+  TryCreateDuplicateRequestIds(shell(), true);\n+}\n+\n }  // namespace content""}<_**next**_>{""sha"": ""9f330134f0b052a4f6a8c6aa85e90c8230edaae6"", ""filename"": ""tools/metrics/histograms/histograms.xml"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/tools/metrics/histograms/histograms.xml"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/tools/metrics/histograms/histograms.xml"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/tools/metrics/histograms/histograms.xml?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -57944,6 +57944,7 @@ http://cs/file:chrome/histograms.xml - but prefer this file for new entries.\n   <int value=\""105\"" label=\""BDH_DEVICE_NOT_ALLOWED_FOR_ORIGIN\""/>\n   <int value=\""106\"" label=\""ACI_WRONG_STORAGE_PARTITION\""/>\n   <int value=\""107\"" label=\""RDHI_WRONG_STORAGE_PARTITION\""/>\n+  <int value=\""108\"" label=\""RDH_INVALID_REQUEST_ID\""/>\n </enum>\n \n <enum name=\""BadMessageReasonExtensions\"" type=\""int\"">""}","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
",C,"  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

",,,"@@ -1187,6 +1187,20 @@ void ResourceDispatcherHostImpl::OnSyncLoad(
                sync_result->routing_id());
 }
 
+bool ResourceDispatcherHostImpl::IsRequestIDInUse(
+    const GlobalRequestID& id) const {
+  if (pending_loaders_.find(id) != pending_loaders_.end())
+    return true;
+  for (const auto& blocked_loaders : blocked_loaders_map_) {
+    for (const auto& loader : *blocked_loaders.second.get()) {
+      ResourceRequestInfoImpl* info = loader->GetRequestInfo();
+      if (info->GetGlobalRequestID() == id)
+        return true;
+    }
+  }
+  return false;
+}
+
 void ResourceDispatcherHostImpl::UpdateRequestForTransfer(
     int child_id,
     int route_id,
@@ -1281,6 +1295,13 @@ void ResourceDispatcherHostImpl::BeginRequest(
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
+  // Reject request id that's currently in use.
+  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
+    bad_message::ReceivedBadMessage(filter_,
+                                    bad_message::RDH_INVALID_REQUEST_ID);
+    return;
+  }
+
   // PlzNavigate: reject invalid renderer main resource request.
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&",Chrome,1af4fada49c4f3890f16daac31d38379a9d782b2,f5d1a9c599c7cd6833455eacb4ec694a47c1f1b0,1,"void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
//fix_flaw_line_below:
//  // Reject request id that's currently in use.
//fix_flaw_line_below:
//  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
//fix_flaw_line_below:
//    bad_message::ReceivedBadMessage(filter_,
//fix_flaw_line_below:
//                                    bad_message::RDH_INVALID_REQUEST_ID);
//fix_flaw_line_below:
//    return;
//fix_flaw_line_below:
//  }
//fix_flaw_line_below:
//
   // PlzNavigate: reject invalid renderer main resource request.
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  // Reject invalid priority.
  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  // If we crash here, figure out what URL the renderer was requesting.
  // http://crbug.com/91398
  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  // If the request that's coming in is being transferred from another process,
  // we want to reuse and resume the old loader rather than start a new one.
  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    // If the request is transferring to a new process, we can update our
    // state and let it resume with its existing ResourceHandlers.
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  // http://crbug.com/90971
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  // Parse the headers before calling ShouldServiceRequest, so that they are
  // available to be validated.
  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  // Allow the observer to block/handle the request.
  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  // Construct the request.
  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  // If the request is a MAIN_FRAME request, the first-party URL gets updated on
  // redirects.
  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  // Resolve elements from request_body and prepare upload data.
  if (request_data.request_body.get()) {
    // |blob_context| could be null when the request is from the plugins because
    // ResourceMessageFilters created in PluginProcessHost don't have the blob
    // context.
    if (blob_context) {
      // Attaches the BlobDataHandles to request_body not to free the blobs and
      // any attached shareable files until upload completion. These data will
      // be used in UploadDataStream and ServiceWorkerURLRequestJob.
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  // Raw headers are sensitive, as they include Cookie/Set-Cookie, so only
  // allow requesting them if requester has ReadRawCookies permission.
  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    // TODO: crbug.com/523063 can we call bad_message::ReceivedBadMessage here?
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    // Prevent third-party image content from prompting for login, as this
    // is often a scam to extract credentials for another domain from the user.
    // Only block image loads, as the attack applies largely to the ""src""
    // property of the <img> tag. It is common for web properties to allow
    // untrusted values for <img src>; this is considered a fair thing for an
    // HTML sanitizer to do. Conversely, any HTML sanitizer that didn't
    // filter sources for <script>, <link>, <embed>, <object>, <iframe> tags
    // would be considered vulnerable in and of itself.
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  // Sync loads should have maximum priority and should be the only
  // requets that have the ignore limits flag set.
  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  // Make extra info and read footer (contains request ID).
  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  // Request takes ownership.
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    // Hang on to a reference to ensure the blob is not released prior
    // to the job being started.
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  // Initialize the service worker handler for the request. We don't use
  // ServiceWorker for synchronous loads to avoid renderer deadlocks.
  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  // Have the appcache associate its extra info with the request.
  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
",186239,"void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","The `BeginRequest` function in the `ResourceDispatcherHostImpl` class initiates a resource request by validating input parameters, managing request priorities, and handling potential transfers of existing requests. It checks for browser-side navigation and URL validity, ensuring that the request adheres to specified priority limits. The function constructs a new `URLRequest` object, setting various properties such as method, headers, and upload data. It also manages security policies, load flags, and service worker interactions. Finally, it creates a resource handler and invokes an internal method to process the request, ensuring proper handling of asynchronous and synchronous requests.","The provided code defines a method that initiates a resource request by validating various parameters, including the request's URL and priority. It checks for specific conditions related to navigation and resource types, ensuring compliance with security policies. The method constructs a new request, setting its properties based on the input data, and manages potential transfers of existing requests. It also handles interactions with service workers and applies appropriate load flags. Finally, the method creates a resource handler to process the request, facilitating both synchronous and asynchronous operations while ensuring proper request management.","The vulnerability in the original code stemmed from a race condition that allowed an attacker to reuse a request ID, potentially leading to arbitrary HTTP requests from a renderer process. This was due to the absence of a check to reject request IDs that were already in use. The fixed code addresses this issue by introducing a validation step that checks if the request ID is currently in use using the `IsRequestIDInUse` function. If the ID is found to be in use, the method responds with an error message, effectively preventing the exploitation of the race condition and enhancing the security of the request handling process.","The vulnerability in the original code was a race condition that allowed attackers to exploit the reuse of request IDs, enabling them to make arbitrary HTTP requests from a renderer process. This occurred because the code did not verify whether a request ID was already in use, leading to potential unauthorized access. The fixed code addresses this issue by implementing a validation check to reject any request IDs that are currently in use. This change effectively prevents the exploitation of the race condition, thereby enhancing the security of the request handling mechanism and safeguarding against unauthorized requests.","The vulnerability was fixed by adding a validation check at the beginning of the `BeginRequest` function to determine if the request ID is currently in use. This is achieved through the `IsRequestIDInUse` function, which checks the status of the request ID constructed with `GlobalRequestID`. If the request ID is found to be in use, the code responds by calling `bad_message::ReceivedBadMessage` with an appropriate error message, effectively preventing further processing of the request. This change ensures that only unique request IDs are processed, mitigating the risk of arbitrary HTTP requests.","The vulnerability was fixed by introducing a validation step that checks whether a request ID is already in use before processing a new request. This additional check ensures that any attempt to reuse an existing request ID is promptly rejected, preventing unauthorized access and arbitrary HTTP requests. If the request ID is found to be in use, the system responds with an error message, halting further processing of that request. This enhancement effectively mitigates the race condition, thereby strengthening the security of the request handling mechanism and protecting against potential exploitation."
