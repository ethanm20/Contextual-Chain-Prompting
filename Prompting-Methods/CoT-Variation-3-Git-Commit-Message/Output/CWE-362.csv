,Unnamed: 0.1,Unnamed: 0,Access Gained,Attack Origin,Authentication Required,Availability,CVE ID,CVE Page,CWE ID,Complexity,Confidentiality,Integrity,Known Exploits,Publish Date,Score,Summary,Update Date,Vulnerability Classification,add_lines,codeLink,commit_id,commit_message,del_lines,file_name,files_changed,func_after,func_before,lang,lines_after,lines_before,parentID,patch,project,project_after,project_before,vul,vul_func_with_fix,idx,primevul_func_before_fix,primevul_func_after_fix,C1_Description_of_Functionality_In_Context,C2_Description_of_Functionality_Generic,C3_Explanation_of_Vulnerability_In_Context,C4_Explanation_of_Vulnerability_Generic,C5_Explanation_Vulnerability_Fixed_In_Context,C6_Explanation_Vulnerability_Fixed_Generic
0,177805,177805,,Local,Not required,Partial,CVE-2018-19489,https://www.cvedetails.com/cve/CVE-2018-19489/,CWE-362,Low,,,,2018-12-13,2.1,v9fs_wstat in hw/9pfs/9p.c in QEMU allows guest OS users to cause a denial of service (crash) because of a race condition during file renaming.,2019-05-31,DoS,1,https://git.qemu.org/?p=qemu.git;a=commit;h=1d20398694a3b67a388d955b7a945ba4aa90a8a8,1d20398694a3b67a388d955b7a945ba4aa90a8a8,,0,,,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
",C,"    V9fsState *s = pdu->s;
",,5298f4d67a911dd9cefa4c4185eed242074d64c2,"@@ -2855,6 +2855,7 @@ static void coroutine_fn v9fs_wstat(void *opaque)
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
+    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
@@ -2920,7 +2921,9 @@ static void coroutine_fn v9fs_wstat(void *opaque)
         }
     }
     if (v9stat.name.size != 0) {
+        v9fs_path_write_lock(s);
         err = v9fs_complete_rename(pdu, fidp, -1, &v9stat.name);
+        v9fs_path_unlock(s);
         if (err < 0) {
             goto out;
         }",qemu,https://git.qemu.org/?p=qemu.git;a=blob;f=hw/9pfs/9p.c;h=bdf7919abfc517ae90b41c66d965137f1c7ee510;hb=bdf7919abfc517ae90b41c66d965137f1c7ee510,https://git.qemu.org/?p=qemu.git;a=blob;f=hw/9pfs/9p.c;h=267a25533b775b7e9a95c7bbd44a17914842e3cf;hb=267a25533b775b7e9a95c7bbd44a17914842e3cf,1,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
//fix_flaw_line_below:
//    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
",177805,"static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","static void coroutine_fn v9fs_wstat(void *opaque)
{
    int32_t fid;
    int err = 0;
    int16_t unused;
    V9fsStat v9stat;
    size_t offset = 7;
     struct stat stbuf;
     V9fsFidState *fidp;
     V9fsPDU *pdu = opaque;
    V9fsState *s = pdu->s;
 
     v9fs_stat_init(&v9stat);
     err = pdu_unmarshal(pdu, offset, ""dwS"", &fid, &unused, &v9stat);
        goto out_nofid;
    }
","The provided code defines a coroutine function named `v9fs_wstat`, which is designed to handle a specific operation related to file status updates in a V9 filesystem context. It initializes a `V9fsStat` structure and attempts to unmarshal data from a protocol data unit (PDU) into local variables, including a file identifier (`fid`) and a status structure (`v9stat`). The function uses an offset to extract the relevant data from the PDU. If the unmarshalling process encounters an error, it proceeds to a cleanup section labeled `out_nofid`, although the cleanup logic is not shown in the provided snippet.","The code defines a coroutine function that is responsible for processing file status updates within a specific filesystem context. It initializes a structure to hold file status information and attempts to extract relevant data from a provided data unit, including a file identifier and additional status information. The extraction process uses a predefined offset to locate the necessary data. If an error occurs during this extraction, the function is designed to handle the situation by transitioning to a designated cleanup section, although the details of that cleanup process are not included in the provided code snippet.","The vulnerability in the original code arises from the lack of proper error handling after the unmarshalling process, which could lead to undefined behavior if the data extraction fails. Specifically, the absence of checks for the return value of the `pdu_unmarshal` function means that the function could proceed with uninitialized or invalid data. In the fixed code, the addition of a variable to hold the state of the PDU enhances the context for error handling, allowing for more robust management of the unmarshalling process and ensuring that any errors can be appropriately addressed before proceeding further in the function.","The vulnerability in the original code stems from inadequate error handling following a critical data extraction process, which could result in the function operating with uninitialized or invalid data if the extraction fails. This oversight could lead to undefined behavior or crashes. In the revised code, additional context is provided to enhance error management, allowing the function to better handle potential failures during the data extraction. This improvement ensures that the function can appropriately respond to errors, thereby preventing the execution of subsequent operations with invalid data and enhancing overall stability and security.","The vulnerability was fixed by enhancing error handling after the data extraction process. In the revised code, the state of the protocol data unit is captured with the variable `s`, which provides additional context for managing errors. The return value of the `pdu_unmarshal` function is now properly checked, ensuring that if an error occurs during the unmarshalling of data into `fid`, `unused`, and `v9stat`, the function can handle it appropriately. This change prevents the function from proceeding with potentially uninitialized or invalid data, thereby improving the robustness and reliability of the code.","The vulnerability was addressed by implementing improved error handling following the data extraction process. The revised code ensures that any potential errors during the extraction are properly checked and managed, preventing the function from proceeding with uninitialized or invalid data. By capturing additional context related to the data unit, the code enhances its ability to respond to errors effectively. This change not only increases the robustness of the function but also safeguards against undefined behavior, thereby improving the overall stability and security of the application."
1,177889,177889,,Remote,Not required,Partial,CVE-2015-3196,https://www.cvedetails.com/cve/CVE-2015-3196/,CWE-362,Medium,,,,2015-12-06,4.3,"ssl/s3_clnt.c in OpenSSL 1.0.0 before 1.0.0t, 1.0.1 before 1.0.1p, and 1.0.2 before 1.0.2d, when used for a multi-threaded client, writes the PSK identity hint to an incorrect data structure, which allows remote servers to cause a denial of service (race condition and double free) via a crafted ServerKeyExchange message.",2019-06-13,DoS,1,https://git.openssl.org/?p=openssl.git;a=commit;h=3c66a669dfc7b3792f7af0758ea26fe8502ce70c,3c66a669dfc7b3792f7af0758ea26fe8502ce70c,,1,,,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strdup((char *)p);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
",C,"        s->session->psk_identity = BUF_strndup((char *)p, i);
","        s->session->psk_identity = BUF_strdup((char *)p);
",f8f75a7e4a7746caba9cab843e14ed6a41dfad21,"@@ -2780,7 +2780,7 @@ int ssl3_get_client_key_exchange(SSL *s)
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
-        s->session->psk_identity = BUF_strdup((char *)p);
+        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
",openssl,https://git.openssl.org/?p=openssl.git;a=blob;f=ssl/s3_srvr.c;h=acd3b9e964a239b8b01f7aa84179c8e4f09a0e14;hb=3c66a669dfc7b3792f7af0758ea26fe8502ce70c,https://git.openssl.org/?p=openssl.git;a=blob;f=ssl/s3_srvr.c;h=8885694c02b34522d8acb980727c4a7841716993,1,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
//flaw_line_below:
        s->session->psk_identity = BUF_strdup((char *)p);
//fix_flaw_line_below:
//        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
",177889,"int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strdup((char *)p);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_client_key_exchange(SSL *s)
{
    int i, al, ok;
    long n;
    unsigned long alg_k;
    unsigned char *p;
#ifndef OPENSSL_NO_RSA
    RSA *rsa = NULL;
    EVP_PKEY *pkey = NULL;
#endif
#ifndef OPENSSL_NO_DH
    BIGNUM *pub = NULL;
    DH *dh_srvr, *dh_clnt = NULL;
#endif
#ifndef OPENSSL_NO_KRB5
    KSSL_ERR kssl_err;
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    EC_KEY *srvr_ecdh = NULL;
    EVP_PKEY *clnt_pub_pkey = NULL;
    EC_POINT *clnt_ecpoint = NULL;
    BN_CTX *bn_ctx = NULL;
#endif

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_SR_KEY_EXCH_A,
                                   SSL3_ST_SR_KEY_EXCH_B,
                                   SSL3_MT_CLIENT_KEY_EXCHANGE, 2048, &ok);

    if (!ok)
        return ((int)n);
    p = (unsigned char *)s->init_msg;

    alg_k = s->s3->tmp.new_cipher->algorithm_mkey;

#ifndef OPENSSL_NO_RSA
    if (alg_k & SSL_kRSA) {
        unsigned char rand_premaster_secret[SSL_MAX_MASTER_KEY_LENGTH];
        int decrypt_len;
        unsigned char decrypt_good, version_good;
        size_t j;

        /* FIX THIS UP EAY EAY EAY EAY */
        if (s->s3->tmp.use_rsa_tmp) {
            if ((s->cert != NULL) && (s->cert->rsa_tmp != NULL))
                rsa = s->cert->rsa_tmp;
            /*
             * Don't do a callback because rsa_tmp should be sent already
             */
            if (rsa == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_RSA_PKEY);
                goto f_err;

            }
        } else {
            pkey = s->cert->pkeys[SSL_PKEY_RSA_ENC].privatekey;
            if ((pkey == NULL) ||
                (pkey->type != EVP_PKEY_RSA) || (pkey->pkey.rsa == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            rsa = pkey->pkey.rsa;
        }

        /* TLS and [incidentally] DTLS{0xFEFF} */
        if (s->version > SSL3_VERSION && s->version != DTLS1_BAD_VER) {
            n2s(p, i);
            if (n != i + 2) {
                if (!(s->options & SSL_OP_TLS_D5_BUG)) {
                    al = SSL_AD_DECODE_ERROR;
                    SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                           SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
                    goto f_err;
                } else
                    p -= 2;
            } else
                n = i;
        }

        /*
         * Reject overly short RSA ciphertext because we want to be sure
         * that the buffer size makes it safe to iterate over the entire
         * size of a premaster secret (SSL_MAX_MASTER_KEY_LENGTH). The
         * actual expected size is larger due to RSA padding, but the
         * bound is sufficient to be safe.
         */
        if (n < SSL_MAX_MASTER_KEY_LENGTH) {
            al = SSL_AD_DECRYPT_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG);
            goto f_err;
        }

        /*
         * We must not leak whether a decryption failure occurs because of
         * Bleichenbacher's attack on PKCS #1 v1.5 RSA padding (see RFC 2246,
         * section 7.4.7.1). The code follows that advice of the TLS RFC and
         * generates a random premaster secret for the case that the decrypt
         * fails. See https://tools.ietf.org/html/rfc5246#section-7.4.7.1
         */

        /*
         * should be RAND_bytes, but we cannot work around a failure.
         */
        if (RAND_pseudo_bytes(rand_premaster_secret,
                              sizeof(rand_premaster_secret)) <= 0)
            goto err;
        decrypt_len =
            RSA_private_decrypt((int)n, p, p, rsa, RSA_PKCS1_PADDING);
        ERR_clear_error();

        /*
         * decrypt_len should be SSL_MAX_MASTER_KEY_LENGTH. decrypt_good will
         * be 0xff if so and zero otherwise.
         */
        decrypt_good =
            constant_time_eq_int_8(decrypt_len, SSL_MAX_MASTER_KEY_LENGTH);

        /*
         * If the version in the decrypted pre-master secret is correct then
         * version_good will be 0xff, otherwise it'll be zero. The
         * Klima-Pokorny-Rosa extension of Bleichenbacher's attack
         * (http://eprint.iacr.org/2003/052/) exploits the version number
         * check as a ""bad version oracle"". Thus version checks are done in
         * constant time and are treated like any other decryption error.
         */
        version_good =
            constant_time_eq_8(p[0], (unsigned)(s->client_version >> 8));
        version_good &=
            constant_time_eq_8(p[1], (unsigned)(s->client_version & 0xff));

        /*
         * The premaster secret must contain the same version number as the
         * ClientHello to detect version rollback attacks (strangely, the
         * protocol does not offer such protection for DH ciphersuites).
         * However, buggy clients exist that send the negotiated protocol
         * version instead if the server does not support the requested
         * protocol version. If SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such
         * clients.
         */
        if (s->options & SSL_OP_TLS_ROLLBACK_BUG) {
            unsigned char workaround_good;
            workaround_good =
                constant_time_eq_8(p[0], (unsigned)(s->version >> 8));
            workaround_good &=
                constant_time_eq_8(p[1], (unsigned)(s->version & 0xff));
            version_good |= workaround_good;
        }

        /*
         * Both decryption and version must be good for decrypt_good to
         * remain non-zero (0xff).
         */
        decrypt_good &= version_good;

        /*
         * Now copy rand_premaster_secret over from p using
         * decrypt_good_mask. If decryption failed, then p does not
         * contain valid plaintext, however, a check above guarantees
         * it is still sufficiently large to read from.
         */
        for (j = 0; j < sizeof(rand_premaster_secret); j++) {
            p[j] = constant_time_select_8(decrypt_good, p[j],
                                          rand_premaster_secret[j]);
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p,
                                                        sizeof
                                                        (rand_premaster_secret));
        OPENSSL_cleanse(p, sizeof(rand_premaster_secret));
    } else
#endif
#ifndef OPENSSL_NO_DH
    if (alg_k & (SSL_kEDH | SSL_kDHr | SSL_kDHd)) {
        int idx = -1;
        EVP_PKEY *skey = NULL;
        if (n > 1) {
            n2s(p, i);
        } else {
            if (alg_k & SSL_kDHE) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto f_err;
            }
            i = 0;
        }
        if (n && n != i + 2) {
            if (!(s->options & SSL_OP_SSLEAY_080_CLIENT_DH_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_DH_PUBLIC_VALUE_LENGTH_IS_WRONG);
                goto err;
            } else {
                p -= 2;
                i = (int)n;
            }
        }
        if (alg_k & SSL_kDHr)
            idx = SSL_PKEY_DH_RSA;
        else if (alg_k & SSL_kDHd)
            idx = SSL_PKEY_DH_DSA;
        if (idx >= 0) {
            skey = s->cert->pkeys[idx].privatekey;
            if ((skey == NULL) ||
                (skey->type != EVP_PKEY_DH) || (skey->pkey.dh == NULL)) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_RSA_CERTIFICATE);
                goto f_err;
            }
            dh_srvr = skey->pkey.dh;
        } else if (s->s3->tmp.dh == NULL) {
            al = SSL_AD_HANDSHAKE_FAILURE;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_MISSING_TMP_DH_KEY);
            goto f_err;
        } else
            dh_srvr = s->s3->tmp.dh;

        if (n == 0L) {
            /* Get pubkey from cert */
            EVP_PKEY *clkey = X509_get_pubkey(s->session->peer);
            if (clkey) {
                if (EVP_PKEY_cmp_parameters(clkey, skey) == 1)
                    dh_clnt = EVP_PKEY_get1_DH(clkey);
            }
            if (dh_clnt == NULL) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_DH_KEY);
                goto f_err;
            }
            EVP_PKEY_free(clkey);
            pub = dh_clnt->pub_key;
        } else
            pub = BN_bin2bn(p, i, NULL);
        if (pub == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_BN_LIB);
            goto err;
        }

        i = DH_compute_key(p, pub, dh_srvr);

        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_DH_LIB);
            BN_clear_free(pub);
            goto err;
        }

        DH_free(s->s3->tmp.dh);
        s->s3->tmp.dh = NULL;
        if (dh_clnt)
            DH_free(dh_clnt);
        else
            BN_clear_free(pub);
        pub = NULL;
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);
        OPENSSL_cleanse(p, i);
        if (dh_clnt)
            return 2;
    } else
#endif
#ifndef OPENSSL_NO_KRB5
    if (alg_k & SSL_kKRB5) {
        krb5_error_code krb5rc;
        krb5_data enc_ticket;
        krb5_data authenticator;
        krb5_data enc_pms;
        KSSL_CTX *kssl_ctx = s->kssl_ctx;
        EVP_CIPHER_CTX ciph_ctx;
        const EVP_CIPHER *enc = NULL;
        unsigned char iv[EVP_MAX_IV_LENGTH];
        unsigned char pms[SSL_MAX_MASTER_KEY_LENGTH + EVP_MAX_BLOCK_LENGTH];
        int padl, outl;
        krb5_timestamp authtime = 0;
        krb5_ticket_times ttimes;
        int kerr = 0;

        EVP_CIPHER_CTX_init(&ciph_ctx);

        if (!kssl_ctx)
            kssl_ctx = kssl_ctx_new();

        n2s(p, i);
        enc_ticket.length = i;

        if (n < (long)(enc_ticket.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        enc_ticket.data = (char *)p;
        p += enc_ticket.length;

        n2s(p, i);
        authenticator.length = i;

        if (n < (long)(enc_ticket.length + authenticator.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        authenticator.data = (char *)p;
        p += authenticator.length;

        n2s(p, i);
        enc_pms.length = i;
        enc_pms.data = (char *)p;
        p += enc_pms.length;

        /*
         * Note that the length is checked again below, ** after decryption
         */
        if (enc_pms.length > sizeof pms) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if (n != (long)(enc_ticket.length + authenticator.length +
                        enc_pms.length + 6)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto err;
        }

        if ((krb5rc = kssl_sget_tkt(kssl_ctx, &enc_ticket, &ttimes,
                                    &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_sget_tkt rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        /*
         * Note: no authenticator is not considered an error, ** but will
         * return authtime == 0.
         */
        if ((krb5rc = kssl_check_authent(kssl_ctx, &authenticator,
                                         &authtime, &kssl_err)) != 0) {
# ifdef KSSL_DEBUG
            fprintf(stderr, ""kssl_check_authent rtn %d [%d]\n"",
                    krb5rc, kssl_err.reason);
            if (kssl_err.text)
                fprintf(stderr, ""kssl_err text= %s\n"", kssl_err.text);
# endif                         /* KSSL_DEBUG */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, kssl_err.reason);
            goto err;
        }

        if ((krb5rc = kssl_validate_times(authtime, &ttimes)) != 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, krb5rc);
            goto err;
        }
# ifdef KSSL_DEBUG
        kssl_ctx_show(kssl_ctx);
# endif                         /* KSSL_DEBUG */

        enc = kssl_map_enc(kssl_ctx->enctype);
        if (enc == NULL)
            goto err;

        memset(iv, 0, sizeof iv); /* per RFC 1510 */

        if (!EVP_DecryptInit_ex(&ciph_ctx, enc, NULL, kssl_ctx->key, iv)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto err;
        }
        if (!EVP_DecryptUpdate(&ciph_ctx, pms, &outl,
                               (unsigned char *)enc_pms.data, enc_pms.length))
        {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!EVP_DecryptFinal_ex(&ciph_ctx, &(pms[outl]), &padl)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            kerr = 1;
            goto kclean;
        }
        outl += padl;
        if (outl > SSL_MAX_MASTER_KEY_LENGTH) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            kerr = 1;
            goto kclean;
        }
        if (!((pms[0] == (s->client_version >> 8))
              && (pms[1] == (s->client_version & 0xff)))) {
            /*
             * The premaster secret must contain the same version number as
             * the ClientHello to detect version rollback attacks (strangely,
             * the protocol does not offer such protection for DH
             * ciphersuites). However, buggy clients exist that send random
             * bytes instead of the protocol version. If
             * SSL_OP_TLS_ROLLBACK_BUG is set, tolerate such clients.
             * (Perhaps we should have a separate BUG value for the Kerberos
             * cipher)
             */
            if (!(s->options & SSL_OP_TLS_ROLLBACK_BUG)) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_AD_DECODE_ERROR);
                kerr = 1;
                goto kclean;
            }
        }

        EVP_CIPHER_CTX_cleanup(&ciph_ctx);

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        pms, outl);

        if (kssl_ctx->client_princ) {
            size_t len = strlen(kssl_ctx->client_princ);
            if (len < SSL_MAX_KRB5_PRINCIPAL_LENGTH) {
                s->session->krb5_client_princ_len = len;
                memcpy(s->session->krb5_client_princ, kssl_ctx->client_princ,
                       len);
            }
        }

        /*- Was doing kssl_ctx_free() here,
         *  but it caused problems for apache.
         *  kssl_ctx = kssl_ctx_free(kssl_ctx);
         *  if (s->kssl_ctx)  s->kssl_ctx = NULL;
         */

 kclean:
        OPENSSL_cleanse(pms, sizeof(pms));
        if (kerr)
            goto err;
    } else
#endif                          /* OPENSSL_NO_KRB5 */

#ifndef OPENSSL_NO_ECDH
    if (alg_k & (SSL_kEECDH | SSL_kECDHr | SSL_kECDHe)) {
        int ret = 1;
        int field_size = 0;
        const EC_KEY *tkey;
        const EC_GROUP *group;
        const BIGNUM *priv_key;

        /* initialize structures for server's ECDH key pair */
        if ((srvr_ecdh = EC_KEY_new()) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        /* Let's get server private key and group information */
        if (alg_k & (SSL_kECDHr | SSL_kECDHe)) {
            /* use the certificate */
            tkey = s->cert->pkeys[SSL_PKEY_ECC].privatekey->pkey.ec;
        } else {
            /*
             * use the ephermeral values we saved when generating the
             * ServerKeyExchange msg.
             */
            tkey = s->s3->tmp.ecdh;
        }

        group = EC_KEY_get0_group(tkey);
        priv_key = EC_KEY_get0_private_key(tkey);

        if (!EC_KEY_set_group(srvr_ecdh, group) ||
            !EC_KEY_set_private_key(srvr_ecdh, priv_key)) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
            goto err;
        }

        /* Let's get client's public key */
        if ((clnt_ecpoint = EC_POINT_new(group)) == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if (n == 0L) {
            /* Client Publickey was in Client Certificate */

            if (alg_k & SSL_kEECDH) {
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_MISSING_TMP_ECDH_KEY);
                goto f_err;
            }
            if (((clnt_pub_pkey = X509_get_pubkey(s->session->peer))
                 == NULL) || (clnt_pub_pkey->type != EVP_PKEY_EC)) {
                /*
                 * XXX: For now, we do not support client authentication
                 * using ECDH certificates so this branch (n == 0L) of the
                 * code is never executed. When that support is added, we
                 * ought to ensure the key received in the certificate is
                 * authorized for key agreement. ECDH_compute_key implicitly
                 * checks that the two ECDH shares are for the same group.
                 */
                al = SSL_AD_HANDSHAKE_FAILURE;
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       SSL_R_UNABLE_TO_DECODE_ECDH_CERTS);
                goto f_err;
            }

            if (EC_POINT_copy(clnt_ecpoint,
                              EC_KEY_get0_public_key(clnt_pub_pkey->
                                                     pkey.ec)) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            ret = 2;            /* Skip certificate verify processing */
        } else {
            /*
             * Get client's public key from encoded point in the
             * ClientKeyExchange message.
             */
            if ((bn_ctx = BN_CTX_new()) == NULL) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                       ERR_R_MALLOC_FAILURE);
                goto err;
            }

            /* Get encoded point length */
            i = *p;
            p += 1;
            if (n != 1 + i) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            if (EC_POINT_oct2point(group, clnt_ecpoint, p, i, bn_ctx) == 0) {
                SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_EC_LIB);
                goto err;
            }
            /*
             * p is pointing to somewhere in the buffer currently, so set it
             * to the start
             */
            p = (unsigned char *)s->init_buf->data;
        }

        /* Compute the shared pre-master secret */
        field_size = EC_GROUP_get_degree(group);
        if (field_size <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }
        i = ECDH_compute_key(p, (field_size + 7) / 8, clnt_ecpoint, srvr_ecdh,
                             NULL);
        if (i <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_ECDH_LIB);
            goto err;
        }

        EVP_PKEY_free(clnt_pub_pkey);
        EC_POINT_free(clnt_ecpoint);
        EC_KEY_free(srvr_ecdh);
        BN_CTX_free(bn_ctx);
        EC_KEY_free(s->s3->tmp.ecdh);
        s->s3->tmp.ecdh = NULL;

        /* Compute the master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        p, i);

        OPENSSL_cleanse(p, i);
        return (ret);
    } else
#endif
#ifndef OPENSSL_NO_PSK
    if (alg_k & SSL_kPSK) {
        unsigned char *t = NULL;
        unsigned char psk_or_pre_ms[PSK_MAX_PSK_LEN * 2 + 4];
        unsigned int pre_ms_len = 0, psk_len = 0;
        int psk_err = 1;
        char tmp_id[PSK_MAX_IDENTITY_LEN + 1];

        al = SSL_AD_HANDSHAKE_FAILURE;

        n2s(p, i);
        if (n != i + 2) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_LENGTH_MISMATCH);
            goto psk_err;
        }
        if (i > PSK_MAX_IDENTITY_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DATA_LENGTH_TOO_LONG);
            goto psk_err;
        }
        if (s->psk_server_callback == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_NO_SERVER_CB);
            goto psk_err;
        }

        /*
         * Create guaranteed NULL-terminated identity string for the callback
         */
        memcpy(tmp_id, p, i);
        memset(tmp_id + i, 0, PSK_MAX_IDENTITY_LEN + 1 - i);
        psk_len = s->psk_server_callback(s, tmp_id,
                                         psk_or_pre_ms,
                                         sizeof(psk_or_pre_ms));
        OPENSSL_cleanse(tmp_id, PSK_MAX_IDENTITY_LEN + 1);

        if (psk_len > PSK_MAX_PSK_LEN) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto psk_err;
        } else if (psk_len == 0) {
            /*
             * PSK related to the given identity not found
             */
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_PSK_IDENTITY_NOT_FOUND);
            al = SSL_AD_UNKNOWN_PSK_IDENTITY;
            goto psk_err;
        }

        /* create PSK pre_master_secret */
        pre_ms_len = 2 + psk_len + 2 + psk_len;
        t = psk_or_pre_ms;
        memmove(psk_or_pre_ms + psk_len + 4, psk_or_pre_ms, psk_len);
        s2n(psk_len, t);
        memset(t, 0, psk_len);
        t += psk_len;
        s2n(psk_len, t);
 
         if (s->session->psk_identity != NULL)
             OPENSSL_free(s->session->psk_identity);
        s->session->psk_identity = BUF_strndup((char *)p, i);
         if (s->session->psk_identity == NULL) {
             SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
             goto psk_err;
        }

        if (s->session->psk_identity_hint != NULL)
            OPENSSL_free(s->session->psk_identity_hint);
        s->session->psk_identity_hint = BUF_strdup(s->ctx->psk_identity_hint);
        if (s->ctx->psk_identity_hint != NULL &&
            s->session->psk_identity_hint == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto psk_err;
        }

        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        psk_or_pre_ms,
                                                        pre_ms_len);
        psk_err = 0;
 psk_err:
        OPENSSL_cleanse(psk_or_pre_ms, sizeof(psk_or_pre_ms));
        if (psk_err != 0)
            goto f_err;
    } else
#endif
#ifndef OPENSSL_NO_SRP
    if (alg_k & SSL_kSRP) {
        int param_len;

        n2s(p, i);
        param_len = i + 2;
        if (param_len > n) {
            al = SSL_AD_DECODE_ERROR;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_A_LENGTH);
            goto f_err;
        }
        if (!(s->srp_ctx.A = BN_bin2bn(p, i, NULL))) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_BN_LIB);
            goto err;
        }
        if (BN_ucmp(s->srp_ctx.A, s->srp_ctx.N) >= 0
            || BN_is_zero(s->srp_ctx.A)) {
            al = SSL_AD_ILLEGAL_PARAMETER;
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_BAD_SRP_PARAMETERS);
            goto f_err;
        }
        if (s->session->srp_username != NULL)
            OPENSSL_free(s->session->srp_username);
        s->session->srp_username = BUF_strdup(s->srp_ctx.login);
        if (s->session->srp_username == NULL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_MALLOC_FAILURE);
            goto err;
        }

        if ((s->session->master_key_length =
             SRP_generate_server_master_secret(s,
                                               s->session->master_key)) < 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, ERR_R_INTERNAL_ERROR);
            goto err;
        }

        p += i;
    } else
#endif                          /* OPENSSL_NO_SRP */
    if (alg_k & SSL_kGOST) {
        int ret = 0;
        EVP_PKEY_CTX *pkey_ctx;
        EVP_PKEY *client_pub_pkey = NULL, *pk = NULL;
        unsigned char premaster_secret[32], *start;
        size_t outlen = 32, inlen;
        unsigned long alg_a;
        int Ttag, Tclass;
        long Tlen;

        /* Get our certificate private key */
        alg_a = s->s3->tmp.new_cipher->algorithm_auth;
        if (alg_a & SSL_aGOST94)
            pk = s->cert->pkeys[SSL_PKEY_GOST94].privatekey;
        else if (alg_a & SSL_aGOST01)
            pk = s->cert->pkeys[SSL_PKEY_GOST01].privatekey;

        pkey_ctx = EVP_PKEY_CTX_new(pk, NULL);
        EVP_PKEY_decrypt_init(pkey_ctx);
        /*
         * If client certificate is present and is of the same type, maybe
         * use it for key exchange.  Don't mind errors from
         * EVP_PKEY_derive_set_peer, because it is completely valid to use a
         * client certificate for authorization only.
         */
        client_pub_pkey = X509_get_pubkey(s->session->peer);
        if (client_pub_pkey) {
            if (EVP_PKEY_derive_set_peer(pkey_ctx, client_pub_pkey) <= 0)
                ERR_clear_error();
        }
        /* Decrypt session key */
        if (ASN1_get_object
            ((const unsigned char **)&p, &Tlen, &Ttag, &Tclass,
             n) != V_ASN1_CONSTRUCTED || Ttag != V_ASN1_SEQUENCE
            || Tclass != V_ASN1_UNIVERSAL) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        start = p;
        inlen = Tlen;
        if (EVP_PKEY_decrypt
            (pkey_ctx, premaster_secret, &outlen, start, inlen) <= 0) {
            SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE,
                   SSL_R_DECRYPTION_FAILED);
            goto gerr;
        }
        /* Generate master secret */
        s->session->master_key_length =
            s->method->ssl3_enc->generate_master_secret(s,
                                                        s->
                                                        session->master_key,
                                                        premaster_secret, 32);
        OPENSSL_cleanse(premaster_secret, sizeof(premaster_secret));
        /* Check if pubkey from client certificate was used */
        if (EVP_PKEY_CTX_ctrl
            (pkey_ctx, -1, -1, EVP_PKEY_CTRL_PEER_KEY, 2, NULL) > 0)
            ret = 2;
        else
            ret = 1;
 gerr:
        EVP_PKEY_free(client_pub_pkey);
        EVP_PKEY_CTX_free(pkey_ctx);
        if (ret)
            return ret;
        else
            goto err;
    } else {
        al = SSL_AD_HANDSHAKE_FAILURE;
        SSLerr(SSL_F_SSL3_GET_CLIENT_KEY_EXCHANGE, SSL_R_UNKNOWN_CIPHER_TYPE);
        goto f_err;
    }

    return (1);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
#if !defined(OPENSSL_NO_DH) || !defined(OPENSSL_NO_RSA) || !defined(OPENSSL_NO_ECDH) || defined(OPENSSL_NO_SRP)
 err:
#endif
#ifndef OPENSSL_NO_ECDH
    EVP_PKEY_free(clnt_pub_pkey);
    EC_POINT_free(clnt_ecpoint);
    if (srvr_ecdh != NULL)
        EC_KEY_free(srvr_ecdh);
    BN_CTX_free(bn_ctx);
#endif
    s->state = SSL_ST_ERR;
    return (-1);
}
","The `ssl3_get_client_key_exchange` function is part of the SSL/TLS handshake process, responsible for handling the client's key exchange message. It begins by retrieving the message and determining the key exchange algorithm used (e.g., RSA, DH, ECDH, PSK). Depending on the algorithm, it performs specific operations such as decrypting a premaster secret using RSA, computing a shared secret with DH or ECDH, or handling PSK identities. The function also includes error handling for various conditions, ensuring secure key generation and preventing potential attacks. Finally, it generates the master secret for session encryption and returns the appropriate status.","The provided code implements a crucial part of the SSL/TLS handshake process, specifically handling the client's key exchange message. It identifies the key exchange method used and performs the necessary operations to establish a secure connection, such as decrypting a premaster secret or computing a shared secret based on the chosen algorithm. The code includes robust error handling to manage various failure scenarios, ensuring that the key exchange is secure and resilient against potential attacks. Ultimately, it generates a master secret that will be used for encrypting the session, facilitating secure communication between the client and server.","The vulnerability in the original code arises from the potential for timing attacks, specifically related to the decryption of the premaster secret using RSA. The code checks the decryption length and the version of the decrypted data in a way that could leak information about whether the decryption was successful, allowing an attacker to infer details about the private key. In the fixed code, the checks for decryption success and version correctness are performed using constant-time comparisons, which mitigate the risk of timing attacks by ensuring that the execution time does not vary based on the input values, thus enhancing security during the key exchange process.","The vulnerability in the original code stems from the potential for timing attacks during the decryption process, where the execution time could reveal whether the decryption was successful or if the input was valid. This could allow an attacker to infer sensitive information about the private key. The fix implemented in the updated code involves using constant-time comparisons for checking the results of the decryption and version validation. By ensuring that the execution time remains consistent regardless of the input values, the revised code effectively mitigates the risk of timing attacks, enhancing the overall security of the key exchange process.","The vulnerability was fixed by replacing direct comparisons with constant-time functions to prevent timing attacks. Specifically, the checks for decryption success and version correctness were modified to use `constant_time_eq_int_8` and `constant_time_eq_8`, ensuring that the execution time remains consistent regardless of the input values. This approach prevents an attacker from inferring information based on how long the decryption process takes. Additionally, the code ensures that both decryption and version checks must pass for the decryption status to be considered valid, further enhancing security during the key exchange process.","The vulnerability was addressed by implementing constant-time comparison techniques to evaluate the results of decryption and version checks. This change ensures that the execution time remains uniform regardless of the input values, thereby preventing attackers from gaining insights into the success or failure of the decryption process based on timing variations. Additionally, the updated code requires both the decryption and version checks to pass for the overall decryption status to be deemed valid, further strengthening the security of the key exchange process and reducing the risk of information leakage during the handshake."
2,178347,178347,,Remote,Not required,Partial,CVE-2014-3509,https://www.cvedetails.com/cve/CVE-2014-3509/,CWE-362,Medium,Partial,Partial,,2014-08-13,6.8,"Race condition in the ssl_parse_serverhello_tlsext function in t1_lib.c in OpenSSL 1.0.0 before 1.0.0n and 1.0.1 before 1.0.1i, when multithreading and session resumption are used, allows remote SSL servers to cause a denial of service (memory overwrite and client application crash) or possibly have unspecified other impact by sending Elliptic Curve (EC) Supported Point Formats Extension data.",2017-11-14,DoS,10,https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,,7,,,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       if (!s->hit)
                                {
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                {
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
",C,"                       if (!s->hit)
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
","                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
",0042fb5fd1c9d257d713b15a1f45da05cf5c1c87,"@@ -2647,15 +2647,18 @@ static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
-                       s->session->tlsext_ecpointformatlist_length = 0;
-                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
-                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
+                       if (!s->hit)
                                {
-                               *al = TLS1_AD_INTERNAL_ERROR;
-                               return 0;
+                               s->session->tlsext_ecpointformatlist_length = 0;
+                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
+                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
+                                       {
+                                       *al = TLS1_AD_INTERNAL_ERROR;
+                                       return 0;
+                                       }
+                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
+                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
-                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
-                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;",openssl,https://git.openssl.org/gitweb/?p=openssl.git;a=blob;f=ssl/t1_lib.c;h=749d88d1a9d9a6faa535437c236104497f0d1a66;hb=fb0bc2b273bcc2d5401dd883fe869af4fc74bb21,https://git.openssl.org/gitweb/?p=openssl.git;a=blob;f=ssl/t1_lib.c;h=4374d6aadd3d5b0804d13d338b27408d8102e8b5,1,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
//flaw_line_below:
                       s->session->tlsext_ecpointformatlist_length = 0;
//flaw_line_below:
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
//flaw_line_below:
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
//fix_flaw_line_below:
//                       if (!s->hit)
                                {
//flaw_line_below:
                               *al = TLS1_AD_INTERNAL_ERROR;
//flaw_line_below:
                               return 0;
//fix_flaw_line_below:
//                               s->session->tlsext_ecpointformatlist_length = 0;
//fix_flaw_line_below:
//                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
//fix_flaw_line_below:
//                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
//fix_flaw_line_below:
//                                       {
//fix_flaw_line_below:
//                                       *al = TLS1_AD_INTERNAL_ERROR;
//fix_flaw_line_below:
//                                       return 0;
//fix_flaw_line_below:
//                                       }
//fix_flaw_line_below:
//                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
//fix_flaw_line_below:
//                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
//flaw_line_below:
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
//flaw_line_below:
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
",178347,"static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = 0;
                       if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                       if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                {
                               *al = TLS1_AD_INTERNAL_ERROR;
                               return 0;
                                }
                       s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                       memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","static int ssl_scan_serverhello_tlsext(SSL *s, unsigned char **p, unsigned char *d, int n, int *al)
	{
	unsigned short length;
	unsigned short type;
	unsigned short size;
	unsigned char *data = *p;
	int tlsext_servername = 0;
	int renegotiate_seen = 0;

#ifndef OPENSSL_NO_NEXTPROTONEG
	s->s3->next_proto_neg_seen = 0;
#endif

	if (s->s3->alpn_selected)
		{
		OPENSSL_free(s->s3->alpn_selected);
		s->s3->alpn_selected = NULL;
		}

#ifndef OPENSSL_NO_HEARTBEATS
	s->tlsext_heartbeat &= ~(SSL_TLSEXT_HB_ENABLED |
	                       SSL_TLSEXT_HB_DONT_SEND_REQUESTS);
#endif

#ifdef TLSEXT_TYPE_encrypt_then_mac
	s->s3->flags &= ~TLS1_FLAGS_ENCRYPT_THEN_MAC;
#endif

	if (data >= (d+n-2))
		goto ri_check;

	n2s(data,length);
	if (data+length != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	while(data <= (d+n-4))
		{
		n2s(data,type);
		n2s(data,size);

		if (data+size > (d+n))
	   		goto ri_check;

		if (s->tlsext_debug_cb)
			s->tlsext_debug_cb(s, 1, type, data, size,
						s->tlsext_debug_arg);

		if (type == TLSEXT_TYPE_server_name)
			{
			if (s->tlsext_hostname == NULL || size > 0)
				{
				*al = TLS1_AD_UNRECOGNIZED_NAME;
				return 0;
				}
			tlsext_servername = 1;   
			}

#ifndef OPENSSL_NO_EC
		else if (type == TLSEXT_TYPE_ec_point_formats)
			{
			unsigned char *sdata = data;
			int ecpointformatlist_length = *(sdata++);

			if (ecpointformatlist_length != size - 1)
				{
                                *al = TLS1_AD_DECODE_ERROR;
                                return 0;
                                }
                       if (!s->hit)
                                {
                               s->session->tlsext_ecpointformatlist_length = 0;
                               if (s->session->tlsext_ecpointformatlist != NULL) OPENSSL_free(s->session->tlsext_ecpointformatlist);
                               if ((s->session->tlsext_ecpointformatlist = OPENSSL_malloc(ecpointformatlist_length)) == NULL)
                                       {
                                       *al = TLS1_AD_INTERNAL_ERROR;
                                       return 0;
                                       }
                               s->session->tlsext_ecpointformatlist_length = ecpointformatlist_length;
                               memcpy(s->session->tlsext_ecpointformatlist, sdata, ecpointformatlist_length);
                                }
 #if 0
                        fprintf(stderr,""ssl_parse_serverhello_tlsext s->session->tlsext_ecpointformatlist "");
                        sdata = s->session->tlsext_ecpointformatlist;
#endif
			}
#endif /* OPENSSL_NO_EC */

		else if (type == TLSEXT_TYPE_session_ticket)
			{
			if (s->tls_session_ticket_ext_cb &&
			    !s->tls_session_ticket_ext_cb(s, data, size, s->tls_session_ticket_ext_cb_arg))
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			if (!tls_use_ticket(s) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			s->tlsext_ticket_expected = 1;
			}
#ifdef TLSEXT_TYPE_opaque_prf_input
		else if (type == TLSEXT_TYPE_opaque_prf_input)
			{
			unsigned char *sdata = data;

			if (size < 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			n2s(sdata, s->s3->server_opaque_prf_input_len);
			if (s->s3->server_opaque_prf_input_len != size - 2)
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			
			if (s->s3->server_opaque_prf_input != NULL) /* shouldn't really happen */
				OPENSSL_free(s->s3->server_opaque_prf_input);
			if (s->s3->server_opaque_prf_input_len == 0)
				s->s3->server_opaque_prf_input = OPENSSL_malloc(1); /* dummy byte just to get non-NULL */
			else
				s->s3->server_opaque_prf_input = BUF_memdup(sdata, s->s3->server_opaque_prf_input_len);

			if (s->s3->server_opaque_prf_input == NULL)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_status_request)
			{
			/* MUST be empty and only sent if we've requested
			 * a status request message.
			 */ 
			if ((s->tlsext_status_type == -1) || (size > 0))
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* Set flag to expect CertificateStatus message */
			s->tlsext_status_expected = 1;
			}
#ifndef OPENSSL_NO_NEXTPROTONEG
		else if (type == TLSEXT_TYPE_next_proto_neg &&
			 s->s3->tmp.finish_md_len == 0)
			{
			unsigned char *selected;
			unsigned char selected_len;

			/* We must have requested it. */
			if (s->ctx->next_proto_select_cb == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			/* The data must be valid */
			if (!ssl_next_proto_validate(data, size))
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->ctx->next_proto_select_cb(s, &selected, &selected_len, data, size, s->ctx->next_proto_select_cb_arg) != SSL_TLSEXT_ERR_OK)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			s->next_proto_negotiated = OPENSSL_malloc(selected_len);
			if (!s->next_proto_negotiated)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->next_proto_negotiated, selected, selected_len);
			s->next_proto_negotiated_len = selected_len;
			s->s3->next_proto_neg_seen = 1;
			}
#endif

		else if (type == TLSEXT_TYPE_application_layer_protocol_negotiation)
			{
			unsigned len;

			/* We must have requested it. */
			if (s->alpn_client_proto_list == NULL)
				{
				*al = TLS1_AD_UNSUPPORTED_EXTENSION;
				return 0;
				}
			if (size < 4)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			/* The extension data consists of:
			 *   uint16 list_length
			 *   uint8 proto_length;
			 *   uint8 proto[proto_length]; */
			len = data[0];
			len <<= 8;
			len |= data[1];
			if (len != (unsigned) size - 2)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			len = data[2];
			if (len != (unsigned) size - 3)
				{
				*al = TLS1_AD_DECODE_ERROR;
				return 0;
				}
			if (s->s3->alpn_selected)
				OPENSSL_free(s->s3->alpn_selected);
			s->s3->alpn_selected = OPENSSL_malloc(len);
			if (!s->s3->alpn_selected)
				{
				*al = TLS1_AD_INTERNAL_ERROR;
				return 0;
				}
			memcpy(s->s3->alpn_selected, data + 3, len);
			s->s3->alpn_selected_len = len;
			}

		else if (type == TLSEXT_TYPE_renegotiate)
			{
			if(!ssl_parse_serverhello_renegotiate_ext(s, data, size, al))
				return 0;
			renegotiate_seen = 1;
			}
#ifndef OPENSSL_NO_HEARTBEATS
		else if (type == TLSEXT_TYPE_heartbeat)
			{
			switch(data[0])
				{
				case 0x01:	/* Server allows us to send HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							break;
				case 0x02:	/* Server doesn't accept HB requests */
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_ENABLED;
							s->tlsext_heartbeat |= SSL_TLSEXT_HB_DONT_SEND_REQUESTS;
							break;
				default:	*al = SSL_AD_ILLEGAL_PARAMETER;
							return 0;
				}
			}
#endif
		else if (type == TLSEXT_TYPE_use_srtp)
                        {
                        if(ssl_parse_serverhello_use_srtp_ext(s, data, size,
							      al))
                                return 0;
                        }
		/* If this extension type was not otherwise handled, but 
		 * matches a custom_cli_ext_record, then send it to the c
		 * callback */
		else if (s->ctx->custom_cli_ext_records_count)
			{
			size_t i;
			custom_cli_ext_record* record;

			for (i = 0; i < s->ctx->custom_cli_ext_records_count; i++)
				{
				record = &s->ctx->custom_cli_ext_records[i];
				if (record->ext_type == type)
					{
					if (record->fn2 && !record->fn2(s, type, data, size, al, record->arg))
						return 0;
					break;
					}
				}			
			}
#ifdef TLSEXT_TYPE_encrypt_then_mac
		else if (type == TLSEXT_TYPE_encrypt_then_mac)
			{
			/* Ignore if inappropriate ciphersuite */
			if (s->s3->tmp.new_cipher->algorithm_mac != SSL_AEAD)
				s->s3->flags |= TLS1_FLAGS_ENCRYPT_THEN_MAC;
			}
#endif
 
		data += size;
		}

	if (data != d+n)
		{
		*al = SSL_AD_DECODE_ERROR;
		return 0;
		}

	if (!s->hit && tlsext_servername == 1)
		{
 		if (s->tlsext_hostname)
			{
			if (s->session->tlsext_hostname == NULL)
				{
				s->session->tlsext_hostname = BUF_strdup(s->tlsext_hostname);	
				if (!s->session->tlsext_hostname)
					{
					*al = SSL_AD_UNRECOGNIZED_NAME;
					return 0;
					}
				}
			else 
				{
				*al = SSL_AD_DECODE_ERROR;
				return 0;
				}
			}
		}

	*p = data;

	ri_check:

	/* Determine if we need to see RI. Strictly speaking if we want to
	 * avoid an attack we should *always* see RI even on initial server
	 * hello because the client doesn't see any renegotiation during an
	 * attack. However this would mean we could not connect to any server
	 * which doesn't support RI so for the immediate future tolerate RI
	 * absence on initial connect only.
	 */
	if (!renegotiate_seen
		&& !(s->options & SSL_OP_LEGACY_SERVER_CONNECT)
		&& !(s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION))
		{
		*al = SSL_AD_HANDSHAKE_FAILURE;
		SSLerr(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT,
				SSL_R_UNSAFE_LEGACY_RENEGOTIATION_DISABLED);
		return 0;
		}

	return 1;
	}
","The `ssl_scan_serverhello_tlsext` function processes the TLS ServerHello message extensions in a secure socket layer (SSL) context. It begins by initializing various parameters and checking for the presence of specific extensions, such as server name indication, application layer protocol negotiation, and session tickets. The function validates the received data, ensuring it adheres to expected formats and lengths, and handles memory allocation for extension data. It also manages flags related to features like heartbeat support and renegotiation. Finally, it updates the pointer to the data buffer and checks for the presence of a required renegotiation indicator, returning appropriate error codes when necessary.","The provided code processes the extensions in a TLS ServerHello message during the SSL handshake. It initializes various parameters and checks for specific extensions, validating their formats and sizes while managing memory allocation for extension data. The code handles features such as server name indication, application layer protocol negotiation, and session tickets, ensuring compliance with expected behaviors. It also sets flags for heartbeat support and renegotiation, updating the data pointer accordingly. Finally, it verifies the presence of a required renegotiation indicator, returning appropriate error codes when discrepancies are found.","The vulnerability in the original code stemmed from inadequate handling of the renegotiation indicator, which could allow an attacker to exploit the absence of this indicator during the initial connection, potentially leading to a man-in-the-middle attack. Specifically, if the renegotiation indicator was not seen, the code would not enforce strict checks, allowing unsafe legacy renegotiation. The fix introduced a check that ensures the presence of the renegotiation indicator, and if it is absent, the code now returns a handshake failure error. This change enhances security by preventing connections to servers that do not support secure renegotiation.","The vulnerability in the original code was related to insufficient validation of the renegotiation indicator during the TLS handshake, which could allow attackers to exploit legacy renegotiation methods, potentially leading to man-in-the-middle attacks. The fix implemented a stricter check to ensure that the renegotiation indicator is present; if it is not, the handshake is aborted with a failure message. This change enhances security by preventing connections to servers that do not support secure renegotiation, thereby mitigating the risk of exploitation through unsafe legacy protocols.","The vulnerability was fixed by adding a check for the presence of the renegotiation indicator within the code. Specifically, after processing the extensions, the code verifies if the `renegotiate_seen` variable is set. If it is not, and the options do not allow legacy server connections, the code now sets the alert variable to indicate a handshake failure and returns an error. This ensures that if the renegotiation indicator is absent, the handshake will not proceed, effectively preventing connections to servers that do not support secure renegotiation and enhancing overall security.","The vulnerability was addressed by implementing a strict validation mechanism for the renegotiation indicator during the TLS handshake process. The updated code checks whether the indicator has been observed; if it is absent, and certain security options are not enabled, the handshake is aborted with a failure alert. This change ensures that connections are only established with servers that support secure renegotiation, thereby mitigating the risk of exploitation through legacy renegotiation methods. The fix enhances the overall security of the protocol by preventing potential man-in-the-middle attacks."
3,178408,178408,,Local,Not required,,CVE-2011-4029,https://www.cvedetails.com/cve/CVE-2011-4029/,CWE-362,Medium,Partial,,,2012-07-03,1.9,"The LockServer function in os/utils.c in X.Org xserver before 1.11.2 allows local users to change the permissions of arbitrary files to 444, read those files, and possibly cause a denial of service (removed execution permission) via a symlink attack on a temporary lock file.",2012-07-17,DoS,1,https://cgit.freedesktop.org/xorg/xserver/commit/?id=b67581cf825940fdf52bf2e0af4330e695d724a4,b67581cf825940fdf52bf2e0af4330e695d724a4,,1,,,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) chmod(tmp, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
",C,"  (void) fchmod(lfd, 0444);
","  (void) chmod(tmp, 0444);
",6ba44b91e37622ef8c146d8f2ac92d708a18ed34,"@@ -297,7 +297,7 @@ LockServer(void)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
-  (void) chmod(tmp, 0444);
+  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*",xserver,https://cgit.freedesktop.org/xorg/xserver/tree/os/utils.c?id=b67581cf825940fdf52bf2e0af4330e695d724a4,https://cgit.freedesktop.org/xorg/xserver/tree/os/utils.c?id=6ba44b91e37622ef8c146d8f2ac92d708a18ed34,1,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
//flaw_line_below:
  (void) chmod(tmp, 0444);
//fix_flaw_line_below:
//  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
",178408,"LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) chmod(tmp, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","LockServer(void)
{
  char tmp[PATH_MAX], pid_str[12];
  int lfd, i, haslock, l_pid, t;
  char *tmppath = NULL;
  int len;
  char port[20];

  if (nolock) return;
  /*
   * Path names
   */
  tmppath = LOCK_DIR;

  sprintf(port, ""%d"", atoi(display));
  len = strlen(LOCK_PREFIX) > strlen(LOCK_TMP_PREFIX) ? strlen(LOCK_PREFIX) :
						strlen(LOCK_TMP_PREFIX);
  len += strlen(tmppath) + strlen(port) + strlen(LOCK_SUFFIX) + 1;
  if (len > sizeof(LockFile))
    FatalError(""Display name `%s' is too long\n"", port);
  (void)sprintf(tmp, ""%s"" LOCK_TMP_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);
  (void)sprintf(LockFile, ""%s"" LOCK_PREFIX ""%s"" LOCK_SUFFIX, tmppath, port);

  /*
   * Create a temporary file containing our PID.  Attempt three times
   * to create the file.
   */
  StillLocking = TRUE;
  i = 0;
  do {
    i++;
    lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
    if (lfd < 0)
       sleep(2);
    else
       break;
  } while (i < 3);
  if (lfd < 0) {
    unlink(tmp);
    i = 0;
    do {
      i++;
      lfd = open(tmp, O_CREAT | O_EXCL | O_WRONLY, 0644);
      if (lfd < 0)
         sleep(2);
      else
         break;
    } while (i < 3);
  }
  if (lfd < 0)
     FatalError(""Could not create lock file in %s\n"", tmp);
   (void) sprintf(pid_str, ""%10ld\n"", (long)getpid());
   (void) write(lfd, pid_str, 11);
  (void) fchmod(lfd, 0444);
   (void) close(lfd);
 
   /*
   * OK.  Now the tmp file exists.  Try three times to move it in place
   * for the lock.
   */
  i = 0;
  haslock = 0;
  while ((!haslock) && (i++ < 3)) {
    haslock = (link(tmp,LockFile) == 0);
    if (haslock) {
      /*
       * We're done.
       */
      break;
    }
    else {
      /*
       * Read the pid from the existing file
       */
      lfd = open(LockFile, O_RDONLY|O_NOFOLLOW);
      if (lfd < 0) {
        unlink(tmp);
        FatalError(""Can't read lock file %s\n"", LockFile);
      }
      pid_str[0] = '\0';
      if (read(lfd, pid_str, 11) != 11) {
        /*
         * Bogus lock file.
         */
        unlink(LockFile);
        close(lfd);
        continue;
      }
      pid_str[11] = '\0';
      sscanf(pid_str, ""%d"", &l_pid);
      close(lfd);

      /*
       * Now try to kill the PID to see if it exists.
       */
      errno = 0;
      t = kill(l_pid, 0);
      if ((t< 0) && (errno == ESRCH)) {
        /*
         * Stale lock file.
         */
        unlink(LockFile);
        continue;
      }
      else if (((t < 0) && (errno == EPERM)) || (t == 0)) {
        /*
         * Process is still active.
         */
        unlink(tmp);
	FatalError(""Server is already active for display %s\n%s %s\n%s\n"",
		   port, ""\tIf this server is no longer running, remove"",
		   LockFile, ""\tand start again."");
      }
    }
  }
  unlink(tmp);
  if (!haslock)
    FatalError(""Could not create server lock file: %s\n"", LockFile);
  StillLocking = FALSE;
}
","The `LockServer` function is designed to manage a locking mechanism for a server process, ensuring that only one instance runs for a given display. It first constructs a temporary lock file path based on predefined constants and the display identifier. The function attempts to create this temporary file, writing the current process ID (PID) to it, and then changes its permissions. It then tries to link this temporary file to a permanent lock file, checking for existing locks by reading the PID from the lock file and verifying if the process is still active. If the lock is valid, it completes successfully; otherwise, it handles stale locks or existing active processes, ultimately ensuring that only one server instance can operate at a time.","The code implements a locking mechanism for a server process to prevent multiple instances from running simultaneously for a specific display. It constructs a temporary file path and attempts to create a temporary lock file, writing the current process ID to it. The code then tries to establish a permanent lock by linking the temporary file to a designated lock file, checking for existing locks by reading the process ID and verifying if the associated process is still active. If a valid lock is established, the function completes; otherwise, it handles stale locks or active processes appropriately.","The vulnerability in the original code arises from the potential for a race condition when creating and linking the lock file, which could allow multiple processes to believe they have exclusive access. Specifically, if a process checks for the existence of the lock file and finds it absent, it may create its own lock file before the first process has completed its operations, leading to conflicts. The fix addresses this by ensuring that the temporary file is created with exclusive flags and by checking the existence of the lock file more robustly, thus preventing the scenario where multiple processes can simultaneously create valid lock files.","The vulnerability in the original code stems from a race condition that allows multiple processes to create lock files simultaneously, potentially leading to conflicts and multiple instances of the server running. This occurs because the code checks for the existence of a lock file and creates a new one without sufficient synchronization, allowing a second process to intervene. The fix enhances the locking mechanism by ensuring that the temporary file is created with exclusive access flags and by implementing more robust checks for existing lock files, thereby preventing multiple processes from successfully creating their own lock files and ensuring proper server instance management.","The vulnerability was fixed by improving the file creation and locking logic. The code now uses the `O_CREAT | O_EXCL` flags when opening the temporary file, ensuring that if the file already exists, the operation fails, thus preventing race conditions. Additionally, the code checks for the existence of the lock file more rigorously by reading the PID from the existing lock file and verifying if the associated process is still active using the `kill` function. This ensures that stale lock files are removed, and only one server instance can successfully create and maintain the lock file, enhancing overall process management.","The vulnerability was addressed by enhancing the file creation process to ensure exclusive access, preventing multiple processes from simultaneously creating lock files. The code now checks for the existence of a lock file more thoroughly, ensuring that if a lock already exists, it cannot be overwritten without proper validation. Additionally, the implementation includes checks to verify whether the process associated with an existing lock is still active, allowing for the removal of stale locks. These changes collectively ensure that only one instance of the server can operate at a time, effectively mitigating the risk of race conditions."
4,178753,178753,,Local,Not required,Complete,CVE-2012-3511,https://www.cvedetails.com/cve/CVE-2012-3511/,CWE-362,High,Complete,Complete,,2012-10-03,6.2,Multiple race conditions in the madvise_remove function in mm/madvise.c in the Linux kernel before 3.4.5 allow local users to cause a denial of service (use-after-free and system crash) via vectors involving a (1) munmap or (2) close system call.,2013-10-23,DoS ,13,https://github.com/torvalds/linux/commit/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,"mm: Hold a file reference in madvise_remove

Otherwise the code races with munmap (causing a use-after-free
of the vma) or with close (causing a use-after-free of the struct
file).

The bug was introduced by commit 90ed52ebe481 (""[PATCH] holepunch: fix
mmap_sem i_mutex deadlock"")

Cc: Hugh Dickins <hugh@veritas.com>
Cc: Miklos Szeredi <mszeredi@suse.cz>
Cc: Badari Pulavarty <pbadari@us.ibm.com>
Cc: Nick Piggin <npiggin@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Andy Lutomirski <luto@amacapital.net>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",4,mm/madvise.c,"{""sha"": ""14d260fa0d17939a2279c244df91789cd30720e4"", ""filename"": ""mm/madvise.c"", ""status"": ""modified"", ""additions"": 14, ""deletions"": 4, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb/mm/madvise.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb/mm/madvise.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/madvise.c?ref=9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb"", ""patch"": ""@@ -15,6 +15,7 @@\n #include <linux/sched.h>\n #include <linux/ksm.h>\n #include <linux/fs.h>\n+#include <linux/file.h>\n \n /*\n  * Any behaviour which results in changes to the vma->vm_flags needs to\n@@ -204,14 +205,16 @@ static long madvise_remove(struct vm_area_struct *vma,\n {\n \tloff_t offset;\n \tint error;\n+\tstruct file *f;\n \n \t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n \n \tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n \t\treturn -EINVAL;\n \n-\tif (!vma->vm_file || !vma->vm_file->f_mapping\n-\t\t|| !vma->vm_file->f_mapping->host) {\n+\tf = vma->vm_file;\n+\n+\tif (!f || !f->f_mapping || !f->f_mapping->host) {\n \t\t\treturn -EINVAL;\n \t}\n \n@@ -221,11 +224,18 @@ static long madvise_remove(struct vm_area_struct *vma,\n \toffset = (loff_t)(start - vma->vm_start)\n \t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n \n-\t/* filesystem's fallocate may need to take i_mutex */\n+\t/*\n+\t * Filesystem's fallocate may need to take i_mutex.  We need to\n+\t * explicitly grab a reference because the vma (and hence the\n+\t * vma's reference to the file) can go away as soon as we drop\n+\t * mmap_sem.\n+\t */\n+\tget_file(f);\n \tup_read(&current->mm->mmap_sem);\n-\terror = do_fallocate(vma->vm_file,\n+\terror = do_fallocate(f,\n \t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n \t\t\t\toffset, end - start);\n+\tfput(f);\n \tdown_read(&current->mm->mmap_sem);\n \treturn error;\n }""}","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/* filesystem's fallocate may need to take i_mutex */
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(vma->vm_file,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
",C,"	struct file *f;
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
	error = do_fallocate(f,
	fput(f);
","	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
	/* filesystem's fallocate may need to take i_mutex */
	error = do_fallocate(vma->vm_file,
",,"@@ -15,6 +15,7 @@
 #include <linux/sched.h>
 #include <linux/ksm.h>
 #include <linux/fs.h>
+#include <linux/file.h>
 
 /*
  * Any behaviour which results in changes to the vma->vm_flags needs to
@@ -204,14 +205,16 @@ static long madvise_remove(struct vm_area_struct *vma,
 {
 	loff_t offset;
 	int error;
+	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
-	if (!vma->vm_file || !vma->vm_file->f_mapping
-		|| !vma->vm_file->f_mapping->host) {
+	f = vma->vm_file;
+
+	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
@@ -221,11 +224,18 @@ static long madvise_remove(struct vm_area_struct *vma,
 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
-	/* filesystem's fallocate may need to take i_mutex */
+	/*
+	 * Filesystem's fallocate may need to take i_mutex.  We need to
+	 * explicitly grab a reference because the vma (and hence the
+	 * vma's reference to the file) can go away as soon as we drop
+	 * mmap_sem.
+	 */
+	get_file(f);
 	up_read(&current->mm->mmap_sem);
-	error = do_fallocate(vma->vm_file,
+	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
+	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }",linux,9ab4233dd08036fe34a89c7dc6f47a8bf2eb29eb,1b7fa4c27111757789b21bb78543317dad4cfd08,1,"static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
//fix_flaw_line_below:
//	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
//flaw_line_below:
	if (!vma->vm_file || !vma->vm_file->f_mapping
//flaw_line_below:
		|| !vma->vm_file->f_mapping->host) {
//fix_flaw_line_below:
//	f = vma->vm_file;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
//flaw_line_below:
	/* filesystem's fallocate may need to take i_mutex */
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * Filesystem's fallocate may need to take i_mutex.  We need to
//fix_flaw_line_below:
//	 * explicitly grab a reference because the vma (and hence the
//fix_flaw_line_below:
//	 * vma's reference to the file) can go away as soon as we drop
//fix_flaw_line_below:
//	 * mmap_sem.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	get_file(f);
 	up_read(&current->mm->mmap_sem);
//flaw_line_below:
	error = do_fallocate(vma->vm_file,
//fix_flaw_line_below:
//	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
//fix_flaw_line_below:
//	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
",178753,"static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/* filesystem's fallocate may need to take i_mutex */
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(vma->vm_file,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","static long madvise_remove(struct vm_area_struct *vma,
				struct vm_area_struct **prev,
				unsigned long start, unsigned long end)
 {
 	loff_t offset;
 	int error;
	struct file *f;
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 		return -EINVAL;
 
	f = vma->vm_file;

	if (!f || !f->f_mapping || !f->f_mapping->host) {
 			return -EINVAL;
 	}
 
	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
		return -EACCES;

 	offset = (loff_t)(start - vma->vm_start)
 			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
 
	/*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
	 */
	get_file(f);
 	up_read(&current->mm->mmap_sem);
	error = do_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
	fput(f);
 	down_read(&current->mm->mmap_sem);
 	return error;
 }
","The `madvise_remove` function is designed to handle memory management advice for a specific virtual memory area (VMA) in a Linux kernel context. It first checks if the VMA has certain flags set, such as being locked or using huge pages, returning an error if so. It also verifies that the VMA is associated with a valid file and that it has the appropriate shared and writable permissions. If these conditions are met, it calculates the offset for the specified range and invokes `do_fallocate` to punch a hole in the file, effectively removing the specified memory range while maintaining the file size.","The provided code defines a function that manages memory advice for a specific area in a virtual memory space. It begins by ensuring that certain conditions regarding memory flags are met, returning an error if they are not. The function checks for the presence of a valid file associated with the memory area and verifies that the necessary permissions are granted. If all checks pass, it calculates the appropriate offset for the specified memory range and proceeds to modify the underlying file by removing the specified memory segment while keeping the overall file size intact.","The vulnerability in the original code arises from the lack of a proper reference to the file associated with the virtual memory area, which can lead to use-after-free errors when the memory area is unmapped or the file is closed. Specifically, if the `mmap_sem` is dropped, the `vma` could be freed, resulting in a dangling pointer when accessing the file. The fix introduces a reference to the file using `get_file(f)` before dropping the semaphore, ensuring that the file remains valid during the operation. After the file operation is complete, the reference is released with `fput(f)`, preventing potential race conditions.","The vulnerability in the original code stems from the absence of a mechanism to maintain a valid reference to the file associated with a memory area, which can lead to use-after-free errors when the memory area is unmapped or the file is closed. This situation arises when the code drops a semaphore, allowing the memory area or file structure to be freed while still being accessed. The fix addresses this issue by explicitly acquiring a reference to the file before releasing the semaphore, ensuring that the file remains valid throughout the operation, and subsequently releasing the reference after the operation is complete to prevent memory leaks.","The vulnerability was fixed by introducing a reference counting mechanism for the file associated with the virtual memory area. Specifically, the code now uses `get_file(f)` to increment the reference count before dropping the `mmap_sem`, ensuring that the file remains valid during the operation. This prevents use-after-free errors that could occur if the memory area was unmapped or the file was closed while the operation was in progress. After the file operation is completed with `do_fallocate`, the reference is released using `fput(f)`, thereby maintaining proper memory management and preventing potential race conditions.","The vulnerability was addressed by implementing a reference counting mechanism for the file associated with the memory area. This involved acquiring a reference to the file before releasing the semaphore, ensuring that the file remains valid and accessible throughout the operation. By doing so, the code prevents potential use-after-free errors that could occur if the memory area was unmapped or the file was closed during execution. After completing the necessary file operations, the reference is released, thereby maintaining proper memory management and safeguarding against race conditions that could lead to instability or crashes."
5,179043,179043,,Local,Not required,Complete,CVE-2011-2183,https://www.cvedetails.com/cve/CVE-2011-2183/,CWE-362,High,,,,2012-06-13,4.0,"Race condition in the scan_get_next_rmap_item function in mm/ksm.c in the Linux kernel before 2.6.39.3, when Kernel SamePage Merging (KSM) is enabled, allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via a crafted application.",2012-06-14,DoS ,6,https://github.com/torvalds/linux/commit/2b472611a32a72f4a118c069c2d62a1a3f087afd,2b472611a32a72f4a118c069c2d62a1a3f087afd,"ksm: fix NULL pointer dereference in scan_get_next_rmap_item()

Andrea Righi reported a case where an exiting task can race against
ksmd::scan_get_next_rmap_item (http://lkml.org/lkml/2011/6/1/742) easily
triggering a NULL pointer dereference in ksmd.

ksm_scan.mm_slot == &ksm_mm_head with only one registered mm

CPU 1 (__ksm_exit)		CPU 2 (scan_get_next_rmap_item)
 				list_empty() is false
lock				slot == &ksm_mm_head
list_del(slot->mm_list)
(list now empty)
unlock
				lock
				slot = list_entry(slot->mm_list.next)
				(list is empty, so slot is still ksm_mm_head)
				unlock
				slot->mm == NULL ... Oops

Close this race by revalidating that the new slot is not simply the list
head again.

Andrea's test case:

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/mman.h>

#define BUFSIZE getpagesize()

int main(int argc, char **argv)
{
	void *ptr;

	if (posix_memalign(&ptr, getpagesize(), BUFSIZE) < 0) {
		perror(""posix_memalign"");
		exit(1);
	}
	if (madvise(ptr, BUFSIZE, MADV_MERGEABLE) < 0) {
		perror(""madvise"");
		exit(1);
	}
	*(char *)NULL = 0;

	return 0;
}

Reported-by: Andrea Righi <andrea@betterlinux.com>
Tested-by: Andrea Righi <andrea@betterlinux.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
Signed-off-by: Hugh Dickins <hughd@google.com>
Signed-off-by: Chris Wright <chrisw@sous-sol.org>
Cc: <stable@kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,mm/ksm.c,"{""sha"": ""9a68b0cf0a1c4c8009ee25d2990530d7e2927132"", ""filename"": ""mm/ksm.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 0, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/2b472611a32a72f4a118c069c2d62a1a3f087afd/mm/ksm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/2b472611a32a72f4a118c069c2d62a1a3f087afd/mm/ksm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/ksm.c?ref=2b472611a32a72f4a118c069c2d62a1a3f087afd"", ""patch"": ""@@ -1302,6 +1302,12 @@ static struct rmap_item *scan_get_next_rmap_item(struct page **page)\n \t\tslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\n \t\tksm_scan.mm_slot = slot;\n \t\tspin_unlock(&ksm_mmlist_lock);\n+\t\t/*\n+\t\t * Although we tested list_empty() above, a racing __ksm_exit\n+\t\t * of the last mm on the list may have removed it since then.\n+\t\t */\n+\t\tif (slot == &ksm_mm_head)\n+\t\t\treturn NULL;\n next_mm:\n \t\tksm_scan.address = 0;\n \t\tksm_scan.rmap_list = &slot->rmap_list;""}","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
",C,"		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
",,,"@@ -1302,6 +1302,12 @@ static struct rmap_item *scan_get_next_rmap_item(struct page **page)
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
+		/*
+		 * Although we tested list_empty() above, a racing __ksm_exit
+		 * of the last mm on the list may have removed it since then.
+		 */
+		if (slot == &ksm_mm_head)
+			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;",linux,2b472611a32a72f4a118c069c2d62a1a3f087afd,c7cbb02222eccb82bfd42696b01abceddae663f2,1,"static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * Although we tested list_empty() above, a racing __ksm_exit
//fix_flaw_line_below:
//		 * of the last mm on the list may have removed it since then.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (slot == &ksm_mm_head)
//fix_flaw_line_below:
//			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
",179043,"static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		root_unstable_tree = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
 		ksm_scan.mm_slot = slot;
 		spin_unlock(&ksm_mmlist_lock);
		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
 next_mm:
 		ksm_scan.address = 0;
 		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hlist_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
","The `scan_get_next_rmap_item` function is designed to traverse and manage memory mapping structures in a Linux kernel context, specifically for Kernel Same-page Merging (KSM). It begins by checking if there are any memory management slots available for scanning. If the current slot is the head, it drains any lingering pages and prepares for a new scan. The function iterates through virtual memory areas (VMAs) to identify mergeable pages, checking their properties and flushing them as necessary. It retrieves the next reverse mapping item (rmap_item) for eligible pages, updating the scanning state accordingly. If no mergeable pages are found, it cleans up the memory slot and prepares for the next iteration, ultimately returning the found rmap_item or NULL when complete.","The provided code implements a function that scans through memory management structures to identify and manage pages eligible for merging in a memory optimization process. It begins by checking for available memory slots and, if necessary, drains lingering pages. The function iterates through virtual memory areas to locate pages that can be merged, checking their properties and performing necessary operations on them. It retrieves relevant mapping information for these pages, updating the scanning state as it progresses. If no suitable pages are found, it cleans up the memory slot and prepares for the next iteration, ultimately returning the identified mapping information or indicating completion.","The vulnerability in the original code arises from a race condition between two threads, where one thread can remove the last memory management structure from the list while another thread is attempting to access it. This leads to a situation where the code attempts to dereference a pointer that may be NULL, specifically when checking if the current slot is still valid after it has been updated. The fix addresses this issue by adding a validation step after updating the slot, ensuring that if the new slot is the list head, the function returns NULL immediately, thus preventing any further dereferencing of a potentially NULL pointer and avoiding the crash.","The vulnerability in the original code stems from a race condition where one thread can remove the last memory management structure from a list while another thread is trying to access it. This can lead to a situation where the code attempts to dereference a pointer that may be NULL, resulting in a crash. The fix addresses this issue by adding a validation step after updating the pointer to ensure that it does not point back to the list head, which indicates an empty list. This additional check prevents any further dereferencing of a potentially NULL pointer, thereby eliminating the risk of a crash.","The vulnerability was fixed by adding a validation check after updating the `slot` variable to ensure it does not point back to `ksm_mm_head`, which indicates that the list is empty. Specifically, after the line that updates `slot` with `list_entry(slot->mm_list.next, struct mm_slot, mm_list)`, the code now checks if `slot` is still equal to `ksm_mm_head`. If it is, the function immediately returns NULL, preventing any further dereferencing of `slot->mm`, which could lead to a NULL pointer dereference. This effectively closes the race condition that could occur during concurrent access.","The vulnerability was addressed by introducing a validation step after updating the pointer that references the memory management structure. This step checks whether the updated pointer points back to the list head, which would indicate that the list is empty. If the pointer is found to be pointing to the list head, the function immediately returns NULL, thereby preventing any further dereferencing of a potentially NULL pointer. This modification effectively mitigates the race condition that could occur when one thread modifies the list while another thread attempts to access it, ensuring safer concurrent operations."
6,179198,179198,,Local,Not required,Partial,CVE-2013-3302,https://www.cvedetails.com/cve/CVE-2013-3302/,CWE-362,Medium,Partial,Partial,,2013-04-29,4.4,Race condition in the smb_send_rqst function in fs/cifs/transport.c in the Linux kernel before 3.7.2 allows local users to cause a denial of service (NULL pointer dereference and OOPS) or possibly have unspecified other impact via vectors involving a reconnection event.,2013-05-03,DoS ,3,https://github.com/torvalds/linux/commit/ea702b80e0bbb2448e201472127288beb82ca2fe,ea702b80e0bbb2448e201472127288beb82ca2fe,"cifs: move check for NULL socket into smb_send_rqst

Cai reported this oops:

[90701.616664] BUG: unable to handle kernel NULL pointer dereference at 0000000000000028
[90701.625438] IP: [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.632167] PGD fea319067 PUD 103fda4067 PMD 0
[90701.637255] Oops: 0000 [#1] SMP
[90701.640878] Modules linked in: des_generic md4 nls_utf8 cifs dns_resolver binfmt_misc tun sg igb iTCO_wdt iTCO_vendor_support lpc_ich pcspkr i2c_i801 i2c_core i7core_edac edac_core ioatdma dca mfd_core coretemp kvm_intel kvm crc32c_intel microcode sr_mod cdrom ata_generic sd_mod pata_acpi crc_t10dif ata_piix libata megaraid_sas dm_mirror dm_region_hash dm_log dm_mod
[90701.677655] CPU 10
[90701.679808] Pid: 9627, comm: ls Tainted: G        W    3.7.1+ #10 QCI QSSC-S4R/QSSC-S4R
[90701.688950] RIP: 0010:[<ffffffff814a343e>]  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.698383] RSP: 0018:ffff88177b431bb8  EFLAGS: 00010206
[90701.704309] RAX: ffff88177b431fd8 RBX: 00007ffffffff000 RCX: ffff88177b431bec
[90701.712271] RDX: 0000000000000003 RSI: 0000000000000006 RDI: 0000000000000000
[90701.720223] RBP: ffff88177b431bc8 R08: 0000000000000004 R09: 0000000000000000
[90701.728185] R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000001
[90701.736147] R13: ffff88184ef92000 R14: 0000000000000023 R15: ffff88177b431c88
[90701.744109] FS:  00007fd56a1a47c0(0000) GS:ffff88105fc40000(0000) knlGS:0000000000000000
[90701.753137] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
[90701.759550] CR2: 0000000000000028 CR3: 000000104f15f000 CR4: 00000000000007e0
[90701.767512] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
[90701.775465] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
[90701.783428] Process ls (pid: 9627, threadinfo ffff88177b430000, task ffff88185ca4cb60)
[90701.792261] Stack:
[90701.794505]  0000000000000023 ffff88177b431c50 ffff88177b431c38 ffffffffa014fcb1
[90701.802809]  ffff88184ef921bc 0000000000000000 00000001ffffffff ffff88184ef921c0
[90701.811123]  ffff88177b431c08 ffffffff815ca3d9 ffff88177b431c18 ffff880857758000
[90701.819433] Call Trace:
[90701.822183]  [<ffffffffa014fcb1>] smb_send_rqst+0x71/0x1f0 [cifs]
[90701.828991]  [<ffffffff815ca3d9>] ? schedule+0x29/0x70
[90701.834736]  [<ffffffffa014fe6d>] smb_sendv+0x3d/0x40 [cifs]
[90701.841062]  [<ffffffffa014fe96>] smb_send+0x26/0x30 [cifs]
[90701.847291]  [<ffffffffa015801f>] send_nt_cancel+0x6f/0xd0 [cifs]
[90701.854102]  [<ffffffffa015075e>] SendReceive+0x18e/0x360 [cifs]
[90701.860814]  [<ffffffffa0134a78>] CIFSFindFirst+0x1a8/0x3f0 [cifs]
[90701.867724]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs]
[90701.875601]  [<ffffffffa013f731>] ? build_path_from_dentry+0xf1/0x260 [cifs]
[90701.883477]  [<ffffffffa01578e6>] cifs_query_dir_first+0x26/0x30 [cifs]
[90701.890869]  [<ffffffffa015480d>] initiate_cifs_search+0xed/0x250 [cifs]
[90701.898354]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.904486]  [<ffffffffa01554cb>] cifs_readdir+0x45b/0x8f0 [cifs]
[90701.911288]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.917410]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.923533]  [<ffffffff81195970>] ? fillonedir+0x100/0x100
[90701.929657]  [<ffffffff81195848>] vfs_readdir+0xb8/0xe0
[90701.935490]  [<ffffffff81195b9f>] sys_getdents+0x8f/0x110
[90701.941521]  [<ffffffff815d3b99>] system_call_fastpath+0x16/0x1b
[90701.948222] Code: 66 90 55 65 48 8b 04 25 f0 c6 00 00 48 89 e5 53 48 83 ec 08 83 fe 01 48 8b 98 48 e0 ff ff 48 c7 80 48 e0 ff ff ff ff ff ff 74 22 <48> 8b 47 28 ff 50 68 65 48 8b 14 25 f0 c6 00 00 48 89 9a 48 e0
[90701.970313] RIP  [<ffffffff814a343e>] kernel_setsockopt+0x2e/0x60
[90701.977125]  RSP <ffff88177b431bb8>
[90701.981018] CR2: 0000000000000028
[90701.984809] ---[ end trace 24bd602971110a43 ]---

This is likely due to a race vs. a reconnection event.

The current code checks for a NULL socket in smb_send_kvec, but that's
too late. By the time that check is done, the socket will already have
been passed to kernel_setsockopt. Move the check into smb_send_rqst, so
that it's checked earlier.

In truth, this is a bit of a half-assed fix. The -ENOTSOCK error
return here looks like it could bubble back up to userspace. The locking
rules around the ssocket pointer are really unclear as well. There are
cases where the ssocket pointer is changed without holding the srv_mutex,
but I'm not clear whether there's a potential race here yet or not.

This code seems like it could benefit from some fundamental re-think of
how the socket handling should behave. Until then though, this patch
should at least fix the above oops in most cases.

Cc: <stable@vger.kernel.org> # 3.7+
Reported-and-Tested-by: CAI Qian <caiqian@redhat.com>
Signed-off-by: Jeff Layton <jlayton@redhat.com>
Signed-off-by: Steve French <smfrench@gmail.com>",0,fs/cifs/transport.c,"{""sha"": ""1a528680ec5a29e4b59485fba5df54568a1e3787"", ""filename"": ""fs/cifs/transport.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 3, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/ea702b80e0bbb2448e201472127288beb82ca2fe/fs/cifs/transport.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/ea702b80e0bbb2448e201472127288beb82ca2fe/fs/cifs/transport.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/cifs/transport.c?ref=ea702b80e0bbb2448e201472127288beb82ca2fe"", ""patch"": ""@@ -144,9 +144,6 @@ smb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n \n \t*sent = 0;\n \n-\tif (ssocket == NULL)\n-\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */\n-\n \tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n \tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n \tsmb_msg.msg_control = NULL;\n@@ -291,6 +288,9 @@ smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)\n \tstruct socket *ssocket = server->ssocket;\n \tint val = 1;\n \n+\tif (ssocket == NULL)\n+\t\treturn -ENOTSOCK;\n+\n \tcFYI(1, \""Sending smb: smb_len=%u\"", smb_buf_length);\n \tdump_smb(iov[0].iov_base, iov[0].iov_len);\n ""}","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
	if (ssocket == NULL)
		return -ENOTSOCK;

 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
",C,"	if (ssocket == NULL)
		return -ENOTSOCK;

",,,"@@ -144,9 +144,6 @@ smb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,
 
 	*sent = 0;
 
-	if (ssocket == NULL)
-		return -ENOTSOCK; /* BB eventually add reconnect code here */
-
 	smb_msg.msg_name = (struct sockaddr *) &server->dstaddr;
 	smb_msg.msg_namelen = sizeof(struct sockaddr);
 	smb_msg.msg_control = NULL;
@@ -291,6 +288,9 @@ smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
+	if (ssocket == NULL)
+		return -ENOTSOCK;
+
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 ",linux,ea702b80e0bbb2448e201472127288beb82ca2fe,ecccd1248d6e6986130ffcc3b0d003cb46a485c0,1,"smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
//fix_flaw_line_below:
//	if (ssocket == NULL)
//fix_flaw_line_below:
//		return -ENOTSOCK;
//fix_flaw_line_below:
//
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
",179198,"smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","smb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)
{
	int rc;
	struct kvec *iov = rqst->rq_iov;
	int n_vec = rqst->rq_nvec;
	unsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);
	unsigned int i;
	size_t total_len = 0, sent;
 	struct socket *ssocket = server->ssocket;
 	int val = 1;
 
	if (ssocket == NULL)
		return -ENOTSOCK;

 	cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 	dump_smb(iov[0].iov_base, iov[0].iov_len);
 
	/* cork the socket */
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	rc = smb_send_kvec(server, iov, n_vec, &sent);
	if (rc < 0)
		goto uncork;

	total_len += sent;

	/* now walk the page array and send each page in it */
	for (i = 0; i < rqst->rq_npages; i++) {
		struct kvec p_iov;

		cifs_rqst_page_to_kvec(rqst, i, &p_iov);
		rc = smb_send_kvec(server, &p_iov, 1, &sent);
		kunmap(rqst->rq_pages[i]);
		if (rc < 0)
			break;

		total_len += sent;
	}

uncork:
	/* uncork it */
	val = 0;
	kernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,
				(char *)&val, sizeof(val));

	if ((total_len > 0) && (total_len != smb_buf_length + 4)) {
		cFYI(1, ""partial send (wanted=%u sent=%zu): terminating ""
			""session"", smb_buf_length + 4, total_len);
		/*
		 * If we have only sent part of an SMB then the next SMB could
		 * be taken as the remainder of this one. We need to kill the
		 * socket so the server throws away the partial SMB
		 */
		server->tcpStatus = CifsNeedReconnect;
	}

	if (rc < 0 && rc != -EINTR)
		cERROR(1, ""Error %d sending data on socket to server"", rc);
	else
		rc = 0;

	return rc;
}
","The `smb_send_rqst` function is responsible for sending an SMB (Server Message Block) request over a TCP socket to a specified server. It begins by preparing the socket for sending data using TCP_CORK to optimize packet transmission. The function then sends the initial SMB data using `smb_send_kvec`, followed by iterating through any additional pages of data, sending each one individually. After sending, it uncorks the socket to allow normal operation. If the total bytes sent do not match the expected length, it marks the server's TCP status for reconnection to handle potential data corruption. Finally, it returns the result of the send operation.","The function facilitates the transmission of a request over a TCP connection to a server. It begins by preparing the socket for efficient data sending and then transmits the initial data segment. Following this, it processes and sends additional data pages sequentially. After completing the data transmission, it reverts the socket settings to allow normal operation. If the total amount of data sent does not match the expected size, it indicates a potential issue, prompting a status update for reconnection to prevent data corruption. Finally, the function returns the outcome of the transmission attempt.","The vulnerability in the original code stemmed from a lack of a check for a NULL socket before invoking a function that sets socket options, which could lead to a kernel NULL pointer dereference if the socket was not properly initialized. This was evidenced by a reported crash when the socket was NULL, causing the system to attempt to access an invalid memory address. The fix involved moving the NULL socket check into the `smb_send_rqst` function, ensuring that the socket is validated before any operations are performed on it, thus preventing the dereference error and improving overall stability.","The vulnerability in the original code arose from the absence of a check for a NULL socket before attempting to set socket options, which could lead to a kernel NULL pointer dereference and crash the system. This issue was highlighted by a reported crash when the socket was NULL, causing an invalid memory access. The fix involved moving the NULL socket check to an earlier point in the code, ensuring that the socket is validated before any operations are performed on it. This change prevents the dereference error and enhances the stability of the code during socket operations.","The vulnerability was fixed by moving the NULL socket check into the `smb_send_rqst` function, specifically before the call to `kernel_setsockopt`. This adjustment ensures that the socket is validated early in the process, preventing any attempts to set options on a NULL socket, which previously led to a kernel NULL pointer dereference. By adding the check for `ssocket == NULL` at the beginning of the function, the code can now return an error code before any unsafe operations are performed, thereby enhancing the overall stability and reliability of the socket handling.","The vulnerability was addressed by implementing an early check for a NULL socket before any operations that could lead to a crash. This change ensures that the code verifies the validity of the socket before attempting to set options or perform any actions that rely on it being properly initialized. By moving the validation to the beginning of the function, the risk of encountering a NULL pointer dereference during execution is eliminated, thereby enhancing the stability and reliability of the socket handling process and preventing potential system crashes."
7,179380,179380,,Remote,Not required,Complete,CVE-2011-4348,https://www.cvedetails.com/cve/CVE-2011-4348/,CWE-362,Medium,,,,2013-06-08,7.1,"Race condition in the sctp_rcv function in net/sctp/input.c in the Linux kernel before 2.6.29 allows remote attackers to cause a denial of service (system hang) via SCTP packets.  NOTE: in some environments, this issue exists because of an incomplete fix for CVE-2011-2482.",2013-07-25,DoS ,13,https://github.com/torvalds/linux/commit/ae53b5bd77719fed58086c5be60ce4f22bffe1c6,ae53b5bd77719fed58086c5be60ce4f22bffe1c6,"sctp: Fix another socket race during accept/peeloff

There is a race between sctp_rcv() and sctp_accept() where we
have moved the association from the listening socket to the
accepted socket, but sctp_rcv() processing cached the old
socket and continues to use it.

The easy solution is to check for the socket mismatch once we've
grabed the socket lock.  If we hit a mis-match, that means
that were are currently holding the lock on the listening socket,
but the association is refrencing a newly accepted socket.  We need
to drop the lock on the old socket and grab the lock on the new one.

A more proper solution might be to create accepted sockets when
the new association is established, similar to TCP.  That would
eliminate the race for 1-to-1 style sockets, but it would still
existing for 1-to-many sockets where a user wished to peeloff an
association.  For now, we'll live with this easy solution as
it addresses the problem.

Reported-by: Michal Hocko <mhocko@suse.cz>
Reported-by: Karsten Keil <kkeil@suse.de>
Signed-off-by: Vlad Yasevich <vladislav.yasevich@hp.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",0,net/sctp/input.c,"{""sha"": ""2e4a8646dbc389dcb55fcce1cf6b3ad0f608af0d"", ""filename"": ""net/sctp/input.c"", ""status"": ""modified"", ""additions"": 13, ""deletions"": 0, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/ae53b5bd77719fed58086c5be60ce4f22bffe1c6/net/sctp/input.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/ae53b5bd77719fed58086c5be60ce4f22bffe1c6/net/sctp/input.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/sctp/input.c?ref=ae53b5bd77719fed58086c5be60ce4f22bffe1c6"", ""patch"": ""@@ -249,6 +249,19 @@ int sctp_rcv(struct sk_buff *skb)\n \t */\n \tsctp_bh_lock_sock(sk);\n \n+\tif (sk != rcvr->sk) {\n+\t\t/* Our cached sk is different from the rcvr->sk.  This is\n+\t\t * because migrate()/accept() may have moved the association\n+\t\t * to a new socket and released all the sockets.  So now we\n+\t\t * are holding a lock on the old socket while the user may\n+\t\t * be doing something with the new socket.  Switch our veiw\n+\t\t * of the current sk.\n+\t\t */\n+\t\tsctp_bh_unlock_sock(sk);\n+\t\tsk = rcvr->sk;\n+\t\tsctp_bh_lock_sock(sk);\n+\t}\n+\n \tif (sock_owned_by_user(sk)) {\n \t\tSCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);\n \t\tsctp_add_backlog(sk, skb);""}","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
",C,"	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

",,,"@@ -249,6 +249,19 @@ int sctp_rcv(struct sk_buff *skb)
 	 */
 	sctp_bh_lock_sock(sk);
 
+	if (sk != rcvr->sk) {
+		/* Our cached sk is different from the rcvr->sk.  This is
+		 * because migrate()/accept() may have moved the association
+		 * to a new socket and released all the sockets.  So now we
+		 * are holding a lock on the old socket while the user may
+		 * be doing something with the new socket.  Switch our veiw
+		 * of the current sk.
+		 */
+		sctp_bh_unlock_sock(sk);
+		sk = rcvr->sk;
+		sctp_bh_lock_sock(sk);
+	}
+
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);",linux,ae53b5bd77719fed58086c5be60ce4f22bffe1c6,759af00ebef858015eb68876ac1f383bcb6a1774,1,"int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
//fix_flaw_line_below:
//	if (sk != rcvr->sk) {
//fix_flaw_line_below:
//		/* Our cached sk is different from the rcvr->sk.  This is
//fix_flaw_line_below:
//		 * because migrate()/accept() may have moved the association
//fix_flaw_line_below:
//		 * to a new socket and released all the sockets.  So now we
//fix_flaw_line_below:
//		 * are holding a lock on the old socket while the user may
//fix_flaw_line_below:
//		 * be doing something with the new socket.  Switch our veiw
//fix_flaw_line_below:
//		 * of the current sk.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		sctp_bh_unlock_sock(sk);
//fix_flaw_line_below:
//		sk = rcvr->sk;
//fix_flaw_line_below:
//		sctp_bh_lock_sock(sk);
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
",179380,"int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","int sctp_rcv(struct sk_buff *skb)
{
	struct sock *sk;
	struct sctp_association *asoc;
	struct sctp_endpoint *ep = NULL;
	struct sctp_ep_common *rcvr;
	struct sctp_transport *transport = NULL;
	struct sctp_chunk *chunk;
	struct sctphdr *sh;
	union sctp_addr src;
	union sctp_addr dest;
	int family;
	struct sctp_af *af;

	if (skb->pkt_type!=PACKET_HOST)
		goto discard_it;

	SCTP_INC_STATS_BH(SCTP_MIB_INSCTPPACKS);

	if (skb_linearize(skb))
		goto discard_it;

	sh = sctp_hdr(skb);

	/* Pull up the IP and SCTP headers. */
	__skb_pull(skb, skb_transport_offset(skb));
	if (skb->len < sizeof(struct sctphdr))
		goto discard_it;
	if (!skb_csum_unnecessary(skb) && sctp_rcv_checksum(skb) < 0)
		goto discard_it;

	skb_pull(skb, sizeof(struct sctphdr));

	/* Make sure we at least have chunk headers worth of data left. */
	if (skb->len < sizeof(struct sctp_chunkhdr))
		goto discard_it;

	family = ipver2af(ip_hdr(skb)->version);
	af = sctp_get_af_specific(family);
	if (unlikely(!af))
		goto discard_it;

	/* Initialize local addresses for lookups. */
	af->from_skb(&src, skb, 1);
	af->from_skb(&dest, skb, 0);

	/* If the packet is to or from a non-unicast address,
	 * silently discard the packet.
	 *
	 * This is not clearly defined in the RFC except in section
	 * 8.4 - OOTB handling.  However, based on the book ""Stream Control
	 * Transmission Protocol"" 2.1, ""It is important to note that the
	 * IP address of an SCTP transport address must be a routable
	 * unicast address.  In other words, IP multicast addresses and
	 * IP broadcast addresses cannot be used in an SCTP transport
	 * address.""
	 */
	if (!af->addr_valid(&src, NULL, skb) ||
	    !af->addr_valid(&dest, NULL, skb))
		goto discard_it;

	asoc = __sctp_rcv_lookup(skb, &src, &dest, &transport);

	if (!asoc)
		ep = __sctp_rcv_lookup_endpoint(&dest);

	/* Retrieve the common input handling substructure. */
	rcvr = asoc ? &asoc->base : &ep->base;
	sk = rcvr->sk;

	/*
	 * If a frame arrives on an interface and the receiving socket is
	 * bound to another interface, via SO_BINDTODEVICE, treat it as OOTB
	 */
	if (sk->sk_bound_dev_if && (sk->sk_bound_dev_if != af->skb_iif(skb)))
	{
		if (asoc) {
			sctp_association_put(asoc);
			asoc = NULL;
		} else {
			sctp_endpoint_put(ep);
			ep = NULL;
		}
		sk = sctp_get_ctl_sock();
		ep = sctp_sk(sk)->ep;
		sctp_endpoint_hold(ep);
		rcvr = &ep->base;
	}

	/*
	 * RFC 2960, 8.4 - Handle ""Out of the blue"" Packets.
	 * An SCTP packet is called an ""out of the blue"" (OOTB)
	 * packet if it is correctly formed, i.e., passed the
	 * receiver's checksum check, but the receiver is not
	 * able to identify the association to which this
	 * packet belongs.
	 */
	if (!asoc) {
		if (sctp_rcv_ootb(skb)) {
			SCTP_INC_STATS_BH(SCTP_MIB_OUTOFBLUES);
			goto discard_release;
		}
	}

	if (!xfrm_policy_check(sk, XFRM_POLICY_IN, skb, family))
		goto discard_release;
	nf_reset(skb);

	if (sk_filter(sk, skb))
		goto discard_release;

	/* Create an SCTP packet structure. */
	chunk = sctp_chunkify(skb, asoc, sk);
	if (!chunk)
		goto discard_release;
	SCTP_INPUT_CB(skb)->chunk = chunk;

	/* Remember what endpoint is to handle this packet. */
	chunk->rcvr = rcvr;

	/* Remember the SCTP header. */
	chunk->sctp_hdr = sh;

	/* Set the source and destination addresses of the incoming chunk.  */
	sctp_init_addrs(chunk, &src, &dest);

	/* Remember where we came from.  */
	chunk->transport = transport;

	/* Acquire access to the sock lock. Note: We are safe from other
	 * bottom halves on this lock, but a user may be in the lock too,
	 * so check if it is busy.
 	 */
 	sctp_bh_lock_sock(sk);
 
	if (sk != rcvr->sk) {
		/* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
		 */
		sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
		sctp_bh_lock_sock(sk);
	}

 	if (sock_owned_by_user(sk)) {
 		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 		sctp_add_backlog(sk, skb);
	} else {
		SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_SOFTIRQ);
		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
	}

	sctp_bh_unlock_sock(sk);

	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	return 0;

discard_it:
	SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_DISCARDS);
	kfree_skb(skb);
	return 0;

discard_release:
	/* Release the asoc/ep ref we took in the lookup calls. */
	if (asoc)
		sctp_association_put(asoc);
	else
		sctp_endpoint_put(ep);

	goto discard_it;
}
","The `sctp_rcv` function processes incoming SCTP (Stream Control Transmission Protocol) packets by first validating the packet type and performing checksum verification. It extracts the SCTP header and addresses, ensuring they are valid and unicast. The function then attempts to identify the associated SCTP association or endpoint. If the packet is unrecognized, it checks for ""out of the blue"" packets and applies necessary security policies. Valid packets are chunkified and queued for processing, while maintaining socket locks to manage concurrent access. Finally, it handles cleanup and statistics updates, discarding invalid packets as needed.","The code processes incoming packets for a specific transport protocol by validating their type and ensuring they meet checksum requirements. It extracts necessary headers and addresses, confirming they are valid and unicast. The function attempts to identify the relevant communication association or endpoint, handling cases where packets are unrecognized. It applies security policies and manages concurrent access to resources. Valid packets are organized into manageable structures and queued for further processing, while invalid packets are discarded, and statistics are updated to reflect the handling of incoming traffic and any discarded packets.","The vulnerability in the original code arises from a race condition between the packet reception function and the socket acceptance process, where the code could continue using a cached socket reference after the associated socket had been changed. This could lead to inconsistencies and potential data corruption. The fix implemented checks for a mismatch between the cached socket and the current socket after acquiring the socket lock. If a mismatch is detected, the code releases the lock on the old socket and acquires the lock on the new socket, ensuring that the correct socket is used for further processing and eliminating the race condition.","The vulnerability in the original code stems from a race condition between the packet reception process and the socket acceptance mechanism, where the code could inadvertently continue to reference an outdated socket after the association had been moved to a new socket. This could lead to inconsistencies and potential data corruption. The fix addresses this issue by checking for a mismatch between the cached socket and the current socket after acquiring the necessary lock. If a mismatch is found, the code releases the lock on the old socket and acquires the lock on the new socket, ensuring that the correct socket is used for subsequent operations.","The vulnerability was fixed by introducing a check for a socket mismatch after acquiring the socket lock. Specifically, after locking the socket with `sctp_bh_lock_sock`, the code compares the cached socket with the current socket referenced by the association. If a mismatch is detected, indicating that the association has moved to a new socket, the code releases the lock on the old socket using `sctp_bh_unlock_sock` and then locks the new socket by setting `sk` to the current socket. This ensures that the processing uses the correct socket, thereby eliminating the race condition.","The vulnerability was fixed by implementing a check for discrepancies between the cached socket and the current socket after acquiring the necessary lock. When a mismatch is detected, indicating that the association has transitioned to a new socket, the code releases the lock on the outdated socket and acquires the lock on the new one. This adjustment ensures that the processing consistently references the correct socket, thereby preventing potential inconsistencies and data corruption that could arise from using an outdated socket during packet reception and handling."
8,179565,179565,,Remote,Not required,Complete,CVE-2014-2706,https://www.cvedetails.com/cve/CVE-2014-2706/,CWE-362,Medium,,,,2014-04-14,7.1,"Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c.",2017-07-10,DoS ,15,https://github.com/torvalds/linux/commit/1d147bfa64293b2723c4fec50922168658e613ba,1d147bfa64293b2723c4fec50922168658e613ba,"mac80211: fix AP powersave TX vs. wakeup race

There is a race between the TX path and the STA wakeup: while
a station is sleeping, mac80211 buffers frames until it wakes
up, then the frames are transmitted. However, the RX and TX
path are concurrent, so the packet indicating wakeup can be
processed while a packet is being transmitted.

This can lead to a situation where the buffered frames list
is emptied on the one side, while a frame is being added on
the other side, as the station is still seen as sleeping in
the TX path.

As a result, the newly added frame will not be send anytime
soon. It might be sent much later (and out of order) when the
station goes to sleep and wakes up the next time.

Additionally, it can lead to the crash below.

Fix all this by synchronising both paths with a new lock.
Both path are not fastpath since they handle PS situations.

In a later patch we'll remove the extra skb queue locks to
reduce locking overhead.

BUG: unable to handle kernel
NULL pointer dereference at 000000b0
IP: [<ff6f1791>] ieee80211_report_used_skb+0x11/0x3e0 [mac80211]
*pde = 00000000
Oops: 0000 [#1] SMP DEBUG_PAGEALLOC
EIP: 0060:[<ff6f1791>] EFLAGS: 00210282 CPU: 1
EIP is at ieee80211_report_used_skb+0x11/0x3e0 [mac80211]
EAX: e5900da0 EBX: 00000000 ECX: 00000001 EDX: 00000000
ESI: e41d00c0 EDI: e5900da0 EBP: ebe458e4 ESP: ebe458b0
 DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
CR0: 8005003b CR2: 000000b0 CR3: 25a78000 CR4: 000407d0
DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
DR6: ffff0ff0 DR7: 00000400
Process iperf (pid: 3934, ti=ebe44000 task=e757c0b0 task.ti=ebe44000)
iwlwifi 0000:02:00.0: I iwl_pcie_enqueue_hcmd Sending command LQ_CMD (#4e), seq: 0x0903, 92 bytes at 3[3]:9
Stack:
 e403b32c ebe458c4 00200002 00200286 e403b338 ebe458cc c10960bb e5900da0
 ff76a6ec ebe458d8 00000000 e41d00c0 e5900da0 ebe458f0 ff6f1b75 e403b210
 ebe4598c ff723dc1 00000000 ff76a6ec e597c978 e403b758 00000002 00000002
Call Trace:
 [<ff6f1b75>] ieee80211_free_txskb+0x15/0x20 [mac80211]
 [<ff723dc1>] invoke_tx_handlers+0x1661/0x1780 [mac80211]
 [<ff7248a5>] ieee80211_tx+0x75/0x100 [mac80211]
 [<ff7249bf>] ieee80211_xmit+0x8f/0xc0 [mac80211]
 [<ff72550e>] ieee80211_subif_start_xmit+0x4fe/0xe20 [mac80211]
 [<c149ef70>] dev_hard_start_xmit+0x450/0x950
 [<c14b9aa9>] sch_direct_xmit+0xa9/0x250
 [<c14b9c9b>] __qdisc_run+0x4b/0x150
 [<c149f732>] dev_queue_xmit+0x2c2/0xca0

Cc: stable@vger.kernel.org
Reported-by: Yaara Rozenblum <yaara.rozenblum@intel.com>
Signed-off-by: Emmanuel Grumbach <emmanuel.grumbach@intel.com>
Reviewed-by: Stanislaw Gruszka <sgruszka@redhat.com>
[reword commit log, use a separate lock]
Signed-off-by: Johannes Berg <johannes.berg@intel.com>",0,net/mac80211/tx.c,"{""sha"": ""62a5f0889583437203a1a580b608829997c70ad1"", ""filename"": ""net/mac80211/sta_info.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 0, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/sta_info.c?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -330,6 +330,7 @@ struct sta_info *sta_info_alloc(struct ieee80211_sub_if_data *sdata,\n \trcu_read_unlock();\n \n \tspin_lock_init(&sta->lock);\n+\tspin_lock_init(&sta->ps_lock);\n \tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n \tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n \tmutex_init(&sta->ampdu_mlme.mtx);\n@@ -1109,6 +1110,8 @@ void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n \n \tskb_queue_head_init(&pending);\n \n+\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n+\tspin_lock(&sta->ps_lock);\n \t/* Send all buffered frames to the station */\n \tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n \t\tint count = skb_queue_len(&pending), tmp;\n@@ -1128,6 +1131,7 @@ void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n \t}\n \n \tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n+\tspin_unlock(&sta->ps_lock);\n \n \t/* This station just woke up and isn't aware of our SMPS state */\n \tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,""}<_**next**_>{""sha"": ""d3a6d8208f2f85f7db331238f41c0da2938f0f8d"", ""filename"": ""net/mac80211/sta_info.h"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 4, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/sta_info.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/sta_info.h?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -267,6 +267,7 @@ struct ieee80211_tx_latency_stat {\n  * @drv_unblock_wk: used for driver PS unblocking\n  * @listen_interval: listen interval of this station, when we're acting as AP\n  * @_flags: STA flags, see &enum ieee80211_sta_info_flags, do not use directly\n+ * @ps_lock: used for powersave (when mac80211 is the AP) related locking\n  * @ps_tx_buf: buffers (per AC) of frames to transmit to this station\n  *\twhen it leaves power saving state or polls\n  * @tx_filtered: buffers (per AC) of frames we already tried to\n@@ -356,10 +357,8 @@ struct sta_info {\n \t/* use the accessors defined below */\n \tunsigned long _flags;\n \n-\t/*\n-\t * STA powersave frame queues, no more than the internal\n-\t * locking required.\n-\t */\n+\t/* STA powersave lock and frame queues */\n+\tspinlock_t ps_lock;\n \tstruct sk_buff_head ps_tx_buf[IEEE80211_NUM_ACS];\n \tstruct sk_buff_head tx_filtered[IEEE80211_NUM_ACS];\n \tunsigned long driver_buffered_tids;""}<_**next**_>{""sha"": ""4080c615636fabf3d430ecd898d4349ccd213464"", ""filename"": ""net/mac80211/tx.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 0, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/tx.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/1d147bfa64293b2723c4fec50922168658e613ba/net/mac80211/tx.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/mac80211/tx.c?ref=1d147bfa64293b2723c4fec50922168658e613ba"", ""patch"": ""@@ -478,6 +478,20 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n \t\t       sta->sta.addr, sta->sta.aid, ac);\n \t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n \t\t\tpurge_old_ps_buffers(tx->local);\n+\n+\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n+\t\tspin_lock(&sta->ps_lock);\n+\t\t/*\n+\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n+\t\t * been queued to pending queue. No reordering can happen, go\n+\t\t * ahead and Tx the packet.\n+\t\t */\n+\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n+\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n+\t\t\tspin_unlock(&sta->ps_lock);\n+\t\t\treturn TX_CONTINUE;\n+\t\t}\n+\n \t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n \t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n \t\t\tps_dbg(tx->sdata,\n@@ -492,6 +506,7 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n \t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n \t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n \t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n+\t\tspin_unlock(&sta->ps_lock);\n \n \t\tif (!timer_pending(&local->sta_cleanup))\n \t\t\tmod_timer(&local->sta_cleanup,""}","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);

		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
",C,"
		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

		spin_unlock(&sta->ps_lock);
",,,"@@ -478,6 +478,20 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
+
+		/* sync with ieee80211_sta_ps_deliver_wakeup */
+		spin_lock(&sta->ps_lock);
+		/*
+		 * STA woke up the meantime and all the frames on ps_tx_buf have
+		 * been queued to pending queue. No reordering can happen, go
+		 * ahead and Tx the packet.
+		 */
+		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
+		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
+			spin_unlock(&sta->ps_lock);
+			return TX_CONTINUE;
+		}
+
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
@@ -492,6 +506,7 @@ ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
+		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,",linux,1d147bfa64293b2723c4fec50922168658e613ba,50c11eb9982554e9f99b7bab322c517cbe5ce1a1,1,"ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/* sync with ieee80211_sta_ps_deliver_wakeup */
//fix_flaw_line_below:
//		spin_lock(&sta->ps_lock);
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * STA woke up the meantime and all the frames on ps_tx_buf have
//fix_flaw_line_below:
//		 * been queued to pending queue. No reordering can happen, go
//fix_flaw_line_below:
//		 * ahead and Tx the packet.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
//fix_flaw_line_below:
//		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
//fix_flaw_line_below:
//			spin_unlock(&sta->ps_lock);
//fix_flaw_line_below:
//			return TX_CONTINUE;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
//fix_flaw_line_below:
//		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
",179565,"ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);
 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","ieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)
{
	struct sta_info *sta = tx->sta;
	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);
	struct ieee80211_local *local = tx->local;

	if (unlikely(!sta))
		return TX_CONTINUE;

	if (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||
		      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&
		     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {
		int ac = skb_get_queue_mapping(tx->skb);

		ps_dbg(sta->sdata, ""STA %pM aid %d: PS buffer for AC %d\n"",
 		       sta->sta.addr, sta->sta.aid, ac);
 		if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 			purge_old_ps_buffers(tx->local);

		/* sync with ieee80211_sta_ps_deliver_wakeup */
		spin_lock(&sta->ps_lock);
		/*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
		 */
		if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
			spin_unlock(&sta->ps_lock);
			return TX_CONTINUE;
		}

 		if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 			struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 			ps_dbg(tx->sdata,
			       ""STA %pM TX buffer for AC %d full - dropping oldest frame\n"",
			       sta->sta.addr, ac);
			ieee80211_free_txskb(&local->hw, old);
		} else
			tx->local->total_ps_buffered++;

		info->control.jiffies = jiffies;
		info->control.vif = &tx->sdata->vif;
 		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
 		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 		skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
		spin_unlock(&sta->ps_lock);
 
 		if (!timer_pending(&local->sta_cleanup))
 			mod_timer(&local->sta_cleanup,
				  round_jiffies(jiffies +
						STA_INFO_CLEANUP_INTERVAL));

		/*
		 * We queued up some frames, so the TIM bit might
		 * need to be set, recalculate it.
		 */
		sta_info_recalc_tim(sta);

		return TX_QUEUED;
	} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {
		ps_dbg(tx->sdata,
		       ""STA %pM in PS mode, but polling/in SP -> send frame\n"",
		       sta->sta.addr);
	}

	return TX_CONTINUE;
}
","The function `ieee80211_tx_h_unicast_ps_buf` manages the transmission of unicast packets for stations in power-saving (PS) mode within a wireless network. It first checks if the station (STA) is valid and whether it is in PS mode, allowing for buffering of packets if certain conditions are met. If the PS buffer for the specific access category (AC) is full, it purges the oldest packet to make space for new ones. The function updates transmission information, queues the packet for later processing, and potentially modifies a timer for station cleanup. It also recalculates the Traffic Indication Map (TIM) to reflect the buffered frames.","The code manages the transmission of unicast packets for devices in power-saving mode within a wireless network. It first verifies the validity of the device and checks if it is in power-saving mode, allowing for the buffering of packets under specific conditions. If the buffer for a particular category is full, it removes the oldest packet to accommodate new ones. The code updates transmission details, queues the packet for future processing, and may adjust a timer for device cleanup. Additionally, it recalculates the Traffic Indication Map to reflect the status of buffered packets, ensuring efficient communication.","The vulnerability in the original code arises from a race condition between the transmission path and the wakeup signal for a sleeping station. While frames are buffered for the station, a wakeup packet could be processed concurrently, leading to a scenario where the buffered frames are cleared while a new frame is being added. This could result in the new frame being delayed or sent out of order, potentially causing a crash due to a null pointer dereference. The fix introduces a locking mechanism to synchronize access to the buffered frames, ensuring that both the transmission and wakeup processes do not interfere with each other, thus preventing the race condition and associated crashes.","The vulnerability in the original code stems from a race condition between the transmission process and the wakeup signal for a sleeping device. While frames are buffered for the device, a wakeup packet could be processed concurrently, leading to a situation where the list of buffered frames is cleared while a new frame is being added. This could result in the new frame being delayed or sent out of order, potentially causing a crash due to a null pointer dereference. The fix introduces a locking mechanism to synchronize access to the buffered frames, ensuring that both the transmission and wakeup processes do not interfere with each other, thus preventing the race condition and associated crashes.","The vulnerability was fixed by introducing a locking mechanism around the critical sections of the code that handle the transmission and wakeup processes. Specifically, a lock was applied using `spin_lock(&sta->ps_lock)` to synchronize access to the power-saving buffer. This ensures that while one process is modifying the buffered frames, the other process cannot interfere, preventing the race condition. The lock is released with `spin_unlock(&sta->ps_lock)` after the necessary operations are completed. This change ensures that the buffered frames are managed safely, eliminating the risk of concurrent modifications that could lead to crashes or out-of-order transmissions.","The vulnerability was addressed by implementing a locking mechanism to synchronize access to the shared resources involved in the transmission and wakeup processes. This ensures that when one process is modifying the list of buffered frames, the other process cannot interfere, thereby preventing race conditions. By locking the critical sections where these modifications occur, the code guarantees that operations are completed without concurrent access issues. This change effectively eliminates the risk of crashes or out-of-order transmissions that could arise from simultaneous modifications, ensuring safer and more reliable handling of power-saving states."
9,179567,179567,,Remote,Not required,Complete,CVE-2014-2672,https://www.cvedetails.com/cve/CVE-2014-2672/,CWE-362,Medium,,,,2014-04-01,7.1,Race condition in the ath_tx_aggr_sleep function in drivers/net/wireless/ath/ath9k/xmit.c in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via a large amount of network traffic that triggers certain list deletions.,2014-04-19,DoS ,5,https://github.com/torvalds/linux/commit/21f8aaee0c62708654988ce092838aa7df4d25d8,21f8aaee0c62708654988ce092838aa7df4d25d8,"ath9k: protect tid->sched check

We check tid->sched without a lock taken on ath_tx_aggr_sleep(). That
is race condition which can result of doing list_del(&tid->list) twice
(second time with poisoned list node) and cause crash like shown below:

[424271.637220] BUG: unable to handle kernel paging request at 00100104
[424271.637328] IP: [<f90fc072>] ath_tx_aggr_sleep+0x62/0xe0 [ath9k]
...
[424271.639953] Call Trace:
[424271.639998]  [<f90f6900>] ? ath9k_get_survey+0x110/0x110 [ath9k]
[424271.640083]  [<f90f6942>] ath9k_sta_notify+0x42/0x50 [ath9k]
[424271.640177]  [<f809cfef>] sta_ps_start+0x8f/0x1c0 [mac80211]
[424271.640258]  [<c10f730e>] ? free_compound_page+0x2e/0x40
[424271.640346]  [<f809e915>] ieee80211_rx_handlers+0x9d5/0x2340 [mac80211]
[424271.640437]  [<c112f048>] ? kmem_cache_free+0x1d8/0x1f0
[424271.640510]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640578]  [<c10fc23c>] ? put_page+0x2c/0x40
[424271.640640]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640706]  [<c1345a84>] ? kfree_skbmem+0x34/0x90
[424271.640787]  [<f809dde3>] ? ieee80211_rx_handlers_result+0x73/0x1d0 [mac80211]
[424271.640897]  [<f80a07a0>] ieee80211_prepare_and_rx_handle+0x520/0xad0 [mac80211]
[424271.641009]  [<f809e22d>] ? ieee80211_rx_handlers+0x2ed/0x2340 [mac80211]
[424271.641104]  [<c13846ce>] ? ip_output+0x7e/0xd0
[424271.641182]  [<f80a1057>] ieee80211_rx+0x307/0x7c0 [mac80211]
[424271.641266]  [<f90fa6ee>] ath_rx_tasklet+0x88e/0xf70 [ath9k]
[424271.641358]  [<f80a0f2c>] ? ieee80211_rx+0x1dc/0x7c0 [mac80211]
[424271.641445]  [<f90f82db>] ath9k_tasklet+0xcb/0x130 [ath9k]

Bug report:
https://bugzilla.kernel.org/show_bug.cgi?id=70551

Reported-and-tested-by: Max Sydorenko <maxim.stargazer@gmail.com>
Cc: stable@vger.kernel.org
Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
Signed-off-by: John W. Linville <linville@tuxdriver.com>",2,drivers/net/wireless/ath/ath9k/xmit.c,"{""sha"": ""4f4ce83f7ab4afd69fb4ed4ae937953711d3343a"", ""filename"": ""drivers/net/wireless/ath/ath9k/xmit.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 3, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/21f8aaee0c62708654988ce092838aa7df4d25d8/drivers/net/wireless/ath/ath9k/xmit.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/21f8aaee0c62708654988ce092838aa7df4d25d8/drivers/net/wireless/ath/ath9k/xmit.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/net/wireless/ath/ath9k/xmit.c?ref=21f8aaee0c62708654988ce092838aa7df4d25d8"", ""patch"": ""@@ -1444,14 +1444,16 @@ void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,\n \tfor (tidno = 0, tid = &an->tid[tidno];\n \t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n \n-\t\tif (!tid->sched)\n-\t\t\tcontinue;\n-\n \t\tac = tid->ac;\n \t\ttxq = ac->txq;\n \n \t\tath_txq_lock(sc, txq);\n \n+\t\tif (!tid->sched) {\n+\t\t\tath_txq_unlock(sc, txq);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tbuffered = ath_tid_has_buffered(tid);\n \n \t\ttid->sched = false;""}","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
		if (!tid->sched)
			continue;
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
",C,"		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

","		if (!tid->sched)
			continue;
",,"@@ -1444,14 +1444,16 @@ void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
-		if (!tid->sched)
-			continue;
-
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
+		if (!tid->sched) {
+			ath_txq_unlock(sc, txq);
+			continue;
+		}
+
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;",linux,21f8aaee0c62708654988ce092838aa7df4d25d8,1c97560f6d751a620978504a4a888c631192b71a,1,"void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
//flaw_line_below:
		if (!tid->sched)
//flaw_line_below:
			continue;
//flaw_line_below:

 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
//fix_flaw_line_below:
//		if (!tid->sched) {
//fix_flaw_line_below:
//			ath_txq_unlock(sc, txq);
//fix_flaw_line_below:
//			continue;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
",179567,"void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
		if (!tid->sched)
			continue;
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,
		       struct ath_node *an)
{
	struct ath_atx_tid *tid;
	struct ath_atx_ac *ac;
	struct ath_txq *txq;
	bool buffered;
	int tidno;

 	for (tidno = 0, tid = &an->tid[tidno];
 	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {
 
 		ac = tid->ac;
 		txq = ac->txq;
 
 		ath_txq_lock(sc, txq);
 
		if (!tid->sched) {
			ath_txq_unlock(sc, txq);
			continue;
		}

 		buffered = ath_tid_has_buffered(tid);
 
 		tid->sched = false;
		list_del(&tid->list);

		if (ac->sched) {
			ac->sched = false;
			list_del(&ac->list);
		}

		ath_txq_unlock(sc, txq);

		ieee80211_sta_set_buffered(sta, tidno, buffered);
	}
}
","The function `ath_tx_aggr_sleep` is designed to manage the transmission aggregation state of a specific station in a wireless network. It iterates through all possible Traffic Identifier (TID) values associated with the station, checking if each TID is scheduled for transmission. If a TID is scheduled, it locks the corresponding transmission queue, determines if there are buffered packets, and then marks the TID and its associated Access Category (AC) as unscheduled, removing them from their respective lists. Finally, it unlocks the transmission queue and updates the station's buffered status based on the presence of any buffered packets.","The provided code manages the transmission aggregation state for a specific station in a wireless network. It checks each traffic identifier associated with the station to determine if it is scheduled for transmission. If so, it locks the corresponding transmission queue, assesses whether there are any buffered packets, and then marks the traffic identifier and its associated category as unscheduled, removing them from their respective lists. After unlocking the transmission queue, it updates the station's status to reflect the presence or absence of buffered packets, ensuring efficient management of data transmission.","The vulnerability in the original code stemmed from checking the scheduling state of a traffic identifier without acquiring a lock, leading to a race condition. This could result in the same traffic identifier being removed from its list multiple times, potentially causing a crash due to dereferencing a poisoned list node. The fix implemented in the modified code involves acquiring a lock before checking the scheduling state, ensuring that the state is stable and preventing concurrent modifications. If the traffic identifier is not scheduled, the lock is released immediately, thus maintaining thread safety and preventing the double deletion issue.","The vulnerability in the original code arose from checking the scheduling state of a traffic identifier without acquiring a lock, leading to a race condition. This could result in the same traffic identifier being removed from its list multiple times, potentially causing a crash due to accessing a corrupted or poisoned list node. The fix implemented in the modified code involves acquiring a lock before checking the scheduling state, ensuring that the state remains consistent and preventing concurrent modifications. If the traffic identifier is not scheduled, the lock is released immediately, thus maintaining thread safety and avoiding the double deletion issue.","The vulnerability was fixed by introducing a lock around the check for the scheduling state of the traffic identifier. In the modified code, the lock is acquired using `ath_txq_lock` before checking `tid->sched`. This ensures that the scheduling state is stable and prevents race conditions that could lead to multiple calls to `list_del(&tid->list)`. If the traffic identifier is not scheduled, the lock is released immediately with `ath_txq_unlock`, allowing for safe continuation of the loop without risking access to a potentially corrupted list node. This change effectively mitigates the risk of crashes due to concurrent modifications.","The vulnerability was addressed by implementing a locking mechanism to ensure thread safety when checking the scheduling state of the traffic identifier. By acquiring a lock before evaluating the state, the code prevents race conditions that could lead to multiple deletions from the list, which might corrupt the data structure. If the traffic identifier is found to be unscheduled, the lock is released immediately, allowing the process to continue safely. This change effectively eliminates the risk of accessing invalid memory or corrupted nodes, thereby preventing potential crashes in the system."
10,179745,179745,,Local,Not required,Complete,CVE-2015-7990,https://www.cvedetails.com/cve/CVE-2015-7990/,CWE-362,Medium,Partial,Partial,,2015-12-28,5.9,Race condition in the rds_sendmsg function in net/rds/sendmsg.c in the Linux kernel before 4.3.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2015-6937.,2018-10-16,DoS ,3,https://github.com/torvalds/linux/commit/8c7188b23474cca017b3ef354c4a58456f68303a,8c7188b23474cca017b3ef354c4a58456f68303a,"RDS: fix race condition when sending a message on unbound socket

Sasha's found a NULL pointer dereference in the RDS connection code when
sending a message to an apparently unbound socket.  The problem is caused
by the code checking if the socket is bound in rds_sendmsg(), which checks
the rs_bound_addr field without taking a lock on the socket.  This opens a
race where rs_bound_addr is temporarily set but where the transport is not
in rds_bind(), leading to a NULL pointer dereference when trying to
dereference 'trans' in __rds_conn_create().

Vegard wrote a reproducer for this issue, so kindly ask him to share if
you're interested.

I cannot reproduce the NULL pointer dereference using Vegard's reproducer
with this patch, whereas I could without.

Complete earlier incomplete fix to CVE-2015-6937:

  74e98eb08588 (""RDS: verify the underlying transport exists before creating a connection"")

Cc: David S. Miller <davem@davemloft.net>
Cc: stable@vger.kernel.org

Reviewed-by: Vegard Nossum <vegard.nossum@oracle.com>
Reviewed-by: Sasha Levin <sasha.levin@oracle.com>
Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Signed-off-by: Quentin Casasnovas <quentin.casasnovas@oracle.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/rds/send.c,"{""sha"": ""e3b118cae81d5e859e0244df1bf323aaa1798b8e"", ""filename"": ""net/rds/connection.c"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 6, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/connection.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/connection.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/connection.c?ref=8c7188b23474cca017b3ef354c4a58456f68303a"", ""patch"": ""@@ -186,12 +186,6 @@ static struct rds_connection *__rds_conn_create(struct net *net,\n \t\t}\n \t}\n \n-\tif (trans == NULL) {\n-\t\tkmem_cache_free(rds_conn_slab, conn);\n-\t\tconn = ERR_PTR(-ENODEV);\n-\t\tgoto out;\n-\t}\n-\n \tconn->c_trans = trans;\n \n \tret = trans->conn_alloc(conn, gfp);""}<_**next**_>{""sha"": ""c9cdb358ea885e3e356cc675b579f1313ed94ff9"", ""filename"": ""net/rds/send.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 1, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/send.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8c7188b23474cca017b3ef354c4a58456f68303a/net/rds/send.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/send.c?ref=8c7188b23474cca017b3ef354c4a58456f68303a"", ""patch"": ""@@ -1013,11 +1013,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n \t\trelease_sock(sk);\n \t}\n \n-\t/* racing with another thread binding seems ok here */\n+\tlock_sock(sk);\n \tif (daddr == 0 || rs->rs_bound_addr == 0) {\n+\t\trelease_sock(sk);\n \t\tret = -ENOTCONN; /* XXX not a great errno */\n \t\tgoto out;\n \t}\n+\trelease_sock(sk);\n \n \tif (payload_len > rds_sk_sndbuf(rs)) {\n \t\tret = -EMSGSIZE;""}","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	/* racing with another thread binding seems ok here */
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
",C,"	lock_sock(sk);
		release_sock(sk);
	release_sock(sk);
","	/* racing with another thread binding seems ok here */
",,"@@ -1013,11 +1013,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 		release_sock(sk);
 	}
 
-	/* racing with another thread binding seems ok here */
+	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
+		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
+	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;",linux,8c7188b23474cca017b3ef354c4a58456f68303a,20f795666d3accbb75969730019aeb03f50ef0ec,1,"int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
//flaw_line_below:
	/* racing with another thread binding seems ok here */
//fix_flaw_line_below:
//	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
//fix_flaw_line_below:
//		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
//fix_flaw_line_below:
//	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
",179745,"int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	/* racing with another thread binding seems ok here */
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
{
	struct sock *sk = sock->sk;
	struct rds_sock *rs = rds_sk_to_rs(sk);
	DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
	__be32 daddr;
	__be16 dport;
	struct rds_message *rm = NULL;
	struct rds_connection *conn;
	int ret = 0;
	int queued = 0, allocated_mr = 0;
	int nonblock = msg->msg_flags & MSG_DONTWAIT;
	long timeo = sock_sndtimeo(sk, nonblock);

	/* Mirror Linux UDP mirror of BSD error message compatibility */
	/* XXX: Perhaps MSG_MORE someday */
	if (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (msg->msg_namelen) {
		/* XXX fail non-unicast destination IPs? */
		if (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {
			ret = -EINVAL;
			goto out;
		}
		daddr = usin->sin_addr.s_addr;
		dport = usin->sin_port;
	} else {
		/* We only care about consistency with ->connect() */
		lock_sock(sk);
		daddr = rs->rs_conn_addr;
		dport = rs->rs_conn_port;
 		release_sock(sk);
 	}
 
	lock_sock(sk);
 	if (daddr == 0 || rs->rs_bound_addr == 0) {
		release_sock(sk);
 		ret = -ENOTCONN; /* XXX not a great errno */
 		goto out;
 	}
	release_sock(sk);
 
 	if (payload_len > rds_sk_sndbuf(rs)) {
 		ret = -EMSGSIZE;
		goto out;
	}

	/* size of rm including all sgs */
	ret = rds_rm_size(msg, payload_len);
	if (ret < 0)
		goto out;

	rm = rds_message_alloc(ret, GFP_KERNEL);
	if (!rm) {
		ret = -ENOMEM;
		goto out;
	}

	/* Attach data to the rm */
	if (payload_len) {
		rm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));
		if (!rm->data.op_sg) {
			ret = -ENOMEM;
			goto out;
		}
		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
		if (ret)
			goto out;
	}
	rm->data.op_active = 1;

	rm->m_daddr = daddr;

	/* rds_conn_create has a spinlock that runs with IRQ off.
	 * Caching the conn in the socket helps a lot. */
	if (rs->rs_conn && rs->rs_conn->c_faddr == daddr)
		conn = rs->rs_conn;
	else {
		conn = rds_conn_create_outgoing(sock_net(sock->sk),
						rs->rs_bound_addr, daddr,
					rs->rs_transport,
					sock->sk->sk_allocation);
		if (IS_ERR(conn)) {
			ret = PTR_ERR(conn);
			goto out;
		}
		rs->rs_conn = conn;
	}

	/* Parse any control messages the user may have included. */
	ret = rds_cmsg_send(rs, rm, msg, &allocated_mr);
	if (ret)
		goto out;

	if (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {
		printk_ratelimited(KERN_NOTICE ""rdma_op %p conn xmit_rdma %p\n"",
			       &rm->rdma, conn->c_trans->xmit_rdma);
		ret = -EOPNOTSUPP;
		goto out;
	}

	if (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {
		printk_ratelimited(KERN_NOTICE ""atomic_op %p conn xmit_atomic %p\n"",
			       &rm->atomic, conn->c_trans->xmit_atomic);
		ret = -EOPNOTSUPP;
		goto out;
	}

	rds_conn_connect_if_down(conn);

	ret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);
	if (ret) {
		rs->rs_seen_congestion = 1;
		goto out;
	}

	while (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,
				  dport, &queued)) {
		rds_stats_inc(s_send_queue_full);

		if (nonblock) {
			ret = -EAGAIN;
			goto out;
		}

		timeo = wait_event_interruptible_timeout(*sk_sleep(sk),
					rds_send_queue_rm(rs, conn, rm,
							  rs->rs_bound_port,
							  dport,
							  &queued),
					timeo);
		rdsdebug(""sendmsg woke queued %d timeo %ld\n"", queued, timeo);
		if (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)
			continue;

		ret = timeo;
		if (ret == 0)
			ret = -ETIMEDOUT;
		goto out;
	}

	/*
	 * By now we've committed to the send.  We reuse rds_send_worker()
	 * to retry sends in the rds thread if the transport asks us to.
	 */
	rds_stats_inc(s_send_queued);

	ret = rds_send_xmit(conn);
	if (ret == -ENOMEM || ret == -EAGAIN)
		queue_delayed_work(rds_wq, &conn->c_send_w, 1);

	rds_message_put(rm);
	return payload_len;

out:
	/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.
	 * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN
	 * or in any other way, we need to destroy the MR again */
	if (allocated_mr)
		rds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);

	if (rm)
		rds_message_put(rm);
	return ret;
}
","The `rds_sendmsg` function is responsible for sending messages over a socket using the Reliable Datagram Sockets (RDS) protocol. It begins by validating the message parameters, including the destination address and port, and checks for connection status. The function allocates a message structure and prepares the data for transmission, handling control messages if present. It manages congestion control and queues the message for sending, retrying if necessary. If the message is successfully queued, it triggers the transmission process. The function also ensures proper resource management, releasing any allocated memory in case of errors, and returns the length of the payload sent or an error code.","The function facilitates the sending of messages over a specific socket using a specialized protocol. It begins by validating the message's parameters and ensuring a valid connection. After allocating necessary resources for the message, it prepares the data for transmission and handles any control messages. The function manages potential congestion and queues the message for sending, retrying as needed. If the message is successfully queued, it initiates the transmission process. Additionally, the function ensures proper cleanup of resources in case of errors, returning either the length of the sent payload or an error code.","The vulnerability in the original code stemmed from a race condition that could lead to a NULL pointer dereference when sending messages on an unbound socket. Specifically, the code checked the `rs_bound_addr` field without acquiring a lock on the socket, allowing a scenario where the address could be temporarily set while the transport was not fully established, resulting in dereferencing a NULL transport pointer. The fix involved ensuring that the socket was locked during the check for `rs_bound_addr`, preventing concurrent modifications and ensuring that the transport was valid before proceeding with the connection creation, thus eliminating the potential for a NULL pointer dereference.","The vulnerability in the original code was due to a race condition that could lead to a NULL pointer dereference when attempting to send messages through an unbound socket. This occurred because the code checked the binding status of the socket without acquiring a lock, allowing the binding state to change unexpectedly. As a result, the code could attempt to access an uninitialized transport pointer, leading to a crash. The fix involved ensuring that the socket was properly locked during the binding status check, thereby preventing concurrent modifications and ensuring that the transport was valid before any further operations were performed.","The vulnerability was fixed by ensuring that the socket is locked during the check of the `rs_bound_addr` field. In the updated code, the function now calls `lock_sock` before verifying if the bound address is valid, which prevents concurrent modifications that could lead to a NULL pointer dereference. This locking mechanism ensures that the transport is fully established before proceeding with the connection creation. Additionally, the code now properly releases the lock after the check, maintaining thread safety and preventing potential race conditions when sending messages on an unbound socket.","The vulnerability was addressed by implementing a locking mechanism to ensure thread safety during the check of the socket's binding status. By acquiring a lock before verifying the binding state, the code prevents concurrent modifications that could lead to accessing an uninitialized transport pointer. This change ensures that the transport is valid and fully established before any further operations are performed, effectively eliminating the risk of a NULL pointer dereference. The fix also includes proper management of the lock to maintain system stability and prevent race conditions when sending messages through the socket."
11,179750,179750,,Local,Not required,Complete,CVE-2015-7613,https://www.cvedetails.com/cve/CVE-2015-7613/,CWE-362,Medium,Complete,Complete,,2015-10-19,6.9,"Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",2016-12-07,+Priv ,7,https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf,b9a532277938798b53178d5a66af6e2915cb27cf,"Initialize msg/shm IPC objects before doing ipc_addid()

As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before
having initialized the IPC object state.  Yes, we initialize the IPC
object in a locked state, but with all the lockless RCU lookup work,
that IPC object lock no longer means that the state cannot be seen.

We already did this for the IPC semaphore code (see commit e8577d1f0329:
""ipc/sem.c: fully initialize sem_array before making it visible"") but we
clearly forgot about msg and shm.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Cc: Manfred Spraul <manfred@colorfullife.com>
Cc: Davidlohr Bueso <dbueso@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",6,ipc/msg.c,"{""sha"": ""1471db9a7e6112b3316ae887b50c6d8d1352f171"", ""filename"": ""ipc/msg.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 7, ""changes"": 14, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/msg.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \t\treturn retval;\n \t}\n \n-\t/* ipc_addid() locks msq upon success. */\n-\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n-\tif (id < 0) {\n-\t\tipc_rcu_putref(msq, msg_rcu_free);\n-\t\treturn id;\n-\t}\n-\n \tmsq->q_stime = msq->q_rtime = 0;\n \tmsq->q_ctime = get_seconds();\n \tmsq->q_cbytes = msq->q_qnum = 0;\n@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \tINIT_LIST_HEAD(&msq->q_receivers);\n \tINIT_LIST_HEAD(&msq->q_senders);\n \n+\t/* ipc_addid() locks msq upon success. */\n+\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n+\tif (id < 0) {\n+\t\tipc_rcu_putref(msq, msg_rcu_free);\n+\t\treturn id;\n+\t}\n+\n \tipc_unlock_object(&msq->q_perm);\n \trcu_read_unlock();\n ""}<_**next**_>{""sha"": ""41787276e14170af7de8261181721991fde528bf"", ""filename"": ""ipc/shm.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 6, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/shm.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -551,12 +551,6 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -565,6 +559,13 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*""}<_**next**_>{""sha"": ""0f401d94b7c657d5e7126fe78f149c94ffea8e24"", ""filename"": ""ipc/util.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/util.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \trcu_read_lock();\n \tspin_lock(&new->lock);\n \n+\tcurrent_euid_egid(&euid, &egid);\n+\tnew->cuid = new->uid = euid;\n+\tnew->gid = new->cgid = egid;\n+\n \tid = idr_alloc(&ids->ipcs_idr, new,\n \t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n \t\t       GFP_NOWAIT);\n@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \n \tids->in_use++;\n \n-\tcurrent_euid_egid(&euid, &egid);\n-\tnew->cuid = new->uid = euid;\n-\tnew->gid = new->cgid = egid;\n-\n \tif (next_id < 0) {\n \t\tnew->seq = ids->seq++;\n \t\tif (ids->seq > IPCID_SEQ_MAX)""}","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
",C,"	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

","	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
",,"@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)
 		return retval;
 	}
 
-	/* ipc_addid() locks msq upon success. */
-	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
-	if (id < 0) {
-		ipc_rcu_putref(msq, msg_rcu_free);
-		return id;
-	}
-
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
+	/* ipc_addid() locks msq upon success. */
+	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
+	if (id < 0) {
+		ipc_rcu_putref(msq, msg_rcu_free);
+		return id;
+	}
+
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 ",linux,b9a532277938798b53178d5a66af6e2915cb27cf,3225031fbeb1e32b269a82eccd815128267a4bfe,1,"static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
//flaw_line_below:
	/* ipc_addid() locks msq upon success. */
//flaw_line_below:
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
//flaw_line_below:
	if (id < 0) {
//flaw_line_below:
		ipc_rcu_putref(msq, msg_rcu_free);
//flaw_line_below:
		return id;
//flaw_line_below:
	}
//flaw_line_below:

 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
//fix_flaw_line_below:
//	/* ipc_addid() locks msq upon success. */
//fix_flaw_line_below:
//	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
//fix_flaw_line_below:
//	if (id < 0) {
//fix_flaw_line_below:
//		ipc_rcu_putref(msq, msg_rcu_free);
//fix_flaw_line_below:
//		return id;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
",179750,"static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","static int newque(struct ipc_namespace *ns, struct ipc_params *params)
{
	struct msg_queue *msq;
	int id, retval;
	key_t key = params->key;
	int msgflg = params->flg;

	msq = ipc_rcu_alloc(sizeof(*msq));
	if (!msq)
		return -ENOMEM;

	msq->q_perm.mode = msgflg & S_IRWXUGO;
	msq->q_perm.key = key;

	msq->q_perm.security = NULL;
	retval = security_msg_queue_alloc(msq);
	if (retval) {
		ipc_rcu_putref(msq, ipc_rcu_free);
 		return retval;
 	}
 
 	msq->q_stime = msq->q_rtime = 0;
 	msq->q_ctime = get_seconds();
 	msq->q_cbytes = msq->q_qnum = 0;
	msq->q_qbytes = ns->msg_ctlmnb;
	msq->q_lspid = msq->q_lrpid = 0;
	INIT_LIST_HEAD(&msq->q_messages);
 	INIT_LIST_HEAD(&msq->q_receivers);
 	INIT_LIST_HEAD(&msq->q_senders);
 
	/* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
	if (id < 0) {
		ipc_rcu_putref(msq, msg_rcu_free);
		return id;
	}

 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 
	return msq->q_perm.id;
}
","The provided code defines a function `newque` that creates a new message queue within a specified IPC (Inter-Process Communication) namespace. It begins by allocating memory for a message queue structure and initializes its permissions based on the provided flags and key. The function then calls a security check for the message queue and, if successful, adds the queue to the IPC identifier list. It initializes various timestamps and counters related to the message queue's state, sets up linked lists for managing messages and senders/receivers, and finally unlocks the IPC object before returning the unique identifier of the newly created message queue.","The code defines a function that creates a new message queue within a specified communication namespace. It begins by allocating memory for the message queue and setting its permissions based on provided flags and a key. After performing a security check, it adds the queue to an identifier list if the check is successful. The function initializes various attributes related to the queue's state, including timestamps and counters for messages. It also sets up linked lists for managing messages and participants. Finally, it unlocks the associated object and returns the unique identifier of the newly created message queue.","The vulnerability in the original code arises from the premature call to `ipc_addid()` before fully initializing the message queue's state. This could lead to a situation where the message queue is visible to other processes while still in an uninitialized state, potentially causing undefined behavior or security issues. The fix involved moving the initialization of various attributes, such as timestamps and counters, to occur before the call to `ipc_addid()`. This ensures that the message queue is completely initialized and in a valid state before it becomes accessible, thereby preventing any inconsistencies or vulnerabilities related to its visibility.","The vulnerability in the original code stems from the fact that an IPC object was made visible to other processes before its internal state was fully initialized. This premature exposure could lead to inconsistent or undefined behavior, as other processes might interact with an object that is not yet in a valid state. The fix involved ensuring that all necessary attributes of the IPC object were initialized prior to making it visible. By rearranging the code to complete the initialization first, the risk of exposing an improperly configured object was mitigated, thereby enhancing the overall stability and security of the IPC mechanism.","The vulnerability was fixed by rearranging the code to ensure that the message queue's state was fully initialized before calling `ipc_addid()`. In the updated code, attributes such as timestamps, counters, and linked lists for messages and participants are set up prior to the call to `ipc_addid()`. This change guarantees that the message queue is in a valid and complete state before it becomes visible to other processes. By doing so, the risk of exposing an uninitialized object is eliminated, thereby preventing potential inconsistencies and security issues associated with premature visibility.","The vulnerability was fixed by ensuring that all necessary attributes of the IPC object were fully initialized before it was made visible to other processes. The code was rearranged to complete the initialization of the object's state, including setting up timestamps, counters, and linked lists, prior to exposing it through the identifier addition function. This adjustment prevents the risk of interacting with an improperly configured object, thereby enhancing the stability and security of the IPC mechanism by ensuring that the object is in a valid state before any external access occurs."
12,179752,179752,,Local,Not required,Complete,CVE-2015-7613,https://www.cvedetails.com/cve/CVE-2015-7613/,CWE-362,Medium,Complete,Complete,,2015-10-19,6.9,"Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",2016-12-07,+Priv ,4,https://github.com/torvalds/linux/commit/b9a532277938798b53178d5a66af6e2915cb27cf,b9a532277938798b53178d5a66af6e2915cb27cf,"Initialize msg/shm IPC objects before doing ipc_addid()

As reported by Dmitry Vyukov, we really shouldn't do ipc_addid() before
having initialized the IPC object state.  Yes, we initialize the IPC
object in a locked state, but with all the lockless RCU lookup work,
that IPC object lock no longer means that the state cannot be seen.

We already did this for the IPC semaphore code (see commit e8577d1f0329:
""ipc/sem.c: fully initialize sem_array before making it visible"") but we
clearly forgot about msg and shm.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Cc: Manfred Spraul <manfred@colorfullife.com>
Cc: Davidlohr Bueso <dbueso@suse.de>
Cc: stable@vger.kernel.org
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",3,ipc/util.c,"{""sha"": ""1471db9a7e6112b3316ae887b50c6d8d1352f171"", ""filename"": ""ipc/msg.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 7, ""changes"": 14, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/msg.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/msg.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -137,13 +137,6 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \t\treturn retval;\n \t}\n \n-\t/* ipc_addid() locks msq upon success. */\n-\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n-\tif (id < 0) {\n-\t\tipc_rcu_putref(msq, msg_rcu_free);\n-\t\treturn id;\n-\t}\n-\n \tmsq->q_stime = msq->q_rtime = 0;\n \tmsq->q_ctime = get_seconds();\n \tmsq->q_cbytes = msq->q_qnum = 0;\n@@ -153,6 +146,13 @@ static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n \tINIT_LIST_HEAD(&msq->q_receivers);\n \tINIT_LIST_HEAD(&msq->q_senders);\n \n+\t/* ipc_addid() locks msq upon success. */\n+\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n+\tif (id < 0) {\n+\t\tipc_rcu_putref(msq, msg_rcu_free);\n+\t\treturn id;\n+\t}\n+\n \tipc_unlock_object(&msq->q_perm);\n \trcu_read_unlock();\n ""}<_**next**_>{""sha"": ""41787276e14170af7de8261181721991fde528bf"", ""filename"": ""ipc/shm.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 6, ""changes"": 13, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/shm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/shm.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -551,12 +551,6 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -565,6 +559,13 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*""}<_**next**_>{""sha"": ""0f401d94b7c657d5e7126fe78f149c94ffea8e24"", ""filename"": ""ipc/util.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a532277938798b53178d5a66af6e2915cb27cf/ipc/util.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/ipc/util.c?ref=b9a532277938798b53178d5a66af6e2915cb27cf"", ""patch"": ""@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \trcu_read_lock();\n \tspin_lock(&new->lock);\n \n+\tcurrent_euid_egid(&euid, &egid);\n+\tnew->cuid = new->uid = euid;\n+\tnew->gid = new->cgid = egid;\n+\n \tid = idr_alloc(&ids->ipcs_idr, new,\n \t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n \t\t       GFP_NOWAIT);\n@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n \n \tids->in_use++;\n \n-\tcurrent_euid_egid(&euid, &egid);\n-\tnew->cuid = new->uid = euid;\n-\tnew->gid = new->cgid = egid;\n-\n \tif (next_id < 0) {\n \t\tnew->seq = ids->seq++;\n \t\tif (ids->seq > IPCID_SEQ_MAX)""}","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
",C,"	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

","	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
",,"@@ -237,6 +237,10 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
+	current_euid_egid(&euid, &egid);
+	new->cuid = new->uid = euid;
+	new->gid = new->cgid = egid;
+
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
@@ -249,10 +253,6 @@ int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
 
 	ids->in_use++;
 
-	current_euid_egid(&euid, &egid);
-	new->cuid = new->uid = euid;
-	new->gid = new->cgid = egid;
-
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)",linux,b9a532277938798b53178d5a66af6e2915cb27cf,3225031fbeb1e32b269a82eccd815128267a4bfe,1,"int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
//fix_flaw_line_below:
//	current_euid_egid(&euid, &egid);
//fix_flaw_line_below:
//	new->cuid = new->uid = euid;
//fix_flaw_line_below:
//	new->gid = new->cgid = egid;
//fix_flaw_line_below:
//
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
//flaw_line_below:
	current_euid_egid(&euid, &egid);
//flaw_line_below:
	new->cuid = new->uid = euid;
//flaw_line_below:
	new->gid = new->cgid = egid;
//flaw_line_below:

 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
",179752,"int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)
{
	kuid_t euid;
	kgid_t egid;
	int id;
	int next_id = ids->next_id;

	if (size > IPCMNI)
		size = IPCMNI;

	if (ids->in_use >= size)
		return -ENOSPC;

	idr_preload(GFP_KERNEL);

	spin_lock_init(&new->lock);
	new->deleted = false;
 	rcu_read_lock();
 	spin_lock(&new->lock);
 
	current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

 	id = idr_alloc(&ids->ipcs_idr, new,
 		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
 		       GFP_NOWAIT);
	idr_preload_end();
	if (id < 0) {
		spin_unlock(&new->lock);
		rcu_read_unlock();
		return id;
	}
 
 	ids->in_use++;
 
 	if (next_id < 0) {
 		new->seq = ids->seq++;
 		if (ids->seq > IPCID_SEQ_MAX)
			ids->seq = 0;
	} else {
		new->seq = ipcid_to_seqx(next_id);
		ids->next_id = -1;
	}

	new->id = ipc_buildid(id, new->seq);
	return id;
}
","The `ipc_addid` function is responsible for adding a new IPC (Inter-Process Communication) identifier to a given set of IPC identifiers, ensuring that the system does not exceed predefined limits. It first checks if the current number of identifiers in use is less than the maximum allowed size. If space is available, it initializes the lock for the new IPC permission structure, allocates a unique identifier using an IDR (Identifier Resource) structure, and updates the ownership attributes (user and group IDs) of the new IPC. Finally, it constructs a unique IPC ID by combining the allocated identifier with a sequence number, returning the new identifier or an error code if allocation fails.","The code defines a function that adds a new identifier to a collection of inter-process communication identifiers while managing resource limits. It first checks if there is available space for a new identifier and initializes necessary locks for thread safety. If space is available, it allocates a unique identifier and updates the ownership information for the new entry. The function also manages a sequence number to ensure uniqueness and handles potential errors during the allocation process. Ultimately, it returns the newly allocated identifier or an error code if the allocation fails.","The vulnerability in the original code stemmed from the fact that the IPC object was not fully initialized before being made visible through the `ipc_addid` function. This could lead to inconsistent or invalid states being observed by other processes, as the object was locked but still subject to lockless RCU lookups. The fix involved ensuring that the IPC object state was completely initialized prior to calling `ipc_addid`, thereby preventing any race conditions or visibility issues. This change aligns with previous fixes made for other IPC mechanisms, ensuring robust initialization before exposure to concurrent access.","The vulnerability in the original code arose from the premature exposure of an IPC object before its state was fully initialized, which could lead to inconsistent behavior when accessed by other processes. Although the object was locked during initialization, the use of lockless mechanisms allowed for potential visibility of uninitialized or invalid states. The fix involved ensuring that the IPC object was completely initialized prior to its addition, thereby preventing any race conditions or inconsistencies. This approach was consistent with previous fixes applied to other IPC mechanisms, reinforcing the importance of proper initialization before making objects visible.","The vulnerability was fixed by ensuring that the IPC object state is fully initialized before invoking the `ipc_addid` function. In the revised code, the initialization of the IPC object occurs prior to the allocation of the identifier, which prevents any race conditions that could arise from concurrent access. Specifically, the code now initializes the ownership attributes, such as user and group IDs, and sets the deletion flag before the identifier is allocated with `idr_alloc`. This guarantees that the object is in a valid state when it becomes visible, thus eliminating the risk of exposing uninitialized data.","The vulnerability was addressed by ensuring that the state of the IPC object is completely initialized before it is made visible to other processes. The fix involved rearranging the order of operations so that all necessary attributes, such as ownership and deletion status, are set prior to the allocation of the identifier. This change prevents any potential race conditions or inconsistencies that could arise from concurrent access, ensuring that the object is in a valid and fully defined state when it is exposed. As a result, the risk of other processes encountering uninitialized or invalid data is eliminated."
13,179797,179797,,Local,Not required,Complete,CVE-2015-3339,https://www.cvedetails.com/cve/CVE-2015-3339/,CWE-362,High,Complete,Complete,,2015-05-27,6.2,"Race condition in the prepare_binprm function in fs/exec.c in the Linux kernel before 3.19.6 allows local users to gain privileges by executing a setuid program at a time instant when a chown to root is in progress, and the ownership is changed but the setuid bit is not yet stripped.",2016-12-30,+Priv ,1,https://github.com/torvalds/linux/commit/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,"fs: take i_mutex during prepare_binprm for set[ug]id executables

This prevents a race between chown() and execve(), where chowning a
setuid-user binary to root would momentarily make the binary setuid
root.

This patch was mostly written by Linus Torvalds.

Signed-off-by: Jann Horn <jann@thejh.net>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",25,fs/exec.c,"{""sha"": ""49a1c61433b73722683cad25eef1fb92045e265a"", ""filename"": ""fs/exec.c"", ""status"": ""modified"", ""additions"": 48, ""deletions"": 28, ""changes"": 76, ""blob_url"": ""https://github.com/torvalds/linux/blob/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543/fs/exec.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8b01fc86b9f425899f8a3a8fc1c47d73c2c20543/fs/exec.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/exec.c?ref=8b01fc86b9f425899f8a3a8fc1c47d73c2c20543"", ""patch"": ""@@ -1275,6 +1275,53 @@ static void check_unsafe_exec(struct linux_binprm *bprm)\n \tspin_unlock(&p->fs->lock);\n }\n \n+static void bprm_fill_uid(struct linux_binprm *bprm)\n+{\n+\tstruct inode *inode;\n+\tunsigned int mode;\n+\tkuid_t uid;\n+\tkgid_t gid;\n+\n+\t/* clear any previous set[ug]id data from a previous binary */\n+\tbprm->cred->euid = current_euid();\n+\tbprm->cred->egid = current_egid();\n+\n+\tif (bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID)\n+\t\treturn;\n+\n+\tif (task_no_new_privs(current))\n+\t\treturn;\n+\n+\tinode = file_inode(bprm->file);\n+\tmode = READ_ONCE(inode->i_mode);\n+\tif (!(mode & (S_ISUID|S_ISGID)))\n+\t\treturn;\n+\n+\t/* Be careful if suid/sgid is set */\n+\tmutex_lock(&inode->i_mutex);\n+\n+\t/* reload atomically mode/uid/gid now that lock held */\n+\tmode = inode->i_mode;\n+\tuid = inode->i_uid;\n+\tgid = inode->i_gid;\n+\tmutex_unlock(&inode->i_mutex);\n+\n+\t/* We ignore suid/sgid if there are no mappings for them in the ns */\n+\tif (!kuid_has_mapping(bprm->cred->user_ns, uid) ||\n+\t\t !kgid_has_mapping(bprm->cred->user_ns, gid))\n+\t\treturn;\n+\n+\tif (mode & S_ISUID) {\n+\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n+\t\tbprm->cred->euid = uid;\n+\t}\n+\n+\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n+\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n+\t\tbprm->cred->egid = gid;\n+\t}\n+}\n+\n /*\n  * Fill the binprm structure from the inode.\n  * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes\n@@ -1283,36 +1330,9 @@ static void check_unsafe_exec(struct linux_binprm *bprm)\n  */\n int prepare_binprm(struct linux_binprm *bprm)\n {\n-\tstruct inode *inode = file_inode(bprm->file);\n-\tumode_t mode = inode->i_mode;\n \tint retval;\n \n-\n-\t/* clear any previous set[ug]id data from a previous binary */\n-\tbprm->cred->euid = current_euid();\n-\tbprm->cred->egid = current_egid();\n-\n-\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n-\t    !task_no_new_privs(current) &&\n-\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n-\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n-\t\t/* Set-uid? */\n-\t\tif (mode & S_ISUID) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->euid = inode->i_uid;\n-\t\t}\n-\n-\t\t/* Set-gid? */\n-\t\t/*\n-\t\t * If setgid is set but no group execute bit then this\n-\t\t * is a candidate for mandatory locking, not a setgid\n-\t\t * executable.\n-\t\t */\n-\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->egid = inode->i_gid;\n-\t\t}\n-\t}\n+\tbprm_fill_uid(bprm);\n \n \t/* fill in binprm security blob */\n \tretval = security_bprm_set_creds(bprm);""}"," int prepare_binprm(struct linux_binprm *bprm)
 {
 	int retval;
 
	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
"," int prepare_binprm(struct linux_binprm *bprm)
 {
	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
 	int retval;
 
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
",C,"	bprm_fill_uid(bprm);
","	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
",,"@@ -1275,6 +1275,53 @@ static void check_unsafe_exec(struct linux_binprm *bprm)
 	spin_unlock(&p->fs->lock);
 }
 
+static void bprm_fill_uid(struct linux_binprm *bprm)
+{
+	struct inode *inode;
+	unsigned int mode;
+	kuid_t uid;
+	kgid_t gid;
+
+	/* clear any previous set[ug]id data from a previous binary */
+	bprm->cred->euid = current_euid();
+	bprm->cred->egid = current_egid();
+
+	if (bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID)
+		return;
+
+	if (task_no_new_privs(current))
+		return;
+
+	inode = file_inode(bprm->file);
+	mode = READ_ONCE(inode->i_mode);
+	if (!(mode & (S_ISUID|S_ISGID)))
+		return;
+
+	/* Be careful if suid/sgid is set */
+	mutex_lock(&inode->i_mutex);
+
+	/* reload atomically mode/uid/gid now that lock held */
+	mode = inode->i_mode;
+	uid = inode->i_uid;
+	gid = inode->i_gid;
+	mutex_unlock(&inode->i_mutex);
+
+	/* We ignore suid/sgid if there are no mappings for them in the ns */
+	if (!kuid_has_mapping(bprm->cred->user_ns, uid) ||
+		 !kgid_has_mapping(bprm->cred->user_ns, gid))
+		return;
+
+	if (mode & S_ISUID) {
+		bprm->per_clear |= PER_CLEAR_ON_SETID;
+		bprm->cred->euid = uid;
+	}
+
+	if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
+		bprm->per_clear |= PER_CLEAR_ON_SETID;
+		bprm->cred->egid = gid;
+	}
+}
+
 /*
  * Fill the binprm structure from the inode.
  * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes
@@ -1283,36 +1330,9 @@ static void check_unsafe_exec(struct linux_binprm *bprm)
  */
 int prepare_binprm(struct linux_binprm *bprm)
 {
-	struct inode *inode = file_inode(bprm->file);
-	umode_t mode = inode->i_mode;
 	int retval;
 
-
-	/* clear any previous set[ug]id data from a previous binary */
-	bprm->cred->euid = current_euid();
-	bprm->cred->egid = current_egid();
-
-	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
-	    !task_no_new_privs(current) &&
-	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
-	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
-		/* Set-uid? */
-		if (mode & S_ISUID) {
-			bprm->per_clear |= PER_CLEAR_ON_SETID;
-			bprm->cred->euid = inode->i_uid;
-		}
-
-		/* Set-gid? */
-		/*
-		 * If setgid is set but no group execute bit then this
-		 * is a candidate for mandatory locking, not a setgid
-		 * executable.
-		 */
-		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
-			bprm->per_clear |= PER_CLEAR_ON_SETID;
-			bprm->cred->egid = inode->i_gid;
-		}
-	}
+	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);",linux,8b01fc86b9f425899f8a3a8fc1c47d73c2c20543,5224b9613b91d937c6948fe977023247afbcc04e,1," int prepare_binprm(struct linux_binprm *bprm)
 {
//flaw_line_below:
	struct inode *inode = file_inode(bprm->file);
//flaw_line_below:
	umode_t mode = inode->i_mode;
 	int retval;
 
//flaw_line_below:

//flaw_line_below:
	/* clear any previous set[ug]id data from a previous binary */
//flaw_line_below:
	bprm->cred->euid = current_euid();
//flaw_line_below:
	bprm->cred->egid = current_egid();
//flaw_line_below:

//flaw_line_below:
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
//flaw_line_below:
	    !task_no_new_privs(current) &&
//flaw_line_below:
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
//flaw_line_below:
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
//flaw_line_below:
		/* Set-uid? */
//flaw_line_below:
		if (mode & S_ISUID) {
//flaw_line_below:
			bprm->per_clear |= PER_CLEAR_ON_SETID;
//flaw_line_below:
			bprm->cred->euid = inode->i_uid;
//flaw_line_below:
		}
//flaw_line_below:

//flaw_line_below:
		/* Set-gid? */
//flaw_line_below:
		/*
//flaw_line_below:
		 * If setgid is set but no group execute bit then this
//flaw_line_below:
		 * is a candidate for mandatory locking, not a setgid
//flaw_line_below:
		 * executable.
//flaw_line_below:
		 */
//flaw_line_below:
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
//flaw_line_below:
			bprm->per_clear |= PER_CLEAR_ON_SETID;
//flaw_line_below:
			bprm->cred->egid = inode->i_gid;
//flaw_line_below:
		}
//flaw_line_below:
	}
//fix_flaw_line_below:
//	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
",179797," int prepare_binprm(struct linux_binprm *bprm)
 {
	struct inode *inode = file_inode(bprm->file);
	umode_t mode = inode->i_mode;
 	int retval;
 
	/* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();
	if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
	    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
	    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
		/* Set-uid? */
		if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}
		/* Set-gid? */
		/*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
		 */
		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
"," int prepare_binprm(struct linux_binprm *bprm)
 {
 	int retval;
 
	bprm_fill_uid(bprm);
 
 	/* fill in binprm security blob */
 	retval = security_bprm_set_creds(bprm);
	if (retval)
		return retval;
	bprm->cred_prepared = 1;

	memset(bprm->buf, 0, BINPRM_BUF_SIZE);
	return kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);
}
","The `prepare_binprm` function initializes a `linux_binprm` structure for executing a binary file in a Linux environment. It retrieves the inode of the file to check its permissions and sets the effective user and group IDs based on the file's attributes, particularly if the file has set-user-ID (SUID) or set-group-ID (SGID) permissions. The function also ensures that the current task is allowed to gain these privileges and prepares a security context for the binary. Finally, it clears the buffer in the `binprm` structure and reads the binary file into this buffer, returning the result of the read operation.","The code initializes a structure used for executing a binary file in a Linux system by checking the file's permissions and setting the appropriate user and group identifiers. It first clears any previous privilege data and verifies if the current task can gain elevated privileges based on the file's attributes. If the file has specific permission flags, it updates the effective user and group identifiers accordingly. The code also prepares a security context for the binary and clears a buffer before reading the binary file into it, ultimately returning the result of this read operation.","The vulnerability in the original code stemmed from a race condition between changing the ownership of a setuid binary and executing it, which could allow an attacker to gain elevated privileges by exploiting the momentary state where the binary was setuid root. This was addressed in the fixed code by introducing a call to `bprm_fill_uid(bprm)`, which ensures that the user and group identifiers are set correctly while holding the `i_mutex` lock. This prevents concurrent modifications to the binary's ownership during execution, thereby mitigating the risk of privilege escalation through the race condition.","The vulnerability in the original code was due to a race condition that occurred when changing the ownership of a setuid binary while it was being executed. This situation allowed an attacker to exploit the brief moment when the binary's ownership was changed to root, potentially granting unauthorized elevated privileges. The fix implemented a mechanism to ensure that the necessary locks were held during the preparation of the binary for execution. By doing so, it effectively prevented concurrent modifications to the binary's ownership, thereby eliminating the risk of privilege escalation through this race condition.","The vulnerability was fixed by introducing a call to `bprm_fill_uid(bprm)` while ensuring that the `i_mutex` lock is held during the preparation of the binary. This change prevents a race condition between the `chown()` and `execve()` functions, which could allow an attacker to exploit the momentary state when a setuid binary is changed to root. By locking the inode during this process, the code ensures that the user and group identifiers are set correctly and consistently, thereby mitigating the risk of privilege escalation that could occur if the binary's ownership were modified concurrently.","The vulnerability was fixed by implementing a locking mechanism during the preparation of the binary for execution, which prevents concurrent modifications to the binary's ownership. This change ensures that the necessary checks and updates to user and group identifiers occur while the binary is safely locked, eliminating the race condition that could arise from simultaneous ownership changes. By securing the critical section of code, the fix effectively mitigates the risk of privilege escalation that could occur if an attacker exploited the brief window when the binary's ownership was altered to grant elevated privileges."
14,179863,179863,,Remote,Not required,Partial,CVE-2015-1791,https://www.cvedetails.com/cve/CVE-2015-1791/,CWE-362,Medium,Partial,Partial,,2015-06-12,6.8,"Race condition in the ssl3_get_new_session_ticket function in ssl/s3_clnt.c in OpenSSL before 0.9.8zg, 1.0.0 before 1.0.0s, 1.0.1 before 1.0.1n, and 1.0.2 before 1.0.2b, when used for a multi-threaded client, allows remote attackers to cause a denial of service (double free and application crash) or possibly have unspecified other impact by providing a NewSessionTicket during an attempt to reuse a ticket that had been obtained earlier.",2017-11-14,DoS ,32,https://github.com/openssl/openssl/commit/98ece4eebfb6cd45cc8d550c6ac0022965071afc,98ece4eebfb6cd45cc8d550c6ac0022965071afc,"Fix race condition in NewSessionTicket

If a NewSessionTicket is received by a multi-threaded client when
attempting to reuse a previous ticket then a race condition can occur
potentially leading to a double free of the ticket data.

CVE-2015-1791

This also fixes RT#3808 where a session ID is changed for a session already
in the client session cache. Since the session ID is the key to the cache
this breaks the cache access.

Parts of this patch were inspired by this Akamai change:
https://github.com/akamai/openssl/commit/c0bf69a791239ceec64509f9f19fcafb2461b0d3

Reviewed-by: Rich Salz <rsalz@openssl.org>",0,ssl/s3_clnt.c,"{""sha"": ""4e18b65605431b7ef0e7b0fec99604d9c77b404c"", ""filename"": ""include/openssl/ssl.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/include/openssl/ssl.h"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/include/openssl/ssl.h"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/include/openssl/ssl.h?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -2048,6 +2048,7 @@ void ERR_load_SSL_strings(void);\n # define SSL_F_SSL_READ                                   223\n # define SSL_F_SSL_SCAN_CLIENTHELLO_TLSEXT                320\n # define SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT                321\n+# define SSL_F_SSL_SESSION_DUP                            348\n # define SSL_F_SSL_SESSION_NEW                            189\n # define SSL_F_SSL_SESSION_PRINT_FP                       190\n # define SSL_F_SSL_SESSION_SET1_ID_CONTEXT                312""}<_**next**_>{""sha"": ""d6f53b0dea846bba4e7e93b37768faea1dfa879f"", ""filename"": ""ssl/s3_clnt.c"", ""status"": ""modified"", ""additions"": 32, ""deletions"": 0, ""changes"": 32, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/s3_clnt.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/s3_clnt.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/s3_clnt.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -2238,6 +2238,38 @@ int ssl3_get_new_session_ticket(SSL *s)\n     }\n \n     p = d = (unsigned char *)s->init_msg;\n+\n+    if (s->session->session_id_length > 0) {\n+        int i = s->session_ctx->session_cache_mode;\n+        SSL_SESSION *new_sess;\n+        /*\n+         * We reused an existing session, so we need to replace it with a new\n+         * one\n+         */\n+        if (i & SSL_SESS_CACHE_CLIENT) {\n+            /*\n+             * Remove the old session from the cache\n+             */\n+            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {\n+                if (s->session_ctx->remove_session_cb != NULL)\n+                    s->session_ctx->remove_session_cb(s->session_ctx,\n+                                                      s->session);\n+            } else {\n+                /* We carry on if this fails */\n+                SSL_CTX_remove_session(s->session_ctx, s->session);\n+            }\n+        }\n+\n+        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {\n+            al = SSL_AD_INTERNAL_ERROR;\n+            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);\n+            goto f_err;\n+        }\n+\n+        SSL_SESSION_free(s->session);\n+        s->session = new_sess;\n+    }\n+\n     n2l(p, s->session->tlsext_tick_lifetime_hint);\n     n2s(p, ticklen);\n     /* ticket_lifetime_hint + ticket_length + ticket */""}<_**next**_>{""sha"": ""4b4d89ce7a1e8c6a596224a60588f1a6b314b051"", ""filename"": ""ssl/ssl_err.c"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_err.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_err.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_err.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -274,6 +274,7 @@ static ERR_STRING_DATA SSL_str_functs[] = {\n      \""SSL_SCAN_CLIENTHELLO_TLSEXT\""},\n     {ERR_FUNC(SSL_F_SSL_SCAN_SERVERHELLO_TLSEXT),\n      \""SSL_SCAN_SERVERHELLO_TLSEXT\""},\n+    {ERR_FUNC(SSL_F_SSL_SESSION_DUP), \""ssl_session_dup\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_NEW), \""SSL_SESSION_new\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_PRINT_FP), \""SSL_SESSION_print_fp\""},\n     {ERR_FUNC(SSL_F_SSL_SESSION_SET1_ID_CONTEXT),""}<_**next**_>{""sha"": ""3252631e1c8981e39a828c59b98c7e63931e4a25"", ""filename"": ""ssl/ssl_locl.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_locl.h"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_locl.h"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_locl.h?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -1860,6 +1860,7 @@ __owur int ssl_set_peer_cert_type(SESS_CERT *c, int type);\n __owur int ssl_get_new_session(SSL *s, int session);\n __owur int ssl_get_prev_session(SSL *s, unsigned char *session, int len,\n                          const unsigned char *limit);\n+__owur SSL_SESSION *ssl_session_dup(SSL_SESSION *src, int ticket);\n __owur int ssl_cipher_id_cmp(const SSL_CIPHER *a, const SSL_CIPHER *b);\n DECLARE_OBJ_BSEARCH_GLOBAL_CMP_FN(SSL_CIPHER, SSL_CIPHER, ssl_cipher_id);\n __owur int ssl_cipher_ptr_id_cmp(const SSL_CIPHER *const *ap,""}<_**next**_>{""sha"": ""fd940541d5bc0fdb02c298da55170b40d8397eb4"", ""filename"": ""ssl/ssl_sess.c"", ""status"": ""modified"", ""additions"": 116, ""deletions"": 0, ""changes"": 116, ""blob_url"": ""https://github.com/openssl/openssl/blob/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_sess.c"", ""raw_url"": ""https://github.com/openssl/openssl/raw/98ece4eebfb6cd45cc8d550c6ac0022965071afc/ssl/ssl_sess.c"", ""contents_url"": ""https://api.github.com/repos/openssl/openssl/contents/ssl/ssl_sess.c?ref=98ece4eebfb6cd45cc8d550c6ac0022965071afc"", ""patch"": ""@@ -225,6 +225,122 @@ SSL_SESSION *SSL_SESSION_new(void)\n     return (ss);\n }\n \n+/*\n+ * Create a new SSL_SESSION and duplicate the contents of |src| into it. If\n+ * ticket == 0 then no ticket information is duplicated, otherwise it is.\n+ */\n+SSL_SESSION *ssl_session_dup(SSL_SESSION *src, int ticket)\n+{\n+    SSL_SESSION *dest;\n+\n+    dest = OPENSSL_malloc(sizeof(*src));\n+    if (dest == NULL) {\n+        goto err;\n+    }\n+    memcpy(dest, src, sizeof(*dest));\n+\n+#ifndef OPENSSL_NO_PSK\n+    if (src->psk_identity_hint) {\n+        dest->psk_identity_hint = BUF_strdup(src->psk_identity_hint);\n+        if (dest->psk_identity_hint == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->psk_identity_hint = NULL;\n+    }\n+    if (src->psk_identity) {\n+        dest->psk_identity = BUF_strdup(src->psk_identity);\n+        if (dest->psk_identity == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->psk_identity = NULL;\n+    }\n+#endif\n+\n+    if (src->sess_cert != NULL)\n+        CRYPTO_add(&src->sess_cert->references, 1, CRYPTO_LOCK_SSL_SESS_CERT);\n+\n+    if (src->peer != NULL)\n+        CRYPTO_add(&src->peer->references, 1, CRYPTO_LOCK_X509);\n+\n+    dest->references = 1;\n+\n+    if(src->ciphers != NULL) {\n+        dest->ciphers = sk_SSL_CIPHER_dup(src->ciphers);\n+        if (dest->ciphers == NULL)\n+            goto err;\n+    } else {\n+        dest->ciphers = NULL;\n+    }\n+\n+    if (!CRYPTO_dup_ex_data(CRYPTO_EX_INDEX_SSL_SESSION,\n+                                            &dest->ex_data, &src->ex_data)) {\n+        goto err;\n+    }\n+\n+    /* We deliberately don't copy the prev and next pointers */\n+    dest->prev = NULL;\n+    dest->next = NULL;\n+\n+#ifndef OPENSSL_NO_TLSEXT\n+    if (src->tlsext_hostname) {\n+        dest->tlsext_hostname = BUF_strdup(src->tlsext_hostname);\n+        if (dest->tlsext_hostname == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->tlsext_hostname = NULL;\n+    }\n+# ifndef OPENSSL_NO_EC\n+    if (src->tlsext_ecpointformatlist) {\n+        dest->tlsext_ecpointformatlist =\n+            BUF_memdup(src->tlsext_ecpointformatlist,\n+                       src->tlsext_ecpointformatlist_length);\n+        if (dest->tlsext_ecpointformatlist == NULL)\n+            goto err;\n+        dest->tlsext_ecpointformatlist_length =\n+            src->tlsext_ecpointformatlist_length;\n+    }\n+    if (src->tlsext_ellipticcurvelist) {\n+        dest->tlsext_ellipticcurvelist =\n+            BUF_memdup(src->tlsext_ellipticcurvelist,\n+                       src->tlsext_ellipticcurvelist_length);\n+        if (dest->tlsext_ellipticcurvelist == NULL)\n+            goto err;\n+        dest->tlsext_ellipticcurvelist_length =\n+            src->tlsext_ellipticcurvelist_length;\n+    }\n+# endif\n+#endif\n+\n+    if (ticket != 0) {\n+        dest->tlsext_tick_lifetime_hint = src->tlsext_tick_lifetime_hint;\n+        dest->tlsext_ticklen = src->tlsext_ticklen;\n+        if((dest->tlsext_tick = OPENSSL_malloc(src->tlsext_ticklen)) == NULL) {\n+            goto err;\n+        }\n+    }\n+\n+#ifndef OPENSSL_NO_SRP\n+    dest->srp_username = NULL;\n+    if (src->srp_username) {\n+        dest->srp_username = BUF_strdup(src->srp_username);\n+        if (dest->srp_username == NULL) {\n+            goto err;\n+        }\n+    } else {\n+        dest->srp_username = NULL;\n+    }\n+#endif\n+\n+    return dest;\n+err:\n+    SSLerr(SSL_F_SSL_SESSION_DUP, ERR_R_MALLOC_FAILURE);\n+    SSL_SESSION_free(dest);\n+    return NULL;\n+}\n+\n const unsigned char *SSL_SESSION_get_id(const SSL_SESSION *s,\n                                         unsigned int *len)\n {""}","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;

    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
",C,"
    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

",,,"@@ -2238,6 +2238,38 @@ int ssl3_get_new_session_ticket(SSL *s)
     }
 
     p = d = (unsigned char *)s->init_msg;
+
+    if (s->session->session_id_length > 0) {
+        int i = s->session_ctx->session_cache_mode;
+        SSL_SESSION *new_sess;
+        /*
+         * We reused an existing session, so we need to replace it with a new
+         * one
+         */
+        if (i & SSL_SESS_CACHE_CLIENT) {
+            /*
+             * Remove the old session from the cache
+             */
+            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
+                if (s->session_ctx->remove_session_cb != NULL)
+                    s->session_ctx->remove_session_cb(s->session_ctx,
+                                                      s->session);
+            } else {
+                /* We carry on if this fails */
+                SSL_CTX_remove_session(s->session_ctx, s->session);
+            }
+        }
+
+        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
+            al = SSL_AD_INTERNAL_ERROR;
+            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
+            goto f_err;
+        }
+
+        SSL_SESSION_free(s->session);
+        s->session = new_sess;
+    }
+
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */",openssl,98ece4eebfb6cd45cc8d550c6ac0022965071afc,8c2b1d872b25f3ec78e04f5cd2de8f21e853c4a6,1,"int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//    if (s->session->session_id_length > 0) {
//fix_flaw_line_below:
//        int i = s->session_ctx->session_cache_mode;
//fix_flaw_line_below:
//        SSL_SESSION *new_sess;
//fix_flaw_line_below:
//        /*
//fix_flaw_line_below:
//         * We reused an existing session, so we need to replace it with a new
//fix_flaw_line_below:
//         * one
//fix_flaw_line_below:
//         */
//fix_flaw_line_below:
//        if (i & SSL_SESS_CACHE_CLIENT) {
//fix_flaw_line_below:
//            /*
//fix_flaw_line_below:
//             * Remove the old session from the cache
//fix_flaw_line_below:
//             */
//fix_flaw_line_below:
//            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
//fix_flaw_line_below:
//                if (s->session_ctx->remove_session_cb != NULL)
//fix_flaw_line_below:
//                    s->session_ctx->remove_session_cb(s->session_ctx,
//fix_flaw_line_below:
//                                                      s->session);
//fix_flaw_line_below:
//            } else {
//fix_flaw_line_below:
//                /* We carry on if this fails */
//fix_flaw_line_below:
//                SSL_CTX_remove_session(s->session_ctx, s->session);
//fix_flaw_line_below:
//            }
//fix_flaw_line_below:
//        }
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
//fix_flaw_line_below:
//            al = SSL_AD_INTERNAL_ERROR;
//fix_flaw_line_below:
//            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
//fix_flaw_line_below:
//            goto f_err;
//fix_flaw_line_below:
//        }
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//        SSL_SESSION_free(s->session);
//fix_flaw_line_below:
//        s->session = new_sess;
//fix_flaw_line_below:
//    }
//fix_flaw_line_below:
//
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
",179863,"int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;
     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","int ssl3_get_new_session_ticket(SSL *s)
{
    int ok, al, ret = 0, ticklen;
    long n;
    const unsigned char *p;
    unsigned char *d;

    n = s->method->ssl_get_message(s,
                                   SSL3_ST_CR_SESSION_TICKET_A,
                                   SSL3_ST_CR_SESSION_TICKET_B,
                                   SSL3_MT_NEWSESSION_TICKET, 16384, &ok);

    if (!ok)
        return ((int)n);

    if (n < 6) {
        /* need at least ticket_lifetime_hint + ticket length */
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
     }
 
     p = d = (unsigned char *)s->init_msg;

    if (s->session->session_id_length > 0) {
        int i = s->session_ctx->session_cache_mode;
        SSL_SESSION *new_sess;
        /*
         * We reused an existing session, so we need to replace it with a new
         * one
         */
        if (i & SSL_SESS_CACHE_CLIENT) {
            /*
             * Remove the old session from the cache
             */
            if (i & SSL_SESS_CACHE_NO_INTERNAL_STORE) {
                if (s->session_ctx->remove_session_cb != NULL)
                    s->session_ctx->remove_session_cb(s->session_ctx,
                                                      s->session);
            } else {
                /* We carry on if this fails */
                SSL_CTX_remove_session(s->session_ctx, s->session);
            }
        }

        if ((new_sess = ssl_session_dup(s->session, 0)) == 0) {
            al = SSL_AD_INTERNAL_ERROR;
            SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
            goto f_err;
        }

        SSL_SESSION_free(s->session);
        s->session = new_sess;
    }

     n2l(p, s->session->tlsext_tick_lifetime_hint);
     n2s(p, ticklen);
     /* ticket_lifetime_hint + ticket_length + ticket */
    if (ticklen + 6 != n) {
        al = SSL_AD_DECODE_ERROR;
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, SSL_R_LENGTH_MISMATCH);
        goto f_err;
    }
    OPENSSL_free(s->session->tlsext_tick);
    s->session->tlsext_ticklen = 0;
    s->session->tlsext_tick = OPENSSL_malloc(ticklen);
    if (!s->session->tlsext_tick) {
        SSLerr(SSL_F_SSL3_GET_NEW_SESSION_TICKET, ERR_R_MALLOC_FAILURE);
        goto err;
    }
    memcpy(s->session->tlsext_tick, p, ticklen);
    s->session->tlsext_ticklen = ticklen;
    /*
     * There are two ways to detect a resumed ticket session. One is to set
     * an appropriate session ID and then the server must return a match in
     * ServerHello. This allows the normal client session ID matching to work
     * and we know much earlier that the ticket has been accepted. The
     * other way is to set zero length session ID when the ticket is
     * presented and rely on the handshake to determine session resumption.
     * We choose the former approach because this fits in with assumptions
     * elsewhere in OpenSSL. The session ID is set to the SHA256 (or SHA1 is
     * SHA256 is disabled) hash of the ticket.
     */
    EVP_Digest(p, ticklen,
               s->session->session_id, &s->session->session_id_length,
               EVP_sha256(), NULL);
    ret = 1;
    return (ret);
 f_err:
    ssl3_send_alert(s, SSL3_AL_FATAL, al);
 err:
    s->state = SSL_ST_ERR;
    return (-1);
}
","The `ssl3_get_new_session_ticket` function processes a new session ticket in an SSL/TLS connection. It begins by retrieving a message containing the session ticket and checks its validity. If the message length is insufficient, it triggers an error. The function extracts the ticket lifetime hint and the ticket length, ensuring they match the expected size. It allocates memory for the new session ticket, copies the ticket data, and computes a session ID based on the ticket's hash. If any errors occur during these steps, appropriate alerts are sent, and the function returns an error code; otherwise, it returns success.","The function processes a new session ticket in an SSL/TLS connection by first retrieving a message that contains the ticket and validating its length. It checks if the message is long enough to include necessary information; if not, it triggers an error. Upon successful validation, it extracts the lifetime hint and ticket length, allocates memory for the ticket, and copies the ticket data. Additionally, it computes a session identifier based on the ticket's content. If any errors arise during these operations, appropriate alerts are sent, and the function returns an error code; otherwise, it indicates success.","The vulnerability in the original code stemmed from a race condition that could occur in a multi-threaded client when a new session ticket was received while attempting to reuse a previous ticket. This situation could lead to a double free of the ticket data, causing memory corruption and potential crashes. The fixed code addresses this issue by checking if a session ID already exists before reusing a session. If it does, the old session is removed from the cache, and a new session is created, ensuring that the session ID remains consistent and preventing the race condition from occurring.","The vulnerability in the original code was a race condition that could occur in a multi-threaded environment when a new session ticket was received while attempting to reuse an existing ticket. This situation could lead to a double free of the ticket data, resulting in memory corruption and potential application crashes. The fix implemented in the updated code involves checking for the existence of a session ID before reusing a session. By ensuring that the old session is properly removed from the cache before creating a new one, the fix prevents the race condition and maintains the integrity of the session management process.","The vulnerability was fixed by introducing checks for the session ID before reusing an existing session. In the updated code, if the session ID length is greater than zero, the code first checks the session cache mode to determine if the old session should be removed. If the cache mode allows it, the old session is removed using the appropriate callback or by calling a removal function. A new session is then created by duplicating the existing session. This ensures that the session ID remains consistent and prevents the double free of the ticket data, thereby eliminating the race condition.","The vulnerability was addressed by implementing a mechanism to check for the existence of a session before reusing it. The updated code ensures that if a session is already present, it is properly removed from the cache before creating a new one. This process involves verifying the cache mode to determine the appropriate method for session removal. By ensuring that the old session is cleared and a new session is created without conflicts, the fix prevents the potential for double freeing of memory, thereby eliminating the race condition and maintaining the integrity of session management."
15,179914,179914,,Local,Not required,Complete,CVE-2015-0239,https://www.cvedetails.com/cve/CVE-2015-0239/,CWE-362,High,,Partial,,2015-03-02,4.7,"The em_sysenter function in arch/x86/kvm/emulate.c in the Linux kernel before 3.18.5, when the guest OS lacks SYSENTER MSR initialization, allows guest OS users to gain guest OS privileges or cause a denial of service (guest OS crash) by triggering use of a 16-bit code segment for emulation of a SYSENTER instruction.",2016-12-23,DoS +Priv ,8,https://github.com/torvalds/linux/commit/f3747379accba8e95d70cec0eae0582c8c182050,f3747379accba8e95d70cec0eae0582c8c182050,"KVM: x86: SYSENTER emulation is broken

SYSENTER emulation is broken in several ways:
1. It misses the case of 16-bit code segments completely (CVE-2015-0239).
2. MSR_IA32_SYSENTER_CS is checked in 64-bit mode incorrectly (bits 0 and 1 can
   still be set without causing #GP).
3. MSR_IA32_SYSENTER_EIP and MSR_IA32_SYSENTER_ESP are not masked in
   legacy-mode.
4. There is some unneeded code.

Fix it.

Cc: stable@vger.linux.org
Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>",19,arch/x86/kvm/emulate.c,"{""sha"": ""de12c1d379f16899645d96a2c3fd75663919c86d"", ""filename"": ""arch/x86/kvm/emulate.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 19, ""changes"": 27, ""blob_url"": ""https://github.com/torvalds/linux/blob/f3747379accba8e95d70cec0eae0582c8c182050/arch/x86/kvm/emulate.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/f3747379accba8e95d70cec0eae0582c8c182050/arch/x86/kvm/emulate.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kvm/emulate.c?ref=f3747379accba8e95d70cec0eae0582c8c182050"", ""patch"": ""@@ -2348,7 +2348,7 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \t * Not recognized on AMD in compat mode (but is recognized in legacy\n \t * mode).\n \t */\n-\tif ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)\n+\tif ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)\n \t    && !vendor_intel(ctxt))\n \t\treturn emulate_ud(ctxt);\n \n@@ -2359,25 +2359,13 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \tsetup_syscalls_segments(ctxt, &cs, &ss);\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n-\tswitch (ctxt->mode) {\n-\tcase X86EMUL_MODE_PROT32:\n-\t\tif ((msr_data & 0xfffc) == 0x0)\n-\t\t\treturn emulate_gp(ctxt, 0);\n-\t\tbreak;\n-\tcase X86EMUL_MODE_PROT64:\n-\t\tif (msr_data == 0x0)\n-\t\t\treturn emulate_gp(ctxt, 0);\n-\t\tbreak;\n-\tdefault:\n-\t\tbreak;\n-\t}\n+\tif ((msr_data & 0xfffc) == 0x0)\n+\t\treturn emulate_gp(ctxt, 0);\n \n \tctxt->eflags &= ~(EFLG_VM | EFLG_IF);\n-\tcs_sel = (u16)msr_data;\n-\tcs_sel &= ~SELECTOR_RPL_MASK;\n+\tcs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;\n \tss_sel = cs_sel + 8;\n-\tss_sel &= ~SELECTOR_RPL_MASK;\n-\tif (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {\n+\tif (efer & EFER_LMA) {\n \t\tcs.d = 0;\n \t\tcs.l = 1;\n \t}\n@@ -2386,10 +2374,11 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n \tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);\n-\tctxt->_eip = msr_data;\n+\tctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;\n \n \tops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);\n-\t*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;\n+\t*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :\n+\t\t\t\t\t\t\t      (u32)msr_data;\n \n \treturn X86EMUL_CONTINUE;\n }""}","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
 
 	return X86EMUL_CONTINUE;
 }
",C,"	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
	if (efer & EFER_LMA) {
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
","	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
	ctxt->_eip = msr_data;
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
",,"@@ -2348,7 +2348,7 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
-	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
+	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
@@ -2359,25 +2359,13 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
-	switch (ctxt->mode) {
-	case X86EMUL_MODE_PROT32:
-		if ((msr_data & 0xfffc) == 0x0)
-			return emulate_gp(ctxt, 0);
-		break;
-	case X86EMUL_MODE_PROT64:
-		if (msr_data == 0x0)
-			return emulate_gp(ctxt, 0);
-		break;
-	default:
-		break;
-	}
+	if ((msr_data & 0xfffc) == 0x0)
+		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
-	cs_sel = (u16)msr_data;
-	cs_sel &= ~SELECTOR_RPL_MASK;
+	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
-	ss_sel &= ~SELECTOR_RPL_MASK;
-	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
+	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}
@@ -2386,10 +2374,11 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
-	ctxt->_eip = msr_data;
+	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
-	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
+	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
+							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }",linux,f3747379accba8e95d70cec0eae0582c8c182050,63ea0a49ae0b145b91ff2b070c01b66fc75854b9,1,"static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
//flaw_line_below:
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
//fix_flaw_line_below:
//	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
//flaw_line_below:
	switch (ctxt->mode) {
//flaw_line_below:
	case X86EMUL_MODE_PROT32:
//flaw_line_below:
		if ((msr_data & 0xfffc) == 0x0)
//flaw_line_below:
			return emulate_gp(ctxt, 0);
//flaw_line_below:
		break;
//flaw_line_below:
	case X86EMUL_MODE_PROT64:
//flaw_line_below:
		if (msr_data == 0x0)
//flaw_line_below:
			return emulate_gp(ctxt, 0);
//flaw_line_below:
		break;
//flaw_line_below:
	default:
//flaw_line_below:
		break;
//flaw_line_below:
	}
//fix_flaw_line_below:
//	if ((msr_data & 0xfffc) == 0x0)
//fix_flaw_line_below:
//		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
//flaw_line_below:
	cs_sel = (u16)msr_data;
//flaw_line_below:
	cs_sel &= ~SELECTOR_RPL_MASK;
//fix_flaw_line_below:
//	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
//flaw_line_below:
	ss_sel &= ~SELECTOR_RPL_MASK;
//flaw_line_below:
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
//fix_flaw_line_below:
//	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
//flaw_line_below:
	ctxt->_eip = msr_data;
//fix_flaw_line_below:
//	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
//flaw_line_below:
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
//fix_flaw_line_below:
//	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
//fix_flaw_line_below:
//							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
",179914,"static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","static int em_sysenter(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	u16 cs_sel, ss_sel;
	u64 efer = 0;

	ops->get_msr(ctxt, MSR_EFER, &efer);
	/* inject #GP if in real mode */
	if (ctxt->mode == X86EMUL_MODE_REAL)
		return emulate_gp(ctxt, 0);

	/*
 	 * Not recognized on AMD in compat mode (but is recognized in legacy
 	 * mode).
 	 */
	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
 	    && !vendor_intel(ctxt))
 		return emulate_ud(ctxt);
 
	/* sysenter/sysexit have not been tested in 64bit mode. */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		return X86EMUL_UNHANDLEABLE;

 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}

	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
","The `em_sysenter` function emulates the `SYSENTER` instruction in an x86 architecture context, handling different operating modes such as real mode, protected mode, and long mode. It first retrieves the Extended Feature Enable Register (EFER) and checks for compatibility issues, injecting a general protection fault if necessary. The function sets up the code and stack segment descriptors, ensuring proper privilege levels, and updates the instruction pointer and stack pointer based on the values from the corresponding model-specific registers (MSRs). Finally, it returns a status indicating whether to continue emulation or handle an error.","The provided code emulates the execution of a specific instruction in an x86 architecture, managing various operating modes such as real, protected, and long mode. It begins by checking compatibility and potential errors, injecting faults when necessary. The code sets up segment descriptors for code and stack, ensuring correct privilege levels, and retrieves values from specific registers to update the instruction and stack pointers. The function ultimately determines whether to continue the emulation process or signal an error, ensuring proper handling of system calls in the emulated environment.","The vulnerability in the original code stemmed from inadequate handling of various operating modes, particularly the failure to properly check for 16-bit code segments and incorrect validation of the `MSR_IA32_SYSENTER_CS` register in 64-bit mode, which could lead to unintended behavior or security issues. The fixed code addresses these vulnerabilities by ensuring that the `MSR_IA32_SYSENTER_CS` is validated correctly, regardless of the mode, and by applying appropriate masking to the `MSR_IA32_SYSENTER_EIP` and `MSR_IA32_SYSENTER_ESP` registers in legacy mode. Additionally, unnecessary code was removed to streamline the emulation process.","The vulnerability in the original code was primarily due to inadequate handling of different operating modes, particularly the omission of checks for 16-bit code segments and incorrect validation of a specific register in 64-bit mode, which could lead to improper execution and potential security risks. Additionally, certain registers were not masked correctly in legacy mode, further exacerbating the issue. The fixed code rectifies these problems by ensuring comprehensive checks for all relevant modes, applying the necessary masking to the affected registers, and removing unnecessary code to enhance clarity and functionality, thereby improving overall security and reliability.","The vulnerability was fixed by implementing comprehensive checks for the `MSR_IA32_SYSENTER_CS` register to ensure it is validated correctly in both 64-bit and legacy modes, addressing the oversight of 16-bit code segments. The code now masks the values retrieved from `MSR_IA32_SYSENTER_EIP` and `MSR_IA32_SYSENTER_ESP` to prevent improper execution in legacy mode. Additionally, unnecessary code was removed to streamline the emulation process. These changes enhance the robustness of the SYSENTER emulation, ensuring that all potential edge cases are handled appropriately and securely.","The vulnerability was fixed by enhancing the validation process for the relevant registers to ensure they are correctly checked across all operating modes, including the previously overlooked 16-bit code segments. The code now includes proper masking for specific registers in legacy mode to prevent unintended behavior during execution. Additionally, unnecessary code was removed to simplify the logic and improve clarity. These modifications collectively strengthen the emulation process, ensuring that all potential edge cases are addressed and that the system operates securely and reliably across different scenarios."
16,179955,179955,,Local,Not required,Complete,CVE-2014-9529,https://www.cvedetails.com/cve/CVE-2014-9529/,CWE-362,Low,Complete,Complete,,2015-01-09,7.2,Race condition in the key_gc_unused_keys function in security/keys/gc.c in the Linux kernel through 3.18.2 allows local users to cause a denial of service (memory corruption or panic) or possibly have unspecified other impact via keyctl commands that trigger access to a key structure member during garbage collection of a key.,2018-01-04,DoS Mem. Corr. ,2,https://github.com/torvalds/linux/commit/a3a8784454692dd72e5d5d34dcdab17b4420e74c,a3a8784454692dd72e5d5d34dcdab17b4420e74c,"KEYS: close race between key lookup and freeing

When a key is being garbage collected, it's key->user would get put before
the ->destroy() callback is called, where the key is removed from it's
respective tracking structures.

This leaves a key hanging in a semi-invalid state which leaves a window open
for a different task to try an access key->user. An example is
find_keyring_by_name() which would dereference key->user for a key that is
in the process of being garbage collected (where key->user was freed but
->destroy() wasn't called yet - so it's still present in the linked list).

This would cause either a panic, or corrupt memory.

Fixes CVE-2014-9529.

Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
Signed-off-by: David Howells <dhowells@redhat.com>",1,security/keys/gc.c,"{""sha"": ""c7952375ac5325cfb4c403fa1020671b5f31a150"", ""filename"": ""security/keys/gc.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 2, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/a3a8784454692dd72e5d5d34dcdab17b4420e74c/security/keys/gc.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/a3a8784454692dd72e5d5d34dcdab17b4420e74c/security/keys/gc.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/security/keys/gc.c?ref=a3a8784454692dd72e5d5d34dcdab17b4420e74c"", ""patch"": ""@@ -148,12 +148,12 @@ static noinline void key_gc_unused_keys(struct list_head *keys)\n \t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n \t\t\tatomic_dec(&key->user->nikeys);\n \n-\t\tkey_user_put(key->user);\n-\n \t\t/* now throw away the key memory */\n \t\tif (key->type->destroy)\n \t\t\tkey->type->destroy(key);\n \n+\t\tkey_user_put(key->user);\n+\n \t\tkfree(key->description);\n \n #ifdef KEY_DEBUGGING""}","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
		key_user_put(key->user);

 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
		key_user_put(key->user);
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
",C,"		key_user_put(key->user);

","		key_user_put(key->user);
",,"@@ -148,12 +148,12 @@ static noinline void key_gc_unused_keys(struct list_head *keys)
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
-		key_user_put(key->user);
-
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
+		key_user_put(key->user);
+
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING",linux,a3a8784454692dd72e5d5d34dcdab17b4420e74c,693a30b8f19a964087a3762d09fb2e1cbad6b0d4,1,"static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
//flaw_line_below:
		key_user_put(key->user);
//flaw_line_below:

 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
//fix_flaw_line_below:
//		key_user_put(key->user);
//fix_flaw_line_below:
//
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
",179955,"static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
		key_user_put(key->user);
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","static noinline void key_gc_unused_keys(struct list_head *keys)
{
	while (!list_empty(keys)) {
		struct key *key =
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);

		kdebug(""- %u"", key->serial);
		key_check(key);

		security_key_free(key);

		/* deal with the user's key tracking and quota */
		if (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {
			spin_lock(&key->user->lock);
			key->user->qnkeys--;
			key->user->qnbytes -= key->quotalen;
			spin_unlock(&key->user->lock);
		}

		atomic_dec(&key->user->nkeys);
 		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 			atomic_dec(&key->user->nikeys);
 
 		/* now throw away the key memory */
 		if (key->type->destroy)
 			key->type->destroy(key);
 
		key_user_put(key->user);

 		kfree(key->description);
 
 #ifdef KEY_DEBUGGING
		key->magic = KEY_DEBUG_MAGIC_X;
#endif
		kmem_cache_free(key_jar, key);
	}
}
","The function `key_gc_unused_keys` is designed to clean up and free unused keys from a linked list, referred to as `keys`. It iterates through the list, removing each key and performing several operations: logging the key's serial number, checking its validity, and freeing its associated resources. If the key is tracked under a user's quota, it updates the user's key count and byte usage accordingly. The function also decrements the user's key statistics if the key is instantiated, releases the user reference, and finally deallocates the key's memory, including its description, while ensuring proper memory management and debugging support.","The provided code defines a function that systematically removes and deallocates unused keys from a linked list. It processes each key by logging its identifier, verifying its integrity, and releasing its associated resources. If the key is part of a user's quota, the function updates the user's statistics regarding key counts and memory usage. Additionally, it adjusts the user's key metrics if the key is active, releases the user reference, and finally frees the memory allocated for the key and its description, ensuring efficient memory management and supporting debugging features.","The vulnerability in the original code arises from the premature release of the user reference associated with a key before the key's destruction process is complete. Specifically, the `key_user_put(key->user)` function is called before the `key->type->destroy(key)` callback, which can leave the key in a semi-invalid state. This creates a race condition where another task could access `key->user`, leading to potential memory corruption or system panic. The fix addresses this by ensuring that the user reference is only released after the destruction callback has been executed, thus maintaining the integrity of the key's state throughout the garbage collection process.","The vulnerability in the original code stems from the improper timing of releasing a user reference associated with a key during the garbage collection process. Specifically, the user reference was released before the key's destruction callback was executed, which could leave the key in an inconsistent state. This inconsistency allowed other tasks to access the user reference while it was still linked to the key, potentially leading to memory corruption or system crashes. The fix ensures that the user reference is only released after the key has been fully destroyed, thereby eliminating the race condition and maintaining the integrity of the key's state throughout the process.","The vulnerability was fixed by rearranging the order of operations in the garbage collection process. Specifically, the call to `key_user_put(key->user)` was moved to occur after the `key->type->destroy(key)` callback. This change ensures that the user reference is not released until after the key has been fully destroyed and removed from its tracking structures. By doing so, the code prevents a race condition where another task could access `key->user` while it is still linked to the key, thereby eliminating the risk of memory corruption or system panic during the garbage collection of keys.","The vulnerability was addressed by changing the sequence of operations during the key's garbage collection process. The release of the user reference was postponed until after the key's destruction process was completed. This adjustment ensures that the key remains in a valid state throughout its removal from tracking structures, preventing other tasks from accessing a potentially invalid reference. By ensuring that the user reference is only released after the key has been fully processed, the fix effectively eliminates the race condition that could lead to memory corruption or system crashes."
17,180092,180092,,Local,Not required,,CVE-2016-7916,https://www.cvedetails.com/cve/CVE-2016-7916/,CWE-362,Medium,Complete,,,2016-11-16,4.7,Race condition in the environ_read function in fs/proc/base.c in the Linux kernel before 4.5.4 allows local users to obtain sensitive information from kernel memory by reading a /proc/*/environ file during a process-setup time interval in which environment-variable copying is incomplete.,2017-01-17,+Info ,2,https://github.com/torvalds/linux/commit/8148a73c9901a8794a50f950083c00ccf97d43b3,8148a73c9901a8794a50f950083c00ccf97d43b3,"proc: prevent accessing /proc/<PID>/environ until it's ready

If /proc/<PID>/environ gets read before the envp[] array is fully set up
in create_{aout,elf,elf_fdpic,flat}_tables(), we might end up trying to
read more bytes than are actually written, as env_start will already be
set but env_end will still be zero, making the range calculation
underflow, allowing to read beyond the end of what has been written.

Fix this as it is done for /proc/<PID>/cmdline by testing env_end for
zero.  It is, apparently, intentionally set last in create_*_tables().

This bug was found by the PaX size_overflow plugin that detected the
arithmetic underflow of 'this_len = env_end - (env_start + src)' when
env_end is still zero.

The expected consequence is that userland trying to access
/proc/<PID>/environ of a not yet fully set up process may get
inconsistent data as we're in the middle of copying in the environment
variables.

Fixes: https://forums.grsecurity.net/viewtopic.php?f=3&t=4363
Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=116461
Signed-off-by: Mathias Krause <minipli@googlemail.com>
Cc: Emese Revfy <re.emese@gmail.com>
Cc: Pax Team <pageexec@freemail.hu>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Cc: Mateusz Guzik <mguzik@redhat.com>
Cc: Alexey Dobriyan <adobriyan@gmail.com>
Cc: Cyrill Gorcunov <gorcunov@openvz.org>
Cc: Jarod Wilson <jarod@redhat.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",1,fs/proc/base.c,"{""sha"": ""92e37e224cd22d66d0b9675f1790e110d5e7b415"", ""filename"": ""fs/proc/base.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 1, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/8148a73c9901a8794a50f950083c00ccf97d43b3/fs/proc/base.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8148a73c9901a8794a50f950083c00ccf97d43b3/fs/proc/base.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/base.c?ref=8148a73c9901a8794a50f950083c00ccf97d43b3"", ""patch"": ""@@ -955,7 +955,8 @@ static ssize_t environ_read(struct file *file, char __user *buf,\n \tstruct mm_struct *mm = file->private_data;\n \tunsigned long env_start, env_end;\n \n-\tif (!mm)\n+\t/* Ensure the process spawned far enough to have an environment. */\n+\tif (!mm || !mm->env_end)\n \t\treturn 0;\n \n \tpage = (char *)__get_free_page(GFP_TEMPORARY);""}","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	if (!mm)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
",C,"	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
","	if (!mm)
",,"@@ -955,7 +955,8 @@ static ssize_t environ_read(struct file *file, char __user *buf,
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
-	if (!mm)
+	/* Ensure the process spawned far enough to have an environment. */
+	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);",linux,8148a73c9901a8794a50f950083c00ccf97d43b3,32a4e169039927bfb6ee9f0ccbbe3a8aaf13a4bc,1,"static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
//flaw_line_below:
	if (!mm)
//fix_flaw_line_below:
//	/* Ensure the process spawned far enough to have an environment. */
//fix_flaw_line_below:
//	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
",180092,"static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	if (!mm)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","static ssize_t environ_read(struct file *file, char __user *buf,
			size_t count, loff_t *ppos)
{
	char *page;
	unsigned long src = *ppos;
	int ret = 0;
 	struct mm_struct *mm = file->private_data;
 	unsigned long env_start, env_end;
 
	/* Ensure the process spawned far enough to have an environment. */
	if (!mm || !mm->env_end)
 		return 0;
 
 	page = (char *)__get_free_page(GFP_TEMPORARY);
	if (!page)
		return -ENOMEM;

	ret = 0;
	if (!atomic_inc_not_zero(&mm->mm_users))
		goto free;

	down_read(&mm->mmap_sem);
	env_start = mm->env_start;
	env_end = mm->env_end;
	up_read(&mm->mmap_sem);

	while (count > 0) {
		size_t this_len, max_len;
		int retval;

		if (src >= (env_end - env_start))
			break;

		this_len = env_end - (env_start + src);

		max_len = min_t(size_t, PAGE_SIZE, count);
		this_len = min(max_len, this_len);

		retval = access_remote_vm(mm, (env_start + src),
			page, this_len, 0);

		if (retval <= 0) {
			ret = retval;
			break;
		}

		if (copy_to_user(buf, page, retval)) {
			ret = -EFAULT;
			break;
		}

		ret += retval;
		src += retval;
		buf += retval;
		count -= retval;
	}
	*ppos = src;
	mmput(mm);

free:
	free_page((unsigned long) page);
	return ret;
}
","The `environ_read` function is designed to read the environment variables of a process in a Linux kernel module context. It takes a file structure, a user buffer, a count of bytes to read, and a position pointer as parameters. The function first retrieves the memory management structure of the target process and allocates a temporary page for data transfer. It then acquires a read lock on the process's memory, calculates the range of environment variables, and reads them into the allocated page using `access_remote_vm`. The data is subsequently copied to the user buffer, updating the position pointer accordingly, and finally, it releases resources and returns the number of bytes read or an error code.","The provided code implements a function that reads the environment variables of a specified process in a kernel module. It begins by checking the validity of the target process's memory structure and allocates a temporary buffer for data storage. After acquiring a read lock on the process's memory, it determines the range of environment variables to be read. The function then retrieves the data from the target process's memory and copies it to a user-supplied buffer, updating the read position accordingly. Finally, it releases allocated resources and returns the number of bytes successfully read or an error code if applicable.","The vulnerability in the original code arises from the potential for an arithmetic underflow when calculating the length of the environment variables to read. Specifically, if the environment array is accessed before it is fully initialized, `env_end` could be zero while `env_start` is set, leading to an incorrect calculation of `this_len` and allowing reads beyond the intended memory range. The fix addresses this by adding a check for `env_end` being zero before proceeding with the read operation, ensuring that the environment variables are only accessed when they are fully set up, thus preventing inconsistent data access and potential memory corruption.","The vulnerability in the original code occurs when attempting to read environment variables from a process that has not yet fully initialized its environment array. If accessed prematurely, the calculation for the length of data to read can result in an underflow, allowing the code to attempt reading beyond the allocated memory, which can lead to inconsistent or corrupted data. The fix addresses this issue by adding a check to ensure that the environment end value is not zero before proceeding with the read operation, thereby preventing access to uninitialized memory and ensuring that the environment variables are only read when they are fully set up.","The vulnerability was fixed by adding a condition to check if `mm->env_end` is zero before proceeding with the read operation. This check ensures that the environment variables are only accessed when they are fully initialized. Specifically, the code now includes a validation step that returns early if the memory management structure is invalid or if `env_end` is zero, preventing the subsequent calculations for `this_len` and the read operation from executing. This change effectively eliminates the risk of arithmetic underflow and ensures that the environment variables are read only when they are ready, thus maintaining data integrity.","The vulnerability was fixed by introducing a validation check to ensure that the environment array is fully initialized before any read operations are attempted. This check prevents the code from proceeding if the end of the environment data is not set, thereby avoiding potential arithmetic underflows that could lead to reading beyond allocated memory. By ensuring that the environment variables are only accessed when they are ready, the fix effectively safeguards against inconsistent data retrieval and protects the integrity of the memory being accessed, thus enhancing the overall stability and security of the code."
18,180189,180189,,Local,Not required,Partial,CVE-2016-6156,https://www.cvedetails.com/cve/CVE-2016-6156/,CWE-362,Medium,,,,2016-08-06,1.9,"Race condition in the ec_device_ioctl_xcmd function in drivers/platform/chrome/cros_ec_dev.c in the Linux kernel before 4.7 allows local users to cause a denial of service (out-of-bounds array access) by changing a certain size value, aka a *double fetch* vulnerability.",2016-11-28,DoS ,7,https://github.com/torvalds/linux/commit/096cdc6f52225835ff503f987a0d68ef770bb78e,096cdc6f52225835ff503f987a0d68ef770bb78e,"platform/chrome: cros_ec_dev - double fetch bug in ioctl

We verify ""u_cmd.outsize"" and ""u_cmd.insize"" but we need to make sure
that those values have not changed between the two copy_from_user()
calls.  Otherwise it could lead to a buffer overflow.

Additionally, cros_ec_cmd_xfer() can set s_cmd->insize to a lower value.
We should use the new smaller value so we don't copy too much data to
the user.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Fixes: a841178445bb ('mfd: cros_ec: Use a zero-length array for command data')
Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
Reviewed-by: Kees Cook <keescook@chromium.org>
Tested-by: Gwendal Grignou <gwendal@chromium.org>
Cc: <stable@vger.kernel.org> # v4.2+
Signed-off-by: Olof Johansson <olof@lixom.net>",1,drivers/platform/chrome/cros_ec_dev.c,"{""sha"": ""8abd80dbcbed7974b4ace4265dd9dad1bca89edd"", ""filename"": ""drivers/platform/chrome/cros_ec_dev.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 1, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/096cdc6f52225835ff503f987a0d68ef770bb78e/drivers/platform/chrome/cros_ec_dev.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/096cdc6f52225835ff503f987a0d68ef770bb78e/drivers/platform/chrome/cros_ec_dev.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/platform/chrome/cros_ec_dev.c?ref=096cdc6f52225835ff503f987a0d68ef770bb78e"", ""patch"": ""@@ -151,13 +151,19 @@ static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n \t\tgoto exit;\n \t}\n \n+\tif (u_cmd.outsize != s_cmd->outsize ||\n+\t    u_cmd.insize != s_cmd->insize) {\n+\t\tret = -EINVAL;\n+\t\tgoto exit;\n+\t}\n+\n \ts_cmd->command += ec->cmd_offset;\n \tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n \t/* Only copy data to userland if data was received. */\n \tif (ret < 0)\n \t\tgoto exit;\n \n-\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))\n+\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n \t\tret = -EFAULT;\n exit:\n \tkfree(s_cmd);""}","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
",C,"	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
","	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
",,"@@ -151,13 +151,19 @@ static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
 		goto exit;
 	}
 
+	if (u_cmd.outsize != s_cmd->outsize ||
+	    u_cmd.insize != s_cmd->insize) {
+		ret = -EINVAL;
+		goto exit;
+	}
+
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
-	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
+	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);",linux,096cdc6f52225835ff503f987a0d68ef770bb78e,33688abb2802ff3a230bd2441f765477b94cc89e,1,"static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
//fix_flaw_line_below:
//	if (u_cmd.outsize != s_cmd->outsize ||
//fix_flaw_line_below:
//	    u_cmd.insize != s_cmd->insize) {
//fix_flaw_line_below:
//		ret = -EINVAL;
//fix_flaw_line_below:
//		goto exit;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
//flaw_line_below:
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
//fix_flaw_line_below:
//	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
",180189,"static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{
	long ret;
	struct cros_ec_command u_cmd;
	struct cros_ec_command *s_cmd;

	if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))
		return -EFAULT;

	if ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||
	    (u_cmd.insize > EC_MAX_MSG_BYTES))
		return -EINVAL;

	s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),
			GFP_KERNEL);
	if (!s_cmd)
		return -ENOMEM;

	if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {
		ret = -EFAULT;
 		goto exit;
 	}
 
	if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
		goto exit;
	}

 	s_cmd->command += ec->cmd_offset;
 	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 	/* Only copy data to userland if data was received. */
 	if (ret < 0)
 		goto exit;
 
	if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
 		ret = -EFAULT;
 exit:
 	kfree(s_cmd);
	return ret;
}
","The `ec_device_ioctl_xcmd` function facilitates communication between user space and a device managed by the Chrome OS Embedded Controller (EC). It begins by copying a command structure from user space and validating its size against predefined limits. If valid, it allocates memory for a command structure, copies the command data, and adjusts the command offset. The function then sends the command to the EC device and, if successful, copies any received data back to user space. It handles errors gracefully, ensuring proper memory management by freeing allocated resources before returning the result of the operation.","The provided code implements a function that facilitates communication between user space and a device managed by an embedded controller. It begins by retrieving a command structure from user space and validating its size against predefined limits. If the command is valid, it allocates memory for the command structure and copies the necessary data. The function then modifies the command before sending it to the embedded controller. Upon receiving a response, it copies the data back to user space if applicable, while also ensuring proper error handling and memory management throughout the process to prevent resource leaks.","The vulnerability in the original code stemmed from the lack of verification that the sizes of the output and input data remained consistent between two calls to copy data from user space. This oversight could lead to a buffer overflow if the user provided altered sizes, allowing for potential exploitation. The fix introduced an additional check after the second copy operation to ensure that the sizes of the output and input data matched the initially retrieved values. Furthermore, it adjusted the data copy to user space to use the potentially modified size returned by the command transfer function, enhancing overall safety and preventing overflow.","The vulnerability in the original code arose from the failure to ensure that the sizes of the output and input data remained unchanged between two separate data retrieval operations from user space. This oversight could lead to a buffer overflow, allowing an attacker to manipulate memory. The fix addressed this issue by adding a verification step to confirm that the sizes had not changed after the second data copy. Additionally, it ensured that the data copied back to user space utilized the potentially updated size from the command transfer, thereby preventing excessive data copying and enhancing overall security.","The vulnerability was fixed by introducing a check to ensure that the sizes of the output and input data, specifically `u_cmd.outsize` and `u_cmd.insize`, remained consistent after the second call to `copy_from_user()`. This prevents potential buffer overflow by verifying that the user-provided sizes have not changed. Additionally, the code was updated to use the potentially modified `s_cmd->insize` value returned by the `cros_ec_cmd_xfer()` function when copying data back to user space. This ensures that only the correct amount of data is copied, further mitigating the risk of overflow.","The vulnerability was addressed by adding a verification step to ensure that the sizes of the output and input data remained unchanged after the second data retrieval operation from user space. This check prevents potential buffer overflow by confirming that the user-provided sizes are consistent. Additionally, the fix involved adjusting the data copying process to utilize the updated size returned by the command transfer operation, ensuring that only the appropriate amount of data is copied back to user space. This combination of measures enhances the overall security and integrity of the data handling process."
19,180191,180191,,Local,Not required,,CVE-2016-6136,https://www.cvedetails.com/cve/CVE-2016-6136/,CWE-362,Medium,,Partial,,2016-08-06,1.9,"Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a *double fetch* vulnerability.",2018-01-04,Bypass ,93,https://github.com/torvalds/linux/commit/43761473c254b45883a64441dd0bc85a42f3645c,43761473c254b45883a64441dd0bc85a42f3645c,"audit: fix a double fetch in audit_log_single_execve_arg()

There is a double fetch problem in audit_log_single_execve_arg()
where we first check the execve(2) argumnets for any ""bad"" characters
which would require hex encoding and then re-fetch the arguments for
logging in the audit record[1].  Of course this leaves a window of
opportunity for an unsavory application to munge with the data.

This patch reworks things by only fetching the argument data once[2]
into a buffer where it is scanned and logged into the audit
records(s).  In addition to fixing the double fetch, this patch
improves on the original code in a few other ways: better handling
of large arguments which require encoding, stricter record length
checking, and some performance improvements (completely unverified,
but we got rid of some strlen() calls, that's got to be a good
thing).

As part of the development of this patch, I've also created a basic
regression test for the audit-testsuite, the test can be tracked on
GitHub at the following link:

 * https://github.com/linux-audit/audit-testsuite/issues/25

[1] If you pay careful attention, there is actually a triple fetch
problem due to a strnlen_user() call at the top of the function.

[2] This is a tiny white lie, we do make a call to strnlen_user()
prior to fetching the argument data.  I don't like it, but due to the
way the audit record is structured we really have no choice unless we
copy the entire argument at once (which would require a rather
wasteful allocation).  The good news is that with this patch the
kernel no longer relies on this strnlen_user() value for anything
beyond recording it in the log, we also update it with a trustworthy
value whenever possible.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Paul Moore <paul@paul-moore.com>",117,kernel/auditsc.c,"{""sha"": ""c65af21a12d6d2e7b56e99ed2fdd6d284a9ec43a"", ""filename"": ""kernel/auditsc.c"", ""status"": ""modified"", ""additions"": 164, ""deletions"": 168, ""changes"": 332, ""blob_url"": ""https://github.com/torvalds/linux/blob/43761473c254b45883a64441dd0bc85a42f3645c/kernel/auditsc.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/43761473c254b45883a64441dd0bc85a42f3645c/kernel/auditsc.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/kernel/auditsc.c?ref=43761473c254b45883a64441dd0bc85a42f3645c"", ""patch"": ""@@ -73,6 +73,7 @@\n #include <linux/compat.h>\n #include <linux/ctype.h>\n #include <linux/string.h>\n+#include <linux/uaccess.h>\n #include <uapi/linux/limits.h>\n \n #include \""audit.h\""\n@@ -82,7 +83,8 @@\n #define AUDITSC_SUCCESS 1\n #define AUDITSC_FAILURE 2\n \n-/* no execve audit message should be longer than this (userspace limits) */\n+/* no execve audit message should be longer than this (userspace limits),\n+ * see the note near the top of audit_log_execve_info() about this value */\n #define MAX_EXECVE_AUDIT_LEN 7500\n \n /* max length to print of cmdline/proctitle value during audit */\n@@ -992,184 +994,178 @@ static int audit_log_pid_context(struct audit_context *context, pid_t pid,\n \treturn rc;\n }\n \n-/*\n- * to_send and len_sent accounting are very loose estimates.  We aren't\n- * really worried about a hard cap to MAX_EXECVE_AUDIT_LEN so much as being\n- * within about 500 bytes (next page boundary)\n- *\n- * why snprintf?  an int is up to 12 digits long.  if we just assumed when\n- * logging that a[%d]= was going to be 16 characters long we would be wasting\n- * space in every audit message.  In one 7500 byte message we can log up to\n- * about 1000 min size arguments.  That comes down to about 50% waste of space\n- * if we didn't do the snprintf to find out how long arg_num_len was.\n- */\n-static int audit_log_single_execve_arg(struct audit_context *context,\n-\t\t\t\t\tstruct audit_buffer **ab,\n-\t\t\t\t\tint arg_num,\n-\t\t\t\t\tsize_t *len_sent,\n-\t\t\t\t\tconst char __user *p,\n-\t\t\t\t\tchar *buf)\n+static void audit_log_execve_info(struct audit_context *context,\n+\t\t\t\t  struct audit_buffer **ab)\n {\n-\tchar arg_num_len_buf[12];\n-\tconst char __user *tmp_p = p;\n-\t/* how many digits are in arg_num? 5 is the length of ' a=\""\""' */\n-\tsize_t arg_num_len = snprintf(arg_num_len_buf, 12, \""%d\"", arg_num) + 5;\n-\tsize_t len, len_left, to_send;\n-\tsize_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;\n-\tunsigned int i, has_cntl = 0, too_long = 0;\n-\tint ret;\n-\n-\t/* strnlen_user includes the null we don't want to send */\n-\tlen_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n-\n-\t/*\n-\t * We just created this mm, if we can't find the strings\n-\t * we just copied into it something is _very_ wrong. Similar\n-\t * for strings that are too long, we should not have created\n-\t * any.\n-\t */\n-\tif (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {\n-\t\tsend_sig(SIGKILL, current, 0);\n-\t\treturn -1;\n+\tlong len_max;\n+\tlong len_rem;\n+\tlong len_full;\n+\tlong len_buf;\n+\tlong len_abuf;\n+\tlong len_tmp;\n+\tbool require_data;\n+\tbool encode;\n+\tunsigned int iter;\n+\tunsigned int arg;\n+\tchar *buf_head;\n+\tchar *buf;\n+\tconst char __user *p = (const char __user *)current->mm->arg_start;\n+\n+\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\n+\t *       data we put in the audit record for this argument (see the\n+\t *       code below) ... at this point in time 96 is plenty */\n+\tchar abuf[96];\n+\n+\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\n+\t *       current value of 7500 is not as important as the fact that it\n+\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\n+\t *       room if we go over a little bit in the logging below */\n+\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\n+\tlen_max = MAX_EXECVE_AUDIT_LEN;\n+\n+\t/* scratch buffer to hold the userspace args */\n+\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n+\tif (!buf_head) {\n+\t\taudit_panic(\""out of memory for argv string\"");\n+\t\treturn;\n \t}\n+\tbuf = buf_head;\n \n-\t/* walk the whole argument looking for non-ascii chars */\n+\taudit_log_format(*ab, \""argc=%d\"", context->execve.argc);\n+\n+\tlen_rem = len_max;\n+\tlen_buf = 0;\n+\tlen_full = 0;\n+\trequire_data = true;\n+\tencode = false;\n+\titer = 0;\n+\targ = 0;\n \tdo {\n-\t\tif (len_left > MAX_EXECVE_AUDIT_LEN)\n-\t\t\tto_send = MAX_EXECVE_AUDIT_LEN;\n-\t\telse\n-\t\t\tto_send = len_left;\n-\t\tret = copy_from_user(buf, tmp_p, to_send);\n-\t\t/*\n-\t\t * There is no reason for this copy to be short. We just\n-\t\t * copied them here, and the mm hasn't been exposed to user-\n-\t\t * space yet.\n-\t\t */\n-\t\tif (ret) {\n-\t\t\tWARN_ON(1);\n-\t\t\tsend_sig(SIGKILL, current, 0);\n-\t\t\treturn -1;\n-\t\t}\n-\t\tbuf[to_send] = '\\0';\n-\t\thas_cntl = audit_string_contains_control(buf, to_send);\n-\t\tif (has_cntl) {\n-\t\t\t/*\n-\t\t\t * hex messages get logged as 2 bytes, so we can only\n-\t\t\t * send half as much in each message\n-\t\t\t */\n-\t\t\tmax_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;\n-\t\t\tbreak;\n-\t\t}\n-\t\tlen_left -= to_send;\n-\t\ttmp_p += to_send;\n-\t} while (len_left > 0);\n-\n-\tlen_left = len;\n-\n-\tif (len > max_execve_audit_len)\n-\t\ttoo_long = 1;\n-\n-\t/* rewalk the argument actually logging the message */\n-\tfor (i = 0; len_left > 0; i++) {\n-\t\tint room_left;\n-\n-\t\tif (len_left > max_execve_audit_len)\n-\t\t\tto_send = max_execve_audit_len;\n-\t\telse\n-\t\t\tto_send = len_left;\n-\n-\t\t/* do we have space left to send this argument in this ab? */\n-\t\troom_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;\n-\t\tif (has_cntl)\n-\t\t\troom_left -= (to_send * 2);\n-\t\telse\n-\t\t\troom_left -= to_send;\n-\t\tif (room_left < 0) {\n-\t\t\t*len_sent = 0;\n-\t\t\taudit_log_end(*ab);\n-\t\t\t*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);\n-\t\t\tif (!*ab)\n-\t\t\t\treturn 0;\n-\t\t}\n+\t\t/* NOTE: we don't ever want to trust this value for anything\n+\t\t *       serious, but the audit record format insists we\n+\t\t *       provide an argument length for really long arguments,\n+\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\n+\t\t *       to use strncpy_from_user() to obtain this value for\n+\t\t *       recording in the log, although we don't use it\n+\t\t *       anywhere here to avoid a double-fetch problem */\n+\t\tif (len_full == 0)\n+\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n+\n+\t\t/* read more data from userspace */\n+\t\tif (require_data) {\n+\t\t\t/* can we make more room in the buffer? */\n+\t\t\tif (buf != buf_head) {\n+\t\t\t\tmemmove(buf_head, buf, len_buf);\n+\t\t\t\tbuf = buf_head;\n+\t\t\t}\n+\n+\t\t\t/* fetch as much as we can of the argument */\n+\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\n+\t\t\t\t\t\t    len_max - len_buf);\n+\t\t\tif (len_tmp == -EFAULT) {\n+\t\t\t\t/* unable to copy from userspace */\n+\t\t\t\tsend_sig(SIGKILL, current, 0);\n+\t\t\t\tgoto out;\n+\t\t\t} else if (len_tmp == (len_max - len_buf)) {\n+\t\t\t\t/* buffer is not large enough */\n+\t\t\t\trequire_data = true;\n+\t\t\t\t/* NOTE: if we are going to span multiple\n+\t\t\t\t *       buffers force the encoding so we stand\n+\t\t\t\t *       a chance at a sane len_full value and\n+\t\t\t\t *       consistent record encoding */\n+\t\t\t\tencode = true;\n+\t\t\t\tlen_full = len_full * 2;\n+\t\t\t\tp += len_tmp;\n+\t\t\t} else {\n+\t\t\t\trequire_data = false;\n+\t\t\t\tif (!encode)\n+\t\t\t\t\tencode = audit_string_contains_control(\n+\t\t\t\t\t\t\t\tbuf, len_tmp);\n+\t\t\t\t/* try to use a trusted value for len_full */\n+\t\t\t\tif (len_full < len_max)\n+\t\t\t\t\tlen_full = (encode ?\n+\t\t\t\t\t\t    len_tmp * 2 : len_tmp);\n+\t\t\t\tp += len_tmp + 1;\n+\t\t\t}\n+\t\t\tlen_buf += len_tmp;\n+\t\t\tbuf_head[len_buf] = '\\0';\n \n-\t\t/*\n-\t\t * first record needs to say how long the original string was\n-\t\t * so we can be sure nothing was lost.\n-\t\t */\n-\t\tif ((i == 0) && (too_long))\n-\t\t\taudit_log_format(*ab, \"" a%d_len=%zu\"", arg_num,\n-\t\t\t\t\t has_cntl ? 2*len : len);\n-\n-\t\t/*\n-\t\t * normally arguments are small enough to fit and we already\n-\t\t * filled buf above when we checked for control characters\n-\t\t * so don't bother with another copy_from_user\n-\t\t */\n-\t\tif (len >= max_execve_audit_len)\n-\t\t\tret = copy_from_user(buf, p, to_send);\n-\t\telse\n-\t\t\tret = 0;\n-\t\tif (ret) {\n-\t\t\tWARN_ON(1);\n-\t\t\tsend_sig(SIGKILL, current, 0);\n-\t\t\treturn -1;\n+\t\t\t/* length of the buffer in the audit record? */\n+\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\n \t\t}\n-\t\tbuf[to_send] = '\\0';\n-\n-\t\t/* actually log it */\n-\t\taudit_log_format(*ab, \"" a%d\"", arg_num);\n-\t\tif (too_long)\n-\t\t\taudit_log_format(*ab, \""[%d]\"", i);\n-\t\taudit_log_format(*ab, \""=\"");\n-\t\tif (has_cntl)\n-\t\t\taudit_log_n_hex(*ab, buf, to_send);\n-\t\telse\n-\t\t\taudit_log_string(*ab, buf);\n-\n-\t\tp += to_send;\n-\t\tlen_left -= to_send;\n-\t\t*len_sent += arg_num_len;\n-\t\tif (has_cntl)\n-\t\t\t*len_sent += to_send * 2;\n-\t\telse\n-\t\t\t*len_sent += to_send;\n-\t}\n-\t/* include the null we didn't log */\n-\treturn len + 1;\n-}\n \n-static void audit_log_execve_info(struct audit_context *context,\n-\t\t\t\t  struct audit_buffer **ab)\n-{\n-\tint i, len;\n-\tsize_t len_sent = 0;\n-\tconst char __user *p;\n-\tchar *buf;\n+\t\t/* write as much as we can to the audit log */\n+\t\tif (len_buf > 0) {\n+\t\t\t/* NOTE: some magic numbers here - basically if we\n+\t\t\t *       can't fit a reasonable amount of data into the\n+\t\t\t *       existing audit buffer, flush it and start with\n+\t\t\t *       a new buffer */\n+\t\t\tif ((sizeof(abuf) + 8) > len_rem) {\n+\t\t\t\tlen_rem = len_max;\n+\t\t\t\taudit_log_end(*ab);\n+\t\t\t\t*ab = audit_log_start(context,\n+\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);\n+\t\t\t\tif (!*ab)\n+\t\t\t\t\tgoto out;\n+\t\t\t}\n \n-\tp = (const char __user *)current->mm->arg_start;\n+\t\t\t/* create the non-arg portion of the arg record */\n+\t\t\tlen_tmp = 0;\n+\t\t\tif (require_data || (iter > 0) ||\n+\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\n+\t\t\t\tif (iter == 0) {\n+\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t\t\"" a%d_len=%lu\"",\n+\t\t\t\t\t\t\targ, len_full);\n+\t\t\t\t}\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \"" a%d[%d]=\"", arg, iter++);\n+\t\t\t} else\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \"" a%d=\"", arg);\n+\t\t\tWARN_ON(len_tmp >= sizeof(abuf));\n+\t\t\tabuf[sizeof(abuf) - 1] = '\\0';\n+\n+\t\t\t/* log the arg in the audit record */\n+\t\t\taudit_log_format(*ab, \""%s\"", abuf);\n+\t\t\tlen_rem -= len_tmp;\n+\t\t\tlen_tmp = len_buf;\n+\t\t\tif (encode) {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */\n+\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp * 2;\n+\t\t\t\tlen_abuf -= len_tmp * 2;\n+\t\t\t} else {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */\n+\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp + 2;\n+\t\t\t\t/* don't subtract the \""2\"" because we still need\n+\t\t\t\t * to add quotes to the remaining string */\n+\t\t\t\tlen_abuf -= len_tmp;\n+\t\t\t}\n+\t\t\tlen_buf -= len_tmp;\n+\t\t\tbuf += len_tmp;\n+\t\t}\n \n-\taudit_log_format(*ab, \""argc=%d\"", context->execve.argc);\n+\t\t/* ready to move to the next argument? */\n+\t\tif ((len_buf == 0) && !require_data) {\n+\t\t\targ++;\n+\t\t\titer = 0;\n+\t\t\tlen_full = 0;\n+\t\t\trequire_data = true;\n+\t\t\tencode = false;\n+\t\t}\n+\t} while (arg < context->execve.argc);\n \n-\t/*\n-\t * we need some kernel buffer to hold the userspace args.  Just\n-\t * allocate one big one rather than allocating one of the right size\n-\t * for every single argument inside audit_log_single_execve_arg()\n-\t * should be <8k allocation so should be pretty safe.\n-\t */\n-\tbuf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n-\tif (!buf) {\n-\t\taudit_panic(\""out of memory for argv string\"");\n-\t\treturn;\n-\t}\n+\t/* NOTE: the caller handles the final audit_log_end() call */\n \n-\tfor (i = 0; i < context->execve.argc; i++) {\n-\t\tlen = audit_log_single_execve_arg(context, ab, i,\n-\t\t\t\t\t\t  &len_sent, p, buf);\n-\t\tif (len <= 0)\n-\t\t\tbreak;\n-\t\tp += len;\n-\t}\n-\tkfree(buf);\n+out:\n+\tkfree(buf_head);\n }\n \n static void show_special(struct audit_context *context, int *call_panic)""}","static int audit_log_single_execve_arg(struct audit_context *context,
static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
 {
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
 	}
	buf = buf_head;
 
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
 	do {
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
 
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
","static int audit_log_single_execve_arg(struct audit_context *context,
					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
 {
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
 	}
 
	/* walk the whole argument looking for non-ascii chars */
 	do {
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
 
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
 		}
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
",C,"static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
	buf = buf_head;
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
","					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
	/* walk the whole argument looking for non-ascii chars */
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
",,"@@ -73,6 +73,7 @@
 #include <linux/compat.h>
 #include <linux/ctype.h>
 #include <linux/string.h>
+#include <linux/uaccess.h>
 #include <uapi/linux/limits.h>
 
 #include ""audit.h""
@@ -82,7 +83,8 @@
 #define AUDITSC_SUCCESS 1
 #define AUDITSC_FAILURE 2
 
-/* no execve audit message should be longer than this (userspace limits) */
+/* no execve audit message should be longer than this (userspace limits),
+ * see the note near the top of audit_log_execve_info() about this value */
 #define MAX_EXECVE_AUDIT_LEN 7500
 
 /* max length to print of cmdline/proctitle value during audit */
@@ -992,184 +994,178 @@ static int audit_log_pid_context(struct audit_context *context, pid_t pid,
 	return rc;
 }
 
-/*
- * to_send and len_sent accounting are very loose estimates.  We aren't
- * really worried about a hard cap to MAX_EXECVE_AUDIT_LEN so much as being
- * within about 500 bytes (next page boundary)
- *
- * why snprintf?  an int is up to 12 digits long.  if we just assumed when
- * logging that a[%d]= was going to be 16 characters long we would be wasting
- * space in every audit message.  In one 7500 byte message we can log up to
- * about 1000 min size arguments.  That comes down to about 50% waste of space
- * if we didn't do the snprintf to find out how long arg_num_len was.
- */
-static int audit_log_single_execve_arg(struct audit_context *context,
-					struct audit_buffer **ab,
-					int arg_num,
-					size_t *len_sent,
-					const char __user *p,
-					char *buf)
+static void audit_log_execve_info(struct audit_context *context,
+				  struct audit_buffer **ab)
 {
-	char arg_num_len_buf[12];
-	const char __user *tmp_p = p;
-	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
-	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
-	size_t len, len_left, to_send;
-	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
-	unsigned int i, has_cntl = 0, too_long = 0;
-	int ret;
-
-	/* strnlen_user includes the null we don't want to send */
-	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
-
-	/*
-	 * We just created this mm, if we can't find the strings
-	 * we just copied into it something is _very_ wrong. Similar
-	 * for strings that are too long, we should not have created
-	 * any.
-	 */
-	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
-		send_sig(SIGKILL, current, 0);
-		return -1;
+	long len_max;
+	long len_rem;
+	long len_full;
+	long len_buf;
+	long len_abuf;
+	long len_tmp;
+	bool require_data;
+	bool encode;
+	unsigned int iter;
+	unsigned int arg;
+	char *buf_head;
+	char *buf;
+	const char __user *p = (const char __user *)current->mm->arg_start;
+
+	/* NOTE: this buffer needs to be large enough to hold all the non-arg
+	 *       data we put in the audit record for this argument (see the
+	 *       code below) ... at this point in time 96 is plenty */
+	char abuf[96];
+
+	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
+	 *       current value of 7500 is not as important as the fact that it
+	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
+	 *       room if we go over a little bit in the logging below */
+	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
+	len_max = MAX_EXECVE_AUDIT_LEN;
+
+	/* scratch buffer to hold the userspace args */
+	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
+	if (!buf_head) {
+		audit_panic(""out of memory for argv string"");
+		return;
 	}
+	buf = buf_head;
 
-	/* walk the whole argument looking for non-ascii chars */
+	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
+
+	len_rem = len_max;
+	len_buf = 0;
+	len_full = 0;
+	require_data = true;
+	encode = false;
+	iter = 0;
+	arg = 0;
 	do {
-		if (len_left > MAX_EXECVE_AUDIT_LEN)
-			to_send = MAX_EXECVE_AUDIT_LEN;
-		else
-			to_send = len_left;
-		ret = copy_from_user(buf, tmp_p, to_send);
-		/*
-		 * There is no reason for this copy to be short. We just
-		 * copied them here, and the mm hasn't been exposed to user-
-		 * space yet.
-		 */
-		if (ret) {
-			WARN_ON(1);
-			send_sig(SIGKILL, current, 0);
-			return -1;
-		}
-		buf[to_send] = '\0';
-		has_cntl = audit_string_contains_control(buf, to_send);
-		if (has_cntl) {
-			/*
-			 * hex messages get logged as 2 bytes, so we can only
-			 * send half as much in each message
-			 */
-			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
-			break;
-		}
-		len_left -= to_send;
-		tmp_p += to_send;
-	} while (len_left > 0);
-
-	len_left = len;
-
-	if (len > max_execve_audit_len)
-		too_long = 1;
-
-	/* rewalk the argument actually logging the message */
-	for (i = 0; len_left > 0; i++) {
-		int room_left;
-
-		if (len_left > max_execve_audit_len)
-			to_send = max_execve_audit_len;
-		else
-			to_send = len_left;
-
-		/* do we have space left to send this argument in this ab? */
-		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
-		if (has_cntl)
-			room_left -= (to_send * 2);
-		else
-			room_left -= to_send;
-		if (room_left < 0) {
-			*len_sent = 0;
-			audit_log_end(*ab);
-			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
-			if (!*ab)
-				return 0;
-		}
+		/* NOTE: we don't ever want to trust this value for anything
+		 *       serious, but the audit record format insists we
+		 *       provide an argument length for really long arguments,
+		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
+		 *       to use strncpy_from_user() to obtain this value for
+		 *       recording in the log, although we don't use it
+		 *       anywhere here to avoid a double-fetch problem */
+		if (len_full == 0)
+			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;
+
+		/* read more data from userspace */
+		if (require_data) {
+			/* can we make more room in the buffer? */
+			if (buf != buf_head) {
+				memmove(buf_head, buf, len_buf);
+				buf = buf_head;
+			}
+
+			/* fetch as much as we can of the argument */
+			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
+						    len_max - len_buf);
+			if (len_tmp == -EFAULT) {
+				/* unable to copy from userspace */
+				send_sig(SIGKILL, current, 0);
+				goto out;
+			} else if (len_tmp == (len_max - len_buf)) {
+				/* buffer is not large enough */
+				require_data = true;
+				/* NOTE: if we are going to span multiple
+				 *       buffers force the encoding so we stand
+				 *       a chance at a sane len_full value and
+				 *       consistent record encoding */
+				encode = true;
+				len_full = len_full * 2;
+				p += len_tmp;
+			} else {
+				require_data = false;
+				if (!encode)
+					encode = audit_string_contains_control(
+								buf, len_tmp);
+				/* try to use a trusted value for len_full */
+				if (len_full < len_max)
+					len_full = (encode ?
+						    len_tmp * 2 : len_tmp);
+				p += len_tmp + 1;
+			}
+			len_buf += len_tmp;
+			buf_head[len_buf] = '\0';
 
-		/*
-		 * first record needs to say how long the original string was
-		 * so we can be sure nothing was lost.
-		 */
-		if ((i == 0) && (too_long))
-			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
-					 has_cntl ? 2*len : len);
-
-		/*
-		 * normally arguments are small enough to fit and we already
-		 * filled buf above when we checked for control characters
-		 * so don't bother with another copy_from_user
-		 */
-		if (len >= max_execve_audit_len)
-			ret = copy_from_user(buf, p, to_send);
-		else
-			ret = 0;
-		if (ret) {
-			WARN_ON(1);
-			send_sig(SIGKILL, current, 0);
-			return -1;
+			/* length of the buffer in the audit record? */
+			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
-		buf[to_send] = '\0';
-
-		/* actually log it */
-		audit_log_format(*ab, "" a%d"", arg_num);
-		if (too_long)
-			audit_log_format(*ab, ""[%d]"", i);
-		audit_log_format(*ab, ""="");
-		if (has_cntl)
-			audit_log_n_hex(*ab, buf, to_send);
-		else
-			audit_log_string(*ab, buf);
-
-		p += to_send;
-		len_left -= to_send;
-		*len_sent += arg_num_len;
-		if (has_cntl)
-			*len_sent += to_send * 2;
-		else
-			*len_sent += to_send;
-	}
-	/* include the null we didn't log */
-	return len + 1;
-}
 
-static void audit_log_execve_info(struct audit_context *context,
-				  struct audit_buffer **ab)
-{
-	int i, len;
-	size_t len_sent = 0;
-	const char __user *p;
-	char *buf;
+		/* write as much as we can to the audit log */
+		if (len_buf > 0) {
+			/* NOTE: some magic numbers here - basically if we
+			 *       can't fit a reasonable amount of data into the
+			 *       existing audit buffer, flush it and start with
+			 *       a new buffer */
+			if ((sizeof(abuf) + 8) > len_rem) {
+				len_rem = len_max;
+				audit_log_end(*ab);
+				*ab = audit_log_start(context,
+						      GFP_KERNEL, AUDIT_EXECVE);
+				if (!*ab)
+					goto out;
+			}
 
-	p = (const char __user *)current->mm->arg_start;
+			/* create the non-arg portion of the arg record */
+			len_tmp = 0;
+			if (require_data || (iter > 0) ||
+			    ((len_abuf + sizeof(abuf)) > len_rem)) {
+				if (iter == 0) {
+					len_tmp += snprintf(&abuf[len_tmp],
+							sizeof(abuf) - len_tmp,
+							"" a%d_len=%lu"",
+							arg, len_full);
+				}
+				len_tmp += snprintf(&abuf[len_tmp],
+						    sizeof(abuf) - len_tmp,
+						    "" a%d[%d]="", arg, iter++);
+			} else
+				len_tmp += snprintf(&abuf[len_tmp],
+						    sizeof(abuf) - len_tmp,
+						    "" a%d="", arg);
+			WARN_ON(len_tmp >= sizeof(abuf));
+			abuf[sizeof(abuf) - 1] = '\0';
+
+			/* log the arg in the audit record */
+			audit_log_format(*ab, ""%s"", abuf);
+			len_rem -= len_tmp;
+			len_tmp = len_buf;
+			if (encode) {
+				if (len_abuf > len_rem)
+					len_tmp = len_rem / 2; /* encoding */
+				audit_log_n_hex(*ab, buf, len_tmp);
+				len_rem -= len_tmp * 2;
+				len_abuf -= len_tmp * 2;
+			} else {
+				if (len_abuf > len_rem)
+					len_tmp = len_rem - 2; /* quotes */
+				audit_log_n_string(*ab, buf, len_tmp);
+				len_rem -= len_tmp + 2;
+				/* don't subtract the ""2"" because we still need
+				 * to add quotes to the remaining string */
+				len_abuf -= len_tmp;
+			}
+			len_buf -= len_tmp;
+			buf += len_tmp;
+		}
 
-	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
+		/* ready to move to the next argument? */
+		if ((len_buf == 0) && !require_data) {
+			arg++;
+			iter = 0;
+			len_full = 0;
+			require_data = true;
+			encode = false;
+		}
+	} while (arg < context->execve.argc);
 
-	/*
-	 * we need some kernel buffer to hold the userspace args.  Just
-	 * allocate one big one rather than allocating one of the right size
-	 * for every single argument inside audit_log_single_execve_arg()
-	 * should be <8k allocation so should be pretty safe.
-	 */
-	buf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
-	if (!buf) {
-		audit_panic(""out of memory for argv string"");
-		return;
-	}
+	/* NOTE: the caller handles the final audit_log_end() call */
 
-	for (i = 0; i < context->execve.argc; i++) {
-		len = audit_log_single_execve_arg(context, ab, i,
-						  &len_sent, p, buf);
-		if (len <= 0)
-			break;
-		p += len;
-	}
-	kfree(buf);
+out:
+	kfree(buf_head);
 }
 
 static void show_special(struct audit_context *context, int *call_panic)",linux,43761473c254b45883a64441dd0bc85a42f3645c,0b7a0fdb29715e38641beb39db4d01695b22b5aa,1,"static int audit_log_single_execve_arg(struct audit_context *context,
//flaw_line_below:
					struct audit_buffer **ab,
//flaw_line_below:
					int arg_num,
//flaw_line_below:
					size_t *len_sent,
//flaw_line_below:
					const char __user *p,
//flaw_line_below:
					char *buf)
//fix_flaw_line_below:
//static void audit_log_execve_info(struct audit_context *context,
//fix_flaw_line_below:
//				  struct audit_buffer **ab)
 {
//flaw_line_below:
	char arg_num_len_buf[12];
//flaw_line_below:
	const char __user *tmp_p = p;
//flaw_line_below:
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
//flaw_line_below:
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
//flaw_line_below:
	size_t len, len_left, to_send;
//flaw_line_below:
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
//flaw_line_below:
	unsigned int i, has_cntl = 0, too_long = 0;
//flaw_line_below:
	int ret;
//flaw_line_below:

//flaw_line_below:
	/* strnlen_user includes the null we don't want to send */
//flaw_line_below:
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
//flaw_line_below:

//flaw_line_below:
	/*
//flaw_line_below:
	 * We just created this mm, if we can't find the strings
//flaw_line_below:
	 * we just copied into it something is _very_ wrong. Similar
//flaw_line_below:
	 * for strings that are too long, we should not have created
//flaw_line_below:
	 * any.
//flaw_line_below:
	 */
//flaw_line_below:
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
//flaw_line_below:
		send_sig(SIGKILL, current, 0);
//flaw_line_below:
		return -1;
//fix_flaw_line_below:
//	long len_max;
//fix_flaw_line_below:
//	long len_rem;
//fix_flaw_line_below:
//	long len_full;
//fix_flaw_line_below:
//	long len_buf;
//fix_flaw_line_below:
//	long len_abuf;
//fix_flaw_line_below:
//	long len_tmp;
//fix_flaw_line_below:
//	bool require_data;
//fix_flaw_line_below:
//	bool encode;
//fix_flaw_line_below:
//	unsigned int iter;
//fix_flaw_line_below:
//	unsigned int arg;
//fix_flaw_line_below:
//	char *buf_head;
//fix_flaw_line_below:
//	char *buf;
//fix_flaw_line_below:
//	const char __user *p = (const char __user *)current->mm->arg_start;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* NOTE: this buffer needs to be large enough to hold all the non-arg
//fix_flaw_line_below:
//	 *       data we put in the audit record for this argument (see the
//fix_flaw_line_below:
//	 *       code below) ... at this point in time 96 is plenty */
//fix_flaw_line_below:
//	char abuf[96];
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
//fix_flaw_line_below:
//	 *       current value of 7500 is not as important as the fact that it
//fix_flaw_line_below:
//	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
//fix_flaw_line_below:
//	 *       room if we go over a little bit in the logging below */
//fix_flaw_line_below:
//	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
//fix_flaw_line_below:
//	len_max = MAX_EXECVE_AUDIT_LEN;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	/* scratch buffer to hold the userspace args */
//fix_flaw_line_below:
//	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
//fix_flaw_line_below:
//	if (!buf_head) {
//fix_flaw_line_below:
//		audit_panic(""out of memory for argv string"");
//fix_flaw_line_below:
//		return;
 	}
//fix_flaw_line_below:
//	buf = buf_head;
 
//flaw_line_below:
	/* walk the whole argument looking for non-ascii chars */
//fix_flaw_line_below:
//	audit_log_format(*ab, ""argc=%d"", context->execve.argc);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	len_rem = len_max;
//fix_flaw_line_below:
//	len_buf = 0;
//fix_flaw_line_below:
//	len_full = 0;
//fix_flaw_line_below:
//	require_data = true;
//fix_flaw_line_below:
//	encode = false;
//fix_flaw_line_below:
//	iter = 0;
//fix_flaw_line_below:
//	arg = 0;
 	do {
//flaw_line_below:
		if (len_left > MAX_EXECVE_AUDIT_LEN)
//flaw_line_below:
			to_send = MAX_EXECVE_AUDIT_LEN;
//flaw_line_below:
		else
//flaw_line_below:
			to_send = len_left;
//flaw_line_below:
		ret = copy_from_user(buf, tmp_p, to_send);
//flaw_line_below:
		/*
//flaw_line_below:
		 * There is no reason for this copy to be short. We just
//flaw_line_below:
		 * copied them here, and the mm hasn't been exposed to user-
//flaw_line_below:
		 * space yet.
//flaw_line_below:
		 */
//flaw_line_below:
		if (ret) {
//flaw_line_below:
			WARN_ON(1);
//flaw_line_below:
			send_sig(SIGKILL, current, 0);
//flaw_line_below:
			return -1;
//flaw_line_below:
		}
//flaw_line_below:
		buf[to_send] = '\0';
//flaw_line_below:
		has_cntl = audit_string_contains_control(buf, to_send);
//flaw_line_below:
		if (has_cntl) {
//flaw_line_below:
			/*
//flaw_line_below:
			 * hex messages get logged as 2 bytes, so we can only
//flaw_line_below:
			 * send half as much in each message
//flaw_line_below:
			 */
//flaw_line_below:
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
//flaw_line_below:
			break;
//flaw_line_below:
		}
//flaw_line_below:
		len_left -= to_send;
//flaw_line_below:
		tmp_p += to_send;
//flaw_line_below:
	} while (len_left > 0);
//flaw_line_below:

//flaw_line_below:
	len_left = len;
//flaw_line_below:

//flaw_line_below:
	if (len > max_execve_audit_len)
//flaw_line_below:
		too_long = 1;
//flaw_line_below:

//flaw_line_below:
	/* rewalk the argument actually logging the message */
//flaw_line_below:
	for (i = 0; len_left > 0; i++) {
//flaw_line_below:
		int room_left;
//flaw_line_below:

//flaw_line_below:
		if (len_left > max_execve_audit_len)
//flaw_line_below:
			to_send = max_execve_audit_len;
//flaw_line_below:
		else
//flaw_line_below:
			to_send = len_left;
//flaw_line_below:

//flaw_line_below:
		/* do we have space left to send this argument in this ab? */
//flaw_line_below:
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			room_left -= (to_send * 2);
//flaw_line_below:
		else
//flaw_line_below:
			room_left -= to_send;
//flaw_line_below:
		if (room_left < 0) {
//flaw_line_below:
			*len_sent = 0;
//flaw_line_below:
			audit_log_end(*ab);
//flaw_line_below:
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
//flaw_line_below:
			if (!*ab)
//flaw_line_below:
				return 0;
//flaw_line_below:
		}
//fix_flaw_line_below:
//		/* NOTE: we don't ever want to trust this value for anything
//fix_flaw_line_below:
//		 *       serious, but the audit record format insists we
//fix_flaw_line_below:
//		 *       provide an argument length for really long arguments,
//fix_flaw_line_below:
//		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
//fix_flaw_line_below:
//		 *       to use strncpy_from_user() to obtain this value for
//fix_flaw_line_below:
//		 *       recording in the log, although we don't use it
//fix_flaw_line_below:
//		 *       anywhere here to avoid a double-fetch problem */
//fix_flaw_line_below:
//		if (len_full == 0)
//fix_flaw_line_below:
//			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/* read more data from userspace */
//fix_flaw_line_below:
//		if (require_data) {
//fix_flaw_line_below:
//			/* can we make more room in the buffer? */
//fix_flaw_line_below:
//			if (buf != buf_head) {
//fix_flaw_line_below:
//				memmove(buf_head, buf, len_buf);
//fix_flaw_line_below:
//				buf = buf_head;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//			/* fetch as much as we can of the argument */
//fix_flaw_line_below:
//			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
//fix_flaw_line_below:
//						    len_max - len_buf);
//fix_flaw_line_below:
//			if (len_tmp == -EFAULT) {
//fix_flaw_line_below:
//				/* unable to copy from userspace */
//fix_flaw_line_below:
//				send_sig(SIGKILL, current, 0);
//fix_flaw_line_below:
//				goto out;
//fix_flaw_line_below:
//			} else if (len_tmp == (len_max - len_buf)) {
//fix_flaw_line_below:
//				/* buffer is not large enough */
//fix_flaw_line_below:
//				require_data = true;
//fix_flaw_line_below:
//				/* NOTE: if we are going to span multiple
//fix_flaw_line_below:
//				 *       buffers force the encoding so we stand
//fix_flaw_line_below:
//				 *       a chance at a sane len_full value and
//fix_flaw_line_below:
//				 *       consistent record encoding */
//fix_flaw_line_below:
//				encode = true;
//fix_flaw_line_below:
//				len_full = len_full * 2;
//fix_flaw_line_below:
//				p += len_tmp;
//fix_flaw_line_below:
//			} else {
//fix_flaw_line_below:
//				require_data = false;
//fix_flaw_line_below:
//				if (!encode)
//fix_flaw_line_below:
//					encode = audit_string_contains_control(
//fix_flaw_line_below:
//								buf, len_tmp);
//fix_flaw_line_below:
//				/* try to use a trusted value for len_full */
//fix_flaw_line_below:
//				if (len_full < len_max)
//fix_flaw_line_below:
//					len_full = (encode ?
//fix_flaw_line_below:
//						    len_tmp * 2 : len_tmp);
//fix_flaw_line_below:
//				p += len_tmp + 1;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//			len_buf += len_tmp;
//fix_flaw_line_below:
//			buf_head[len_buf] = '\0';
 
//flaw_line_below:
		/*
//flaw_line_below:
		 * first record needs to say how long the original string was
//flaw_line_below:
		 * so we can be sure nothing was lost.
//flaw_line_below:
		 */
//flaw_line_below:
		if ((i == 0) && (too_long))
//flaw_line_below:
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
//flaw_line_below:
					 has_cntl ? 2*len : len);
//flaw_line_below:

//flaw_line_below:
		/*
//flaw_line_below:
		 * normally arguments are small enough to fit and we already
//flaw_line_below:
		 * filled buf above when we checked for control characters
//flaw_line_below:
		 * so don't bother with another copy_from_user
//flaw_line_below:
		 */
//flaw_line_below:
		if (len >= max_execve_audit_len)
//flaw_line_below:
			ret = copy_from_user(buf, p, to_send);
//flaw_line_below:
		else
//flaw_line_below:
			ret = 0;
//flaw_line_below:
		if (ret) {
//flaw_line_below:
			WARN_ON(1);
//flaw_line_below:
			send_sig(SIGKILL, current, 0);
//flaw_line_below:
			return -1;
//fix_flaw_line_below:
//			/* length of the buffer in the audit record? */
//fix_flaw_line_below:
//			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
//flaw_line_below:
		buf[to_send] = '\0';
//flaw_line_below:

//flaw_line_below:
		/* actually log it */
//flaw_line_below:
		audit_log_format(*ab, "" a%d"", arg_num);
//flaw_line_below:
		if (too_long)
//flaw_line_below:
			audit_log_format(*ab, ""[%d]"", i);
//flaw_line_below:
		audit_log_format(*ab, ""="");
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			audit_log_n_hex(*ab, buf, to_send);
//flaw_line_below:
		else
//flaw_line_below:
			audit_log_string(*ab, buf);
//flaw_line_below:

//flaw_line_below:
		p += to_send;
//flaw_line_below:
		len_left -= to_send;
//flaw_line_below:
		*len_sent += arg_num_len;
//flaw_line_below:
		if (has_cntl)
//flaw_line_below:
			*len_sent += to_send * 2;
//flaw_line_below:
		else
//flaw_line_below:
			*len_sent += to_send;
//flaw_line_below:
	}
//flaw_line_below:
	/* include the null we didn't log */
//flaw_line_below:
	return len + 1;
//flaw_line_below:
}
",180191,"static int audit_log_single_execve_arg(struct audit_context *context,
					struct audit_buffer **ab,
					int arg_num,
					size_t *len_sent,
					const char __user *p,
					char *buf)
 {
	char arg_num_len_buf[12];
	const char __user *tmp_p = p;
	/* how many digits are in arg_num? 5 is the length of ' a=""""' */
	size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
	size_t len, len_left, to_send;
	size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
	unsigned int i, has_cntl = 0, too_long = 0;
	int ret;
	/* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;
	/*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
	 */
	if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
		send_sig(SIGKILL, current, 0);
		return -1;
 	}
 
	/* walk the whole argument looking for non-ascii chars */
 	do {
		if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
		else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
		/*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
		 */
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
		if (has_cntl) {
			/*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
			 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
			break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);
	len_left = len;
	if (len > max_execve_audit_len)
		too_long = 1;
	/* rewalk the argument actually logging the message */
	for (i = 0; len_left > 0; i++) {
		int room_left;
		if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
		else
			to_send = len_left;
		/* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
		if (has_cntl)
			room_left -= (to_send * 2);
		else
			room_left -= to_send;
		if (room_left < 0) {
			*len_sent = 0;
			audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
			if (!*ab)
				return 0;
		}
 
		/*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
		 */
		if ((i == 0) && (too_long))
			audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);
		/*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
		 */
		if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
		else
			ret = 0;
		if (ret) {
			WARN_ON(1);
			send_sig(SIGKILL, current, 0);
			return -1;
 		}
		buf[to_send] = '\0';
		/* actually log it */
		audit_log_format(*ab, "" a%d"", arg_num);
		if (too_long)
			audit_log_format(*ab, ""[%d]"", i);
		audit_log_format(*ab, ""="");
		if (has_cntl)
			audit_log_n_hex(*ab, buf, to_send);
		else
			audit_log_string(*ab, buf);
		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
		if (has_cntl)
			*len_sent += to_send * 2;
		else
			*len_sent += to_send;
	}
	/* include the null we didn't log */
	return len + 1;
}
","static int audit_log_single_execve_arg(struct audit_context *context,
static void audit_log_execve_info(struct audit_context *context,
				  struct audit_buffer **ab)
 {
	long len_max;
	long len_rem;
	long len_full;
	long len_buf;
	long len_abuf;
	long len_tmp;
	bool require_data;
	bool encode;
	unsigned int iter;
	unsigned int arg;
	char *buf_head;
	char *buf;
	const char __user *p = (const char __user *)current->mm->arg_start;

	/* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
	char abuf[96];

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
	WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

	/* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
	if (!buf_head) {
		audit_panic(""out of memory for argv string"");
		return;
 	}
	buf = buf_head;
 
	audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
 	do {
		/* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
		if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

		/* read more data from userspace */
		if (require_data) {
			/* can we make more room in the buffer? */
			if (buf != buf_head) {
				memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

			/* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
			if (len_tmp == -EFAULT) {
				/* unable to copy from userspace */
				send_sig(SIGKILL, current, 0);
				goto out;
			} else if (len_tmp == (len_max - len_buf)) {
				/* buffer is not large enough */
				require_data = true;
				/* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
				if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
				/* try to use a trusted value for len_full */
				if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';
 
			/* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);
 		}
","The `audit_log_single_execve_arg` function is designed to log a single argument from an `execve` system call for auditing purposes. It takes various parameters, including the audit context, a pointer to an audit buffer, the argument number, and a user-space pointer to the argument string. The function first checks the length of the argument and ensures it does not exceed predefined limits. It then copies the argument from user space, checking for control characters, and logs the argument in segments if necessary. The function handles potential errors by sending a signal to terminate the process and returns the total length of the argument logged, including a null terminator.","The provided code is responsible for logging a single argument from an `execve` system call for auditing purposes. It begins by determining the length of the argument and validating it against predefined limits. The code then copies the argument from user space while checking for control characters that may require special handling. If the argument is too long, it is logged in segments to ensure complete capture. Throughout the process, the function manages potential errors by terminating the process if issues arise, ultimately returning the total length of the logged argument, including a null terminator.","The vulnerability in the original code stemmed from a double fetch issue, where the argument data was retrieved multiple times from user space, creating a window for malicious applications to alter the data between fetches. Specifically, the code first checked for control characters and then re-fetched the argument for logging, which could lead to inconsistent or tampered data being logged. The fixed code addresses this by fetching the argument data only once into a buffer, which is then scanned and logged, thereby eliminating the double fetch and enhancing the handling of large arguments, while also improving performance and ensuring stricter record length checks.","The vulnerability in the original code arose from a double fetch issue, where the argument data was retrieved multiple times from user space, allowing a malicious application to potentially alter the data between these fetches. This inconsistency could lead to incorrect or tampered information being logged. The revised code addresses this vulnerability by ensuring that the argument data is fetched only once into a dedicated buffer, which is then scanned and logged, thereby eliminating the risk of data manipulation. Additionally, the fix enhances the handling of large arguments and implements stricter checks on record lengths, improving overall reliability.","The vulnerability was fixed by restructuring the code to fetch the argument data only once into a buffer, specifically using `strncpy_from_user` to read the data directly into `buf_head`. This change eliminates the double fetch problem, as the argument is no longer retrieved multiple times, which previously allowed for potential data tampering. The revised code also improves handling of large arguments by adjusting the encoding logic and ensuring that the length of the buffer in the audit record is accurately calculated. Additionally, the reliance on `strnlen_user` is minimized, ensuring that the kernel uses a trustworthy value for logging.","The vulnerability was fixed by modifying the code to fetch the argument data only once, thereby eliminating the risk of data manipulation between multiple fetches. This was achieved by reading the argument directly into a dedicated buffer, which is then used for both scanning and logging. The revised approach ensures that the data remains consistent and reliable throughout the process. Additionally, the fix enhances the handling of large arguments and improves the accuracy of length calculations for the audit records, ultimately strengthening the overall integrity and security of the logging mechanism."
20,180192,180192,,Local,Not required,,CVE-2016-6130,https://www.cvedetails.com/cve/CVE-2016-6130/,CWE-362,Medium,Partial,,,2016-07-03,1.9,"Race condition in the sclp_ctl_ioctl_sccb function in drivers/s390/char/sclp_ctl.c in the Linux kernel before 4.6 allows local users to obtain sensitive information from kernel memory by changing a certain length value, aka a *double fetch* vulnerability.",2016-11-28,+Info ,7,https://github.com/torvalds/linux/commit/532c34b5fbf1687df63b3fcd5b2846312ac943c6,532c34b5fbf1687df63b3fcd5b2846312ac943c6,"s390/sclp_ctl: fix potential information leak with /dev/sclp

The sclp_ctl_ioctl_sccb function uses two copy_from_user calls to
retrieve the sclp request from user space. The first copy_from_user
fetches the length of the request which is stored in the first two
bytes of the request. The second copy_from_user gets the complete
sclp request, but this copies the length field a second time.
A malicious user may have changed the length in the meantime.

Reported-by: Pengfei Wang <wpengfeinudt@gmail.com>
Reviewed-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>",5,drivers/s390/char/sclp_ctl.c,"{""sha"": ""ea607a4a1bddaf3e41165aebed1fd787b87d754e"", ""filename"": ""drivers/s390/char/sclp_ctl.c"", ""status"": ""modified"", ""additions"": 7, ""deletions"": 5, ""changes"": 12, ""blob_url"": ""https://github.com/torvalds/linux/blob/532c34b5fbf1687df63b3fcd5b2846312ac943c6/drivers/s390/char/sclp_ctl.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/532c34b5fbf1687df63b3fcd5b2846312ac943c6/drivers/s390/char/sclp_ctl.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/s390/char/sclp_ctl.c?ref=532c34b5fbf1687df63b3fcd5b2846312ac943c6"", ""patch"": ""@@ -56,6 +56,7 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)\n {\n \tstruct sclp_ctl_sccb ctl_sccb;\n \tstruct sccb_header *sccb;\n+\tunsigned long copied;\n \tint rc;\n \n \tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n@@ -65,14 +66,15 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)\n \tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n \tif (!sccb)\n \t\treturn -ENOMEM;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {\n+\tcopied = PAGE_SIZE -\n+\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);\n+\tif (offsetof(struct sccb_header, length) +\n+\t    sizeof(sccb->length) > copied || sccb->length > copied) {\n \t\trc = -EFAULT;\n \t\tgoto out_free;\n \t}\n-\tif (sccb->length > PAGE_SIZE || sccb->length < 8)\n-\t\treturn -EINVAL;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {\n-\t\trc = -EFAULT;\n+\tif (sccb->length < 8) {\n+\t\trc = -EINVAL;\n \t\tgoto out_free;\n \t}\n \trc = sclp_sync_request(ctl_sccb.cmdw, sccb);""}","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length < 8) {
		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
",C,"	unsigned long copied;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
	if (sccb->length < 8) {
		rc = -EINVAL;
","	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
",,"@@ -56,6 +56,7 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
+	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
@@ -65,14 +66,15 @@ static int sclp_ctl_ioctl_sccb(void __user *user_area)
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
-	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
+	copied = PAGE_SIZE -
+		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
+	if (offsetof(struct sccb_header, length) +
+	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
-	if (sccb->length > PAGE_SIZE || sccb->length < 8)
-		return -EINVAL;
-	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
-		rc = -EFAULT;
+	if (sccb->length < 8) {
+		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);",linux,532c34b5fbf1687df63b3fcd5b2846312ac943c6,723cacbd9dc79582e562c123a0bacf8bfc69e72a,1,"static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
//fix_flaw_line_below:
//	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
//flaw_line_below:
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
//fix_flaw_line_below:
//	copied = PAGE_SIZE -
//fix_flaw_line_below:
//		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
//fix_flaw_line_below:
//	if (offsetof(struct sccb_header, length) +
//fix_flaw_line_below:
//	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
//flaw_line_below:
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
//flaw_line_below:
		return -EINVAL;
//flaw_line_below:
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
//flaw_line_below:
		rc = -EFAULT;
//fix_flaw_line_below:
//	if (sccb->length < 8) {
//fix_flaw_line_below:
//		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
",180192,"static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length > PAGE_SIZE || sccb->length < 8)
		return -EINVAL;
	if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","static int sclp_ctl_ioctl_sccb(void __user *user_area)
 {
 	struct sclp_ctl_sccb ctl_sccb;
 	struct sccb_header *sccb;
	unsigned long copied;
 	int rc;
 
 	if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
		return -EFAULT;
	if (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))
		return -EOPNOTSUPP;
 	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!sccb)
 		return -ENOMEM;
	copied = PAGE_SIZE -
		copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
	if (offsetof(struct sccb_header, length) +
	    sizeof(sccb->length) > copied || sccb->length > copied) {
 		rc = -EFAULT;
 		goto out_free;
 	}
	if (sccb->length < 8) {
		rc = -EINVAL;
 		goto out_free;
 	}
 	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
	if (rc)
		goto out_free;
	if (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))
		rc = -EFAULT;
out_free:
	free_page((unsigned long) sccb);
	return rc;
}
","The `sclp_ctl_ioctl_sccb` function handles an IOCTL request for a System Control Program (SCP) Control Subsystem Control Block (SCCB) in a Linux kernel module. It begins by copying data from user space into a local structure and checks for command support. If valid, it allocates memory for an SCCB and copies the initial data from user space. The function validates the SCCB's length, performs a synchronous request using the command and SCCB, and finally copies the response back to user space. It ensures proper error handling and memory management throughout the process, freeing allocated memory before returning the result.","The function processes an IOCTL request related to a control block in a kernel module. It begins by retrieving data from user space and verifying command support. If the command is valid, it allocates memory for the control block and copies initial data from user space. The function checks the length of the control block for validity, performs a synchronous operation based on the command and control block, and then transfers the response back to user space. Throughout the process, it includes error handling and ensures that allocated memory is properly freed before returning the result.","The vulnerability in the original code arises from the use of two separate `copy_from_user` calls, which allows a malicious user to modify the length field between the two calls, potentially leading to an information leak or buffer overflow. Specifically, the first call retrieves the control block, while the second call fetches the entire request, including the length field again, which could have been altered. The fixed code addresses this by consolidating the copy operation into a single call that retrieves the entire request at once, ensuring that the length is consistent and preventing any malicious modifications between copies.","The vulnerability in the original code stems from the use of two separate calls to retrieve data from user space, which allows a malicious user to alter the length of the request between these calls. This could lead to inconsistencies, potentially causing an information leak or buffer overflow, as the length could be manipulated after the initial retrieval. The fixed code resolves this issue by consolidating the data retrieval into a single operation, ensuring that the length remains consistent and preventing any malicious modifications during the process, thereby enhancing the security of the function.","The vulnerability was fixed by modifying the data retrieval process to use a single call to `copy_from_user`, which now fetches the entire request in one operation instead of two separate calls. This change ensures that the length field is read only once, preventing a malicious user from altering it between the two calls. Specifically, the updated code calculates the number of bytes copied and checks that the length field, along with the total length of the request, is valid before proceeding. This approach eliminates the risk of inconsistencies and potential information leaks associated with the previous implementation.","The vulnerability was fixed by consolidating the data retrieval process into a single operation that fetches the entire request from user space at once. This approach eliminates the risk of a malicious user modifying the length of the request between two separate retrieval calls, which could lead to inconsistencies and potential information leaks. By ensuring that the length is read only once and validating it against the total bytes copied, the updated implementation enhances security and maintains data integrity throughout the process, effectively mitigating the previously identified risks."
21,180581,180581,,Local,Not required,Complete,CVE-2016-2544,https://www.cvedetails.com/cve/CVE-2016-2544/,CWE-362,Medium,,,,2016-04-27,4.7,Race condition in the queue_delete function in sound/core/seq/seq_queue.c in the Linux kernel before 4.4.1 allows local users to cause a denial of service (use-after-free and system crash) by making an ioctl call at a certain time.,2017-09-06,DoS ,2,https://github.com/torvalds/linux/commit/3567eb6af614dac436c4b16a8d426f9faed639b3,3567eb6af614dac436c4b16a8d426f9faed639b3,"ALSA: seq: Fix race at timer setup and close

ALSA sequencer code has an open race between the timer setup ioctl and
the close of the client.  This was triggered by syzkaller fuzzer, and
a use-after-free was caught there as a result.

This patch papers over it by adding a proper queue->timer_mutex lock
around the timer-related calls in the relevant code path.

Reported-by: Dmitry Vyukov <dvyukov@google.com>
Tested-by: Dmitry Vyukov <dvyukov@google.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Takashi Iwai <tiwai@suse.de>",0,sound/core/seq/seq_queue.c,"{""sha"": ""0bec02e89d5118b3dffe1e22e88e423baa037491"", ""filename"": ""sound/core/seq/seq_queue.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 0, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/3567eb6af614dac436c4b16a8d426f9faed639b3/sound/core/seq/seq_queue.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/3567eb6af614dac436c4b16a8d426f9faed639b3/sound/core/seq/seq_queue.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/sound/core/seq/seq_queue.c?ref=3567eb6af614dac436c4b16a8d426f9faed639b3"", ""patch"": ""@@ -142,8 +142,10 @@ static struct snd_seq_queue *queue_new(int owner, int locked)\n static void queue_delete(struct snd_seq_queue *q)\n {\n \t/* stop and release the timer */\n+\tmutex_lock(&q->timer_mutex);\n \tsnd_seq_timer_stop(q->timer);\n \tsnd_seq_timer_close(q);\n+\tmutex_unlock(&q->timer_mutex);\n \t/* wait until access free */\n \tsnd_use_lock_sync(&q->use_lock);\n \t/* release resources... */""}"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
",C,"	mutex_lock(&q->timer_mutex);
	mutex_unlock(&q->timer_mutex);
",,,"@@ -142,8 +142,10 @@ static struct snd_seq_queue *queue_new(int owner, int locked)
 static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
+	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
+	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */",linux,3567eb6af614dac436c4b16a8d426f9faed639b3,030e2c78d3a91dd0d27fef37e91950dde333eba1,1," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
//fix_flaw_line_below:
//	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
//fix_flaw_line_below:
//	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
",180581," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
"," static void queue_delete(struct snd_seq_queue *q)
 {
 	/* stop and release the timer */
	mutex_lock(&q->timer_mutex);
 	snd_seq_timer_stop(q->timer);
 	snd_seq_timer_close(q);
	mutex_unlock(&q->timer_mutex);
 	/* wait until access free */
 	snd_use_lock_sync(&q->use_lock);
 	/* release resources... */
	snd_seq_prioq_delete(&q->tickq);
	snd_seq_prioq_delete(&q->timeq);
	snd_seq_timer_delete(&q->timer);

	kfree(q);
}
","The `queue_delete` function is responsible for safely deallocating a sound sequence queue structure in a sound system. It first stops and closes the associated timer to ensure no ongoing operations are affected. The function then synchronizes access to the queue using a lock to prevent concurrent modifications. After ensuring safe access, it deletes priority queues for ticks and time, effectively releasing any resources they hold. Finally, it frees the memory allocated for the queue structure itself, ensuring that all associated resources are properly cleaned up to prevent memory leaks.","The provided code defines a function that handles the deletion of a sound sequence queue. It begins by stopping and closing an associated timer to prevent any ongoing operations. The function then ensures that access to the queue is synchronized, allowing safe modifications without interference from other processes. After confirming safe access, it proceeds to remove priority queues related to timing and ticks, effectively releasing their resources. Finally, the function frees the memory allocated for the queue itself, ensuring that all resources are properly cleaned up to avoid memory leaks and maintain system stability.","The vulnerability in the original code stemmed from a race condition between the timer setup and the closure of the client, which could lead to a use-after-free error. Specifically, if the timer was accessed while it was being closed, it could result in dereferencing a freed memory area. The fix implemented a mutex lock around the timer-related operations, specifically using `mutex_lock(&q->timer_mutex)` before stopping and closing the timer, and `mutex_unlock(&q->timer_mutex)` afterward. This ensures that the timer cannot be accessed concurrently, thus preventing the race condition and enhancing the overall stability of the code.","The vulnerability in the original code was due to a race condition between the setup of a timer and the closure of a client, which could lead to a use-after-free error. This situation arose when the timer was accessed while it was being closed, potentially causing the program to reference memory that had already been freed. The fix involved introducing a mutex lock around the timer-related operations, ensuring that these operations could not be executed concurrently. This change effectively synchronized access to the timer, preventing the race condition and enhancing the stability and safety of the code.","The vulnerability was fixed by introducing a mutex lock around the timer-related operations in the code. Specifically, the addition of `mutex_lock(&q->timer_mutex)` before the calls to stop and close the timer ensures that these operations are executed in a thread-safe manner. This prevents concurrent access to the timer while it is being manipulated, thereby eliminating the race condition that could lead to a use-after-free error. The corresponding `mutex_unlock(&q->timer_mutex)` after the timer operations ensures that the lock is released, allowing other processes to safely access the timer only when it is not in use.","The vulnerability was addressed by implementing a mutex lock around the operations related to the timer, ensuring that these critical sections of code could not be accessed simultaneously by multiple threads. This synchronization prevents any race conditions that could occur when one thread attempts to manipulate the timer while another is closing it, thereby eliminating the risk of referencing freed memory. By locking the timer operations, the fix guarantees that all timer-related actions are completed safely before allowing other processes to interact with the timer, thus enhancing the overall stability and safety of the code."
22,180625,180625,,Local,Not required,Complete,CVE-2016-0723,https://www.cvedetails.com/cve/CVE-2016-0723/,CWE-362,Low,Partial,,,2016-02-07,5.6,Race condition in the tty_ioctl function in drivers/tty/tty_io.c in the Linux kernel through 4.4.1 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free and system crash) by making a TIOCGETD ioctl call during processing of a TIOCSETD ioctl call.,2016-12-05,DoS +Info ,1,https://github.com/torvalds/linux/commit/5c17c861a357e9458001f021a7afa7aab9937439,5c17c861a357e9458001f021a7afa7aab9937439,"tty: Fix unsafe ldisc reference via ioctl(TIOCGETD)

ioctl(TIOCGETD) retrieves the line discipline id directly from the
ldisc because the line discipline id (c_line) in termios is untrustworthy;
userspace may have set termios via ioctl(TCSETS*) without actually
changing the line discipline via ioctl(TIOCSETD).

However, directly accessing the current ldisc via tty->ldisc is
unsafe; the ldisc ptr dereferenced may be stale if the line discipline
is changing via ioctl(TIOCSETD) or hangup.

Wait for the line discipline reference (just like read() or write())
to retrieve the ""current"" line discipline id.

Cc: <stable@vger.kernel.org>
Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",1,drivers/tty/tty_io.c,"{""sha"": ""5cec01c75691a6b7ee58ea65346c05572db1bab1"", ""filename"": ""drivers/tty/tty_io.c"", ""status"": ""modified"", ""additions"": 23, ""deletions"": 1, ""changes"": 24, ""blob_url"": ""https://github.com/torvalds/linux/blob/5c17c861a357e9458001f021a7afa7aab9937439/drivers/tty/tty_io.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/5c17c861a357e9458001f021a7afa7aab9937439/drivers/tty/tty_io.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/tty/tty_io.c?ref=5c17c861a357e9458001f021a7afa7aab9937439"", ""patch"": ""@@ -2658,6 +2658,28 @@ static int tiocsetd(struct tty_struct *tty, int __user *p)\n \treturn ret;\n }\n \n+/**\n+ *\ttiocgetd\t-\tget line discipline\n+ *\t@tty: tty device\n+ *\t@p: pointer to user data\n+ *\n+ *\tRetrieves the line discipline id directly from the ldisc.\n+ *\n+ *\tLocking: waits for ldisc reference (in case the line discipline\n+ *\t\tis changing or the tty is being hungup)\n+ */\n+\n+static int tiocgetd(struct tty_struct *tty, int __user *p)\n+{\n+\tstruct tty_ldisc *ld;\n+\tint ret;\n+\n+\tld = tty_ldisc_ref_wait(tty);\n+\tret = put_user(ld->ops->num, p);\n+\ttty_ldisc_deref(ld);\n+\treturn ret;\n+}\n+\n /**\n  *\tsend_break\t-\tperformed time break\n  *\t@tty: device to break on\n@@ -2884,7 +2906,7 @@ long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n \tcase TIOCGSID:\n \t\treturn tiocgsid(tty, real_tty, p);\n \tcase TIOCGETD:\n-\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n+\t\treturn tiocgetd(tty, p);\n \tcase TIOCSETD:\n \t\treturn tiocsetd(tty, p);\n \tcase TIOCVHANGUP:""}","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
",C,"		return tiocgetd(tty, p);
","		return put_user(tty->ldisc->ops->num, (int __user *)p);
",,"@@ -2658,6 +2658,28 @@ static int tiocsetd(struct tty_struct *tty, int __user *p)
 	return ret;
 }
 
+/**
+ *	tiocgetd	-	get line discipline
+ *	@tty: tty device
+ *	@p: pointer to user data
+ *
+ *	Retrieves the line discipline id directly from the ldisc.
+ *
+ *	Locking: waits for ldisc reference (in case the line discipline
+ *		is changing or the tty is being hungup)
+ */
+
+static int tiocgetd(struct tty_struct *tty, int __user *p)
+{
+	struct tty_ldisc *ld;
+	int ret;
+
+	ld = tty_ldisc_ref_wait(tty);
+	ret = put_user(ld->ops->num, p);
+	tty_ldisc_deref(ld);
+	return ret;
+}
+
 /**
  *	send_break	-	performed time break
  *	@tty: device to break on
@@ -2884,7 +2906,7 @@ long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
-		return put_user(tty->ldisc->ops->num, (int __user *)p);
+		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:",linux,5c17c861a357e9458001f021a7afa7aab9937439,7f22f6c935cda600660e623a411fe380015d28d9,1,"long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
//flaw_line_below:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
//fix_flaw_line_below:
//		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
",180625,"long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return put_user(tty->ldisc->ops->num, (int __user *)p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
	struct tty_struct *tty = file_tty(file);
	struct tty_struct *real_tty;
	void __user *p = (void __user *)arg;
	int retval;
	struct tty_ldisc *ld;

	if (tty_paranoia_check(tty, file_inode(file), ""tty_ioctl""))
		return -EINVAL;

	real_tty = tty_pair_get_tty(tty);

	/*
	 * Factor out some common prep work
	 */
	switch (cmd) {
	case TIOCSETD:
	case TIOCSBRK:
	case TIOCCBRK:
	case TCSBRK:
	case TCSBRKP:
		retval = tty_check_change(tty);
		if (retval)
			return retval;
		if (cmd != TIOCCBRK) {
			tty_wait_until_sent(tty, 0);
			if (signal_pending(current))
				return -EINTR;
		}
		break;
	}

	/*
	 *	Now do the stuff.
	 */
	switch (cmd) {
	case TIOCSTI:
		return tiocsti(tty, p);
	case TIOCGWINSZ:
		return tiocgwinsz(real_tty, p);
	case TIOCSWINSZ:
		return tiocswinsz(real_tty, p);
	case TIOCCONS:
		return real_tty != tty ? -EINVAL : tioccons(file);
	case FIONBIO:
		return fionbio(file, p);
	case TIOCEXCL:
		set_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCNXCL:
		clear_bit(TTY_EXCLUSIVE, &tty->flags);
		return 0;
	case TIOCGEXCL:
	{
		int excl = test_bit(TTY_EXCLUSIVE, &tty->flags);
		return put_user(excl, (int __user *)p);
	}
	case TIOCNOTTY:
		if (current->signal->tty != tty)
			return -ENOTTY;
		no_tty();
		return 0;
	case TIOCSCTTY:
		return tiocsctty(real_tty, file, arg);
	case TIOCGPGRP:
		return tiocgpgrp(tty, real_tty, p);
	case TIOCSPGRP:
		return tiocspgrp(tty, real_tty, p);
 	case TIOCGSID:
 		return tiocgsid(tty, real_tty, p);
 	case TIOCGETD:
		return tiocgetd(tty, p);
 	case TIOCSETD:
 		return tiocsetd(tty, p);
 	case TIOCVHANGUP:
		if (!capable(CAP_SYS_ADMIN))
			return -EPERM;
		tty_vhangup(tty);
		return 0;
	case TIOCGDEV:
	{
		unsigned int ret = new_encode_dev(tty_devnum(real_tty));
		return put_user(ret, (unsigned int __user *)p);
	}
	/*
	 * Break handling
	 */
	case TIOCSBRK:	/* Turn break on, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, -1);
		return 0;
	case TIOCCBRK:	/* Turn break off, unconditionally */
		if (tty->ops->break_ctl)
			return tty->ops->break_ctl(tty, 0);
		return 0;
	case TCSBRK:   /* SVID version: non-zero arg --> no break */
		/* non-zero arg means wait for all output data
		 * to be sent (performed above) but don't send break.
		 * This is used by the tcdrain() termios function.
		 */
		if (!arg)
			return send_break(tty, 250);
		return 0;
	case TCSBRKP:	/* support for POSIX tcsendbreak() */
		return send_break(tty, arg ? arg*100 : 250);

	case TIOCMGET:
		return tty_tiocmget(tty, p);
	case TIOCMSET:
	case TIOCMBIC:
	case TIOCMBIS:
		return tty_tiocmset(tty, cmd, p);
	case TIOCGICOUNT:
		retval = tty_tiocgicount(tty, p);
		/* For the moment allow fall through to the old method */
        	if (retval != -EINVAL)
			return retval;
		break;
	case TCFLSH:
		switch (arg) {
		case TCIFLUSH:
		case TCIOFLUSH:
		/* flush tty buffer and allow ldisc to process ioctl */
			tty_buffer_flush(tty, NULL);
			break;
		}
		break;
	case TIOCSSERIAL:
		tty_warn_deprecated_flags(p);
		break;
	}
	if (tty->ops->ioctl) {
		retval = tty->ops->ioctl(tty, cmd, arg);
		if (retval != -ENOIOCTLCMD)
			return retval;
	}
	ld = tty_ldisc_ref_wait(tty);
	retval = -EINVAL;
	if (ld->ops->ioctl) {
		retval = ld->ops->ioctl(tty, file, cmd, arg);
		if (retval == -ENOIOCTLCMD)
			retval = -ENOTTY;
	}
	tty_ldisc_deref(ld);
	return retval;
}
","The `tty_ioctl` function handles various input/output control commands for terminal devices in a Linux kernel environment. It begins by validating the terminal structure associated with the file and retrieves the real terminal if applicable. The function processes different commands, such as setting terminal attributes, managing window sizes, and handling break signals. It also manages exclusive access to the terminal and retrieves or sets various terminal parameters. If the command is not recognized, it delegates the operation to the terminal's specific operations or line discipline. The function ultimately returns the result of the executed command or an error code if applicable.","The provided code implements a function that processes various control commands for terminal devices within a Linux kernel. It begins by validating the terminal associated with a file and retrieves the actual terminal if necessary. The function handles a range of commands, including those for setting terminal attributes, managing window sizes, and controlling signal breaks. It also manages exclusive access to the terminal and retrieves or modifies terminal parameters as needed. If a command is unrecognized, it defers to the terminal's specific operations or line discipline, ultimately returning the result of the command execution or an appropriate error code.","The vulnerability in the original code arises from directly accessing the line discipline pointer, which could be stale if the line discipline is being changed concurrently through other ioctl commands, such as setting or hanging up. This could lead to unsafe behavior or crashes due to dereferencing an invalid pointer. The fix implemented in the updated code involves waiting for a reference to the line discipline before accessing it, ensuring that the pointer is valid and current. This change enhances safety by preventing potential race conditions and ensuring that the retrieved line discipline ID is accurate and reliable.","The vulnerability in the original code stems from directly accessing the line discipline pointer, which could lead to dereferencing a stale pointer if the line discipline is modified concurrently through other commands. This could result in unsafe behavior or crashes, as the retrieved information may not reflect the current state. The fix implemented in the updated code involves waiting for a valid reference to the line discipline before accessing it, ensuring that the pointer is accurate and up-to-date. This change mitigates the risk of race conditions and enhances the overall safety and reliability of the code.","The vulnerability was fixed by replacing the direct access to the line discipline pointer with a mechanism that waits for a valid reference to the line discipline. In the updated code, the function now uses `tty_ldisc_ref_wait` to safely obtain a reference to the current line discipline before accessing its properties, such as the line discipline ID. This ensures that the pointer is not stale, preventing potential crashes or unsafe behavior when the line discipline is being modified by other ioctl commands like `TIOCSETD` or during hangup operations. This change enhances the safety and reliability of the code.","The vulnerability was addressed by implementing a mechanism that ensures a valid reference to the line discipline is obtained before accessing its properties. Instead of directly accessing the line discipline pointer, the updated code now waits for a safe reference, which guarantees that the pointer reflects the current state and is not stale. This change prevents potential crashes or unsafe behavior that could arise from concurrent modifications to the line discipline. By ensuring that the access is synchronized, the code enhances its safety and reliability, effectively mitigating the risk of race conditions."
23,180738,180738,,Local,Not required,Complete,CVE-2015-4170,https://www.cvedetails.com/cve/CVE-2015-4170/,CWE-362,Medium,,,,2016-05-02,4.7,Race condition in the ldsem_cmpxchg function in drivers/tty/tty_ldsem.c in the Linux kernel before 3.13-rc4-next-20131218 allows local users to cause a denial of service (ldsem_down_read and ldsem_down_write deadlock) by establishing a new tty thread during shutdown of a previous tty thread.,2017-11-11,DoS ,8,https://github.com/torvalds/linux/commit/cf872776fc84128bb779ce2b83a37c884c3203ae,cf872776fc84128bb779ce2b83a37c884c3203ae,"tty: Fix hang at ldsem_down_read()

When a controlling tty is being hung up and the hang up is
waiting for a just-signalled tty reader or writer to exit, and a new tty
reader/writer tries to acquire an ldisc reference concurrently with the
ldisc reference release from the signalled reader/writer, the hangup
can hang. The new reader/writer is sleeping in ldsem_down_read() and the
hangup is sleeping in ldsem_down_write() [1].

The new reader/writer fails to wakeup the waiting hangup because the
wrong lock count value is checked (the old lock count rather than the new
lock count) to see if the lock is unowned.

Change helper function to return the new lock count if the cmpxchg was
successful; document this behavior.

[1] edited dmesg log from reporter

SysRq : Show Blocked State
  task                        PC stack   pid father
systemd         D ffff88040c4f0000     0     1      0 0x00000000
 ffff88040c49fbe0 0000000000000046 ffff88040c4a0000 ffff88040c49ffd8
 00000000001d3980 00000000001d3980 ffff88040c4a0000 ffff88040593d840
 ffff88040c49fb40 ffffffff810a4cc0 0000000000000006 0000000000000023
Call Trace:
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817a6649>] schedule+0x24/0x5e
 [<ffffffff817a588b>] schedule_timeout+0x15b/0x1ec
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817aa691>] ? _raw_spin_unlock_irq+0x24/0x26
 [<ffffffff817aa10c>] down_read_failed+0xe3/0x1b9
 [<ffffffff817aa26d>] ldsem_down_read+0x8b/0xa5
 [<ffffffff8142b5ca>] ? tty_ldisc_ref_wait+0x1b/0x44
 [<ffffffff8142b5ca>] tty_ldisc_ref_wait+0x1b/0x44
 [<ffffffff81423f5b>] tty_write+0x7d/0x28a
 [<ffffffff814241f5>] redirected_tty_write+0x8d/0x98
 [<ffffffff81424168>] ? tty_write+0x28a/0x28a
 [<ffffffff8115d03f>] do_loop_readv_writev+0x56/0x79
 [<ffffffff8115e604>] do_readv_writev+0x1b0/0x1ff
 [<ffffffff8116ea0b>] ? do_vfs_ioctl+0x32a/0x489
 [<ffffffff81167d9d>] ? final_putname+0x1d/0x3a
 [<ffffffff8115e6c7>] vfs_writev+0x2e/0x49
 [<ffffffff8115e7d3>] SyS_writev+0x47/0xaa
 [<ffffffff817ab822>] system_call_fastpath+0x16/0x1b
bash            D ffffffff81c104c0     0  5469   5302 0x00000082
 ffff8800cf817ac0 0000000000000046 ffff8804086b22a0 ffff8800cf817fd8
 00000000001d3980 00000000001d3980 ffff8804086b22a0 ffff8800cf817a48
 000000000000b9a0 ffff8800cf817a78 ffffffff81004675 ffff8800cf817a44
Call Trace:
 [<ffffffff81004675>] ? dump_trace+0x165/0x29c
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff8100edda>] ? save_stack_trace+0x26/0x41
 [<ffffffff817a6649>] schedule+0x24/0x5e
 [<ffffffff817a588b>] schedule_timeout+0x15b/0x1ec
 [<ffffffff810a4cc0>] ? sched_clock_cpu+0x9f/0xe4
 [<ffffffff817a9f03>] ? down_write_failed+0xa3/0x1c9
 [<ffffffff817aa691>] ? _raw_spin_unlock_irq+0x24/0x26
 [<ffffffff817a9f0b>] down_write_failed+0xab/0x1c9
 [<ffffffff817aa300>] ldsem_down_write+0x79/0xb1
 [<ffffffff817aada3>] ? tty_ldisc_lock_pair_timeout+0xa5/0xd9
 [<ffffffff817aada3>] tty_ldisc_lock_pair_timeout+0xa5/0xd9
 [<ffffffff8142bf33>] tty_ldisc_hangup+0xc4/0x218
 [<ffffffff81423ab3>] __tty_hangup+0x2e2/0x3ed
 [<ffffffff81424a76>] disassociate_ctty+0x63/0x226
 [<ffffffff81078aa7>] do_exit+0x79f/0xa11
 [<ffffffff81086bdb>] ? get_signal_to_deliver+0x206/0x62f
 [<ffffffff810b4bfb>] ? lock_release_holdtime.part.8+0xf/0x16e
 [<ffffffff81079b05>] do_group_exit+0x47/0xb5
 [<ffffffff81086c16>] get_signal_to_deliver+0x241/0x62f
 [<ffffffff810020a7>] do_signal+0x43/0x59d
 [<ffffffff810f2af7>] ? __audit_syscall_exit+0x21a/0x2a8
 [<ffffffff810b4bfb>] ? lock_release_holdtime.part.8+0xf/0x16e
 [<ffffffff81002655>] do_notify_resume+0x54/0x6c
 [<ffffffff817abaf8>] int_signal+0x12/0x17

Reported-by: Sami Farin <sami.farin@gmail.com>
Cc: <stable@vger.kernel.org> # 3.12.x
Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",3,drivers/tty/tty_ldsem.c,"{""sha"": ""d8a55e87877f06f3141602e4f08cdcb668c465b0"", ""filename"": ""drivers/tty/tty_ldsem.c"", ""status"": ""modified"", ""additions"": 13, ""deletions"": 3, ""changes"": 16, ""blob_url"": ""https://github.com/torvalds/linux/blob/cf872776fc84128bb779ce2b83a37c884c3203ae/drivers/tty/tty_ldsem.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cf872776fc84128bb779ce2b83a37c884c3203ae/drivers/tty/tty_ldsem.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/tty/tty_ldsem.c?ref=cf872776fc84128bb779ce2b83a37c884c3203ae"", ""patch"": ""@@ -86,11 +86,21 @@ static inline long ldsem_atomic_update(long delta, struct ld_semaphore *sem)\n \treturn atomic_long_add_return(delta, (atomic_long_t *)&sem->count);\n }\n \n+/*\n+ * ldsem_cmpxchg() updates @*old with the last-known sem->count value.\n+ * Returns 1 if count was successfully changed; @*old will have @new value.\n+ * Returns 0 if count was not changed; @*old will have most recent sem->count\n+ */\n static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n {\n-\tlong tmp = *old;\n-\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n-\treturn *old == tmp;\n+\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n+\tif (tmp == *old) {\n+\t\t*old = new;\n+\t\treturn 1;\n+\t} else {\n+\t\t*old = tmp;\n+\t\treturn 0;\n+\t}\n }\n \n /*""}"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
 }
"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
 }
",C,"	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
","	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
",,"@@ -86,11 +86,21 @@ static inline long ldsem_atomic_update(long delta, struct ld_semaphore *sem)
 	return atomic_long_add_return(delta, (atomic_long_t *)&sem->count);
 }
 
+/*
+ * ldsem_cmpxchg() updates @*old with the last-known sem->count value.
+ * Returns 1 if count was successfully changed; @*old will have @new value.
+ * Returns 0 if count was not changed; @*old will have most recent sem->count
+ */
 static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
-	long tmp = *old;
-	*old = atomic_long_cmpxchg(&sem->count, *old, new);
-	return *old == tmp;
+	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
+	if (tmp == *old) {
+		*old = new;
+		return 1;
+	} else {
+		*old = tmp;
+		return 0;
+	}
 }
 
 /*",linux,cf872776fc84128bb779ce2b83a37c884c3203ae,319e2e3f63c348a9b66db4667efa73178e18b17d,1," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
//flaw_line_below:
	long tmp = *old;
//flaw_line_below:
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
//flaw_line_below:
	return *old == tmp;
//fix_flaw_line_below:
//	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
//fix_flaw_line_below:
//	if (tmp == *old) {
//fix_flaw_line_below:
//		*old = new;
//fix_flaw_line_below:
//		return 1;
//fix_flaw_line_below:
//	} else {
//fix_flaw_line_below:
//		*old = tmp;
//fix_flaw_line_below:
//		return 0;
//fix_flaw_line_below:
//	}
 }
",180738," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
	return *old == tmp;
 }
"," static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
 {
	long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
	if (tmp == *old) {
		*old = new;
		return 1;
	} else {
		*old = tmp;
		return 0;
	}
 }
","The `ldsem_cmpxchg` function performs an atomic compare-and-swap operation on a semaphore's count value. It takes a pointer to a long integer `old`, a new long integer `new`, and a pointer to a `ld_semaphore` structure `sem`. The function first stores the current value of `*old` in a temporary variable `tmp`. It then attempts to update the semaphore's count using `atomic_long_cmpxchg`, which replaces the count with `new` only if it matches `*old`. Finally, the function returns a boolean indicating whether the original value of `*old` was equal to `tmp`, signaling if the swap was successful.","The provided code implements an atomic compare-and-swap operation for a semaphore's count value. It retrieves the current value of a specified variable and stores it temporarily. The code then attempts to update the semaphore's count with a new value, but only if it matches the previously retrieved value. This ensures that the update occurs safely in a concurrent environment. Finally, the function returns a boolean result indicating whether the original value was successfully replaced, thereby signaling if the operation was executed as intended without interference from other processes.","The vulnerability in the original code arises from the incorrect handling of the semaphore's count during a compare-and-swap operation, which could lead to a deadlock situation. Specifically, the function checked the old lock count instead of the updated count, causing a new reader or writer to fail to wake up a waiting hangup, resulting in a hang. The fixed code addresses this by directly assigning the result of the atomic compare-and-swap operation to a temporary variable. If the operation is successful, it updates the original variable with the new value, ensuring that the correct lock count is used for subsequent operations, thus preventing the deadlock.","The vulnerability in the original code stemmed from checking the old lock count during a critical operation, which could lead to a deadlock when a new reader or writer attempted to acquire a reference while another was being released. This situation prevented the new reader or writer from waking up a waiting hangup, causing the system to hang. The fix involved modifying the helper function to return the new lock count if the compare-and-swap operation was successful. This change ensured that the correct, updated lock count was used for subsequent checks, thereby preventing the deadlock scenario.","The vulnerability was fixed by modifying the `ldsem_cmpxchg` function to directly assign the result of the `atomic_long_cmpxchg` operation to a temporary variable. In the updated code, if the compare-and-swap operation successfully updates the semaphore's count, the function checks if the returned value matches the old count. If it does, the original variable is updated with the new value, ensuring that the correct lock count is used for subsequent operations. This change prevents the scenario where the old lock count was incorrectly checked, thereby eliminating the potential for deadlock during concurrent access.","The vulnerability was addressed by altering the logic of the atomic operation to ensure that the updated lock count is correctly utilized. The revised implementation now checks the result of the atomic operation directly, allowing it to determine if the update was successful. If the operation succeeds, the original variable is updated with the new value, ensuring that subsequent checks reference the correct lock count. This change effectively prevents the scenario where an outdated lock count could lead to deadlock, thereby improving the reliability of concurrent access and ensuring proper synchronization in the system."
24,180825,180825,,Local,Not required,Complete,CVE-2017-17712,https://www.cvedetails.com/cve/CVE-2017-17712/,CWE-362,Medium,Complete,Complete,,2017-12-15,6.9,The raw_sendmsg() function in net/ipv4/raw.c in the Linux kernel through 4.14.6 has a race condition in inet->hdrincl that leads to uninitialized stack pointer usage; this allows a local user to execute code and gain privileges.,2018-04-03,Exec Code +Priv ,10,https://github.com/torvalds/linux/commit/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,"net: ipv4: fix for a race condition in raw_sendmsg

inet->hdrincl is racy, and could lead to uninitialized stack pointer
usage, so its value should be read only once.

Fixes: c008ba5bdc9f (""ipv4: Avoid reading user iov twice after raw_probe_proto_opt"")
Signed-off-by: Mohamed Ghannam <simo.ghannam@gmail.com>
Reviewed-by: Eric Dumazet <edumazet@google.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",5,net/ipv4/raw.c,"{""sha"": ""125c1eab3eaa6d894804c3aa8918aa7fcc736ca0"", ""filename"": ""net/ipv4/raw.c"", ""status"": ""modified"", ""additions"": 10, ""deletions"": 5, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483/net/ipv4/raw.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/8f659a03a0ba9289b9aeb9b4470e6fb263d6f483/net/ipv4/raw.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/ipv4/raw.c?ref=8f659a03a0ba9289b9aeb9b4470e6fb263d6f483"", ""patch"": ""@@ -513,11 +513,16 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \tint err;\n \tstruct ip_options_data opt_copy;\n \tstruct raw_frag_vec rfv;\n+\tint hdrincl;\n \n \terr = -EMSGSIZE;\n \tif (len > 0xFFFF)\n \t\tgoto out;\n \n+\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n+\t * but READ_ONCE() doesn't work with bit fields\n+\t */\n+\thdrincl = inet->hdrincl;\n \t/*\n \t *\tCheck the flags.\n \t */\n@@ -593,7 +598,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\t/* Linux does not mangle headers on raw sockets,\n \t\t * so that IP options + IP_HDRINCL is non-sense.\n \t\t */\n-\t\tif (inet->hdrincl)\n+\t\tif (hdrincl)\n \t\t\tgoto done;\n \t\tif (ipc.opt->opt.srr) {\n \t\t\tif (!daddr)\n@@ -615,12 +620,12 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \n \tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n \t\t\t   RT_SCOPE_UNIVERSE,\n-\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n+\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n \t\t\t   inet_sk_flowi_flags(sk) |\n-\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n+\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n \t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n \n-\tif (!inet->hdrincl) {\n+\tif (!hdrincl) {\n \t\trfv.msg = msg;\n \t\trfv.hlen = 0;\n \n@@ -645,7 +650,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\tgoto do_confirm;\n back_from_confirm:\n \n-\tif (inet->hdrincl)\n+\tif (hdrincl)\n \t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n \t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n ""}","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (inet->hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!inet->hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (inet->hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
",C,"	int hdrincl;
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
		if (hdrincl)
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
	if (!hdrincl) {
	if (hdrincl)
","		if (inet->hdrincl)
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
	if (!inet->hdrincl) {
	if (inet->hdrincl)
",,"@@ -513,11 +513,16 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
+	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
+	/* hdrincl should be READ_ONCE(inet->hdrincl)
+	 * but READ_ONCE() doesn't work with bit fields
+	 */
+	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */
@@ -593,7 +598,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
-		if (inet->hdrincl)
+		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
@@ -615,12 +620,12 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
-			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
+			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
-			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
+			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
-	if (!inet->hdrincl) {
+	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
@@ -645,7 +650,7 @@ static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 		goto do_confirm;
 back_from_confirm:
 
-	if (inet->hdrincl)
+	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 ",linux,8f659a03a0ba9289b9aeb9b4470e6fb263d6f483,93c647643b48f0131f02e45da3bd367d80443291,1,"static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
//fix_flaw_line_below:
//	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
//fix_flaw_line_below:
//	/* hdrincl should be READ_ONCE(inet->hdrincl)
//fix_flaw_line_below:
//	 * but READ_ONCE() doesn't work with bit fields
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
//flaw_line_below:
		if (inet->hdrincl)
//fix_flaw_line_below:
//		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
//flaw_line_below:
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
//fix_flaw_line_below:
//			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
//flaw_line_below:
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
//fix_flaw_line_below:
//			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
//flaw_line_below:
	if (!inet->hdrincl) {
//fix_flaw_line_below:
//	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
//flaw_line_below:
	if (inet->hdrincl)
//fix_flaw_line_below:
//	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
",180825,"static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (inet->hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!inet->hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (inet->hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
	struct inet_sock *inet = inet_sk(sk);
	struct net *net = sock_net(sk);
	struct ipcm_cookie ipc;
	struct rtable *rt = NULL;
	struct flowi4 fl4;
	int free = 0;
	__be32 daddr;
	__be32 saddr;
	u8  tos;
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
	int hdrincl;
 
 	err = -EMSGSIZE;
 	if (len > 0xFFFF)
 		goto out;
 
	/* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
	 */
	hdrincl = inet->hdrincl;
 	/*
 	 *	Check the flags.
 	 */

	err = -EOPNOTSUPP;
	if (msg->msg_flags & MSG_OOB)	/* Mirror BSD error message */
		goto out;               /* compatibility */

	/*
	 *	Get and verify the address.
	 */

	if (msg->msg_namelen) {
		DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(*usin))
			goto out;
		if (usin->sin_family != AF_INET) {
			pr_info_once(""%s: %s forgot to set AF_INET. Fix it!\n"",
				     __func__, current->comm);
			err = -EAFNOSUPPORT;
			if (usin->sin_family)
				goto out;
		}
		daddr = usin->sin_addr.s_addr;
		/* ANK: I did not forget to get protocol from port field.
		 * I just do not know, who uses this weirdness.
		 * IP_HDRINCL is much more convenient.
		 */
	} else {
		err = -EDESTADDRREQ;
		if (sk->sk_state != TCP_ESTABLISHED)
			goto out;
		daddr = inet->inet_daddr;
	}

	ipc.sockc.tsflags = sk->sk_tsflags;
	ipc.addr = inet->inet_saddr;
	ipc.opt = NULL;
	ipc.tx_flags = 0;
	ipc.ttl = 0;
	ipc.tos = -1;
	ipc.oif = sk->sk_bound_dev_if;

	if (msg->msg_controllen) {
		err = ip_cmsg_send(sk, msg, &ipc, false);
		if (unlikely(err)) {
			kfree(ipc.opt);
			goto out;
		}
		if (ipc.opt)
			free = 1;
	}

	saddr = ipc.addr;
	ipc.addr = daddr;

	if (!ipc.opt) {
		struct ip_options_rcu *inet_opt;

		rcu_read_lock();
		inet_opt = rcu_dereference(inet->inet_opt);
		if (inet_opt) {
			memcpy(&opt_copy, inet_opt,
			       sizeof(*inet_opt) + inet_opt->opt.optlen);
			ipc.opt = &opt_copy.opt;
		}
		rcu_read_unlock();
	}

	if (ipc.opt) {
		err = -EINVAL;
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
		if (hdrincl)
 			goto done;
 		if (ipc.opt->opt.srr) {
 			if (!daddr)
				goto done;
			daddr = ipc.opt->opt.faddr;
		}
	}
	tos = get_rtconn_flags(&ipc, sk);
	if (msg->msg_flags & MSG_DONTROUTE)
		tos |= RTO_ONLINK;

	if (ipv4_is_multicast(daddr)) {
		if (!ipc.oif)
			ipc.oif = inet->mc_index;
		if (!saddr)
			saddr = inet->mc_addr;
	} else if (!ipc.oif)
		ipc.oif = inet->uc_index;
 
 	flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 			   inet_sk_flowi_flags(sk) |
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
 			   daddr, saddr, 0, 0, sk->sk_uid);
 
	if (!hdrincl) {
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
		err = raw_probe_proto_opt(&rfv, &fl4);
		if (err)
			goto done;
	}

	security_sk_classify_flow(sk, flowi4_to_flowi(&fl4));
	rt = ip_route_output_flow(net, &fl4, sk);
	if (IS_ERR(rt)) {
		err = PTR_ERR(rt);
		rt = NULL;
		goto done;
	}

	err = -EACCES;
	if (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))
		goto done;

	if (msg->msg_flags & MSG_CONFIRM)
 		goto do_confirm;
 back_from_confirm:
 
	if (hdrincl)
 		err = raw_send_hdrinc(sk, &fl4, msg, len,
 				      &rt, msg->msg_flags, &ipc.sockc);
 
	 else {
		sock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);

		if (!ipc.addr)
			ipc.addr = fl4.daddr;
		lock_sock(sk);
		err = ip_append_data(sk, &fl4, raw_getfrag,
				     &rfv, len, 0,
				     &ipc, &rt, msg->msg_flags);
		if (err)
			ip_flush_pending_frames(sk);
		else if (!(msg->msg_flags & MSG_MORE)) {
			err = ip_push_pending_frames(sk, &fl4);
			if (err == -ENOBUFS && !inet->recverr)
				err = 0;
		}
		release_sock(sk);
	}
done:
	if (free)
		kfree(ipc.opt);
	ip_rt_put(rt);

out:
	if (err < 0)
		return err;
	return len;

do_confirm:
	if (msg->msg_flags & MSG_PROBE)
		dst_confirm_neigh(&rt->dst, &fl4.daddr);
	if (!(msg->msg_flags & MSG_PROBE) || len)
		goto back_from_confirm;
	err = 0;
	goto done;
}
","The `raw_sendmsg` function is responsible for sending raw IP packets over a socket in a Linux networking context. It begins by validating the message length and flags, ensuring the socket is properly configured for IPv4 communication. The function retrieves the destination address and prepares various parameters, including IP options and routing information. It checks for multicast addresses and sets the appropriate output interface. The function then constructs a flow structure for routing and handles the transmission of the packet, including managing timestamps and potential fragmentation. Finally, it cleans up resources and returns the result of the send operation, indicating success or failure.","The provided code implements a function that facilitates the sending of raw IP packets through a socket in a networking environment. It begins by validating the input parameters, including the message length and socket configuration. The function retrieves the destination address and prepares necessary routing and transmission options. It handles special cases for multicast addresses and manages packet options, ensuring proper handling of timestamps and potential fragmentation. Ultimately, the function attempts to transmit the packet, performing necessary cleanup and resource management, and returns the outcome of the send operation, indicating whether it was successful or encountered an error.","The vulnerability in the original code stemmed from a race condition involving the `hdrincl` variable, which was accessed multiple times without proper synchronization, potentially leading to the use of an uninitialized stack pointer. This could occur if the value of `inet->hdrincl` changed between reads, resulting in inconsistent behavior. The fix involved reading the value of `inet->hdrincl` only once and storing it in a local variable, `hdrincl`, ensuring that the same value is used throughout the function. This change mitigates the risk of race conditions and ensures consistent behavior during packet processing.","The vulnerability in the original code was due to a race condition that arose from accessing a specific variable multiple times without proper synchronization. This could lead to the use of an uninitialized stack pointer, resulting in unpredictable behavior and potential security issues. The fix involved modifying the code to read the value of the variable only once and store it in a local context. This change ensures that the same value is consistently used throughout the function, effectively eliminating the risk of race conditions and enhancing the stability and security of the packet processing logic.","The vulnerability was fixed by introducing a local variable, `hdrincl`, to store the value of `inet->hdrincl` after reading it once. This change prevents multiple accesses to the potentially racy variable, ensuring that the same value is used consistently throughout the function. By replacing direct references to `inet->hdrincl` with the local variable, the code eliminates the risk of encountering an uninitialized stack pointer due to concurrent modifications. This adjustment enhances the reliability of the packet processing logic, particularly in scenarios where the value could change unexpectedly during execution.","The vulnerability was addressed by modifying the code to read a specific value only once and store it in a local variable. This change ensures that the same value is consistently used throughout the function, thereby preventing potential inconsistencies that could arise from concurrent modifications. By eliminating multiple accesses to the original source, the fix effectively mitigates the risk of encountering uninitialized data, enhancing the stability and security of the packet processing logic. This approach ensures that the function behaves predictably, even in scenarios where the underlying value might change unexpectedly during execution."
25,180889,180889,,Local,Not required,Partial,CVE-2017-15649,https://www.cvedetails.com/cve/CVE-2017-15649/,CWE-362,Low,Partial,Partial,,2017-10-19,4.6,"net/packet/af_packet.c in the Linux kernel before 4.13.6 allows local users to gain privileges via crafted system calls that trigger mishandling of packet_fanout data structures, because of a race condition (involving fanout_add and packet_do_bind) that leads to a use-after-free, a different vulnerability than CVE-2017-6346.",2018-08-24,+Priv ,5,https://github.com/torvalds/linux/commit/4971613c1639d8e5f102c4e797c3bf8f83a5a69e,4971613c1639d8e5f102c4e797c3bf8f83a5a69e,"packet: in packet_do_bind, test fanout with bind_lock held

Once a socket has po->fanout set, it remains a member of the group
until it is destroyed. The prot_hook must be constant and identical
across sockets in the group.

If fanout_add races with packet_do_bind between the test of po->fanout
and taking the lock, the bind call may make type or dev inconsistent
with that of the fanout group.

Hold po->bind_lock when testing po->fanout to avoid this race.

I had to introduce artificial delay (local_bh_enable) to actually
observe the race.

Fixes: dc99f600698d (""packet: Add fanout support."")
Signed-off-by: Willem de Bruijn <willemb@google.com>
Reviewed-by: Eric Dumazet <edumazet@google.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",2,net/packet/af_packet.c,"{""sha"": ""a10c2836465cf499e2b723419da25bc3ad07d69f"", ""filename"": ""net/packet/af_packet.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 3, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/4971613c1639d8e5f102c4e797c3bf8f83a5a69e/net/packet/af_packet.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/4971613c1639d8e5f102c4e797c3bf8f83a5a69e/net/packet/af_packet.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/packet/af_packet.c?ref=4971613c1639d8e5f102c4e797c3bf8f83a5a69e"", ""patch"": ""@@ -3069,13 +3069,15 @@ static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n \tint ret = 0;\n \tbool unlisted = false;\n \n-\tif (po->fanout)\n-\t\treturn -EINVAL;\n-\n \tlock_sock(sk);\n \tspin_lock(&po->bind_lock);\n \trcu_read_lock();\n \n+\tif (po->fanout) {\n+\t\tret = -EINVAL;\n+\t\tgoto out_unlock;\n+\t}\n+\n \tif (name) {\n \t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n \t\tif (!dev) {""}","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
	if (po->fanout)
		return -EINVAL;
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
",C,"	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

","	if (po->fanout)
		return -EINVAL;
",,"@@ -3069,13 +3069,15 @@ static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
 	int ret = 0;
 	bool unlisted = false;
 
-	if (po->fanout)
-		return -EINVAL;
-
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
+	if (po->fanout) {
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {",linux,4971613c1639d8e5f102c4e797c3bf8f83a5a69e,1579f678fb4397f9e439d2e373d4ade036c673b4,1,"static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
//flaw_line_below:
	if (po->fanout)
//flaw_line_below:
		return -EINVAL;
//flaw_line_below:

 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
//fix_flaw_line_below:
//	if (po->fanout) {
//fix_flaw_line_below:
//		ret = -EINVAL;
//fix_flaw_line_below:
//		goto out_unlock;
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
",180889,"static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
	if (po->fanout)
		return -EINVAL;
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","static int packet_do_bind(struct sock *sk, const char *name, int ifindex,
			  __be16 proto)
{
	struct packet_sock *po = pkt_sk(sk);
	struct net_device *dev_curr;
	__be16 proto_curr;
	bool need_rehook;
	struct net_device *dev = NULL;
 	int ret = 0;
 	bool unlisted = false;
 
 	lock_sock(sk);
 	spin_lock(&po->bind_lock);
 	rcu_read_lock();
 
	if (po->fanout) {
		ret = -EINVAL;
		goto out_unlock;
	}

 	if (name) {
 		dev = dev_get_by_name_rcu(sock_net(sk), name);
 		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	} else if (ifindex) {
		dev = dev_get_by_index_rcu(sock_net(sk), ifindex);
		if (!dev) {
			ret = -ENODEV;
			goto out_unlock;
		}
	}

	if (dev)
		dev_hold(dev);

	proto_curr = po->prot_hook.type;
	dev_curr = po->prot_hook.dev;

	need_rehook = proto_curr != proto || dev_curr != dev;

	if (need_rehook) {
		if (po->running) {
			rcu_read_unlock();
			__unregister_prot_hook(sk, true);
			rcu_read_lock();
			dev_curr = po->prot_hook.dev;
			if (dev)
				unlisted = !dev_get_by_index_rcu(sock_net(sk),
								 dev->ifindex);
		}

		po->num = proto;
		po->prot_hook.type = proto;

		if (unlikely(unlisted)) {
			dev_put(dev);
			po->prot_hook.dev = NULL;
			po->ifindex = -1;
			packet_cached_dev_reset(po);
		} else {
			po->prot_hook.dev = dev;
			po->ifindex = dev ? dev->ifindex : 0;
			packet_cached_dev_assign(po, dev);
		}
	}
	if (dev_curr)
		dev_put(dev_curr);

	if (proto == 0 || !need_rehook)
		goto out_unlock;

	if (!unlisted && (!dev || (dev->flags & IFF_UP))) {
		register_prot_hook(sk);
	} else {
		sk->sk_err = ENETDOWN;
		if (!sock_flag(sk, SOCK_DEAD))
			sk->sk_error_report(sk);
	}

out_unlock:
	rcu_read_unlock();
	spin_unlock(&po->bind_lock);
	release_sock(sk);
	return ret;
}
","The `packet_do_bind` function is responsible for binding a socket to a specific network device and protocol in a Linux kernel networking context. It first checks if the socket is already associated with a fanout, returning an error if so. The function then attempts to retrieve the network device either by name or index, handling errors if the device is not found. It evaluates whether a rehook is necessary based on the current protocol and device. If a rehook is needed and the socket is running, it unregisters the current protocol hook. Finally, it registers the new protocol hook if the device is up, or reports an error if the device is down, ensuring proper resource management throughout.","The function facilitates the binding of a socket to a specified network device and protocol within the Linux kernel networking framework. It begins by verifying the absence of a specific configuration and then attempts to locate the network device using either its name or index, handling errors if the device is not found. The function assesses whether a reconfiguration is necessary based on the current settings. If a change is required and the socket is active, it unregisters the existing configuration before applying the new one. Finally, it registers the new configuration if the device is operational or reports an error if it is not.","The vulnerability in the original code arises from a race condition between checking the `po->fanout` variable and acquiring the `bind_lock`. If `fanout_add` is executed concurrently, it could modify the fanout state after the check but before the lock is acquired, leading to inconsistent protocol hooks across sockets in the group. The fix addresses this issue by ensuring that the check for `po->fanout` is performed while holding the `bind_lock`, thus preventing any changes to the fanout state during this critical section. This change guarantees that the protocol hook remains consistent and avoids potential data corruption or unexpected behavior.","The vulnerability in the original code stems from a race condition that occurs when checking the fanout state of a socket and acquiring the necessary lock. This situation allows for concurrent modifications to the fanout state, which can lead to inconsistencies in the protocol hooks across different sockets within the same group. The fix addresses this issue by ensuring that the check for the fanout state is performed while holding the lock, thereby preventing any changes during this critical section. This change guarantees that the protocol hooks remain consistent and prevents potential data corruption or unexpected behavior in the system.","The vulnerability was fixed by ensuring that the check for `po->fanout` is performed while holding the `bind_lock`. This change prevents any race conditions that could occur between the test of `po->fanout` and the acquisition of the lock, which could lead to inconsistencies in the protocol hooks. By locking the `bind_lock` before checking `po->fanout`, the code guarantees that the fanout state remains stable during the binding process. This adjustment effectively eliminates the risk of concurrent modifications affecting the protocol hook's consistency across sockets in the group, thereby enhancing the overall reliability of the function.","The vulnerability was fixed by modifying the code to ensure that the check for the fanout state occurs while holding the appropriate lock. This adjustment prevents any race conditions that could arise from concurrent modifications to the fanout state during the binding process. By securing the lock before evaluating the fanout condition, the code guarantees that the state remains consistent and stable, thereby avoiding potential inconsistencies in the protocol hooks across different sockets. This change enhances the reliability of the function and protects against unexpected behavior caused by simultaneous operations."
26,181435,181435,,Local,Not required,Complete,CVE-2017-7533,https://www.cvedetails.com/cve/CVE-2017-7533/,CWE-362,Medium,Complete,Complete,,2017-08-05,6.9,Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions.,2018-01-04,DoS +Priv Mem. Corr. ,4,https://github.com/torvalds/linux/commit/49d31c2f389acfe83417083e1208422b4091cd9e,49d31c2f389acfe83417083e1208422b4091cd9e,"dentry name snapshots

take_dentry_name_snapshot() takes a safe snapshot of dentry name;
if the name is a short one, it gets copied into caller-supplied
structure, otherwise an extra reference to external name is grabbed
(those are never modified).  In either case the pointer to stable
string is stored into the same structure.

dentry must be held by the caller of take_dentry_name_snapshot(),
but may be freely dropped afterwards - the snapshot will stay
until destroyed by release_dentry_name_snapshot().

Intended use:
	struct name_snapshot s;

	take_dentry_name_snapshot(&s, dentry);
	...
	access s.name
	...
	release_dentry_name_snapshot(&s);

Replaces fsnotify_oldname_...(), gets used in fsnotify to obtain the name
to pass down with event.

Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>",4,fs/namei.c,"{""sha"": ""831f3a9a8f05282b8ba9aae9c6ed963334b9b8f3"", ""filename"": ""fs/dcache.c"", ""status"": ""modified"", ""additions"": 27, ""deletions"": 0, ""changes"": 27, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/dcache.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/dcache.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/dcache.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -277,6 +277,33 @@ static inline int dname_external(const struct dentry *dentry)\n \treturn dentry->d_name.name != dentry->d_iname;\n }\n \n+void take_dentry_name_snapshot(struct name_snapshot *name, struct dentry *dentry)\n+{\n+\tspin_lock(&dentry->d_lock);\n+\tif (unlikely(dname_external(dentry))) {\n+\t\tstruct external_name *p = external_name(dentry);\n+\t\tatomic_inc(&p->u.count);\n+\t\tspin_unlock(&dentry->d_lock);\n+\t\tname->name = p->name;\n+\t} else {\n+\t\tmemcpy(name->inline_name, dentry->d_iname, DNAME_INLINE_LEN);\n+\t\tspin_unlock(&dentry->d_lock);\n+\t\tname->name = name->inline_name;\n+\t}\n+}\n+EXPORT_SYMBOL(take_dentry_name_snapshot);\n+\n+void release_dentry_name_snapshot(struct name_snapshot *name)\n+{\n+\tif (unlikely(name->name != name->inline_name)) {\n+\t\tstruct external_name *p;\n+\t\tp = container_of(name->name, struct external_name, name[0]);\n+\t\tif (unlikely(atomic_dec_and_test(&p->u.count)))\n+\t\t\tkfree_rcu(p, u.head);\n+\t}\n+}\n+EXPORT_SYMBOL(release_dentry_name_snapshot);\n+\n static inline void __d_set_inode_and_type(struct dentry *dentry,\n \t\t\t\t\t  struct inode *inode,\n \t\t\t\t\t  unsigned type_flags)""}<_**next**_>{""sha"": ""acd3be2cc691bb577916aa552668d2c1d179ed69"", ""filename"": ""fs/debugfs/inode.c"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 5, ""changes"": 10, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/debugfs/inode.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/debugfs/inode.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/debugfs/inode.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -766,7 +766,7 @@ struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n {\n \tint error;\n \tstruct dentry *dentry = NULL, *trap;\n-\tconst char *old_name;\n+\tstruct name_snapshot old_name;\n \n \ttrap = lock_rename(new_dir, old_dir);\n \t/* Source or destination directories don't exist? */\n@@ -781,19 +781,19 @@ struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n \tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n \t\tgoto exit;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \n \terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n \t\t\t      dentry, 0);\n \tif (error) {\n-\t\tfsnotify_oldname_free(old_name);\n+\t\trelease_dentry_name_snapshot(&old_name);\n \t\tgoto exit;\n \t}\n \td_move(old_dentry, dentry);\n-\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,\n+\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,\n \t\td_is_dir(old_dentry),\n \t\tNULL, old_dentry);\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \tunlock_rename(new_dir, old_dir);\n \tdput(dentry);\n \treturn old_dentry;""}<_**next**_>{""sha"": ""c5588e837b1532bfc201a5b8d6bfcdedad9f107f"", ""filename"": ""fs/namei.c"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 4, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/namei.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/namei.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/namei.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -4362,11 +4362,11 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n {\n \tint error;\n \tbool is_dir = d_is_dir(old_dentry);\n-\tconst unsigned char *old_name;\n \tstruct inode *source = old_dentry->d_inode;\n \tstruct inode *target = new_dentry->d_inode;\n \tbool new_is_dir = false;\n \tunsigned max_links = new_dir->i_sb->s_max_links;\n+\tstruct name_snapshot old_name;\n \n \tif (source == target)\n \t\treturn 0;\n@@ -4413,7 +4413,7 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n \tif (error)\n \t\treturn error;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \tdget(new_dentry);\n \tif (!is_dir || (flags & RENAME_EXCHANGE))\n \t\tlock_two_nondirectories(source, target);\n@@ -4468,14 +4468,14 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n \t\tinode_unlock(target);\n \tdput(new_dentry);\n \tif (!error) {\n-\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n+\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,\n \t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n \t\tif (flags & RENAME_EXCHANGE) {\n \t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n \t\t\t\t      new_is_dir, NULL, new_dentry);\n \t\t}\n \t}\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \n \treturn error;\n }""}<_**next**_>{""sha"": ""0c4583b61717646e491a28d879b5762faeb8956e"", ""filename"": ""fs/notify/fsnotify.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 2, ""changes"": 8, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/fs/notify/fsnotify.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/fs/notify/fsnotify.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/notify/fsnotify.c?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -161,16 +161,20 @@ int __fsnotify_parent(const struct path *path, struct dentry *dentry, __u32 mask\n \tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n \t\t__fsnotify_update_child_dentry_flags(p_inode);\n \telse if (p_inode->i_fsnotify_mask & mask) {\n+\t\tstruct name_snapshot name;\n+\n \t\t/* we are notifying a parent so come up with the new mask which\n \t\t * specifies these are events which came from a child. */\n \t\tmask |= FS_EVENT_ON_CHILD;\n \n+\t\ttake_dentry_name_snapshot(&name, dentry);\n \t\tif (path)\n \t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n \t\telse\n \t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n+\t\trelease_dentry_name_snapshot(&name);\n \t}\n \n \tdput(parent);""}<_**next**_>{""sha"": ""025727bf679745e507bb427f14198bf1e607ae8e"", ""filename"": ""include/linux/dcache.h"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 0, ""changes"": 6, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/dcache.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/dcache.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/dcache.h?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -591,5 +591,11 @@ static inline struct inode *d_real_inode(const struct dentry *dentry)\n \treturn d_backing_inode(d_real((struct dentry *) dentry, NULL, 0));\n }\n \n+struct name_snapshot {\n+\tconst char *name;\n+\tchar inline_name[DNAME_INLINE_LEN];\n+};\n+void take_dentry_name_snapshot(struct name_snapshot *, struct dentry *);\n+void release_dentry_name_snapshot(struct name_snapshot *);\n \n #endif\t/* __LINUX_DCACHE_H */""}<_**next**_>{""sha"": ""b78aa7ac77ce1d6920aa196f4cd71d6d6a7aee7b"", ""filename"": ""include/linux/fsnotify.h"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 31, ""changes"": 31, ""blob_url"": ""https://github.com/torvalds/linux/blob/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/fsnotify.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/49d31c2f389acfe83417083e1208422b4091cd9e/include/linux/fsnotify.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/fsnotify.h?ref=49d31c2f389acfe83417083e1208422b4091cd9e"", ""patch"": ""@@ -293,35 +293,4 @@ static inline void fsnotify_change(struct dentry *dentry, unsigned int ia_valid)\n \t}\n }\n \n-#if defined(CONFIG_FSNOTIFY)\t/* notify helpers */\n-\n-/*\n- * fsnotify_oldname_init - save off the old filename before we change it\n- */\n-static inline const unsigned char *fsnotify_oldname_init(const unsigned char *name)\n-{\n-\treturn kstrdup(name, GFP_KERNEL);\n-}\n-\n-/*\n- * fsnotify_oldname_free - free the name we got from fsnotify_oldname_init\n- */\n-static inline void fsnotify_oldname_free(const unsigned char *old_name)\n-{\n-\tkfree(old_name);\n-}\n-\n-#else\t/* CONFIG_FSNOTIFY */\n-\n-static inline const char *fsnotify_oldname_init(const unsigned char *name)\n-{\n-\treturn NULL;\n-}\n-\n-static inline void fsnotify_oldname_free(const unsigned char *old_name)\n-{\n-}\n-\n-#endif\t/*  CONFIG_FSNOTIFY */\n-\n #endif\t/* _LINUX_FS_NOTIFY_H */""}","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	fsnotify_oldname_free(old_name);
 
 	return error;
 }
",C,"	struct name_snapshot old_name;
	take_dentry_name_snapshot(&old_name, old_dentry);
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
	release_dentry_name_snapshot(&old_name);
","	const unsigned char *old_name;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
	fsnotify_oldname_free(old_name);
",,"@@ -4362,11 +4362,11 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
-	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
+	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;
@@ -4413,7 +4413,7 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 	if (error)
 		return error;
 
-	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
+	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
@@ -4468,14 +4468,14 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
-		fsnotify_move(old_dir, new_dir, old_name, is_dir,
+		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
-	fsnotify_oldname_free(old_name);
+	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }",linux,49d31c2f389acfe83417083e1208422b4091cd9e,a8e2b6367794e6cee9eecba6d5ff425f338e0754,1,"int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
//flaw_line_below:
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
//fix_flaw_line_below:
//	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
//flaw_line_below:
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
//fix_flaw_line_below:
//	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
//flaw_line_below:
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
//fix_flaw_line_below:
//		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
//flaw_line_below:
	fsnotify_oldname_free(old_name);
//fix_flaw_line_below:
//	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
",181435,"int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
	const unsigned char *old_name;
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	fsnotify_oldname_free(old_name);
 
 	return error;
 }
","int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
	       struct inode *new_dir, struct dentry *new_dentry,
	       struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
 	struct inode *source = old_dentry->d_inode;
 	struct inode *target = new_dentry->d_inode;
 	bool new_is_dir = false;
 	unsigned max_links = new_dir->i_sb->s_max_links;
	struct name_snapshot old_name;
 
 	if (source == target)
 		return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
	if (error)
		return error;

	if (!target) {
		error = may_create(new_dir, new_dentry);
	} else {
		new_is_dir = d_is_dir(new_dentry);

		if (!(flags & RENAME_EXCHANGE))
			error = may_delete(new_dir, new_dentry, is_dir);
		else
			error = may_delete(new_dir, new_dentry, new_is_dir);
	}
	if (error)
		return error;

	if (!old_dir->i_op->rename)
		return -EPERM;

	/*
	 * If we are going to change the parent - check write permissions,
	 * we'll need to flip '..'.
	 */
	if (new_dir != old_dir) {
		if (is_dir) {
			error = inode_permission(source, MAY_WRITE);
			if (error)
				return error;
		}
		if ((flags & RENAME_EXCHANGE) && new_is_dir) {
			error = inode_permission(target, MAY_WRITE);
			if (error)
				return error;
		}
	}

	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
				      flags);
 	if (error)
 		return error;
 
	take_dentry_name_snapshot(&old_name, old_dentry);
 	dget(new_dentry);
 	if (!is_dir || (flags & RENAME_EXCHANGE))
 		lock_two_nondirectories(source, target);
	else if (target)
		inode_lock(target);

	error = -EBUSY;
	if (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))
		goto out;

	if (max_links && new_dir != old_dir) {
		error = -EMLINK;
		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
			goto out;
		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
		    old_dir->i_nlink >= max_links)
			goto out;
	}
	if (is_dir && !(flags & RENAME_EXCHANGE) && target)
		shrink_dcache_parent(new_dentry);
	if (!is_dir) {
		error = try_break_deleg(source, delegated_inode);
		if (error)
			goto out;
	}
	if (target && !new_is_dir) {
		error = try_break_deleg(target, delegated_inode);
		if (error)
			goto out;
	}
	error = old_dir->i_op->rename(old_dir, old_dentry,
				       new_dir, new_dentry, flags);
	if (error)
		goto out;

	if (!(flags & RENAME_EXCHANGE) && target) {
		if (is_dir)
			target->i_flags |= S_DEAD;
		dont_mount(new_dentry);
		detach_mounts(new_dentry);
	}
	if (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {
		if (!(flags & RENAME_EXCHANGE))
			d_move(old_dentry, new_dentry);
		else
			d_exchange(old_dentry, new_dentry);
	}
out:
	if (!is_dir || (flags & RENAME_EXCHANGE))
		unlock_two_nondirectories(source, target);
	else if (target)
 		inode_unlock(target);
 	dput(new_dentry);
 	if (!error) {
		fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
 			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
 				      new_is_dir, NULL, new_dentry);
 		}
 	}
	release_dentry_name_snapshot(&old_name);
 
 	return error;
 }
","The `vfs_rename` function implements the renaming of files or directories within a virtual file system. It checks permissions for deleting the old entry and creating or replacing the new entry, handling cases for directory and file exchanges. The function ensures that the source and target inodes are not the same and verifies link limits for directories. It manages locks on inodes to prevent concurrent modifications and invokes the appropriate rename operation defined by the inode's operations. Finally, it notifies the file system of the move operation and cleans up resources, returning any errors encountered during the process.","The provided code implements a function that facilitates the renaming of files or directories within a virtual file system. It performs checks to ensure that the user has the necessary permissions to delete the old entry and create or replace the new one, while also managing potential conflicts and link limits for directories. The function handles locking mechanisms to prevent concurrent access issues and invokes the appropriate operations for renaming. Additionally, it notifies the file system of the changes made and ensures proper cleanup of resources, returning any errors encountered during the renaming process.","The vulnerability in the original code stemmed from the handling of the old name of a directory entry, which could lead to inconsistencies or use-after-free errors when the name was accessed after the dentry was potentially modified or released. This was particularly problematic in the context of file system notifications. The fix introduced a new mechanism that safely captures a snapshot of the dentry name, ensuring that the name remains stable and accessible throughout the renaming process. By using the `take_dentry_name_snapshot` function, the code now maintains a reliable reference to the name, which is released only after it is no longer needed, thus enhancing stability and security.","The vulnerability in the original code arose from the potential for accessing an outdated or invalid reference to a directory entry's name after modifications or deletions, which could lead to inconsistencies or crashes. This issue was particularly critical during file system notifications, where the name needed to remain stable. The fix implemented a mechanism that safely captures a snapshot of the directory entry's name, ensuring that it remains valid throughout the renaming process. By allowing the original reference to be released while maintaining a stable copy, the code enhances reliability and prevents errors related to name access during critical operations.","The vulnerability was fixed by introducing the `take_dentry_name_snapshot` function, which safely captures a snapshot of the directory entry's name. This function checks if the name is short enough to be copied into a caller-supplied structure; otherwise, it grabs an extra reference to the external name, ensuring that the pointer to the stable string remains valid. The snapshot allows the original dentry to be released without losing access to the name, and it is only destroyed by calling `release_dentry_name_snapshot`. This approach prevents inconsistencies and ensures that the name remains accessible during file system notifications.","The vulnerability was addressed by implementing a mechanism that safely captures and stores a stable reference to the directory entry's name during the renaming process. This approach ensures that even if the original entry is modified or released, the name remains accessible and valid for the duration of its use. By allowing the original reference to be dropped while maintaining a reliable snapshot, the code prevents potential inconsistencies and access errors. This fix enhances the stability of file system notifications and ensures that the name can be safely utilized without risk of invalid access."
27,181539,181539,,Remote,Not required,Complete,CVE-2017-6001,https://www.cvedetails.com/cve/CVE-2017-6001/,CWE-362,High,Complete,Complete,,2017-02-18,7.6,Race condition in kernel/events/core.c in the Linux kernel before 4.9.7 allows local users to gain privileges via a crafted application that makes concurrent perf_event_open system calls for moving a software group into a hardware context.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-6786.,2018-06-19,+Priv ,23,https://github.com/torvalds/linux/commit/321027c1fe77f892f4ea07846aeae08cefbbb290,321027c1fe77f892f4ea07846aeae08cefbbb290,"perf/core: Fix concurrent sys_perf_event_open() vs. 'move_group' race

Di Shen reported a race between two concurrent sys_perf_event_open()
calls where both try and move the same pre-existing software group
into a hardware context.

The problem is exactly that described in commit:

  f63a8daa5812 (""perf: Fix event->ctx locking"")

... where, while we wait for a ctx->mutex acquisition, the event->ctx
relation can have changed under us.

That very same commit failed to recognise sys_perf_event_context() as an
external access vector to the events and thereby didn't apply the
established locking rules correctly.

So while one sys_perf_event_open() call is stuck waiting on
mutex_lock_double(), the other (which owns said locks) moves the group
about. So by the time the former sys_perf_event_open() acquires the
locks, the context we've acquired is stale (and possibly dead).

Apply the established locking rules as per perf_event_ctx_lock_nested()
to the mutex_lock_double() for the 'move_group' case. This obviously means
we need to validate state after we acquire the locks.

Reported-by: Di Shen (Keen Lab)
Tested-by: John Dias <joaodias@google.com>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
Cc: Jiri Olsa <jolsa@redhat.com>
Cc: Kees Cook <keescook@chromium.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Min Chong <mchong@google.com>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Stephane Eranian <eranian@google.com>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Vince Weaver <vincent.weaver@maine.edu>
Fixes: f63a8daa5812 (""perf: Fix event->ctx locking"")
Link: http://lkml.kernel.org/r/20170106131444.GZ3174@twins.programming.kicks-ass.net
Signed-off-by: Ingo Molnar <mingo@kernel.org>",4,kernel/events/core.c,"{""sha"": ""cbc5937265da842bc714cbccf8091484caa5f536"", ""filename"": ""kernel/events/core.c"", ""status"": ""modified"", ""additions"": 54, ""deletions"": 4, ""changes"": 58, ""blob_url"": ""https://github.com/torvalds/linux/blob/321027c1fe77f892f4ea07846aeae08cefbbb290/kernel/events/core.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/321027c1fe77f892f4ea07846aeae08cefbbb290/kernel/events/core.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/kernel/events/core.c?ref=321027c1fe77f892f4ea07846aeae08cefbbb290"", ""patch"": ""@@ -9529,6 +9529,37 @@ static int perf_event_set_clock(struct perf_event *event, clockid_t clk_id)\n \treturn 0;\n }\n \n+/*\n+ * Variation on perf_event_ctx_lock_nested(), except we take two context\n+ * mutexes.\n+ */\n+static struct perf_event_context *\n+__perf_event_ctx_lock_double(struct perf_event *group_leader,\n+\t\t\t     struct perf_event_context *ctx)\n+{\n+\tstruct perf_event_context *gctx;\n+\n+again:\n+\trcu_read_lock();\n+\tgctx = READ_ONCE(group_leader->ctx);\n+\tif (!atomic_inc_not_zero(&gctx->refcount)) {\n+\t\trcu_read_unlock();\n+\t\tgoto again;\n+\t}\n+\trcu_read_unlock();\n+\n+\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\n+\tif (group_leader->ctx != gctx) {\n+\t\tmutex_unlock(&ctx->mutex);\n+\t\tmutex_unlock(&gctx->mutex);\n+\t\tput_ctx(gctx);\n+\t\tgoto again;\n+\t}\n+\n+\treturn gctx;\n+}\n+\n /**\n  * sys_perf_event_open - open a performance event, associate it to a task/cpu\n  *\n@@ -9772,12 +9803,31 @@ SYSCALL_DEFINE5(perf_event_open,\n \t}\n \n \tif (move_group) {\n-\t\tgctx = group_leader->ctx;\n-\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);\n+\n \t\tif (gctx->task == TASK_TOMBSTONE) {\n \t\t\terr = -ESRCH;\n \t\t\tgoto err_locked;\n \t\t}\n+\n+\t\t/*\n+\t\t * Check if we raced against another sys_perf_event_open() call\n+\t\t * moving the software group underneath us.\n+\t\t */\n+\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n+\t\t\t/*\n+\t\t\t * If someone moved the group out from under us, check\n+\t\t\t * if this new event wound up on the same ctx, if so\n+\t\t\t * its the regular !move_group case, otherwise fail.\n+\t\t\t */\n+\t\t\tif (gctx != ctx) {\n+\t\t\t\terr = -EINVAL;\n+\t\t\t\tgoto err_locked;\n+\t\t\t} else {\n+\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n+\t\t\t\tmove_group = 0;\n+\t\t\t}\n+\t\t}\n \t} else {\n \t\tmutex_lock(&ctx->mutex);\n \t}\n@@ -9879,7 +9929,7 @@ SYSCALL_DEFINE5(perf_event_open,\n \tperf_unpin_context(ctx);\n \n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n \n \tif (task) {\n@@ -9905,7 +9955,7 @@ SYSCALL_DEFINE5(perf_event_open,\n \n err_locked:\n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n /* err_file: */\n \tfput(event_file);""}","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = __perf_event_ctx_lock_double(group_leader, ctx);

 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}

		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
",C,"		gctx = __perf_event_ctx_lock_double(group_leader, ctx);


		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
		perf_event_ctx_unlock(group_leader, gctx);
		perf_event_ctx_unlock(group_leader, gctx);
","		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
		mutex_unlock(&gctx->mutex);
		mutex_unlock(&gctx->mutex);
",,"@@ -9529,6 +9529,37 @@ static int perf_event_set_clock(struct perf_event *event, clockid_t clk_id)
 	return 0;
 }
 
+/*
+ * Variation on perf_event_ctx_lock_nested(), except we take two context
+ * mutexes.
+ */
+static struct perf_event_context *
+__perf_event_ctx_lock_double(struct perf_event *group_leader,
+			     struct perf_event_context *ctx)
+{
+	struct perf_event_context *gctx;
+
+again:
+	rcu_read_lock();
+	gctx = READ_ONCE(group_leader->ctx);
+	if (!atomic_inc_not_zero(&gctx->refcount)) {
+		rcu_read_unlock();
+		goto again;
+	}
+	rcu_read_unlock();
+
+	mutex_lock_double(&gctx->mutex, &ctx->mutex);
+
+	if (group_leader->ctx != gctx) {
+		mutex_unlock(&ctx->mutex);
+		mutex_unlock(&gctx->mutex);
+		put_ctx(gctx);
+		goto again;
+	}
+
+	return gctx;
+}
+
 /**
  * sys_perf_event_open - open a performance event, associate it to a task/cpu
  *
@@ -9772,12 +9803,31 @@ SYSCALL_DEFINE5(perf_event_open,
 	}
 
 	if (move_group) {
-		gctx = group_leader->ctx;
-		mutex_lock_double(&gctx->mutex, &ctx->mutex);
+		gctx = __perf_event_ctx_lock_double(group_leader, ctx);
+
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
+
+		/*
+		 * Check if we raced against another sys_perf_event_open() call
+		 * moving the software group underneath us.
+		 */
+		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
+			/*
+			 * If someone moved the group out from under us, check
+			 * if this new event wound up on the same ctx, if so
+			 * its the regular !move_group case, otherwise fail.
+			 */
+			if (gctx != ctx) {
+				err = -EINVAL;
+				goto err_locked;
+			} else {
+				perf_event_ctx_unlock(group_leader, gctx);
+				move_group = 0;
+			}
+		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}
@@ -9879,7 +9929,7 @@ SYSCALL_DEFINE5(perf_event_open,
 	perf_unpin_context(ctx);
 
 	if (move_group)
-		mutex_unlock(&gctx->mutex);
+		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
@@ -9905,7 +9955,7 @@ SYSCALL_DEFINE5(perf_event_open,
 
 err_locked:
 	if (move_group)
-		mutex_unlock(&gctx->mutex);
+		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);",linux,321027c1fe77f892f4ea07846aeae08cefbbb290,63cae12bce9861cec309798d34701cf3da20bc71,1,"SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
//flaw_line_below:
		gctx = group_leader->ctx;
//flaw_line_below:
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
//fix_flaw_line_below:
//		gctx = __perf_event_ctx_lock_double(group_leader, ctx);
//fix_flaw_line_below:
//
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 * Check if we raced against another sys_perf_event_open() call
//fix_flaw_line_below:
//		 * moving the software group underneath us.
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
//fix_flaw_line_below:
//			/*
//fix_flaw_line_below:
//			 * If someone moved the group out from under us, check
//fix_flaw_line_below:
//			 * if this new event wound up on the same ctx, if so
//fix_flaw_line_below:
//			 * its the regular !move_group case, otherwise fail.
//fix_flaw_line_below:
//			 */
//fix_flaw_line_below:
//			if (gctx != ctx) {
//fix_flaw_line_below:
//				err = -EINVAL;
//fix_flaw_line_below:
//				goto err_locked;
//fix_flaw_line_below:
//			} else {
//fix_flaw_line_below:
//				perf_event_ctx_unlock(group_leader, gctx);
//fix_flaw_line_below:
//				move_group = 0;
//fix_flaw_line_below:
//			}
//fix_flaw_line_below:
//		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
//flaw_line_below:
		mutex_unlock(&gctx->mutex);
//fix_flaw_line_below:
//		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
//flaw_line_below:
		mutex_unlock(&gctx->mutex);
//fix_flaw_line_below:
//		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
",181539,"SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = group_leader->ctx;
		mutex_lock_double(&gctx->mutex, &ctx->mutex);
 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		mutex_unlock(&gctx->mutex);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
	struct perf_event *group_leader = NULL, *output_event = NULL;
	struct perf_event *event, *sibling;
	struct perf_event_attr attr;
	struct perf_event_context *ctx, *uninitialized_var(gctx);
	struct file *event_file = NULL;
	struct fd group = {NULL, 0};
	struct task_struct *task = NULL;
	struct pmu *pmu;
	int event_fd;
	int move_group = 0;
	int err;
	int f_flags = O_RDWR;
	int cgroup_fd = -1;

	/* for future expandability... */
	if (flags & ~PERF_FLAG_ALL)
		return -EINVAL;

	err = perf_copy_attr(attr_uptr, &attr);
	if (err)
		return err;

	if (!attr.exclude_kernel) {
		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
			return -EACCES;
	}

	if (attr.freq) {
		if (attr.sample_freq > sysctl_perf_event_sample_rate)
			return -EINVAL;
	} else {
		if (attr.sample_period & (1ULL << 63))
			return -EINVAL;
	}

	if (!attr.sample_max_stack)
		attr.sample_max_stack = sysctl_perf_event_max_stack;

	/*
	 * In cgroup mode, the pid argument is used to pass the fd
	 * opened to the cgroup directory in cgroupfs. The cpu argument
	 * designates the cpu on which to monitor threads from that
	 * cgroup.
	 */
	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
		return -EINVAL;

	if (flags & PERF_FLAG_FD_CLOEXEC)
		f_flags |= O_CLOEXEC;

	event_fd = get_unused_fd_flags(f_flags);
	if (event_fd < 0)
		return event_fd;

	if (group_fd != -1) {
		err = perf_fget_light(group_fd, &group);
		if (err)
			goto err_fd;
		group_leader = group.file->private_data;
		if (flags & PERF_FLAG_FD_OUTPUT)
			output_event = group_leader;
		if (flags & PERF_FLAG_FD_NO_GROUP)
			group_leader = NULL;
	}

	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}

	if (task && group_leader &&
	    group_leader->attr.inherit != attr.inherit) {
		err = -EINVAL;
		goto err_task;
	}

	get_online_cpus();

	if (task) {
		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
		if (err)
			goto err_cpus;

		/*
		 * Reuse ptrace permission checks for now.
		 *
		 * We must hold cred_guard_mutex across this and any potential
		 * perf_install_in_context() call for this new event to
		 * serialize against exec() altering our credentials (and the
		 * perf_event_exit_task() that could imply).
		 */
		err = -EACCES;
		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
			goto err_cred;
	}

	if (flags & PERF_FLAG_PID_CGROUP)
		cgroup_fd = pid;

	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
	if (IS_ERR(event)) {
		err = PTR_ERR(event);
		goto err_cred;
	}

	if (is_sampling_event(event)) {
		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
			err = -EOPNOTSUPP;
			goto err_alloc;
		}
	}

	/*
	 * Special case software events and allow them to be part of
	 * any hardware group.
	 */
	pmu = event->pmu;

	if (attr.use_clockid) {
		err = perf_event_set_clock(event, attr.clockid);
		if (err)
			goto err_alloc;
	}

	if (pmu->task_ctx_nr == perf_sw_context)
		event->event_caps |= PERF_EV_CAP_SOFTWARE;

	if (group_leader &&
	    (is_software_event(event) != is_software_event(group_leader))) {
		if (is_software_event(event)) {
			/*
			 * If event and group_leader are not both a software
			 * event, and event is, then group leader is not.
			 *
			 * Allow the addition of software events to !software
			 * groups, this is safe because software events never
			 * fail to schedule.
			 */
			pmu = group_leader->pmu;
		} else if (is_software_event(group_leader) &&
			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * In case the group is a pure software group, and we
			 * try to add a hardware event, move the whole group to
			 * the hardware context.
			 */
			move_group = 1;
		}
	}

	/*
	 * Get the target context (task or percpu):
	 */
	ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
		err = -EBUSY;
		goto err_context;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
		err = -EINVAL;

		/*
		 * Do not allow a recursive hierarchy (this new sibling
		 * becoming part of another group-sibling):
		 */
		if (group_leader->group_leader != group_leader)
			goto err_context;

		/* All events in a group should have the same clock */
		if (group_leader->clock != event->clock)
			goto err_context;

		/*
		 * Do not allow to attach to a group in a different
		 * task or CPU context:
		 */
		if (move_group) {
			/*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
			 */
			if (group_leader->ctx->task != ctx->task)
				goto err_context;

			/*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
			 */
			if (group_leader->cpu != event->cpu)
				goto err_context;
		} else {
			if (group_leader->ctx != ctx)
				goto err_context;
		}

		/*
		 * Only a group leader can be exclusive or pinned
		 */
		if (attr.exclusive || attr.pinned)
			goto err_context;
	}

	if (output_event) {
		err = perf_event_set_output(event, output_event);
		if (err)
			goto err_context;
	}

	event_file = anon_inode_getfile(""[perf_event]"", &perf_fops, event,
					f_flags);
	if (IS_ERR(event_file)) {
		err = PTR_ERR(event_file);
		event_file = NULL;
		goto err_context;
 	}
 
 	if (move_group) {
		gctx = __perf_event_ctx_lock_double(group_leader, ctx);

 		if (gctx->task == TASK_TOMBSTONE) {
 			err = -ESRCH;
 			goto err_locked;
 		}

		/*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
		 */
		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
			/*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
			 */
			if (gctx != ctx) {
				err = -EINVAL;
				goto err_locked;
			} else {
				perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
 	} else {
 		mutex_lock(&ctx->mutex);
 	}

	if (ctx->task == TASK_TOMBSTONE) {
		err = -ESRCH;
		goto err_locked;
	}

	if (!perf_event_validate_size(event)) {
		err = -E2BIG;
		goto err_locked;
	}

	/*
	 * Must be under the same ctx::mutex as perf_install_in_context(),
	 * because we need to serialize with concurrent event creation.
	 */
	if (!exclusive_event_installable(event, ctx)) {
		/* exclusive and group stuff are assumed mutually exclusive */
		WARN_ON_ONCE(move_group);

		err = -EBUSY;
		goto err_locked;
	}

	WARN_ON_ONCE(ctx->parent_ctx);

	/*
	 * This is the point on no return; we cannot fail hereafter. This is
	 * where we start modifying current state.
	 */

	if (move_group) {
		/*
		 * See perf_event_ctx_lock() for comments on the details
		 * of swizzling perf_event::ctx.
		 */
		perf_remove_from_context(group_leader, 0);

		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_remove_from_context(sibling, 0);
			put_ctx(gctx);
		}

		/*
		 * Wait for everybody to stop referencing the events through
		 * the old lists, before installing it on new lists.
		 */
		synchronize_rcu();

		/*
		 * Install the group siblings before the group leader.
		 *
		 * Because a group leader will try and install the entire group
		 * (through the sibling list, which is still in-tact), we can
		 * end up with siblings installed in the wrong context.
		 *
		 * By installing siblings first we NO-OP because they're not
		 * reachable through the group lists.
		 */
		list_for_each_entry(sibling, &group_leader->sibling_list,
				    group_entry) {
			perf_event__state_init(sibling);
			perf_install_in_context(ctx, sibling, sibling->cpu);
			get_ctx(ctx);
		}

		/*
		 * Removing from the context ends up with disabled
		 * event. What we want here is event in the initial
		 * startup state, ready to be add into new context.
		 */
		perf_event__state_init(group_leader);
		perf_install_in_context(ctx, group_leader, group_leader->cpu);
		get_ctx(ctx);

		/*
		 * Now that all events are installed in @ctx, nothing
		 * references @gctx anymore, so drop the last reference we have
		 * on it.
		 */
		put_ctx(gctx);
	}

	/*
	 * Precalculate sample_data sizes; do while holding ctx::mutex such
	 * that we're serialized against further additions and before
	 * perf_install_in_context() which is the point the event is active and
	 * can use these values.
	 */
	perf_event__header_size(event);
	perf_event__id_header_size(event);

	event->owner = current;

	perf_install_in_context(ctx, event, event->cpu);
 	perf_unpin_context(ctx);
 
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 
 	if (task) {
		mutex_unlock(&task->signal->cred_guard_mutex);
		put_task_struct(task);
	}

	put_online_cpus();

	mutex_lock(&current->perf_event_mutex);
	list_add_tail(&event->owner_entry, &current->perf_event_list);
	mutex_unlock(&current->perf_event_mutex);

	/*
	 * Drop the reference on the group_event after placing the
	 * new event on the sibling_list. This ensures destruction
	 * of the group leader will find the pointer to itself in
	 * perf_group_detach().
	 */
	fdput(group);
	fd_install(event_fd, event_file);
	return event_fd;
 
 err_locked:
 	if (move_group)
		perf_event_ctx_unlock(group_leader, gctx);
 	mutex_unlock(&ctx->mutex);
 /* err_file: */
 	fput(event_file);
err_context:
	perf_unpin_context(ctx);
	put_ctx(ctx);
err_alloc:
	/*
	 * If event_file is set, the fput() above will have called ->release()
	 * and that will take care of freeing the event.
	 */
	if (!event_file)
		free_event(event);
err_cred:
	if (task)
		mutex_unlock(&task->signal->cred_guard_mutex);
err_cpus:
	put_online_cpus();
err_task:
	if (task)
		put_task_struct(task);
err_group_fd:
	fdput(group);
err_fd:
	put_unused_fd(event_fd);
	return err;
}
","The provided code implements the `perf_event_open` system call, which is used to create and configure performance monitoring events in a Linux environment. It allows users to monitor various performance metrics of processes or CPUs by specifying attributes such as event type, sampling frequency, and target process. The function validates input parameters, manages permissions, and handles event grouping. It allocates a new performance event, associates it with a specified task or CPU, and ensures proper context management. If successful, it returns a file descriptor for the newly created event, enabling further interaction with the performance monitoring system.","The code implements a system call that facilitates the creation and configuration of performance monitoring events in a Linux environment. It allows users to specify various attributes for monitoring, such as the type of event and sampling frequency, while ensuring that permissions and input parameters are validated. The function manages the association of events with specific processes or CPUs and handles grouping of events when necessary. Upon successful execution, it returns a file descriptor that enables further interaction with the performance monitoring system, allowing users to track and analyze system performance metrics effectively.","The vulnerability in the original code stemmed from a race condition between concurrent calls to the performance event open system call, where both attempts could move the same software group into a hardware context. This situation could lead to one call acquiring stale context information while waiting for a mutex lock, resulting in potential inconsistencies or crashes. The fix involved applying established locking rules to the mutex acquisition for the ""move_group"" case, ensuring that the state is validated after acquiring the locks. This change prevents the race condition by ensuring that the context remains valid throughout the operation.","The vulnerability in the original code arose from a race condition between concurrent calls to a system function that attempted to move the same software group into a hardware context. This situation could lead to one call acquiring outdated context information while waiting for a lock, resulting in inconsistencies or crashes. The fix involved implementing established locking rules to ensure that the state is validated after acquiring the necessary locks. This adjustment prevents the race condition by ensuring that the context remains valid throughout the operation, thereby maintaining the integrity of the performance monitoring system.","The vulnerability was fixed by applying established locking rules to the `mutex_lock_double()` function for the ""move_group"" case, ensuring that the context remains valid during concurrent operations. Specifically, the code now validates the state after acquiring the locks, which prevents one call from proceeding with stale context information while another call modifies the group. Additionally, the use of `perf_event_ctx_lock_nested()` ensures that the locking mechanism is correctly applied, thereby maintaining the integrity of the event context and preventing potential inconsistencies or crashes during concurrent executions of the system call.","The vulnerability was addressed by implementing stricter locking mechanisms to ensure that the context remains valid during concurrent operations. The fix involved applying established locking rules to prevent one operation from acquiring outdated context information while another modifies the group. By validating the state after acquiring the necessary locks, the code ensures that any changes to the context are properly synchronized, thereby eliminating the race condition. This adjustment enhances the overall stability and integrity of the performance monitoring system, preventing inconsistencies or crashes that could arise from concurrent access."
28,182360,182360,,Local,Not required,Complete,CVE-2018-12633,https://www.cvedetails.com/cve/CVE-2018-12633/,CWE-362,Medium,Complete,,,2018-06-21,6.3,"An issue was discovered in the Linux kernel through 4.17.2. vbg_misc_device_ioctl() in drivers/virt/vboxguest/vboxguest_linux.c reads the same user data twice with copy_from_user. The header part of the user data is double-fetched, and a malicious user thread can tamper with the critical variables (hdr.size_in and hdr.size_out) in the header between the two fetches because of a race condition, leading to severe kernel errors, such as buffer over-accesses. This bug can cause a local denial of service and information leakage.",2018-08-21,DoS +Info ,3,https://github.com/torvalds/linux/commit/bd23a7269834dc7c1f93e83535d16ebc44b75eba,bd23a7269834dc7c1f93e83535d16ebc44b75eba,"virt: vbox: Only copy_from_user the request-header once

In vbg_misc_device_ioctl(), the header of the ioctl argument is copied from
the userspace pointer 'arg' and saved to the kernel object 'hdr'. Then the
'version', 'size_in', and 'size_out' fields of 'hdr' are verified.

Before this commit, after the checks a buffer for the entire request would
be allocated and then all data including the verified header would be
copied from the userspace 'arg' pointer again.

Given that the 'arg' pointer resides in userspace, a malicious userspace
process can race to change the data pointed to by 'arg' between the two
copies. By doing so, the user can bypass the verifications on the ioctl
argument.

This commit fixes this by using the already checked copy of the header
to fill the header part of the allocated buffer and only copying the
remainder of the data from userspace.

Signed-off-by: Wenwen Wang <wang6495@umn.edu>
Reviewed-by: Hans de Goede <hdegoede@redhat.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",1,drivers/virt/vboxguest/vboxguest_linux.c,"{""sha"": ""6e2a9619192d2317f8f449fbb5f9c24d0699e3f8"", ""filename"": ""drivers/virt/vboxguest/vboxguest_linux.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 1, ""changes"": 4, ""blob_url"": ""https://github.com/torvalds/linux/blob/bd23a7269834dc7c1f93e83535d16ebc44b75eba/drivers/virt/vboxguest/vboxguest_linux.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/bd23a7269834dc7c1f93e83535d16ebc44b75eba/drivers/virt/vboxguest/vboxguest_linux.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/virt/vboxguest/vboxguest_linux.c?ref=bd23a7269834dc7c1f93e83535d16ebc44b75eba"", ""patch"": ""@@ -121,7 +121,9 @@ static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,\n \tif (!buf)\n \t\treturn -ENOMEM;\n \n-\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {\n+\t*((struct vbg_ioctl_hdr *)buf) = hdr;\n+\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),\n+\t\t\t   hdr.size_in - sizeof(hdr))) {\n \t\tret = -EFAULT;\n \t\tgoto out;\n \t}""}","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
",C,"	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
","	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
",,"@@ -121,7 +121,9 @@ static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
 	if (!buf)
 		return -ENOMEM;
 
-	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
+	*((struct vbg_ioctl_hdr *)buf) = hdr;
+	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
+			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}",linux,bd23a7269834dc7c1f93e83535d16ebc44b75eba,bdeeed098811b36d1f988521600a89a400830a4b,1,"static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
//flaw_line_below:
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
//fix_flaw_line_below:
//	*((struct vbg_ioctl_hdr *)buf) = hdr;
//fix_flaw_line_below:
//	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
//fix_flaw_line_below:
//			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
",182360,"static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	if (copy_from_user(buf, (void *)arg, hdr.size_in)) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,
				  unsigned long arg)
{
	struct vbg_session *session = filp->private_data;
	size_t returned_size, size;
	struct vbg_ioctl_hdr hdr;
	bool is_vmmdev_req;
	int ret = 0;
	void *buf;

	if (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))
		return -EFAULT;

	if (hdr.version != VBG_IOCTL_HDR_VERSION)
		return -EINVAL;

	if (hdr.size_in < sizeof(hdr) ||
	    (hdr.size_out && hdr.size_out < sizeof(hdr)))
		return -EINVAL;

	size = max(hdr.size_in, hdr.size_out);
	if (_IOC_SIZE(req) && _IOC_SIZE(req) != size)
		return -EINVAL;
	if (size > SZ_16M)
		return -E2BIG;

	/*
	 * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid
	 * the need for a bounce-buffer and another copy later on.
	 */
	is_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||
			 req == VBG_IOCTL_VMMDEV_REQUEST_BIG;

	if (is_vmmdev_req)
		buf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);
	else
		buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
	*((struct vbg_ioctl_hdr *)buf) = hdr;
	if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
 		ret = -EFAULT;
 		goto out;
 	}
	if (hdr.size_in < size)
		memset(buf + hdr.size_in, 0, size -  hdr.size_in);

	ret = vbg_core_ioctl(session, req, buf);
	if (ret)
		goto out;

	returned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;
	if (returned_size > size) {
		vbg_debug(""%s: too much output data %zu > %zu\n"",
			  __func__, returned_size, size);
		returned_size = size;
	}
	if (copy_to_user((void *)arg, buf, returned_size) != 0)
		ret = -EFAULT;

out:
	if (is_vmmdev_req)
		vbg_req_free(buf, size);
	else
		kfree(buf);

	return ret;
}
","The `vbg_misc_device_ioctl` function handles IOCTL (Input/Output Control) requests for a virtual device in a Linux kernel module. It begins by validating the input data structure, ensuring the version and sizes are correct. The function allocates a buffer for the request, either using a specialized allocation for VMMDEV requests or standard kernel memory allocation. It then copies data from user space to the kernel buffer, processes the request through `vbg_core_ioctl`, and retrieves the output size. Finally, it copies the response back to user space, managing memory cleanup and error handling throughout the process.","The provided code implements a function that processes IOCTL requests for a virtual device in a kernel module. It begins by validating the input data structure to ensure compatibility and correct sizes. The function allocates a buffer for the request, using a specific allocation method for certain types of requests. It then copies data from user space to the allocated buffer, processes the request, and retrieves the output size. Finally, it copies the response back to user space while managing memory cleanup and handling any errors that may arise during the process.","The vulnerability in the original code stemmed from copying the IOCTL header from user space into a kernel structure and then subsequently copying the entire request data again from the same user space pointer. This allowed a malicious user process to modify the data at the user space address between the two copy operations, potentially bypassing the earlier validation checks on the header fields. The fix implemented in the updated code involves using the already validated header to populate the corresponding part of the allocated buffer, thus ensuring that only the verified data is used, while copying the remaining data from user space afterward, mitigating the race condition.","The vulnerability in the original code arose from the practice of copying data from a user space pointer into a kernel structure and then copying the entire request data again from the same pointer. This created a window of opportunity for a malicious user process to alter the data between the two copy operations, potentially allowing it to bypass the earlier validation checks. The fix implemented in the updated code involves using the already validated header to populate the corresponding part of the allocated buffer, ensuring that only the verified data is utilized, while the remaining data is copied afterward, effectively eliminating the race condition.","The vulnerability was fixed by modifying the way the header is handled in the code. Instead of copying the entire request data from the user space pointer again after validation, the updated code uses the already checked header to fill the header part of the allocated buffer. Specifically, the line that assigns the header to the buffer ensures that the validated data is used directly, while the subsequent copy operation only transfers the remaining data from user space. This change effectively prevents a malicious user process from altering the data between the two copy operations, thereby closing the race condition.","The vulnerability was fixed by changing the approach to handling the IOCTL request header. Instead of copying the entire request data from the user space pointer after the initial validation, the updated code utilizes the already validated header to populate the corresponding section of the allocated buffer. This ensures that the data used for processing is consistent and secure, as it eliminates the opportunity for a malicious user process to modify the data between the two copy operations. Consequently, the fix effectively mitigates the risk of bypassing validation checks and enhances the overall security of the function."
29,182375,182375,,Remote,Not required,Complete,CVE-2018-12232,https://www.cvedetails.com/cve/CVE-2018-12232/,CWE-362,Medium,,,,2018-06-12,7.1,"In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",2018-10-31,,1,https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14,6d8c50dcb029872b298eea68cc6209c866fd3e14,"socket: close race condition between sock_close() and sockfs_setattr()

fchownat() doesn't even hold refcnt of fd until it figures out
fd is really needed (otherwise is ignored) and releases it after
it resolves the path. This means sock_close() could race with
sockfs_setattr(), which leads to a NULL pointer dereference
since typically we set sock->sk to NULL in ->release().

As pointed out by Al, this is unique to sockfs. So we can fix this
in socket layer by acquiring inode_lock in sock_close() and
checking against NULL in sockfs_setattr().

sock_release() is called in many places, only the sock_close()
path matters here. And fortunately, this should not affect normal
sock_close() as it is only called when the last fd refcnt is gone.
It only affects sock_close() with a parallel sockfs_setattr() in
progress, which is not common.

Fixes: 86741ec25462 (""net: core: Add a UID field to struct sock."")
Reported-by: shankarapailoor <shankarapailoor@gmail.com>
Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
Cc: Lorenzo Colitti <lorenzo@google.com>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/socket.c,"{""sha"": ""8a109012608a6132a65293c86cd175426b851cbe"", ""filename"": ""net/socket.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 3, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/socket.c?ref=6d8c50dcb029872b298eea68cc6209c866fd3e14"", ""patch"": ""@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n \tif (!err && (iattr->ia_valid & ATTR_UID)) {\n \t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n \n-\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\tif (sock->sk)\n+\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\telse\n+\t\t\terr = -ENOENT;\n \t}\n \n \treturn err;\n@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);\n  *\tan inode not a file.\n  */\n \n-void sock_release(struct socket *sock)\n+static void __sock_release(struct socket *sock, struct inode *inode)\n {\n \tif (sock->ops) {\n \t\tstruct module *owner = sock->ops->owner;\n \n+\t\tif (inode)\n+\t\t\tinode_lock(inode);\n \t\tsock->ops->release(sock);\n+\t\tif (inode)\n+\t\t\tinode_unlock(inode);\n \t\tsock->ops = NULL;\n \t\tmodule_put(owner);\n \t}\n@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)\n \t}\n \tsock->file = NULL;\n }\n+\n+void sock_release(struct socket *sock)\n+{\n+\t__sock_release(sock, NULL);\n+}\n EXPORT_SYMBOL(sock_release);\n \n void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)\n@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)\n \n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }\n ""}"," static int sock_close(struct inode *inode, struct file *filp)
 {
	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
"," static int sock_close(struct inode *inode, struct file *filp)
 {
	sock_release(SOCKET_I(inode));
 	return 0;
 }
",C,"	__sock_release(SOCKET_I(inode), inode);
","	sock_release(SOCKET_I(inode));
",,"@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
-		sock->sk->sk_uid = iattr->ia_uid;
+		if (sock->sk)
+			sock->sk->sk_uid = iattr->ia_uid;
+		else
+			err = -ENOENT;
 	}
 
 	return err;
@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);
  *	an inode not a file.
  */
 
-void sock_release(struct socket *sock)
+static void __sock_release(struct socket *sock, struct inode *inode)
 {
 	if (sock->ops) {
 		struct module *owner = sock->ops->owner;
 
+		if (inode)
+			inode_lock(inode);
 		sock->ops->release(sock);
+		if (inode)
+			inode_unlock(inode);
 		sock->ops = NULL;
 		module_put(owner);
 	}
@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)
 	}
 	sock->file = NULL;
 }
+
+void sock_release(struct socket *sock)
+{
+	__sock_release(sock, NULL);
+}
 EXPORT_SYMBOL(sock_release);
 
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)
@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)
 
 static int sock_close(struct inode *inode, struct file *filp)
 {
-	sock_release(SOCKET_I(inode));
+	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
 ",linux,6d8c50dcb029872b298eea68cc6209c866fd3e14,873aca2ee86e533665a73292c4e308ded1e9bafe,1," static int sock_close(struct inode *inode, struct file *filp)
 {
//flaw_line_below:
	sock_release(SOCKET_I(inode));
//fix_flaw_line_below:
//	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
",182375," static int sock_close(struct inode *inode, struct file *filp)
 {
	sock_release(SOCKET_I(inode));
 	return 0;
 }
"," static int sock_close(struct inode *inode, struct file *filp)
 {
	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
","The provided code defines a function `sock_close`, which is responsible for closing a socket in a Linux kernel module. It takes two parameters: a pointer to an `inode` structure and a pointer to a `file` structure. The function retrieves the socket associated with the given inode using the `SOCKET_I` macro and then calls `sock_release` to properly release the socket resources. This ensures that any allocated memory and associated data structures are cleaned up, preventing memory leaks. The function returns 0, indicating successful completion of the socket closure operation.","The code defines a function that handles the closure of a socket within a kernel module. It takes two parameters related to the file and its associated inode. The function retrieves the socket linked to the inode and invokes a method to release the resources allocated to that socket. This process ensures that all memory and data structures associated with the socket are properly cleaned up, thereby preventing potential memory leaks. The function concludes by returning a value that indicates the successful completion of the socket closure operation.","The vulnerability in the original code arises from a race condition between the `sock_close()` function and `sockfs_setattr()`, which can lead to a NULL pointer dereference. This occurs because the reference count for the file descriptor is not held during the execution of `sock_close()`, allowing it to potentially access a socket that has already been released, resulting in the socket's `sk` pointer being set to NULL. The fix involves using `__sock_release()` instead of `sock_release()`, along with acquiring `inode_lock` in `sock_close()` to ensure that the socket remains valid during the closure process, thus preventing the race condition.","The vulnerability in the original code stems from a race condition that occurs when closing a socket while another operation is modifying its attributes. This situation can lead to a NULL pointer dereference, as the socket may be released and its associated data structure modified concurrently, resulting in invalid access. The fix addresses this issue by implementing a locking mechanism to ensure that the socket remains valid during the closure process. By acquiring a lock before performing the closure, the code prevents simultaneous modifications, thereby eliminating the race condition and ensuring safe access to the socket's resources.","The vulnerability was fixed by modifying the `sock_close()` function to acquire `inode_lock` before executing the closure process. This ensures that the socket remains valid while the function checks against NULL in `sockfs_setattr()`, preventing a race condition between these two operations. Additionally, the code now uses `__sock_release()` instead of `sock_release()`, which allows for a more controlled release of the socket resources. This change ensures that the socket's state is consistent and prevents the possibility of dereferencing a NULL pointer, thereby enhancing the overall safety and stability of the socket management process.","The vulnerability was addressed by implementing a locking mechanism to ensure that the socket remains valid during the closure process. By acquiring a lock before executing the closure, the code prevents simultaneous modifications that could lead to a race condition. This change allows the function to safely check the socket's state and ensures that it does not access any invalid or NULL pointers. Additionally, the release process was refined to provide better control over resource management, ultimately enhancing the stability and safety of socket operations in the presence of concurrent attribute modifications."
30,182377,182377,,Remote,Not required,Complete,CVE-2018-12232,https://www.cvedetails.com/cve/CVE-2018-12232/,CWE-362,Medium,,,,2018-06-12,7.1,"In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",2018-10-31,,4,https://github.com/torvalds/linux/commit/6d8c50dcb029872b298eea68cc6209c866fd3e14,6d8c50dcb029872b298eea68cc6209c866fd3e14,"socket: close race condition between sock_close() and sockfs_setattr()

fchownat() doesn't even hold refcnt of fd until it figures out
fd is really needed (otherwise is ignored) and releases it after
it resolves the path. This means sock_close() could race with
sockfs_setattr(), which leads to a NULL pointer dereference
since typically we set sock->sk to NULL in ->release().

As pointed out by Al, this is unique to sockfs. So we can fix this
in socket layer by acquiring inode_lock in sock_close() and
checking against NULL in sockfs_setattr().

sock_release() is called in many places, only the sock_close()
path matters here. And fortunately, this should not affect normal
sock_close() as it is only called when the last fd refcnt is gone.
It only affects sock_close() with a parallel sockfs_setattr() in
progress, which is not common.

Fixes: 86741ec25462 (""net: core: Add a UID field to struct sock."")
Reported-by: shankarapailoor <shankarapailoor@gmail.com>
Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
Cc: Lorenzo Colitti <lorenzo@google.com>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/socket.c,"{""sha"": ""8a109012608a6132a65293c86cd175426b851cbe"", ""filename"": ""net/socket.c"", ""status"": ""modified"", ""additions"": 15, ""deletions"": 3, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/6d8c50dcb029872b298eea68cc6209c866fd3e14/net/socket.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/socket.c?ref=6d8c50dcb029872b298eea68cc6209c866fd3e14"", ""patch"": ""@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n \tif (!err && (iattr->ia_valid & ATTR_UID)) {\n \t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n \n-\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\tif (sock->sk)\n+\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\telse\n+\t\t\terr = -ENOENT;\n \t}\n \n \treturn err;\n@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);\n  *\tan inode not a file.\n  */\n \n-void sock_release(struct socket *sock)\n+static void __sock_release(struct socket *sock, struct inode *inode)\n {\n \tif (sock->ops) {\n \t\tstruct module *owner = sock->ops->owner;\n \n+\t\tif (inode)\n+\t\t\tinode_lock(inode);\n \t\tsock->ops->release(sock);\n+\t\tif (inode)\n+\t\t\tinode_unlock(inode);\n \t\tsock->ops = NULL;\n \t\tmodule_put(owner);\n \t}\n@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)\n \t}\n \tsock->file = NULL;\n }\n+\n+void sock_release(struct socket *sock)\n+{\n+\t__sock_release(sock, NULL);\n+}\n EXPORT_SYMBOL(sock_release);\n \n void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)\n@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)\n \n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }\n ""}","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
 	}
 
 	return err;
}
","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		sock->sk->sk_uid = iattr->ia_uid;
 	}
 
 	return err;
}
",C,"		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
","		sock->sk->sk_uid = iattr->ia_uid;
",,"@@ -541,7 +541,10 @@ static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
-		sock->sk->sk_uid = iattr->ia_uid;
+		if (sock->sk)
+			sock->sk->sk_uid = iattr->ia_uid;
+		else
+			err = -ENOENT;
 	}
 
 	return err;
@@ -590,12 +593,16 @@ EXPORT_SYMBOL(sock_alloc);
  *	an inode not a file.
  */
 
-void sock_release(struct socket *sock)
+static void __sock_release(struct socket *sock, struct inode *inode)
 {
 	if (sock->ops) {
 		struct module *owner = sock->ops->owner;
 
+		if (inode)
+			inode_lock(inode);
 		sock->ops->release(sock);
+		if (inode)
+			inode_unlock(inode);
 		sock->ops = NULL;
 		module_put(owner);
 	}
@@ -609,6 +616,11 @@ void sock_release(struct socket *sock)
 	}
 	sock->file = NULL;
 }
+
+void sock_release(struct socket *sock)
+{
+	__sock_release(sock, NULL);
+}
 EXPORT_SYMBOL(sock_release);
 
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)
@@ -1171,7 +1183,7 @@ static int sock_mmap(struct file *file, struct vm_area_struct *vma)
 
 static int sock_close(struct inode *inode, struct file *filp)
 {
-	sock_release(SOCKET_I(inode));
+	__sock_release(SOCKET_I(inode), inode);
 	return 0;
 }
 ",linux,6d8c50dcb029872b298eea68cc6209c866fd3e14,873aca2ee86e533665a73292c4e308ded1e9bafe,1,"static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
//flaw_line_below:
		sock->sk->sk_uid = iattr->ia_uid;
//fix_flaw_line_below:
//		if (sock->sk)
//fix_flaw_line_below:
//			sock->sk->sk_uid = iattr->ia_uid;
//fix_flaw_line_below:
//		else
//fix_flaw_line_below:
//			err = -ENOENT;
 	}
 
 	return err;
}
",182377,"static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		sock->sk->sk_uid = iattr->ia_uid;
 	}
 
 	return err;
}
","static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)
{
	int err = simple_setattr(dentry, iattr);

 	if (!err && (iattr->ia_valid & ATTR_UID)) {
 		struct socket *sock = SOCKET_I(d_inode(dentry));
 
		if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
		else
			err = -ENOENT;
 	}
 
 	return err;
}
","The `sockfs_setattr` function is designed to update the attributes of a socket file in a Linux filesystem. It first calls `simple_setattr` to handle general attribute changes based on the provided `iattr` structure. If the operation is successful and the user ID attribute (`ATTR_UID`) is specified for modification, it retrieves the socket associated with the given `dentry` and updates its user ID (`sk_uid`) to the new value from `iattr`. The function ultimately returns any error code from the attribute setting process, ensuring that both general and specific updates are managed appropriately.","The provided code defines a function that updates the attributes of a socket file within a filesystem. It begins by attempting to set general attributes based on the input parameters. If this initial operation is successful and a specific user ID attribute is included for modification, the function retrieves the associated socket and updates its user ID accordingly. The function ensures that both general and specific attribute changes are processed, ultimately returning any error encountered during the attribute update process, thereby maintaining the integrity of the socket's metadata.","The vulnerability in the original code arises from a race condition between the `sock_close()` and `sockfs_setattr()` functions, which could lead to a NULL pointer dereference. Specifically, if `sock_close()` is executed and sets the socket's `sk` pointer to NULL while `sockfs_setattr()` is trying to access it, this results in an attempt to dereference a NULL pointer. The fix addresses this issue by adding a check to ensure that the `sk` pointer is not NULL before attempting to update the user ID. Additionally, it acquires a lock during the `sock_close()` operation to prevent concurrent modifications, thus ensuring safer access to the socket's attributes.","The vulnerability in the original code stems from a race condition that occurs when two operations, one closing a socket and the other modifying its attributes, are executed concurrently. This situation can lead to a NULL pointer dereference if the socket is closed and its associated data structure is modified simultaneously, resulting in an attempt to access a non-existent pointer. The fix addresses this issue by implementing a check to ensure that the pointer is valid before any modifications are made. Additionally, a locking mechanism is introduced during the socket closing process to prevent concurrent access, thereby enhancing the safety and stability of the operations.","The vulnerability was fixed by adding a check for the validity of the socket's `sk` pointer in the `sockfs_setattr()` function before attempting to update the user ID. Specifically, the code now verifies that `sock->sk` is not NULL, preventing a NULL pointer dereference if the socket has been closed concurrently. Additionally, the fix involves acquiring the `inode_lock` in the `sock_close()` function to ensure that the socket's state remains consistent during the closing process. This locking mechanism prevents simultaneous modifications, thereby safeguarding against the race condition that previously existed between `sock_close()` and `sockfs_setattr()`.","The vulnerability was addressed by implementing a validation check to ensure that the pointer associated with the socket is not NULL before any modifications are made. This prevents the possibility of dereferencing a NULL pointer, which could occur if the socket is closed while its attributes are being modified. Additionally, a locking mechanism was introduced during the socket closing process to prevent concurrent access to the socket's state. This ensures that the operations are executed in a controlled manner, effectively eliminating the race condition that could lead to instability and crashes in the system."
31,182442,182442,,Local,Not required,Complete,CVE-2018-8897,https://www.cvedetails.com/cve/CVE-2018-8897/,CWE-362,Low,Complete,Complete,,2018-05-08,7.2,"A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs.",2019-10-02,,8,https://github.com/torvalds/linux/commit/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,"x86/entry/64: Don't use IST entry for #BP stack

There's nothing IST-worthy about #BP/int3.  We don't allow kprobes
in the small handful of places in the kernel that run at CPL0 with
an invalid stack, and 32-bit kernels have used normal interrupt
gates for #BP forever.

Furthermore, we don't allow kprobes in places that have usergs while
in kernel mode, so ""paranoid"" is also unnecessary.

Signed-off-by: Andy Lutomirski <luto@kernel.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Cc: stable@vger.kernel.org",6,arch/x86/kernel/traps.c,"{""sha"": ""9b114675fbc05e1f589e6838f4603de871c0565d"", ""filename"": ""arch/x86/entry/entry_64.S"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 1, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/entry/entry_64.S"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/entry/entry_64.S"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/entry/entry_64.S?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -1138,7 +1138,7 @@ apicinterrupt3 HYPERV_REENLIGHTENMENT_VECTOR \\\n #endif /* CONFIG_HYPERV */\n \n idtentry debug\t\t\tdo_debug\t\thas_error_code=0\tparanoid=1 shift_ist=DEBUG_STACK\n-idtentry int3\t\t\tdo_int3\t\t\thas_error_code=0\tparanoid=1 shift_ist=DEBUG_STACK\n+idtentry int3\t\t\tdo_int3\t\t\thas_error_code=0\n idtentry stack_segment\t\tdo_stack_segment\thas_error_code=1\n \n #ifdef CONFIG_XEN""}<_**next**_>{""sha"": ""50bee5fe114013622ee858e4a3b7b16d1ffa8a05"", ""filename"": ""arch/x86/kernel/idt.c"", ""status"": ""modified"", ""additions"": 0, ""deletions"": 2, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/idt.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/idt.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kernel/idt.c?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -160,7 +160,6 @@ static const __initconst struct idt_data early_pf_idts[] = {\n  */\n static const __initconst struct idt_data dbg_idts[] = {\n \tINTG(X86_TRAP_DB,\tdebug),\n-\tINTG(X86_TRAP_BP,\tint3),\n };\n #endif\n \n@@ -183,7 +182,6 @@ gate_desc debug_idt_table[IDT_ENTRIES] __page_aligned_bss;\n static const __initconst struct idt_data ist_idts[] = {\n \tISTG(X86_TRAP_DB,\tdebug,\t\tDEBUG_STACK),\n \tISTG(X86_TRAP_NMI,\tnmi,\t\tNMI_STACK),\n-\tSISTG(X86_TRAP_BP,\tint3,\t\tDEBUG_STACK),\n \tISTG(X86_TRAP_DF,\tdouble_fault,\tDOUBLEFAULT_STACK),\n #ifdef CONFIG_X86_MCE\n \tISTG(X86_TRAP_MC,\t&machine_check,\tMCE_STACK),""}<_**next**_>{""sha"": ""03f3d7695daccae1af7265f2aab221da81573ce5"", ""filename"": ""arch/x86/kernel/traps.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 7, ""changes"": 15, ""blob_url"": ""https://github.com/torvalds/linux/blob/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/traps.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9/arch/x86/kernel/traps.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/arch/x86/kernel/traps.c?ref=d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9"", ""patch"": ""@@ -577,7 +577,6 @@ do_general_protection(struct pt_regs *regs, long error_code)\n }\n NOKPROBE_SYMBOL(do_general_protection);\n \n-/* May run on IST stack. */\n dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n {\n #ifdef CONFIG_DYNAMIC_FTRACE\n@@ -592,6 +591,13 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n \tif (poke_int3_handler(regs))\n \t\treturn;\n \n+\t/*\n+\t * Use ist_enter despite the fact that we don't use an IST stack.\n+\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel\n+\t * mode or even during context tracking state changes.\n+\t *\n+\t * This means that we can't schedule.  That's okay.\n+\t */\n \tist_enter(regs);\n \tRCU_LOCKDEP_WARN(!rcu_is_watching(), \""entry code didn't wake RCU\"");\n #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n@@ -609,15 +615,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n \t\t\tSIGTRAP) == NOTIFY_STOP)\n \t\tgoto exit;\n \n-\t/*\n-\t * Let others (NMI) know that the debug stack is in use\n-\t * as we may switch to the interrupt stack.\n-\t */\n-\tdebug_stack_usage_inc();\n \tcond_local_irq_enable(regs);\n \tdo_trap(X86_TRAP_BP, SIGTRAP, \""int3\"", regs, error_code, NULL);\n \tcond_local_irq_disable(regs);\n-\tdebug_stack_usage_dec();\n+\n exit:\n \tist_exit(regs);\n }""}"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);

 exit:
 	ist_exit(regs);
 }
"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
	debug_stack_usage_dec();
 exit:
 	ist_exit(regs);
 }
",C,"	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */

","	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
	debug_stack_usage_dec();
",,"@@ -577,7 +577,6 @@ do_general_protection(struct pt_regs *regs, long error_code)
 }
 NOKPROBE_SYMBOL(do_general_protection);
 
-/* May run on IST stack. */
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -592,6 +591,13 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	if (poke_int3_handler(regs))
 		return;
 
+	/*
+	 * Use ist_enter despite the fact that we don't use an IST stack.
+	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
+	 * mode or even during context tracking state changes.
+	 *
+	 * This means that we can't schedule.  That's okay.
+	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
@@ -609,15 +615,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
-	/*
-	 * Let others (NMI) know that the debug stack is in use
-	 * as we may switch to the interrupt stack.
-	 */
-	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
-	debug_stack_usage_dec();
+
 exit:
 	ist_exit(regs);
 }",linux,d8ba61ba58c88d5207c1ba2f7d9a2280e7d03be9,06ace26f4e6fcf747e890a39193be811777a048a,1," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * Use ist_enter despite the fact that we don't use an IST stack.
//fix_flaw_line_below:
//	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
//fix_flaw_line_below:
//	 * mode or even during context tracking state changes.
//fix_flaw_line_below:
//	 *
//fix_flaw_line_below:
//	 * This means that we can't schedule.  That's okay.
//fix_flaw_line_below:
//	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
//flaw_line_below:
	/*
//flaw_line_below:
	 * Let others (NMI) know that the debug stack is in use
//flaw_line_below:
	 * as we may switch to the interrupt stack.
//flaw_line_below:
	 */
//flaw_line_below:
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
//flaw_line_below:
	debug_stack_usage_dec();
//fix_flaw_line_below:
//
 exit:
 	ist_exit(regs);
 }
",182442," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
	/*
	 * Let others (NMI) know that the debug stack is in use
	 * as we may switch to the interrupt stack.
	 */
	debug_stack_usage_inc();
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);
	debug_stack_usage_dec();
 exit:
 	ist_exit(regs);
 }
"," dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
 #ifdef CONFIG_DYNAMIC_FTRACE
	/*
	 * ftrace must be first, everything else may cause a recursive crash.
	 * See note by declaration of modifying_ftrace_code in ftrace.c
	 */
	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
	    ftrace_int3_handler(regs))
		return;
#endif
 	if (poke_int3_handler(regs))
 		return;
 
	/*
	 * Use ist_enter despite the fact that we don't use an IST stack.
	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
	 * mode or even during context tracking state changes.
	 *
	 * This means that we can't schedule.  That's okay.
	 */
 	ist_enter(regs);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), ""entry code didn't wake RCU"");
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
	if (kgdb_ll_trap(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
				SIGTRAP) == NOTIFY_STOP)
		goto exit;
#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */

#ifdef CONFIG_KPROBES
	if (kprobe_int3_handler(regs))
		goto exit;
#endif

	if (notify_die(DIE_INT3, ""int3"", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
 		goto exit;
 
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, ""int3"", regs, error_code, NULL);
 	cond_local_irq_disable(regs);

 exit:
 	ist_exit(regs);
 }
","The provided code defines a function `do_int3`, which handles the INT3 (breakpoint) exception in a kernel environment. It first checks if dynamic ftrace is active and handles it accordingly to prevent recursive crashes. The function then attempts to invoke various handlers, such as `poke_int3_handler`, `kgdb_ll_trap`, and `kprobe_int3_handler`, to manage the breakpoint. If none of these handlers stop the process, it notifies the system of the INT3 event, increments the debug stack usage, and processes the trap while managing local interrupts. Finally, it ensures proper exit from the interrupt service routine.","The code defines a function that manages the handling of a specific type of exception related to breakpoints in a kernel environment. It begins by checking for certain conditions to prevent recursive issues during execution. The function then attempts to invoke various handlers to address the exception, and if none are successful, it notifies the system of the occurrence. Additionally, it tracks the usage of a debug stack and manages the enabling and disabling of local interrupts during the process. Finally, it ensures a proper exit from the interrupt service routine, maintaining system stability throughout.","The vulnerability in the original code stemmed from the inappropriate use of an Interrupt Stack Table (IST) entry for handling breakpoint exceptions, which could lead to instability when invoked from certain contexts, such as kprobes or during context tracking state changes. This misuse could potentially allow for recursive crashes or improper handling of the exception. The fix involved removing the IST entry usage and ensuring that the function could safely handle breakpoints without relying on an IST stack, thus maintaining system stability and preventing the execution of kprobes in invalid stack contexts, as highlighted in the commit message.","The vulnerability in the original code arose from the inappropriate use of an Interrupt Stack Table entry for handling breakpoint exceptions, which could lead to instability when invoked from certain contexts, such as during kprobes or context tracking changes. This misuse risked recursive crashes and improper exception handling. The fix involved eliminating the reliance on the IST entry for breakpoint handling, ensuring that the system could manage these exceptions safely without the complications associated with invalid stack contexts. This change aligns with established practices in 32-bit kernels, which have successfully used standard interrupt gates for similar scenarios.","The vulnerability was fixed by removing the use of the Interrupt Stack Table entry in the handling of breakpoint exceptions. The updated code no longer calls `ist_enter` for this purpose, recognizing that there is no need for an IST stack in this context. Instead, it allows for normal handling of breakpoints without the complications that arise from invalid stack scenarios. The change ensures that kprobes are not invoked in contexts that could lead to instability, thereby maintaining system integrity and preventing potential recursive crashes during breakpoint processing. This adjustment aligns with the practices used in 32-bit kernels.","The vulnerability was addressed by eliminating the reliance on an Interrupt Stack Table entry for handling breakpoint exceptions, which was deemed unnecessary. The updated code ensures that the system can manage these exceptions without the complications associated with invalid stack contexts, thereby preventing potential instability and recursive crashes. By allowing for standard handling of breakpoints, the fix also ensures that certain debugging features are not invoked in inappropriate contexts, thus maintaining overall system integrity and aligning with established practices used in similar scenarios within the kernel. This change enhances the robustness of the exception handling mechanism."
32,182476,182476,,Remote,Not required,Partial,CVE-2018-7998,https://www.cvedetails.com/cve/CVE-2018-7998/,CWE-362,High,Partial,Partial,,2018-03-09,5.1,"In libvips before 8.6.3, a NULL function pointer dereference vulnerability was found in the vips_region_generate function in region.c, which allows remote attackers to cause a denial of service or possibly have unspecified other impact via a crafted image file. This occurs because of a race condition involving a failed delayed load and other worker threads.",2018-03-27,DoS ,17,https://github.com/jcupitt/libvips/commit/20d840e6da15c1574b3ed998bc92f91d1e36c2a5,20d840e6da15c1574b3ed998bc92f91d1e36c2a5,"fix a crash with delayed load

If a delayed load failed, it could leave the pipeline only half-set up.
Sebsequent threads could then segv.

Set a load-has-failed flag and test before generate.

See https://github.com/jcupitt/libvips/issues/893",5,libvips/foreign/foreign.c,"{""sha"": ""08aaab8c21980577a74498f3b19bc3a155cbec9f"", ""filename"": ""ChangeLog"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/ChangeLog"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/ChangeLog"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/ChangeLog?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -4,6 +4,7 @@\n   writing twice to memory\n - better rounding behaviour in convolution means we hit the vector path more\n   often\n+- fix a crash if a delayed load failed [gsharpsh00ter]\n \n 5/1/18 started 8.6.2\n - vips_sink_screen() keeps a ref to the input image ... stops a rare race""}<_**next**_>{""sha"": ""fb03fd746990f1e8f7ff129a4e4db1e7320d3bb2"", ""filename"": ""libvips/foreign/foreign.c"", ""status"": ""modified"", ""additions"": 19, ""deletions"": 6, ""changes"": 25, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/foreign/foreign.c"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/foreign/foreign.c"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/libvips/foreign/foreign.c?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -18,6 +18,8 @@\n  * \t- transform cmyk->rgb if there's an embedded profile\n  * 16/6/17\n  * \t- add page_height\n+ * 5/3/18\n+ * \t- block _start if one start fails, see #893\n  */\n \n /*\n@@ -796,6 +798,11 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )\n \tVipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );\n \tVipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );\n \n+\t/* If this start has failed before in another thread, we can fail now.\n+\t */\n+\tif( load->error )\n+\t\treturn( NULL );\n+\n \tif( !load->real ) {\n \t\tif( !(load->real = vips_foreign_load_temp( load )) )\n \t\t\treturn( NULL );\n@@ -819,19 +826,25 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )\n \t\tg_object_set_qdata( G_OBJECT( load->real ), \n \t\t\tvips__foreign_load_operation, load ); \n \n-\t\tif( class->load( load ) ||\n-\t\t\tvips_image_pio_input( load->real ) ) \n-\t\t\treturn( NULL );\n-\n-\t\t/* ->header() read the header into @out, load has read the\n+\t\t/* Load the image and check the result.\n+\t\t *\n+\t\t * ->header() read the header into @out, load has read the\n \t\t * image into @real. They must match exactly in size, bands,\n \t\t * format and coding for the copy to work.  \n \t\t *\n \t\t * Some versions of ImageMagick give different results between\n \t\t * Ping and Load for some formats, for example.\n+\t\t *\n+\t\t * If the load fails, we need to stop\n \t\t */\n-\t\tif( !vips_foreign_load_iscompat( load->real, out ) )\n+\t\tif( class->load( load ) ||\n+\t\t\tvips_image_pio_input( load->real ) || \n+\t\t\tvips_foreign_load_iscompat( load->real, out ) ) {\n+\t\t\tvips_operation_invalidate( VIPS_OPERATION( load ) ); \n+\t\t\tload->error = TRUE;\n+\n \t\t\treturn( NULL );\n+\t\t}\n \n \t\t/* We have to tell vips that out depends on real. We've set\n \t\t * the demand hint below, but not given an input there.""}<_**next**_>{""sha"": ""dcbf21c82c7b70bba5714a9be2b33ee59aa6d51b"", ""filename"": ""libvips/include/vips/foreign.h"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 0, ""changes"": 5, ""blob_url"": ""https://github.com/libvips/libvips/blob/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/include/vips/foreign.h"", ""raw_url"": ""https://github.com/libvips/libvips/raw/20d840e6da15c1574b3ed998bc92f91d1e36c2a5/libvips/include/vips/foreign.h"", ""contents_url"": ""https://api.github.com/repos/libvips/libvips/contents/libvips/include/vips/foreign.h?ref=20d840e6da15c1574b3ed998bc92f91d1e36c2a5"", ""patch"": ""@@ -158,6 +158,11 @@ typedef struct _VipsForeignLoad {\n \t * TRUE.\n \t */\n \tgboolean disc;\n+\n+\t/* Set if a start function fails. We want to prevent the other starts\n+\t * from also triggering the load.\n+\t */\n+\tgboolean error;\n } VipsForeignLoad;\n \n typedef struct _VipsForeignLoadClass {""}","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
		 *
		 * If the load fails, we need to stop
 		 */
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

 			return( NULL );
		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
 		 */
		if( !vips_foreign_load_iscompat( load->real, out ) )
 			return( NULL );
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
",C,"	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
		 *
		 * If the load fails, we need to stop
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

		}
","		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
		if( !vips_foreign_load_iscompat( load->real, out ) )
",,"@@ -18,6 +18,8 @@
  * 	- transform cmyk->rgb if there's an embedded profile
  * 16/6/17
  * 	- add page_height
+ * 5/3/18
+ * 	- block _start if one start fails, see #893
  */
 
 /*
@@ -796,6 +798,11 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
+	/* If this start has failed before in another thread, we can fail now.
+	 */
+	if( load->error )
+		return( NULL );
+
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );
@@ -819,19 +826,25 @@ vips_foreign_load_start( VipsImage *out, void *a, void *b )
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
-		if( class->load( load ) ||
-			vips_image_pio_input( load->real ) ) 
-			return( NULL );
-
-		/* ->header() read the header into @out, load has read the
+		/* Load the image and check the result.
+		 *
+		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
+		 *
+		 * If the load fails, we need to stop
 		 */
-		if( !vips_foreign_load_iscompat( load->real, out ) )
+		if( class->load( load ) ||
+			vips_image_pio_input( load->real ) || 
+			vips_foreign_load_iscompat( load->real, out ) ) {
+			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
+			load->error = TRUE;
+
 			return( NULL );
+		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.",libvips,20d840e6da15c1574b3ed998bc92f91d1e36c2a5,eefb2dad980c12922ad15d987be7c4b42a44d5cf,1,"vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
//fix_flaw_line_below:
//	/* If this start has failed before in another thread, we can fail now.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	if( load->error )
//fix_flaw_line_below:
//		return( NULL );
//fix_flaw_line_below:
//
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
//flaw_line_below:
		if( class->load( load ) ||
//flaw_line_below:
			vips_image_pio_input( load->real ) ) 
//flaw_line_below:
			return( NULL );
//flaw_line_below:

//flaw_line_below:
		/* ->header() read the header into @out, load has read the
//fix_flaw_line_below:
//		/* Load the image and check the result.
//fix_flaw_line_below:
//		 *
//fix_flaw_line_below:
//		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
//fix_flaw_line_below:
//		 *
//fix_flaw_line_below:
//		 * If the load fails, we need to stop
 		 */
//flaw_line_below:
		if( !vips_foreign_load_iscompat( load->real, out ) )
//fix_flaw_line_below:
//		if( class->load( load ) ||
//fix_flaw_line_below:
//			vips_image_pio_input( load->real ) || 
//fix_flaw_line_below:
//			vips_foreign_load_iscompat( load->real, out ) ) {
//fix_flaw_line_below:
//			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
//fix_flaw_line_below:
//			load->error = TRUE;
//fix_flaw_line_below:
//
 			return( NULL );
//fix_flaw_line_below:
//		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
",182476,"vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) ) 
			return( NULL );
		/* ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
 		 */
		if( !vips_foreign_load_iscompat( load->real, out ) )
 			return( NULL );
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","vips_foreign_load_start( VipsImage *out, void *a, void *b )
{
 	VipsForeignLoad *load = VIPS_FOREIGN_LOAD( b );
 	VipsForeignLoadClass *class = VIPS_FOREIGN_LOAD_GET_CLASS( load );
 
	/* If this start has failed before in another thread, we can fail now.
	 */
	if( load->error )
		return( NULL );

 	if( !load->real ) {
 		if( !(load->real = vips_foreign_load_temp( load )) )
 			return( NULL );

#ifdef DEBUG
		printf( ""vips_foreign_load_start: triggering ->load()\n"" );
#endif /*DEBUG*/

		/* Read the image in. This may involve a long computation and
		 * will finish with load->real holding the decompressed image. 
		 *
		 * We want our caller to be able to see this computation on
		 * @out, so eval signals on ->real need to appear on ->out.
		 */
		load->real->progress_signal = load->out;

		/* Note the load object on the image. Loaders can use 
		 * this to signal invalidate if they hit a load error. See
		 * vips_foreign_load_invalidate() below.
		 */
 		g_object_set_qdata( G_OBJECT( load->real ), 
 			vips__foreign_load_operation, load ); 
 
		/* Load the image and check the result.
		 *
		 * ->header() read the header into @out, load has read the
 		 * image into @real. They must match exactly in size, bands,
 		 * format and coding for the copy to work.  
 		 *
 		 * Some versions of ImageMagick give different results between
 		 * Ping and Load for some formats, for example.
		 *
		 * If the load fails, we need to stop
 		 */
		if( class->load( load ) ||
			vips_image_pio_input( load->real ) || 
			vips_foreign_load_iscompat( load->real, out ) ) {
			vips_operation_invalidate( VIPS_OPERATION( load ) ); 
			load->error = TRUE;

 			return( NULL );
		}
 
 		/* We have to tell vips that out depends on real. We've set
 		 * the demand hint below, but not given an input there.
		 */
		vips_image_pipelinev( load->out, load->out->dhint, 
			load->real, NULL );
	}

	return( vips_region_new( load->real ) );
}
","The `vips_foreign_load_start` function initiates the loading of an image using the VIPS image processing library. It checks if the image has already been loaded; if not, it attempts to load the image into a temporary storage. The function sets up progress signals to allow the caller to monitor the loading process and associates the load object with the image for error handling. It ensures that the loaded image's properties match those of the output image before establishing a dependency between them. Finally, it returns a new region representing the loaded image, facilitating further processing.","The provided code defines a function that begins the process of loading an image from a foreign format. It first checks if the image has already been loaded; if not, it attempts to load it into temporary storage while allowing for progress monitoring. The function ensures that the properties of the loaded image match those of the intended output image to prevent inconsistencies. It establishes a dependency between the loaded image and the output, facilitating error handling and further processing. Finally, it returns a new region that represents the loaded image for subsequent operations.","The vulnerability in the original code arises from the lack of error handling for failed image loading, which could lead to a crash when multiple threads attempt to access a partially set-up pipeline. If the loading process fails, the code does not check for this failure before proceeding, potentially causing segmentation faults in subsequent threads. The fix introduces an error flag that is checked before any further processing occurs. If a load has previously failed, the function immediately returns `NULL`, preventing any unsafe operations on an invalid state and ensuring thread safety.","The vulnerability in the original code stems from inadequate error handling during the image loading process, which could result in a crash when multiple threads attempt to access an improperly configured pipeline. If the loading operation fails, the system may continue processing, leading to segmentation faults in other threads that rely on the incomplete state. The fix addresses this issue by introducing a flag that indicates whether the loading has failed. Before proceeding with any further operations, the code checks this flag, ensuring that any attempts to generate output are safely aborted if a failure has occurred, thus enhancing stability and thread safety.","The vulnerability was fixed by introducing an error flag within the loading structure to track whether a loading operation has failed. In the modified code, before proceeding with any image processing, the function checks the `load->error` flag. If this flag is set, indicating a previous failure, the function immediately returns `NULL`, preventing any further operations on an incomplete pipeline. Additionally, if the loading process fails during execution, the code sets `load->error` to `TRUE` and invalidates the operation, ensuring that subsequent threads do not encounter segmentation faults due to accessing an improperly configured state.","The vulnerability was addressed by implementing an error-checking mechanism that tracks whether a loading operation has previously failed. The updated code introduces a flag that is checked before any further processing occurs. If the flag indicates a failure, the function immediately halts execution and returns an error, preventing any unsafe operations on an incomplete or invalid state. Additionally, if a loading error is encountered during execution, the code marks the operation as invalid, ensuring that subsequent attempts to access the pipeline do not lead to crashes or segmentation faults, thereby enhancing overall stability and thread safety."
33,182551,182551,,Local,Not required,Partial,CVE-2017-18249,https://www.cvedetails.com/cve/CVE-2017-18249/,CWE-362,Medium,Partial,Partial,,2018-03-26,4.4,"The add_free_nid function in fs/f2fs/node.c in the Linux kernel before 4.12 does not properly track an allocated nid, which allows local users to cause a denial of service (race condition) or possibly have unspecified other impact via concurrent threads.",2019-04-02,DoS ,45,https://github.com/torvalds/linux/commit/30a61ddf8117c26ac5b295e1233eaa9629a94ca3,30a61ddf8117c26ac5b295e1233eaa9629a94ca3,"f2fs: fix race condition in between free nid allocator/initializer

In below concurrent case, allocated nid can be loaded into free nid cache
and be allocated again.

Thread A				Thread B
- f2fs_create
 - f2fs_new_inode
  - alloc_nid
   - __insert_nid_to_list(ALLOC_NID_LIST)
					- f2fs_balance_fs_bg
					 - build_free_nids
					  - __build_free_nids
					   - scan_nat_page
					    - add_free_nid
					     - __lookup_nat_cache
 - f2fs_add_link
  - init_inode_metadata
   - new_inode_page
    - new_node_page
     - set_node_addr
 - alloc_nid_done
  - __remove_nid_from_list(ALLOC_NID_LIST)
					     - __insert_nid_to_list(FREE_NID_LIST)

This patch makes nat cache lookup and free nid list operation being atomical
to avoid this race condition.

Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
Signed-off-by: Chao Yu <yuchao0@huawei.com>
Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>",17,fs/f2fs/node.c,"{""sha"": ""29dc996b573c7ecfbbb748a79ce8378784f635dc"", ""filename"": ""fs/f2fs/node.c"", ""status"": ""modified"", ""additions"": 45, ""deletions"": 18, ""changes"": 63, ""blob_url"": ""https://github.com/torvalds/linux/blob/30a61ddf8117c26ac5b295e1233eaa9629a94ca3/fs/f2fs/node.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/30a61ddf8117c26ac5b295e1233eaa9629a94ca3/fs/f2fs/node.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/f2fs/node.c?ref=30a61ddf8117c26ac5b295e1233eaa9629a94ca3"", ""patch"": ""@@ -1761,40 +1761,67 @@ static void __remove_nid_from_list(struct f2fs_sb_info *sbi,\n static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n {\n \tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n-\tstruct free_nid *i;\n+\tstruct free_nid *i, *e;\n \tstruct nat_entry *ne;\n-\tint err;\n+\tint err = -EINVAL;\n+\tbool ret = false;\n \n \t/* 0 nid should not be used */\n \tif (unlikely(nid == 0))\n \t\treturn false;\n \n-\tif (build) {\n-\t\t/* do not add allocated nids */\n-\t\tne = __lookup_nat_cache(nm_i, nid);\n-\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n-\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n-\t\t\treturn false;\n-\t}\n-\n \ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n \ti->nid = nid;\n \ti->state = NID_NEW;\n \n-\tif (radix_tree_preload(GFP_NOFS)) {\n-\t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n+\tif (radix_tree_preload(GFP_NOFS))\n+\t\tgoto err;\n \n \tspin_lock(&nm_i->nid_list_lock);\n+\n+\tif (build) {\n+\t\t/*\n+\t\t *   Thread A             Thread B\n+\t\t *  - f2fs_create\n+\t\t *   - f2fs_new_inode\n+\t\t *    - alloc_nid\n+\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)\n+\t\t *                     - f2fs_balance_fs_bg\n+\t\t *                      - build_free_nids\n+\t\t *                       - __build_free_nids\n+\t\t *                        - scan_nat_page\n+\t\t *                         - add_free_nid\n+\t\t *                          - __lookup_nat_cache\n+\t\t *  - f2fs_add_link\n+\t\t *   - init_inode_metadata\n+\t\t *    - new_inode_page\n+\t\t *     - new_node_page\n+\t\t *      - set_node_addr\n+\t\t *  - alloc_nid_done\n+\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)\n+\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)\n+\t\t */\n+\t\tne = __lookup_nat_cache(nm_i, nid);\n+\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n+\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n+\t\t\tgoto err_out;\n+\n+\t\te = __lookup_free_nid_list(nm_i, nid);\n+\t\tif (e) {\n+\t\t\tif (e->state == NID_NEW)\n+\t\t\t\tret = true;\n+\t\t\tgoto err_out;\n+\t\t}\n+\t}\n+\tret = true;\n \terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\n+err_out:\n \tspin_unlock(&nm_i->nid_list_lock);\n \tradix_tree_preload_end();\n-\tif (err) {\n+err:\n+\tif (err)\n \t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n-\treturn true;\n+\treturn ret;\n }\n \n static void remove_free_nid(struct f2fs_sb_info *sbi, nid_t nid)""}"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i, *e;
 	struct nat_entry *ne;
	int err = -EINVAL;
	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS))
		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
err:
	if (err)
 		kmem_cache_free(free_nid_slab, i);
	return ret;
 }
"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i;
 	struct nat_entry *ne;
	int err;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
 
 	spin_lock(&nm_i->nid_list_lock);
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
	if (err) {
 		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	return true;
 }
",C,"	struct free_nid *i, *e;
	int err = -EINVAL;
	bool ret = false;
	if (radix_tree_preload(GFP_NOFS))
		goto err;

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
err_out:
err:
	if (err)
	return ret;
","	struct free_nid *i;
	int err;
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	if (err) {
		return true;
	}
	return true;
",,"@@ -1761,40 +1761,67 @@ static void __remove_nid_from_list(struct f2fs_sb_info *sbi,
 static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
-	struct free_nid *i;
+	struct free_nid *i, *e;
 	struct nat_entry *ne;
-	int err;
+	int err = -EINVAL;
+	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
-	if (build) {
-		/* do not add allocated nids */
-		ne = __lookup_nat_cache(nm_i, nid);
-		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
-				nat_get_blkaddr(ne) != NULL_ADDR))
-			return false;
-	}
-
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
-	if (radix_tree_preload(GFP_NOFS)) {
-		kmem_cache_free(free_nid_slab, i);
-		return true;
-	}
+	if (radix_tree_preload(GFP_NOFS))
+		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);
+
+	if (build) {
+		/*
+		 *   Thread A             Thread B
+		 *  - f2fs_create
+		 *   - f2fs_new_inode
+		 *    - alloc_nid
+		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
+		 *                     - f2fs_balance_fs_bg
+		 *                      - build_free_nids
+		 *                       - __build_free_nids
+		 *                        - scan_nat_page
+		 *                         - add_free_nid
+		 *                          - __lookup_nat_cache
+		 *  - f2fs_add_link
+		 *   - init_inode_metadata
+		 *    - new_inode_page
+		 *     - new_node_page
+		 *      - set_node_addr
+		 *  - alloc_nid_done
+		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
+		 *                         - __insert_nid_to_list(FREE_NID_LIST)
+		 */
+		ne = __lookup_nat_cache(nm_i, nid);
+		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
+				nat_get_blkaddr(ne) != NULL_ADDR))
+			goto err_out;
+
+		e = __lookup_free_nid_list(nm_i, nid);
+		if (e) {
+			if (e->state == NID_NEW)
+				ret = true;
+			goto err_out;
+		}
+	}
+	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
+err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
-	if (err) {
+err:
+	if (err)
 		kmem_cache_free(free_nid_slab, i);
-		return true;
-	}
-	return true;
+	return ret;
 }
 
 static void remove_free_nid(struct f2fs_sb_info *sbi, nid_t nid)",linux,30a61ddf8117c26ac5b295e1233eaa9629a94ca3,5f4c3dec225e03273e1f74816e676cccc1a54329,1," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
//flaw_line_below:
	struct free_nid *i;
//fix_flaw_line_below:
//	struct free_nid *i, *e;
 	struct nat_entry *ne;
//flaw_line_below:
	int err;
//fix_flaw_line_below:
//	int err = -EINVAL;
//fix_flaw_line_below:
//	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
//flaw_line_below:
	if (build) {
//flaw_line_below:
		/* do not add allocated nids */
//flaw_line_below:
		ne = __lookup_nat_cache(nm_i, nid);
//flaw_line_below:
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
//flaw_line_below:
				nat_get_blkaddr(ne) != NULL_ADDR))
//flaw_line_below:
			return false;
//flaw_line_below:
	}
//flaw_line_below:

 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
//flaw_line_below:
	if (radix_tree_preload(GFP_NOFS)) {
//flaw_line_below:
		kmem_cache_free(free_nid_slab, i);
//flaw_line_below:
		return true;
//flaw_line_below:
	}
//fix_flaw_line_below:
//	if (radix_tree_preload(GFP_NOFS))
//fix_flaw_line_below:
//		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	if (build) {
//fix_flaw_line_below:
//		/*
//fix_flaw_line_below:
//		 *   Thread A             Thread B
//fix_flaw_line_below:
//		 *  - f2fs_create
//fix_flaw_line_below:
//		 *   - f2fs_new_inode
//fix_flaw_line_below:
//		 *    - alloc_nid
//fix_flaw_line_below:
//		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
//fix_flaw_line_below:
//		 *                     - f2fs_balance_fs_bg
//fix_flaw_line_below:
//		 *                      - build_free_nids
//fix_flaw_line_below:
//		 *                       - __build_free_nids
//fix_flaw_line_below:
//		 *                        - scan_nat_page
//fix_flaw_line_below:
//		 *                         - add_free_nid
//fix_flaw_line_below:
//		 *                          - __lookup_nat_cache
//fix_flaw_line_below:
//		 *  - f2fs_add_link
//fix_flaw_line_below:
//		 *   - init_inode_metadata
//fix_flaw_line_below:
//		 *    - new_inode_page
//fix_flaw_line_below:
//		 *     - new_node_page
//fix_flaw_line_below:
//		 *      - set_node_addr
//fix_flaw_line_below:
//		 *  - alloc_nid_done
//fix_flaw_line_below:
//		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
//fix_flaw_line_below:
//		 *                         - __insert_nid_to_list(FREE_NID_LIST)
//fix_flaw_line_below:
//		 */
//fix_flaw_line_below:
//		ne = __lookup_nat_cache(nm_i, nid);
//fix_flaw_line_below:
//		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
//fix_flaw_line_below:
//				nat_get_blkaddr(ne) != NULL_ADDR))
//fix_flaw_line_below:
//			goto err_out;
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//		e = __lookup_free_nid_list(nm_i, nid);
//fix_flaw_line_below:
//		if (e) {
//fix_flaw_line_below:
//			if (e->state == NID_NEW)
//fix_flaw_line_below:
//				ret = true;
//fix_flaw_line_below:
//			goto err_out;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
//fix_flaw_line_below:
//err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
//flaw_line_below:
	if (err) {
//fix_flaw_line_below:
//err:
//fix_flaw_line_below:
//	if (err)
 		kmem_cache_free(free_nid_slab, i);
//flaw_line_below:
		return true;
//flaw_line_below:
	}
//flaw_line_below:
	return true;
//fix_flaw_line_below:
//	return ret;
 }
",182551," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i;
 	struct nat_entry *ne;
	int err;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
	if (build) {
		/* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			return false;
	}
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS)) {
		kmem_cache_free(free_nid_slab, i);
		return true;
	}
 
 	spin_lock(&nm_i->nid_list_lock);
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
	if (err) {
 		kmem_cache_free(free_nid_slab, i);
		return true;
	}
	return true;
 }
"," static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
 {
 	struct f2fs_nm_info *nm_i = NM_I(sbi);
	struct free_nid *i, *e;
 	struct nat_entry *ne;
	int err = -EINVAL;
	bool ret = false;
 
 	/* 0 nid should not be used */
 	if (unlikely(nid == 0))
 		return false;
 
 	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
 	i->nid = nid;
 	i->state = NID_NEW;
 
	if (radix_tree_preload(GFP_NOFS))
		goto err;
 
 	spin_lock(&nm_i->nid_list_lock);

	if (build) {
		/*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
		 */
		ne = __lookup_nat_cache(nm_i, nid);
		if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
				nat_get_blkaddr(ne) != NULL_ADDR))
			goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
		if (e) {
			if (e->state == NID_NEW)
				ret = true;
			goto err_out;
		}
	}
	ret = true;
 	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
err_out:
 	spin_unlock(&nm_i->nid_list_lock);
 	radix_tree_preload_end();
err:
	if (err)
 		kmem_cache_free(free_nid_slab, i);
	return ret;
 }
","The `add_free_nid` function is designed to manage the addition of free node IDs (NIDs) in a file system's namespace management structure. It first checks if the provided NID is valid (not zero) and, if the `build` flag is set, ensures that the NID is not already allocated by looking it up in the NAT cache. If the NID is valid and not allocated, the function allocates memory for a new `free_nid` structure, initializes it, and attempts to preload a radix tree. It then locks the NID list, inserts the new NID into the free list, and unlocks the list, ensuring thread safety. Finally, it handles memory cleanup in case of errors.","The function is responsible for adding a free node identifier to a file system's management structure. It first verifies that the provided identifier is valid and, if a specific condition is met, checks that the identifier is not already in use. If the identifier is valid and available, it allocates memory for a new structure, initializes it, and attempts to prepare a data structure for efficient access. The function then locks a list to safely insert the new identifier, unlocks the list afterward, and ensures proper memory management in case of any errors during the process.","The vulnerability in the original code stemmed from a race condition where two threads could concurrently allocate the same node identifier, leading to potential inconsistencies in the free node identifier cache. Specifically, while one thread was inserting a node identifier into the allocated list, another thread could simultaneously check and insert the same identifier into the free list, resulting in double allocation. The fixed code addresses this issue by ensuring that the lookup of the NAT cache and the operation on the free node identifier list are performed atomically within a locked section, thus preventing concurrent access and ensuring thread safety.","The vulnerability in the original code arose from a race condition that allowed two threads to simultaneously allocate the same identifier, leading to potential inconsistencies in the management of node identifiers. Specifically, while one thread was inserting an identifier into the allocated list, another thread could check and insert the same identifier into the free list, resulting in double allocation. The fix implemented in the updated code ensures that the operations related to checking the cache and modifying the free list are performed atomically within a locked context, thereby preventing concurrent access and ensuring the integrity of the identifier management process.","The vulnerability was fixed by ensuring that the operations involving the NAT cache lookup and the free node identifier list are performed atomically within a locked context. In the updated code, the spin lock is acquired before checking the NAT cache and the free list, which prevents concurrent threads from interfering with each other during these critical operations. Specifically, the code checks the NAT cache and the free list for the identifier while holding the lock, ensuring that no other thread can modify the lists until the current operation is complete. This effectively eliminates the race condition that could lead to double allocation.","The vulnerability was addressed by implementing a locking mechanism that ensures critical operations are executed atomically, preventing concurrent access by multiple threads. By acquiring a lock before checking the cache and modifying the list of identifiers, the code guarantees that no other thread can interfere during these operations. This change effectively synchronizes access to shared resources, ensuring that once a thread begins to check or modify the identifier lists, it completes the operation without interruption. As a result, the risk of double allocation due to race conditions is eliminated, enhancing the overall integrity of the identifier management process."
34,182583,182583,,Local,Not required,Partial,CVE-2017-18203,https://www.cvedetails.com/cve/CVE-2017-18203/,CWE-362,Medium,,,,2018-02-27,1.9,The dm_get_from_kobject function in drivers/md/dm.c in the Linux kernel before 4.14.3 allow local users to cause a denial of service (BUG) by leveraging a race condition with __dm_destroy during creation and removal of DM devices.,2018-06-19,DoS ,8,https://github.com/torvalds/linux/commit/b9a41d21dceadf8104812626ef85dc56ee8a60ed,b9a41d21dceadf8104812626ef85dc56ee8a60ed,"dm: fix race between dm_get_from_kobject() and __dm_destroy()

The following BUG_ON was hit when testing repeat creation and removal of
DM devices:

    kernel BUG at drivers/md/dm.c:2919!
    CPU: 7 PID: 750 Comm: systemd-udevd Not tainted 4.1.44
    Call Trace:
     [<ffffffff81649e8b>] dm_get_from_kobject+0x34/0x3a
     [<ffffffff81650ef1>] dm_attr_show+0x2b/0x5e
     [<ffffffff817b46d1>] ? mutex_lock+0x26/0x44
     [<ffffffff811df7f5>] sysfs_kf_seq_show+0x83/0xcf
     [<ffffffff811de257>] kernfs_seq_show+0x23/0x25
     [<ffffffff81199118>] seq_read+0x16f/0x325
     [<ffffffff811de994>] kernfs_fop_read+0x3a/0x13f
     [<ffffffff8117b625>] __vfs_read+0x26/0x9d
     [<ffffffff8130eb59>] ? security_file_permission+0x3c/0x44
     [<ffffffff8117bdb8>] ? rw_verify_area+0x83/0xd9
     [<ffffffff8117be9d>] vfs_read+0x8f/0xcf
     [<ffffffff81193e34>] ? __fdget_pos+0x12/0x41
     [<ffffffff8117c686>] SyS_read+0x4b/0x76
     [<ffffffff817b606e>] system_call_fastpath+0x12/0x71

The bug can be easily triggered, if an extra delay (e.g. 10ms) is added
between the test of DMF_FREEING & DMF_DELETING and dm_get() in
dm_get_from_kobject().

To fix it, we need to ensure the test of DMF_FREEING & DMF_DELETING and
dm_get() are done in an atomic way, so _minor_lock is used.

The other callers of dm_get() have also been checked to be OK: some
callers invoke dm_get() under _minor_lock, some callers invoke it under
_hash_lock, and dm_start_request() invoke it after increasing
md->open_count.

Cc: stable@vger.kernel.org
Signed-off-by: Hou Tao <houtao1@huawei.com>
Signed-off-by: Mike Snitzer <snitzer@redhat.com>",3,drivers/md/dm.c,"{""sha"": ""dcfa1a8c93909cad5bc07c257b0b70af8f3a722e"", ""filename"": ""drivers/md/dm.c"", ""status"": ""modified"", ""additions"": 8, ""deletions"": 4, ""changes"": 12, ""blob_url"": ""https://github.com/torvalds/linux/blob/b9a41d21dceadf8104812626ef85dc56ee8a60ed/drivers/md/dm.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/b9a41d21dceadf8104812626ef85dc56ee8a60ed/drivers/md/dm.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/md/dm.c?ref=b9a41d21dceadf8104812626ef85dc56ee8a60ed"", ""patch"": ""@@ -2711,11 +2711,15 @@ struct mapped_device *dm_get_from_kobject(struct kobject *kobj)\n \n \tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n \n-\tif (test_bit(DMF_FREEING, &md->flags) ||\n-\t    dm_deleting_md(md))\n-\t\treturn NULL;\n-\n+\tspin_lock(&_minor_lock);\n+\tif (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {\n+\t\tmd = NULL;\n+\t\tgoto out;\n+\t}\n \tdm_get(md);\n+out:\n+\tspin_unlock(&_minor_lock);\n+\n \treturn md;\n }\n ""}","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
 	dm_get(md);
out:
	spin_unlock(&_minor_lock);

 	return md;
 }
","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
 	dm_get(md);
 	return md;
 }
",C,"	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
out:
	spin_unlock(&_minor_lock);

","	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
",,"@@ -2711,11 +2711,15 @@ struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
-	if (test_bit(DMF_FREEING, &md->flags) ||
-	    dm_deleting_md(md))
-		return NULL;
-
+	spin_lock(&_minor_lock);
+	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
+		md = NULL;
+		goto out;
+	}
 	dm_get(md);
+out:
+	spin_unlock(&_minor_lock);
+
 	return md;
 }
 ",linux,b9a41d21dceadf8104812626ef85dc56ee8a60ed,856eb0916d181da6d043cc33e03f54d5c5bbe54a,1,"struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
//flaw_line_below:
	if (test_bit(DMF_FREEING, &md->flags) ||
//flaw_line_below:
	    dm_deleting_md(md))
//flaw_line_below:
		return NULL;
//flaw_line_below:

//fix_flaw_line_below:
//	spin_lock(&_minor_lock);
//fix_flaw_line_below:
//	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
//fix_flaw_line_below:
//		md = NULL;
//fix_flaw_line_below:
//		goto out;
//fix_flaw_line_below:
//	}
 	dm_get(md);
//fix_flaw_line_below:
//out:
//fix_flaw_line_below:
//	spin_unlock(&_minor_lock);
//fix_flaw_line_below:
//
 	return md;
 }
",182583,"struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	if (test_bit(DMF_FREEING, &md->flags) ||
	    dm_deleting_md(md))
		return NULL;
 	dm_get(md);
 	return md;
 }
","struct mapped_device *dm_get_from_kobject(struct kobject *kobj)
{
	struct mapped_device *md;
 
 	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);
 
	spin_lock(&_minor_lock);
	if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
		goto out;
	}
 	dm_get(md);
out:
	spin_unlock(&_minor_lock);

 	return md;
 }
","The function `dm_get_from_kobject` retrieves a pointer to a `mapped_device` structure from a given kernel object (`kobject`). It uses the `container_of` macro to obtain the `mapped_device` instance associated with the provided `kobject`. Before returning the pointer, it checks if the device is in the process of being freed or deleted by evaluating specific flags. If either condition is true, the function returns `NULL` to indicate that the device is no longer valid. If the device is valid, it increments the reference count of the `mapped_device` and returns the pointer to it.","The provided code defines a function that retrieves a specific structure associated with a kernel object. It first determines the structure's address based on the kernel object provided. Before proceeding, the function checks if the structure is marked for freeing or deletion, returning a null pointer if either condition is met, indicating that the structure is no longer valid. If the structure is valid, the function increments its reference count to ensure it remains accessible and then returns a pointer to the structure, allowing further operations to be performed on it safely.","The vulnerability in the original code arises from a race condition between checking the flags for freeing or deletion and incrementing the reference count of the mapped device. This could lead to dereferencing a potentially invalid structure, resulting in a kernel crash. The fix introduces a spin lock to ensure that the checks for the flags and the call to increment the reference count are performed atomically. By locking around these operations, the code prevents concurrent modifications that could occur during the execution of `dm_get_from_kobject`, thus ensuring the integrity of the mapped device's state before it is accessed.","The vulnerability in the original code stems from a race condition that occurs when checking the state of a device and subsequently accessing it. This can lead to dereferencing an invalid structure if the device is freed or deleted between these two operations, potentially causing a kernel crash. The fix addresses this issue by introducing a locking mechanism that ensures both the state check and the access to the device are performed atomically. This prevents concurrent modifications that could compromise the integrity of the device's state, thereby eliminating the risk of accessing an invalid reference.","The vulnerability was fixed by introducing a spin lock around the critical section of the code that checks the flags for freeing or deletion and the subsequent call to increment the reference count. Specifically, the code now acquires the `_minor_lock` before evaluating the conditions using `test_bit` and `dm_deleting_md`. If either condition indicates that the device is being freed or deleted, the pointer is set to `NULL`, and the function exits safely. This ensures that the state of the mapped device remains consistent and prevents race conditions that could lead to dereferencing an invalid structure.","The vulnerability was fixed by implementing a locking mechanism that ensures the critical operations of checking the device's state and accessing it are performed atomically. By introducing a lock around these operations, the code prevents concurrent modifications that could occur between the state check and the subsequent access. This ensures that if the device is in the process of being freed or deleted, the access will not occur, thereby avoiding the risk of dereferencing an invalid structure. The fix effectively maintains the integrity of the device's state throughout the execution of the function."
35,182607,182607,,Local,Not required,Complete,CVE-2017-2616,https://www.cvedetails.com/cve/CVE-2017-2616/,CWE-362,Medium,,,,2018-07-27,4.7,A race condition was found in util-linux before 2.32.1 in the way su handled the management of child processes. A local authenticated attacker could use this flaw to kill other processes with root privileges under specific conditions.,2019-10-09,,10,https://github.com/karelzak/util-linux/commit/dffab154d29a288aa171ff50263ecc8f2e14a891,dffab154d29a288aa171ff50263ecc8f2e14a891,"su: properly clear child PID

Reported-by: Tobias Stöckmann <tobias@stoeckmann.org>
Signed-off-by: Karel Zak <kzak@redhat.com>",4,login-utils/su-common.c,"{""sha"": ""696adc8888ad901efbb98f7878c31e4a21c4cf27"", ""filename"": ""login-utils/su-common.c"", ""status"": ""modified"", ""additions"": 10, ""deletions"": 4, ""changes"": 14, ""blob_url"": ""https://github.com/karelzak/util-linux/blob/dffab154d29a288aa171ff50263ecc8f2e14a891/login-utils/su-common.c"", ""raw_url"": ""https://github.com/karelzak/util-linux/raw/dffab154d29a288aa171ff50263ecc8f2e14a891/login-utils/su-common.c"", ""contents_url"": ""https://api.github.com/repos/karelzak/util-linux/contents/login-utils/su-common.c?ref=dffab154d29a288aa171ff50263ecc8f2e14a891"", ""patch"": ""@@ -368,6 +368,9 @@ create_watching_parent (void)\n             }\n           else\n             status = WEXITSTATUS (status);\n+\n+\t  /* child is gone, don't use the PID anymore */\n+\t  child = (pid_t) -1;\n         }\n       else if (caught_signal)\n         status = caught_signal + 128;\n@@ -377,7 +380,7 @@ create_watching_parent (void)\n   else\n     status = 1;\n \n-  if (caught_signal)\n+  if (caught_signal && child != (pid_t)-1)\n     {\n       fprintf (stderr, _(\""\\nSession terminated, killing shell...\""));\n       kill (child, SIGTERM);\n@@ -387,9 +390,12 @@ create_watching_parent (void)\n \n   if (caught_signal)\n     {\n-      sleep (2);\n-      kill (child, SIGKILL);\n-      fprintf (stderr, _(\"" ...killed.\\n\""));\n+      if (child != (pid_t)-1)\n+\t{\n+\t  sleep (2);\n+\t  kill (child, SIGKILL);\n+\t  fprintf (stderr, _(\"" ...killed.\\n\""));\n+\t}\n \n       /* Let's terminate itself with the received signal.\n        *""}","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);

	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
",C,"
	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
  if (caught_signal && child != (pid_t)-1)
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
","  if (caught_signal)
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
",,"@@ -368,6 +368,9 @@ create_watching_parent (void)
             }
           else
             status = WEXITSTATUS (status);
+
+	  /* child is gone, don't use the PID anymore */
+	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
@@ -377,7 +380,7 @@ create_watching_parent (void)
   else
     status = 1;
 
-  if (caught_signal)
+  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
@@ -387,9 +390,12 @@ create_watching_parent (void)
 
   if (caught_signal)
     {
-      sleep (2);
-      kill (child, SIGKILL);
-      fprintf (stderr, _("" ...killed.\n""));
+      if (child != (pid_t)-1)
+	{
+	  sleep (2);
+	  kill (child, SIGKILL);
+	  fprintf (stderr, _("" ...killed.\n""));
+	}
 
       /* Let's terminate itself with the received signal.
        *",util-linux,dffab154d29a288aa171ff50263ecc8f2e14a891,904ffe1fd2ba6c2092e00b4b1b95fcc30be77b45,1,"create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
//fix_flaw_line_below:
//
//fix_flaw_line_below:
//	  /* child is gone, don't use the PID anymore */
//fix_flaw_line_below:
//	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
//flaw_line_below:
  if (caught_signal)
//fix_flaw_line_below:
//  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
//flaw_line_below:
      sleep (2);
//flaw_line_below:
      kill (child, SIGKILL);
//flaw_line_below:
      fprintf (stderr, _("" ...killed.\n""));
//fix_flaw_line_below:
//      if (child != (pid_t)-1)
//fix_flaw_line_below:
//	{
//fix_flaw_line_below:
//	  sleep (2);
//fix_flaw_line_below:
//	  kill (child, SIGKILL);
//fix_flaw_line_below:
//	  fprintf (stderr, _("" ...killed.\n""));
//fix_flaw_line_below:
//	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
",182607,"create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      sleep (2);
      kill (child, SIGKILL);
      fprintf (stderr, _("" ...killed.\n""));
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","create_watching_parent (void)
{
  pid_t child;
  sigset_t ourset;
  struct sigaction oldact[3];
  int status = 0;
  int retval;

  retval = pam_open_session (pamh, 0);
  if (is_pam_failure(retval))
    {
      cleanup_pam (retval);
      errx (EXIT_FAILURE, _(""cannot open session: %s""),
	     pam_strerror (pamh, retval));
    }
  else
    _pam_session_opened = 1;

  memset(oldact, 0, sizeof(oldact));

  child = fork ();
  if (child == (pid_t) -1)
    {
      cleanup_pam (PAM_ABORT);
      err (EXIT_FAILURE, _(""cannot create child process""));
    }

  /* the child proceeds to run the shell */
  if (child == 0)
    return;

  /* In the parent watch the child.  */

  /* su without pam support does not have a helper that keeps
     sitting on any directory so let's go to /.  */
  if (chdir (""/"") != 0)
    warn (_(""cannot change directory to %s""), ""/"");

  sigfillset (&ourset);
  if (sigprocmask (SIG_BLOCK, &ourset, NULL))
    {
      warn (_(""cannot block signals""));
      caught_signal = true;
    }
  if (!caught_signal)
    {
      struct sigaction action;
      action.sa_handler = su_catch_sig;
      sigemptyset (&action.sa_mask);
      action.sa_flags = 0;
      sigemptyset (&ourset);
    if (!same_session)
      {
        if (sigaddset(&ourset, SIGINT) || sigaddset(&ourset, SIGQUIT))
          {
            warn (_(""cannot set signal handler""));
            caught_signal = true;
          }
      }
    if (!caught_signal && (sigaddset(&ourset, SIGTERM)
                    || sigaddset(&ourset, SIGALRM)
                    || sigaction(SIGTERM, &action, &oldact[0])
                    || sigprocmask(SIG_UNBLOCK, &ourset, NULL))) {
	  warn (_(""cannot set signal handler""));
	  caught_signal = true;
	}
    if (!caught_signal && !same_session && (sigaction(SIGINT, &action, &oldact[1])
                                     || sigaction(SIGQUIT, &action, &oldact[2])))
      {
        warn (_(""cannot set signal handler""));
        caught_signal = true;
      }
    }
  if (!caught_signal)
    {
      pid_t pid;
      for (;;)
	{
	  pid = waitpid (child, &status, WUNTRACED);

	  if (pid != (pid_t)-1 && WIFSTOPPED (status))
	    {
	      kill (getpid (), SIGSTOP);
	      /* once we get here, we must have resumed */
	      kill (pid, SIGCONT);
	    }
	  else
	    break;
	}
      if (pid != (pid_t)-1)
        {
          if (WIFSIGNALED (status))
            {
              fprintf (stderr, ""%s%s\n"", strsignal (WTERMSIG (status)),
                       WCOREDUMP (status) ? _("" (core dumped)"") : """");
              status = WTERMSIG (status) + 128;
             }
           else
             status = WEXITSTATUS (status);

	  /* child is gone, don't use the PID anymore */
	  child = (pid_t) -1;
         }
       else if (caught_signal)
         status = caught_signal + 128;
      else
        status = 1;
    }
   else
     status = 1;
 
  if (caught_signal && child != (pid_t)-1)
     {
       fprintf (stderr, _(""\nSession terminated, killing shell...""));
       kill (child, SIGTERM);
    }

  cleanup_pam (PAM_SUCCESS);
 
   if (caught_signal)
     {
      if (child != (pid_t)-1)
	{
	  sleep (2);
	  kill (child, SIGKILL);
	  fprintf (stderr, _("" ...killed.\n""));
	}
 
       /* Let's terminate itself with the received signal.
        *
       * It seems that shells use WIFSIGNALED() rather than our exit status
       * value to detect situations when is necessary to cleanup (reset)
       * terminal settings (kzak -- Jun 2013).
       */
      switch (caught_signal) {
        case SIGTERM:
          sigaction(SIGTERM, &oldact[0], NULL);
          break;
        case SIGINT:
          sigaction(SIGINT, &oldact[1], NULL);
          break;
        case SIGQUIT:
          sigaction(SIGQUIT, &oldact[2], NULL);
          break;
        default:
	  /* just in case that signal stuff initialization failed and
	   * caught_signal = true */
          caught_signal = SIGKILL;
          break;
      }
      kill(getpid(), caught_signal);
    }
  exit (status);
}
","The `create_watching_parent` function manages a child process that runs a shell while the parent process monitors it. Initially, it opens a PAM session and handles any errors. The function then forks a child process; if successful, the child executes the shell, while the parent changes its working directory to the root and sets up signal handling for various termination signals. The parent continuously waits for the child process to change state, handling signals appropriately. If a signal is caught, it terminates the child and cleans up the PAM session before exiting with the appropriate status code.","The code defines a function that creates a child process to run a shell while the parent process monitors its execution. It begins by establishing a session and handling any potential errors. After forking, the child process executes the shell, while the parent changes its working directory and sets up signal handling for termination signals. The parent continuously waits for the child to change state, managing signals as they occur. If a signal is caught, the parent terminates the child process, cleans up the session, and exits with an appropriate status code, ensuring proper resource management.","The vulnerability in the original code stemmed from the improper handling of the child process's PID after it terminated. Specifically, the code continued to use the `child` variable without ensuring it was cleared, which could lead to undefined behavior if the parent attempted to interact with a non-existent child process. In the fixed code, after checking the termination status of the child, the `child` variable is explicitly set to `(pid_t) -1`, ensuring that any subsequent checks for the child's existence are valid. This change prevents potential misuse of an invalid PID, enhancing the robustness of the process management.","The vulnerability in the original code arose from the failure to properly clear the child process identifier after the child process terminated. This oversight could lead to undefined behavior if the parent process attempted to interact with a non-existent child, potentially causing crashes or incorrect handling of process states. In the revised code, the child process identifier is explicitly set to an invalid value after checking the termination status, ensuring that any future references to the child are valid. This fix enhances the reliability of process management by preventing the use of stale or invalid identifiers.","The vulnerability was fixed by explicitly setting the `child` variable to `(pid_t) -1` after checking the termination status of the child process. In the revised code, after determining whether the child was signaled or exited normally, the code ensures that the `child` variable is cleared to prevent any further use of an invalid PID. This change occurs in the section where the termination status is evaluated, ensuring that subsequent checks for the child's existence are valid. By doing this, the code prevents potential misuse of the `child` variable, enhancing the overall robustness of process management.","The vulnerability was addressed by ensuring that the identifier for the child process is explicitly cleared after its termination status is evaluated. This fix prevents any further references to the child process after it has exited, eliminating the risk of interacting with an invalid or non-existent process. By setting the identifier to an invalid state, the code guarantees that any subsequent checks for the child's existence will be accurate, thereby enhancing the reliability of process management and preventing potential crashes or undefined behavior associated with stale process identifiers."
36,182851,182851,,Remote,Not required,Complete,CVE-2019-11815,https://www.cvedetails.com/cve/CVE-2019-11815/,CWE-362,Medium,Complete,Complete,,2019-05-08,9.3,"An issue was discovered in rds_tcp_kill_sock in net/rds/tcp.c in the Linux kernel before 5.0.8. There is a race condition leading to a use-after-free, related to net namespace cleanup.",2019-06-07,,1,https://github.com/torvalds/linux/commit/cb66ddd156203daefb8d71158036b27b0e2caf63,cb66ddd156203daefb8d71158036b27b0e2caf63,"net: rds: force to destroy connection if t_sock is NULL in rds_tcp_kill_sock().

When it is to cleanup net namespace, rds_tcp_exit_net() will call
rds_tcp_kill_sock(), if t_sock is NULL, it will not call
rds_conn_destroy(), rds_conn_path_destroy() and rds_tcp_conn_free() to free
connection, and the worker cp_conn_w is not stopped, afterwards the net is freed in
net_drop_ns(); While cp_conn_w rds_connect_worker() will call rds_tcp_conn_path_connect()
and reference 'net' which has already been freed.

In rds_tcp_conn_path_connect(), rds_tcp_set_callbacks() will set t_sock = sock before
sock->ops->connect, but if connect() is failed, it will call
rds_tcp_restore_callbacks() and set t_sock = NULL, if connect is always
failed, rds_connect_worker() will try to reconnect all the time, so
rds_tcp_kill_sock() will never to cancel worker cp_conn_w and free the
connections.

Therefore, the condition !tc->t_sock is not needed if it is going to do
cleanup_net->rds_tcp_exit_net->rds_tcp_kill_sock, because tc->t_sock is always
NULL, and there is on other path to cancel cp_conn_w and free
connection. So this patch is to fix this.

rds_tcp_kill_sock():
...
if (net != c_net || !tc->t_sock)
...
Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

==================================================================
BUG: KASAN: use-after-free in inet_create+0xbcc/0xd28
net/ipv4/af_inet.c:340
Read of size 4 at addr ffff8003496a4684 by task kworker/u8:4/3721

CPU: 3 PID: 3721 Comm: kworker/u8:4 Not tainted 5.1.0 #11
Hardware name: linux,dummy-virt (DT)
Workqueue: krdsd rds_connect_worker
Call trace:
 dump_backtrace+0x0/0x3c0 arch/arm64/kernel/time.c:53
 show_stack+0x28/0x38 arch/arm64/kernel/traps.c:152
 __dump_stack lib/dump_stack.c:77 [inline]
 dump_stack+0x120/0x188 lib/dump_stack.c:113
 print_address_description+0x68/0x278 mm/kasan/report.c:253
 kasan_report_error mm/kasan/report.c:351 [inline]
 kasan_report+0x21c/0x348 mm/kasan/report.c:409
 __asan_report_load4_noabort+0x30/0x40 mm/kasan/report.c:429
 inet_create+0xbcc/0xd28 net/ipv4/af_inet.c:340
 __sock_create+0x4f8/0x770 net/socket.c:1276
 sock_create_kern+0x50/0x68 net/socket.c:1322
 rds_tcp_conn_path_connect+0x2b4/0x690 net/rds/tcp_connect.c:114
 rds_connect_worker+0x108/0x1d0 net/rds/threads.c:175
 process_one_work+0x6e8/0x1700 kernel/workqueue.c:2153
 worker_thread+0x3b0/0xdd0 kernel/workqueue.c:2296
 kthread+0x2f0/0x378 kernel/kthread.c:255
 ret_from_fork+0x10/0x18 arch/arm64/kernel/entry.S:1117

Allocated by task 687:
 save_stack mm/kasan/kasan.c:448 [inline]
 set_track mm/kasan/kasan.c:460 [inline]
 kasan_kmalloc+0xd4/0x180 mm/kasan/kasan.c:553
 kasan_slab_alloc+0x14/0x20 mm/kasan/kasan.c:490
 slab_post_alloc_hook mm/slab.h:444 [inline]
 slab_alloc_node mm/slub.c:2705 [inline]
 slab_alloc mm/slub.c:2713 [inline]
 kmem_cache_alloc+0x14c/0x388 mm/slub.c:2718
 kmem_cache_zalloc include/linux/slab.h:697 [inline]
 net_alloc net/core/net_namespace.c:384 [inline]
 copy_net_ns+0xc4/0x2d0 net/core/net_namespace.c:424
 create_new_namespaces+0x300/0x658 kernel/nsproxy.c:107
 unshare_nsproxy_namespaces+0xa0/0x198 kernel/nsproxy.c:206
 ksys_unshare+0x340/0x628 kernel/fork.c:2577
 __do_sys_unshare kernel/fork.c:2645 [inline]
 __se_sys_unshare kernel/fork.c:2643 [inline]
 __arm64_sys_unshare+0x38/0x58 kernel/fork.c:2643
 __invoke_syscall arch/arm64/kernel/syscall.c:35 [inline]
 invoke_syscall arch/arm64/kernel/syscall.c:47 [inline]
 el0_svc_common+0x168/0x390 arch/arm64/kernel/syscall.c:83
 el0_svc_handler+0x60/0xd0 arch/arm64/kernel/syscall.c:129
 el0_svc+0x8/0xc arch/arm64/kernel/entry.S:960

Freed by task 264:
 save_stack mm/kasan/kasan.c:448 [inline]
 set_track mm/kasan/kasan.c:460 [inline]
 __kasan_slab_free+0x114/0x220 mm/kasan/kasan.c:521
 kasan_slab_free+0x10/0x18 mm/kasan/kasan.c:528
 slab_free_hook mm/slub.c:1370 [inline]
 slab_free_freelist_hook mm/slub.c:1397 [inline]
 slab_free mm/slub.c:2952 [inline]
 kmem_cache_free+0xb8/0x3a8 mm/slub.c:2968
 net_free net/core/net_namespace.c:400 [inline]
 net_drop_ns.part.6+0x78/0x90 net/core/net_namespace.c:407
 net_drop_ns net/core/net_namespace.c:406 [inline]
 cleanup_net+0x53c/0x6d8 net/core/net_namespace.c:569
 process_one_work+0x6e8/0x1700 kernel/workqueue.c:2153
 worker_thread+0x3b0/0xdd0 kernel/workqueue.c:2296
 kthread+0x2f0/0x378 kernel/kthread.c:255
 ret_from_fork+0x10/0x18 arch/arm64/kernel/entry.S:1117

The buggy address belongs to the object at ffff8003496a3f80
 which belongs to the cache net_namespace of size 7872
The buggy address is located 1796 bytes inside of
 7872-byte region [ffff8003496a3f80, ffff8003496a5e40)
The buggy address belongs to the page:
page:ffff7e000d25a800 count:1 mapcount:0 mapping:ffff80036ce4b000
index:0x0 compound_mapcount: 0
flags: 0xffffe0000008100(slab|head)
raw: 0ffffe0000008100 dead000000000100 dead000000000200 ffff80036ce4b000
raw: 0000000000000000 0000000080040004 00000001ffffffff 0000000000000000
page dumped because: kasan: bad access detected

Memory state around the buggy address:
 ffff8003496a4580: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
 ffff8003496a4600: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
>ffff8003496a4680: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
                   ^
 ffff8003496a4700: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
 ffff8003496a4780: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
==================================================================

Fixes: 467fa15356ac(""RDS-TCP: Support multiple RDS-TCP listen endpoints, one per netns."")
Reported-by: Hulk Robot <hulkci@huawei.com>
Signed-off-by: Mao Wenan <maowenan@huawei.com>
Signed-off-by: David S. Miller <davem@davemloft.net>",1,net/rds/tcp.c,"{""sha"": ""faf726e00e27c75b11721dbc55518ca60bdf00a6"", ""filename"": ""net/rds/tcp.c"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 1, ""changes"": 2, ""blob_url"": ""https://github.com/torvalds/linux/blob/cb66ddd156203daefb8d71158036b27b0e2caf63/net/rds/tcp.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cb66ddd156203daefb8d71158036b27b0e2caf63/net/rds/tcp.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/net/rds/tcp.c?ref=cb66ddd156203daefb8d71158036b27b0e2caf63"", ""patch"": ""@@ -608,7 +608,7 @@ static void rds_tcp_kill_sock(struct net *net)\n \tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n \t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n \n-\t\tif (net != c_net || !tc->t_sock)\n+\t\tif (net != c_net)\n \t\t\tcontinue;\n \t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n \t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);""}","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net || !tc->t_sock)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
",C,"		if (net != c_net)
","		if (net != c_net || !tc->t_sock)
",,"@@ -608,7 +608,7 @@ static void rds_tcp_kill_sock(struct net *net)
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
-		if (net != c_net || !tc->t_sock)
+		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);",linux,cb66ddd156203daefb8d71158036b27b0e2caf63,f28cd2af22a0c134e4aa1c64a70f70d815d473fb,1,"static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
//flaw_line_below:
		if (net != c_net || !tc->t_sock)
//fix_flaw_line_below:
//		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
",182851,"static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net || !tc->t_sock)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","static void rds_tcp_kill_sock(struct net *net)
{
	struct rds_tcp_connection *tc, *_tc;
	LIST_HEAD(tmp_list);
	struct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);
	struct socket *lsock = rtn->rds_tcp_listen_sock;

	rtn->rds_tcp_listen_sock = NULL;
	rds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);
	spin_lock_irq(&rds_tcp_conn_lock);
 	list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 		struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);
 
		if (net != c_net)
 			continue;
 		if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 			list_move_tail(&tc->t_tcp_node, &tmp_list);
		} else {
			list_del(&tc->t_tcp_node);
			tc->t_tcp_node_detached = true;
		}
	}
	spin_unlock_irq(&rds_tcp_conn_lock);
	list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)
		rds_conn_destroy(tc->t_cpath->cp_conn);
}
","The provided code defines a function `rds_tcp_kill_sock` that is responsible for terminating TCP connections associated with a specific network namespace. It first retrieves the listening socket and stops it, ensuring no new connections can be accepted. The function then acquires a lock to safely iterate through a list of active TCP connections, checking if each connection belongs to the specified network. If a connection is found, it is either moved to a temporary list for destruction or marked as detached if it already exists in the temporary list. Finally, the function cleans up by destroying the connections in the temporary list.","The code defines a function that manages the termination of TCP connections within a specific network context. It begins by stopping the listening socket to prevent new connections. The function then locks access to a list of active connections and iterates through them, checking if they belong to the specified network. For each relevant connection, it either moves it to a temporary list for later destruction or marks it as detached if it is already present. Finally, the function cleans up by destroying all connections stored in the temporary list, ensuring proper resource management and preventing leaks.","The vulnerability in the original code stemmed from the condition that prevented the destruction of connections when the socket was NULL, which could lead to a use-after-free error. Specifically, if a connection's socket was NULL, the cleanup function would skip calling the necessary destruction functions, leaving the connection in an inconsistent state. This could result in a worker thread attempting to access freed memory, causing crashes. The fix removed the check for the socket being NULL, ensuring that connections are always destroyed during cleanup, thus preventing the use-after-free scenario and ensuring proper resource management.","The vulnerability in the original code arose from the failure to destroy connections when a specific condition was met, particularly when a socket was NULL. This oversight meant that connections could remain active even after their associated network namespace was freed, leading to a use-after-free error when a worker thread attempted to access these connections. The fix involved removing the condition that checked for the socket's presence, ensuring that connections would be destroyed regardless of their state. This change guarantees that all resources are properly cleaned up, preventing the potential for accessing freed memory and ensuring stability.","The vulnerability was fixed by modifying the condition in the `rds_tcp_kill_sock` function that checked if the socket (`t_sock`) was NULL. The original code contained a check that prevented the destruction of connections when `t_sock` was NULL, which could lead to use-after-free errors. The updated code removed this check, allowing the function to always proceed with calling `rds_conn_destroy`, `rds_conn_path_destroy`, and `rds_tcp_conn_free` for all connections, regardless of the socket's state. This ensures that all connections are properly cleaned up, preventing any access to freed memory and enhancing overall stability.","The vulnerability was addressed by removing the condition that prevented the cleanup of connections when a specific state was detected. Previously, if a connection was in a certain state, the cleanup process would skip the necessary destruction steps, leaving resources allocated and potentially leading to access violations. The fix ensured that the cleanup process would always execute the destruction steps, regardless of the connection's state. This change guarantees that all resources are properly released, preventing any attempts to access freed memory and thereby enhancing the stability and reliability of the system during network namespace cleanup."
37,182856,182856,,Local,Not required,Complete,CVE-2019-11599,https://www.cvedetails.com/cve/CVE-2019-11599/,CWE-362,Medium,Complete,Complete,,2019-04-29,6.9,"The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",2019-05-28,DoS +Info ,3,https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a,04f5866e41fb70690e28397487d8bd8eea7d712a,"coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping

The core dumping code has always run without holding the mmap_sem for
writing, despite that is the only way to ensure that the entire vma
layout will not change from under it.  Only using some signal
serialization on the processes belonging to the mm is not nearly enough.
This was pointed out earlier.  For example in Hugh's post from Jul 2017:

  https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils

  ""Not strictly relevant here, but a related note: I was very surprised
   to discover, only quite recently, how handle_mm_fault() may be called
   without down_read(mmap_sem) - when core dumping. That seems a
   misguided optimization to me, which would also be nice to correct""

In particular because the growsdown and growsup can move the
vm_start/vm_end the various loops the core dump does around the vma will
not be consistent if page faults can happen concurrently.

Pretty much all users calling mmget_not_zero()/get_task_mm() and then
taking the mmap_sem had the potential to introduce unexpected side
effects in the core dumping code.

Adding mmap_sem for writing around the ->core_dump invocation is a
viable long term fix, but it requires removing all copy user and page
faults and to replace them with get_dump_page() for all binary formats
which is not suitable as a short term fix.

For the time being this solution manually covers the places that can
confuse the core dump either by altering the vma layout or the vma flags
while it runs.  Once ->core_dump runs under mmap_sem for writing the
function mmget_still_valid() can be dropped.

Allowing mmap_sem protected sections to run in parallel with the
coredump provides some minor parallelism advantage to the swapoff code
(which seems to be safe enough by never mangling any vma field and can
keep doing swapins in parallel to the core dumping) and to some other
corner case.

In order to facilitate the backporting I added ""Fixes: 86039bd3b4e6""
however the side effect of this same race condition in /proc/pid/mem
should be reproducible since before 2.6.12-rc2 so I couldn't add any
other ""Fixes:"" because there's no hash beyond the git genesis commit.

Because find_extend_vma() is the only location outside of the process
context that could modify the ""mm"" structures under mmap_sem for
reading, by adding the mmget_still_valid() check to it, all other cases
that take the mmap_sem for reading don't need the new check after
mmget_not_zero()/get_task_mm().  The expand_stack() in page fault
context also doesn't need the new check, because all tasks under core
dumping are frozen.

Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com
Fixes: 86039bd3b4e6 (""userfaultfd: add new syscall to provide memory externalization"")
Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Reported-by: Jann Horn <jannh@google.com>
Suggested-by: Oleg Nesterov <oleg@redhat.com>
Acked-by: Peter Xu <peterx@redhat.com>
Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
Reviewed-by: Oleg Nesterov <oleg@redhat.com>
Reviewed-by: Jann Horn <jannh@google.com>
Acked-by: Jason Gunthorpe <jgg@mellanox.com>
Acked-by: Michal Hocko <mhocko@suse.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,drivers/infiniband/core/uverbs_main.c,"{""sha"": ""f2e7ffe6fc546612f62da9cde853b9c1bf37d8bb"", ""filename"": ""drivers/infiniband/core/uverbs_main.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 0, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/infiniband/core/uverbs_main.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t * will only be one mm, so no big deal.\n \t\t */\n \t\tdown_write(&mm->mmap_sem);\n+\t\tif (!mmget_still_valid(mm))\n+\t\t\tgoto skip_mm;\n \t\tmutex_lock(&ufile->umap_lock);\n \t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n \t\t\t\t\t  list) {\n@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n \t\t}\n \t\tmutex_unlock(&ufile->umap_lock);\n+\tskip_mm:\n \t\tup_write(&mm->mmap_sem);\n \t\tmmput(mm);\n \t}""}<_**next**_>{""sha"": ""95ca1fe7283cff265247c6f3a84e5fa573299fca"", ""filename"": ""fs/proc/task_mmu.c"", ""status"": ""modified"", ""additions"": 18, ""deletions"": 0, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/task_mmu.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,\n \t\t\t\t\tcount = -EINTR;\n \t\t\t\t\tgoto out_mm;\n \t\t\t\t}\n+\t\t\t\t/*\n+\t\t\t\t * Avoid to modify vma->vm_flags\n+\t\t\t\t * without locked ops while the\n+\t\t\t\t * coredump reads the vm_flags.\n+\t\t\t\t */\n+\t\t\t\tif (!mmget_still_valid(mm)) {\n+\t\t\t\t\t/*\n+\t\t\t\t\t * Silently return \""count\""\n+\t\t\t\t\t * like if get_task_mm()\n+\t\t\t\t\t * failed. FIXME: should this\n+\t\t\t\t\t * function have returned\n+\t\t\t\t\t * -ESRCH if get_task_mm()\n+\t\t\t\t\t * failed like if\n+\t\t\t\t\t * get_proc_task() fails?\n+\t\t\t\t\t */\n+\t\t\t\t\tup_write(&mm->mmap_sem);\n+\t\t\t\t\tgoto out_mm;\n+\t\t\t\t}\n \t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n \t\t\t\t\tvma_set_page_prot(vma);""}<_**next**_>{""sha"": ""f5de1e726356a51c27ff529f98d99032650eb839"", ""filename"": ""fs/userfaultfd.c"", ""status"": ""modified"", ""additions"": 9, ""deletions"": 0, ""changes"": 9, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/userfaultfd.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -629,6 +629,8 @@ static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n \n \t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n \t\tdown_write(&mm->mmap_sem);\n+\t\t/* no task can run (and in turn coredump) yet */\n+\t\tVM_WARN_ON(!mmget_still_valid(mm));\n \t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n \t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n \t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n@@ -883,6 +885,8 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t * taking the mmap_sem for writing.\n \t */\n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto skip_mm;\n \tprev = NULL;\n \tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\tcond_resched();\n@@ -905,6 +909,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t\tvma->vm_flags = new_flags;\n \t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n \t}\n+skip_mm:\n \tup_write(&mm->mmap_sem);\n \tmmput(mm);\n wakeup:\n@@ -1333,6 +1338,8 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;\n@@ -1520,6 +1527,8 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;""}<_**next**_>{""sha"": ""a3fda9f024c3c1988b6ff60954d7f7e74a9c1ecf"", ""filename"": ""include/linux/sched/mm.h"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/sched/mm.h?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -49,6 +49,27 @@ static inline void mmdrop(struct mm_struct *mm)\n \t\t__mmdrop(mm);\n }\n \n+/*\n+ * This has to be called after a get_task_mm()/mmget_not_zero()\n+ * followed by taking the mmap_sem for writing before modifying the\n+ * vmas or anything the coredump pretends not to change from under it.\n+ *\n+ * NOTE: find_extend_vma() called from GUP context is the only place\n+ * that can modify the \""mm\"" (notably the vm_start/end) under mmap_sem\n+ * for reading and outside the context of the process, so it is also\n+ * the only case that holds the mmap_sem for reading that must call\n+ * this function. Generally if the mmap_sem is hold for reading\n+ * there's no need of this check after get_task_mm()/mmget_not_zero().\n+ *\n+ * This function can be obsoleted and the check can be removed, after\n+ * the coredump code will hold the mmap_sem for writing before\n+ * invoking the ->core_dump methods.\n+ */\n+static inline bool mmget_still_valid(struct mm_struct *mm)\n+{\n+\treturn likely(!mm->core_state);\n+}\n+\n /**\n  * mmget() - Pin the address space associated with a &struct mm_struct.\n  * @mm: The address space to pin.""}<_**next**_>{""sha"": ""bd7b9f293b391f22b85810e48bc7c0679b217f05"", ""filename"": ""mm/mmap.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 1, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/mmap.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -45,6 +45,7 @@\n #include <linux/moduleparam.h>\n #include <linux/pkeys.h>\n #include <linux/oom.h>\n+#include <linux/sched/mm.h>\n \n #include <linux/uaccess.h>\n #include <asm/cacheflush.h>\n@@ -2525,7 +2526,8 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \tvma = find_vma_prev(mm, addr, &prev);\n \tif (vma && (vma->vm_start <= addr))\n \t\treturn vma;\n-\tif (!prev || expand_stack(prev, addr))\n+\t/* don't alter vm_end if the coredump is running */\n+\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n \t\treturn NULL;\n \tif (prev->vm_flags & VM_LOCKED)\n \t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n@@ -2551,6 +2553,9 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \t\treturn vma;\n \tif (!(vma->vm_flags & VM_GROWSDOWN))\n \t\treturn NULL;\n+\t/* don't alter vm_start if the coredump is running */\n+\tif (!mmget_still_valid(mm))\n+\t\treturn NULL;\n \tstart = vma->vm_start;\n \tif (expand_stack(vma, addr))\n \t\treturn NULL;""}","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
		if (!mmget_still_valid(mm))
			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
",C,"		if (!mmget_still_valid(mm))
			goto skip_mm;
	skip_mm:
",,,"@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
+		if (!mmget_still_valid(mm))
+			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
+	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}",linux,04f5866e41fb70690e28397487d8bd8eea7d712a,dce5b0bdeec61bdbee56121ceb1d014151d5cab1,1,"void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
//fix_flaw_line_below:
//		if (!mmget_still_valid(mm))
//fix_flaw_line_below:
//			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
//fix_flaw_line_below:
//	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
",182856,"void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
{
	struct rdma_umap_priv *priv, *next_priv;

	lockdep_assert_held(&ufile->hw_destroy_rwsem);

	while (1) {
		struct mm_struct *mm = NULL;

		/* Get an arbitrary mm pointer that hasn't been cleaned yet */
		mutex_lock(&ufile->umap_lock);
		while (!list_empty(&ufile->umaps)) {
			int ret;

			priv = list_first_entry(&ufile->umaps,
						struct rdma_umap_priv, list);
			mm = priv->vma->vm_mm;
			ret = mmget_not_zero(mm);
			if (!ret) {
				list_del_init(&priv->list);
				mm = NULL;
				continue;
			}
			break;
		}
		mutex_unlock(&ufile->umap_lock);
		if (!mm)
			return;

		/*
		 * The umap_lock is nested under mmap_sem since it used within
		 * the vma_ops callbacks, so we have to clean the list one mm
		 * at a time to get the lock ordering right. Typically there
 		 * will only be one mm, so no big deal.
 		 */
 		down_write(&mm->mmap_sem);
		if (!mmget_still_valid(mm))
			goto skip_mm;
 		mutex_lock(&ufile->umap_lock);
 		list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
 					  list) {
			struct vm_area_struct *vma = priv->vma;

			if (vma->vm_mm != mm)
				continue;
			list_del_init(&priv->list);

			zap_vma_ptes(vma, vma->vm_start,
				     vma->vm_end - vma->vm_start);
 			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
 		}
 		mutex_unlock(&ufile->umap_lock);
	skip_mm:
 		up_write(&mm->mmap_sem);
 		mmput(mm);
 	}
}
","The function `uverbs_user_mmap_disassociate` is designed to disassociate user memory mappings from an RDMA (Remote Direct Memory Access) file structure in a concurrent environment. It first acquires a lock to ensure thread safety while iterating through a list of user mappings associated with the file. For each mapping, it checks if the corresponding memory area (mm_struct) is still valid; if not, it removes the mapping from the list. Once a valid memory area is found, it acquires a write lock on the memory's semaphore, cleans up the associated virtual memory areas (VMAs) by removing page table entries, and updates the VMA flags to prevent sharing. Finally, it releases the locks and decrements the reference count on the memory area.","The provided code implements a function that disassociates user memory mappings from a specific file structure in a multi-threaded environment. It ensures thread safety by acquiring necessary locks while iterating through a list of user mappings. The function checks the validity of each memory area associated with the mappings, removing any that are no longer valid. Upon finding a valid memory area, it acquires a write lock to safely clean up the associated virtual memory areas by removing page table entries and updating flags to prevent sharing. Finally, it releases the locks and decrements the reference count on the memory area.","The vulnerability in the original code stemmed from a race condition between the functions that check the validity of memory areas and the core dumping process, which could lead to inconsistent virtual memory area (VMA) layouts during execution. Specifically, the use of `mmget_not_zero()` without proper locking allowed concurrent modifications to the VMA structure, risking unexpected behavior during core dumps. The fix introduced a check using `mmget_still_valid()` to ensure that the memory area remains valid while the core dump is being processed, thus preventing alterations to the VMA layout or flags during critical operations and ensuring thread safety.","The vulnerability in the original code arose from a race condition during the core dumping process, where the memory layout could be altered concurrently, leading to inconsistencies. Specifically, the core dump operations did not hold the necessary locks to prevent changes to the virtual memory areas while they were being accessed, which could result in unexpected behavior. The fix involved introducing a check to ensure that the memory area remained valid during the core dump, thereby preventing any alterations to the memory layout or flags while the core dump was in progress, thus enhancing overall thread safety.","The vulnerability was fixed by adding a check using `mmget_still_valid()` to ensure that the memory area remains valid during the core dump process. This check was introduced after acquiring the write lock on the memory's semaphore with `down_write()`, preventing any concurrent modifications to the virtual memory area layout while the core dump is being executed. Additionally, the code was structured to handle the cleanup of virtual memory areas safely, ensuring that the flags and layout remain consistent throughout the core dump operation, thus eliminating the race condition that could lead to unexpected behavior.","The vulnerability was addressed by implementing a validation check to ensure that the memory area remains intact during the core dump process. This involved acquiring a write lock to prevent any concurrent modifications to the memory layout while the core dump is being executed. The code was restructured to ensure that any cleanup of memory areas occurs safely, maintaining the integrity of the memory flags and layout throughout the operation. This approach effectively eliminated the race condition that could lead to inconsistencies and unexpected behavior during core dumping."
38,182857,182857,,Local,Not required,Complete,CVE-2019-11599,https://www.cvedetails.com/cve/CVE-2019-11599/,CWE-362,Medium,Complete,Complete,,2019-04-29,6.9,"The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",2019-05-28,DoS +Info ,18,https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a,04f5866e41fb70690e28397487d8bd8eea7d712a,"coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping

The core dumping code has always run without holding the mmap_sem for
writing, despite that is the only way to ensure that the entire vma
layout will not change from under it.  Only using some signal
serialization on the processes belonging to the mm is not nearly enough.
This was pointed out earlier.  For example in Hugh's post from Jul 2017:

  https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils

  ""Not strictly relevant here, but a related note: I was very surprised
   to discover, only quite recently, how handle_mm_fault() may be called
   without down_read(mmap_sem) - when core dumping. That seems a
   misguided optimization to me, which would also be nice to correct""

In particular because the growsdown and growsup can move the
vm_start/vm_end the various loops the core dump does around the vma will
not be consistent if page faults can happen concurrently.

Pretty much all users calling mmget_not_zero()/get_task_mm() and then
taking the mmap_sem had the potential to introduce unexpected side
effects in the core dumping code.

Adding mmap_sem for writing around the ->core_dump invocation is a
viable long term fix, but it requires removing all copy user and page
faults and to replace them with get_dump_page() for all binary formats
which is not suitable as a short term fix.

For the time being this solution manually covers the places that can
confuse the core dump either by altering the vma layout or the vma flags
while it runs.  Once ->core_dump runs under mmap_sem for writing the
function mmget_still_valid() can be dropped.

Allowing mmap_sem protected sections to run in parallel with the
coredump provides some minor parallelism advantage to the swapoff code
(which seems to be safe enough by never mangling any vma field and can
keep doing swapins in parallel to the core dumping) and to some other
corner case.

In order to facilitate the backporting I added ""Fixes: 86039bd3b4e6""
however the side effect of this same race condition in /proc/pid/mem
should be reproducible since before 2.6.12-rc2 so I couldn't add any
other ""Fixes:"" because there's no hash beyond the git genesis commit.

Because find_extend_vma() is the only location outside of the process
context that could modify the ""mm"" structures under mmap_sem for
reading, by adding the mmget_still_valid() check to it, all other cases
that take the mmap_sem for reading don't need the new check after
mmget_not_zero()/get_task_mm().  The expand_stack() in page fault
context also doesn't need the new check, because all tasks under core
dumping are frozen.

Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com
Fixes: 86039bd3b4e6 (""userfaultfd: add new syscall to provide memory externalization"")
Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Reported-by: Jann Horn <jannh@google.com>
Suggested-by: Oleg Nesterov <oleg@redhat.com>
Acked-by: Peter Xu <peterx@redhat.com>
Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
Reviewed-by: Oleg Nesterov <oleg@redhat.com>
Reviewed-by: Jann Horn <jannh@google.com>
Acked-by: Jason Gunthorpe <jgg@mellanox.com>
Acked-by: Michal Hocko <mhocko@suse.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",0,fs/proc/task_mmu.c,"{""sha"": ""f2e7ffe6fc546612f62da9cde853b9c1bf37d8bb"", ""filename"": ""drivers/infiniband/core/uverbs_main.c"", ""status"": ""modified"", ""additions"": 3, ""deletions"": 0, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/drivers/infiniband/core/uverbs_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/drivers/infiniband/core/uverbs_main.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -993,6 +993,8 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t * will only be one mm, so no big deal.\n \t\t */\n \t\tdown_write(&mm->mmap_sem);\n+\t\tif (!mmget_still_valid(mm))\n+\t\t\tgoto skip_mm;\n \t\tmutex_lock(&ufile->umap_lock);\n \t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n \t\t\t\t\t  list) {\n@@ -1007,6 +1009,7 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n \t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n \t\t}\n \t\tmutex_unlock(&ufile->umap_lock);\n+\tskip_mm:\n \t\tup_write(&mm->mmap_sem);\n \t\tmmput(mm);\n \t}""}<_**next**_>{""sha"": ""95ca1fe7283cff265247c6f3a84e5fa573299fca"", ""filename"": ""fs/proc/task_mmu.c"", ""status"": ""modified"", ""additions"": 18, ""deletions"": 0, ""changes"": 18, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/proc/task_mmu.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/proc/task_mmu.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,\n \t\t\t\t\tcount = -EINTR;\n \t\t\t\t\tgoto out_mm;\n \t\t\t\t}\n+\t\t\t\t/*\n+\t\t\t\t * Avoid to modify vma->vm_flags\n+\t\t\t\t * without locked ops while the\n+\t\t\t\t * coredump reads the vm_flags.\n+\t\t\t\t */\n+\t\t\t\tif (!mmget_still_valid(mm)) {\n+\t\t\t\t\t/*\n+\t\t\t\t\t * Silently return \""count\""\n+\t\t\t\t\t * like if get_task_mm()\n+\t\t\t\t\t * failed. FIXME: should this\n+\t\t\t\t\t * function have returned\n+\t\t\t\t\t * -ESRCH if get_task_mm()\n+\t\t\t\t\t * failed like if\n+\t\t\t\t\t * get_proc_task() fails?\n+\t\t\t\t\t */\n+\t\t\t\t\tup_write(&mm->mmap_sem);\n+\t\t\t\t\tgoto out_mm;\n+\t\t\t\t}\n \t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n \t\t\t\t\tvma_set_page_prot(vma);""}<_**next**_>{""sha"": ""f5de1e726356a51c27ff529f98d99032650eb839"", ""filename"": ""fs/userfaultfd.c"", ""status"": ""modified"", ""additions"": 9, ""deletions"": 0, ""changes"": 9, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/fs/userfaultfd.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/userfaultfd.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -629,6 +629,8 @@ static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n \n \t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n \t\tdown_write(&mm->mmap_sem);\n+\t\t/* no task can run (and in turn coredump) yet */\n+\t\tVM_WARN_ON(!mmget_still_valid(mm));\n \t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n \t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n \t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n@@ -883,6 +885,8 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t * taking the mmap_sem for writing.\n \t */\n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto skip_mm;\n \tprev = NULL;\n \tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\tcond_resched();\n@@ -905,6 +909,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)\n \t\tvma->vm_flags = new_flags;\n \t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n \t}\n+skip_mm:\n \tup_write(&mm->mmap_sem);\n \tmmput(mm);\n wakeup:\n@@ -1333,6 +1338,8 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;\n@@ -1520,6 +1527,8 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;""}<_**next**_>{""sha"": ""a3fda9f024c3c1988b6ff60954d7f7e74a9c1ecf"", ""filename"": ""include/linux/sched/mm.h"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/include/linux/sched/mm.h"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/include/linux/sched/mm.h?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -49,6 +49,27 @@ static inline void mmdrop(struct mm_struct *mm)\n \t\t__mmdrop(mm);\n }\n \n+/*\n+ * This has to be called after a get_task_mm()/mmget_not_zero()\n+ * followed by taking the mmap_sem for writing before modifying the\n+ * vmas or anything the coredump pretends not to change from under it.\n+ *\n+ * NOTE: find_extend_vma() called from GUP context is the only place\n+ * that can modify the \""mm\"" (notably the vm_start/end) under mmap_sem\n+ * for reading and outside the context of the process, so it is also\n+ * the only case that holds the mmap_sem for reading that must call\n+ * this function. Generally if the mmap_sem is hold for reading\n+ * there's no need of this check after get_task_mm()/mmget_not_zero().\n+ *\n+ * This function can be obsoleted and the check can be removed, after\n+ * the coredump code will hold the mmap_sem for writing before\n+ * invoking the ->core_dump methods.\n+ */\n+static inline bool mmget_still_valid(struct mm_struct *mm)\n+{\n+\treturn likely(!mm->core_state);\n+}\n+\n /**\n  * mmget() - Pin the address space associated with a &struct mm_struct.\n  * @mm: The address space to pin.""}<_**next**_>{""sha"": ""bd7b9f293b391f22b85810e48bc7c0679b217f05"", ""filename"": ""mm/mmap.c"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 1, ""changes"": 7, ""blob_url"": ""https://github.com/torvalds/linux/blob/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04f5866e41fb70690e28397487d8bd8eea7d712a/mm/mmap.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/mm/mmap.c?ref=04f5866e41fb70690e28397487d8bd8eea7d712a"", ""patch"": ""@@ -45,6 +45,7 @@\n #include <linux/moduleparam.h>\n #include <linux/pkeys.h>\n #include <linux/oom.h>\n+#include <linux/sched/mm.h>\n \n #include <linux/uaccess.h>\n #include <asm/cacheflush.h>\n@@ -2525,7 +2526,8 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \tvma = find_vma_prev(mm, addr, &prev);\n \tif (vma && (vma->vm_start <= addr))\n \t\treturn vma;\n-\tif (!prev || expand_stack(prev, addr))\n+\t/* don't alter vm_end if the coredump is running */\n+\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n \t\treturn NULL;\n \tif (prev->vm_flags & VM_LOCKED)\n \t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n@@ -2551,6 +2553,9 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)\n \t\treturn vma;\n \tif (!(vma->vm_flags & VM_GROWSDOWN))\n \t\treturn NULL;\n+\t/* don't alter vm_start if the coredump is running */\n+\tif (!mmget_still_valid(mm))\n+\t\treturn NULL;\n \tstart = vma->vm_start;\n \tif (expand_stack(vma, addr))\n \t\treturn NULL;""}","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
",C,"				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
",,,"@@ -1143,6 +1143,24 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 					count = -EINTR;
 					goto out_mm;
 				}
+				/*
+				 * Avoid to modify vma->vm_flags
+				 * without locked ops while the
+				 * coredump reads the vm_flags.
+				 */
+				if (!mmget_still_valid(mm)) {
+					/*
+					 * Silently return ""count""
+					 * like if get_task_mm()
+					 * failed. FIXME: should this
+					 * function have returned
+					 * -ESRCH if get_task_mm()
+					 * failed like if
+					 * get_proc_task() fails?
+					 */
+					up_write(&mm->mmap_sem);
+					goto out_mm;
+				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);",linux,04f5866e41fb70690e28397487d8bd8eea7d712a,dce5b0bdeec61bdbee56121ceb1d014151d5cab1,1,"static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
//fix_flaw_line_below:
//				/*
//fix_flaw_line_below:
//				 * Avoid to modify vma->vm_flags
//fix_flaw_line_below:
//				 * without locked ops while the
//fix_flaw_line_below:
//				 * coredump reads the vm_flags.
//fix_flaw_line_below:
//				 */
//fix_flaw_line_below:
//				if (!mmget_still_valid(mm)) {
//fix_flaw_line_below:
//					/*
//fix_flaw_line_below:
//					 * Silently return ""count""
//fix_flaw_line_below:
//					 * like if get_task_mm()
//fix_flaw_line_below:
//					 * failed. FIXME: should this
//fix_flaw_line_below:
//					 * function have returned
//fix_flaw_line_below:
//					 * -ESRCH if get_task_mm()
//fix_flaw_line_below:
//					 * failed like if
//fix_flaw_line_below:
//					 * get_proc_task() fails?
//fix_flaw_line_below:
//					 */
//fix_flaw_line_below:
//					up_write(&mm->mmap_sem);
//fix_flaw_line_below:
//					goto out_mm;
//fix_flaw_line_below:
//				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
",182857,"static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","static ssize_t clear_refs_write(struct file *file, const char __user *buf,
				size_t count, loff_t *ppos)
{
	struct task_struct *task;
	char buffer[PROC_NUMBUF];
	struct mm_struct *mm;
	struct vm_area_struct *vma;
	enum clear_refs_types type;
	struct mmu_gather tlb;
	int itype;
	int rv;

	memset(buffer, 0, sizeof(buffer));
	if (count > sizeof(buffer) - 1)
		count = sizeof(buffer) - 1;
	if (copy_from_user(buffer, buf, count))
		return -EFAULT;
	rv = kstrtoint(strstrip(buffer), 10, &itype);
	if (rv < 0)
		return rv;
	type = (enum clear_refs_types)itype;
	if (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)
		return -EINVAL;

	task = get_proc_task(file_inode(file));
	if (!task)
		return -ESRCH;
	mm = get_task_mm(task);
	if (mm) {
		struct mmu_notifier_range range;
		struct clear_refs_private cp = {
			.type = type,
		};
		struct mm_walk clear_refs_walk = {
			.pmd_entry = clear_refs_pte_range,
			.test_walk = clear_refs_test_walk,
			.mm = mm,
			.private = &cp,
		};

		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
			if (down_write_killable(&mm->mmap_sem)) {
				count = -EINTR;
				goto out_mm;
			}

			/*
			 * Writing 5 to /proc/pid/clear_refs resets the peak
			 * resident set size to this mm's current rss value.
			 */
			reset_mm_hiwater_rss(mm);
			up_write(&mm->mmap_sem);
			goto out_mm;
		}

		down_read(&mm->mmap_sem);
		tlb_gather_mmu(&tlb, mm, 0, -1);
		if (type == CLEAR_REFS_SOFT_DIRTY) {
			for (vma = mm->mmap; vma; vma = vma->vm_next) {
				if (!(vma->vm_flags & VM_SOFTDIRTY))
					continue;
				up_read(&mm->mmap_sem);
				if (down_write_killable(&mm->mmap_sem)) {
 					count = -EINTR;
 					goto out_mm;
 				}
				/*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
				 */
				if (!mmget_still_valid(mm)) {
					/*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
					 */
					up_write(&mm->mmap_sem);
					goto out_mm;
				}
 				for (vma = mm->mmap; vma; vma = vma->vm_next) {
 					vma->vm_flags &= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
				}
				downgrade_write(&mm->mmap_sem);
				break;
			}

			mmu_notifier_range_init(&range, mm, 0, -1UL);
			mmu_notifier_invalidate_range_start(&range);
		}
		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
		if (type == CLEAR_REFS_SOFT_DIRTY)
			mmu_notifier_invalidate_range_end(&range);
		tlb_finish_mmu(&tlb, 0, -1);
		up_read(&mm->mmap_sem);
out_mm:
		mmput(mm);
	}
	put_task_struct(task);

	return count;
}
","The `clear_refs_write` function is designed to handle write operations to a specific file in the `/proc` filesystem, allowing users to clear reference types in a process's memory management structure. It begins by reading user input to determine the type of reference to clear, validating the input against predefined types. If the type is valid, it retrieves the associated task and its memory management structure. Depending on the reference type, it may reset the peak resident set size or clear soft dirty flags from virtual memory areas. The function utilizes synchronization mechanisms to ensure safe access to memory structures and performs necessary cleanup before returning the number of bytes processed or an error code.","The provided code implements a function that processes write requests to a specific file in the system's process directory, enabling the clearing of memory reference types for a specified process. It begins by reading and validating user input to determine the reference type. Upon confirming the validity, it retrieves the target process's memory management structure and performs operations based on the reference type, such as resetting memory usage metrics or clearing specific memory flags. The function ensures safe access to memory structures through synchronization mechanisms and concludes by returning the number of bytes processed or an error code if applicable.","The vulnerability in the original code stemmed from a race condition between memory management operations and core dumping, where concurrent modifications to the virtual memory area (VMA) layout could occur without proper synchronization. Specifically, the code did not hold the necessary write lock on the memory map semaphore during critical operations, allowing potential inconsistencies in VMA flags and layout. The fix introduced a check using `mmget_still_valid()` to ensure that the memory management structure remains valid while modifying VMA flags, thus preventing unexpected side effects during core dumps and ensuring that the VMA layout remains stable throughout the operation.","The vulnerability in the original code arose from a race condition during core dumping, where concurrent modifications to the virtual memory area layout could occur without proper synchronization, leading to inconsistencies. Specifically, the core dumping process did not hold the necessary write lock on the memory map, allowing potential alterations to the memory structure while it was being accessed. The fix involved introducing a validation check to ensure that the memory management structure remains valid during critical operations, thereby preventing unexpected side effects and ensuring the stability of the memory layout throughout the core dump process.","The vulnerability was fixed by adding a check using `mmget_still_valid()` to ensure that the memory management structure remains valid while modifying virtual memory area flags. This check was introduced in the context of operations that could alter the VMA layout, specifically around the core dump process. By requiring the memory management structure to be valid before proceeding with modifications, the code prevents inconsistencies that could arise from concurrent access. Additionally, the fix ensures that the memory map semaphore is held for writing during critical operations, thereby stabilizing the memory layout and avoiding race conditions during core dumping.","The vulnerability was addressed by implementing a validation mechanism to ensure that the memory management structure remains intact during critical operations that could alter the virtual memory layout. This involved adding a check to confirm the validity of the memory structure before making any modifications, thereby preventing inconsistencies that could arise from concurrent access. Additionally, the fix required holding the necessary write lock during key operations, which stabilized the memory layout and eliminated the race conditions that could occur during the core dump process, ensuring safe and consistent memory management."
39,182913,182913,,Remote,Not required,Partial,CVE-2019-6974,https://www.cvedetails.com/cve/CVE-2019-6974/,CWE-362,Medium,Partial,Partial,,2019-02-15,6.8,"In the Linux kernel before 4.20.8, kvm_ioctl_create_device in virt/kvm/kvm_main.c mishandles reference counting because of a race condition, leading to a use-after-free.",2019-09-20,,2,https://github.com/torvalds/linux/commit/cfa39381173d5f969daf43582c95ad679189cbc9,cfa39381173d5f969daf43582c95ad679189cbc9,"kvm: fix kvm_ioctl_create_device() reference counting (CVE-2019-6974)

kvm_ioctl_create_device() does the following:

1. creates a device that holds a reference to the VM object (with a borrowed
   reference, the VM's refcount has not been bumped yet)
2. initializes the device
3. transfers the reference to the device to the caller's file descriptor table
4. calls kvm_get_kvm() to turn the borrowed reference to the VM into a real
   reference

The ownership transfer in step 3 must not happen before the reference to the VM
becomes a proper, non-borrowed reference, which only happens in step 4.
After step 3, an attacker can close the file descriptor and drop the borrowed
reference, which can cause the refcount of the kvm object to drop to zero.

This means that we need to grab a reference for the device before
anon_inode_getfd(), otherwise the VM can disappear from under us.

Fixes: 852b6d57dc7f (""kvm: add device control API"")
Cc: stable@kernel.org
Signed-off-by: Jann Horn <jannh@google.com>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>",1,virt/kvm/kvm_main.c,"{""sha"": ""585845203db89e5d2bc90722fd75bfcf595671d9"", ""filename"": ""virt/kvm/kvm_main.c"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 1, ""changes"": 3, ""blob_url"": ""https://github.com/torvalds/linux/blob/cfa39381173d5f969daf43582c95ad679189cbc9/virt/kvm/kvm_main.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/cfa39381173d5f969daf43582c95ad679189cbc9/virt/kvm/kvm_main.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/virt/kvm/kvm_main.c?ref=cfa39381173d5f969daf43582c95ad679189cbc9"", ""patch"": ""@@ -3000,16 +3000,17 @@ static int kvm_ioctl_create_device(struct kvm *kvm,\n \tif (ops->init)\n \t\tops->init(dev);\n \n+\tkvm_get_kvm(kvm);\n \tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n \tif (ret < 0) {\n+\t\tkvm_put_kvm(kvm);\n \t\tmutex_lock(&kvm->lock);\n \t\tlist_del(&dev->vm_node);\n \t\tmutex_unlock(&kvm->lock);\n \t\tops->destroy(dev);\n \t\treturn ret;\n \t}\n \n-\tkvm_get_kvm(kvm);\n \tcd->fd = ret;\n \treturn 0;\n }""}","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
 	cd->fd = ret;
 	return 0;
 }
","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
",C,"	kvm_get_kvm(kvm);
		kvm_put_kvm(kvm);
","	kvm_get_kvm(kvm);
",,"@@ -3000,16 +3000,17 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 	if (ops->init)
 		ops->init(dev);
 
+	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
+		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
-	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }",linux,cfa39381173d5f969daf43582c95ad679189cbc9,8834f5600cf3c8db365e18a3d5cac2c2780c81e5,1,"static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
//fix_flaw_line_below:
//	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
//fix_flaw_line_below:
//		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
//flaw_line_below:
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
",182913,"static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
	kvm_get_kvm(kvm);
 	cd->fd = ret;
 	return 0;
 }
","static int kvm_ioctl_create_device(struct kvm *kvm,
				   struct kvm_create_device *cd)
{
	struct kvm_device_ops *ops = NULL;
	struct kvm_device *dev;
	bool test = cd->flags & KVM_CREATE_DEVICE_TEST;
	int ret;

	if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
		return -ENODEV;

	ops = kvm_device_ops_table[cd->type];
	if (ops == NULL)
		return -ENODEV;

	if (test)
		return 0;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return -ENOMEM;

	dev->ops = ops;
	dev->kvm = kvm;

	mutex_lock(&kvm->lock);
	ret = ops->create(dev, cd->type);
	if (ret < 0) {
		mutex_unlock(&kvm->lock);
		kfree(dev);
		return ret;
	}
	list_add(&dev->vm_node, &kvm->devices);
	mutex_unlock(&kvm->lock);

 	if (ops->init)
 		ops->init(dev);
 
	kvm_get_kvm(kvm);
 	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 	if (ret < 0) {
		kvm_put_kvm(kvm);
 		mutex_lock(&kvm->lock);
 		list_del(&dev->vm_node);
 		mutex_unlock(&kvm->lock);
 		ops->destroy(dev);
 		return ret;
 	}
 
 	cd->fd = ret;
 	return 0;
 }
","The `kvm_ioctl_create_device` function is responsible for creating a virtual device within a Kernel-based Virtual Machine (KVM) environment. It first checks if the requested device type is valid and retrieves the corresponding device operations. If the function is called in test mode, it returns immediately without creating a device. Otherwise, it allocates memory for the device structure and locks the KVM instance to safely create the device using the specified operations. If successful, it initializes the device, adds it to the KVM's device list, and retrieves a file descriptor for the device, returning it to the caller.","The provided code defines a function that facilitates the creation of a virtual device within a virtual machine environment. It begins by validating the requested device type and checking if the operation is in test mode. If not in test mode, it allocates memory for the device and locks the virtual machine instance to ensure thread safety. The function then attempts to create the device using specified operations and, upon success, initializes it and adds it to the list of devices. Finally, it retrieves a file descriptor for the device, which is returned to the caller, indicating successful creation.","The vulnerability in the original code arises from improper reference counting related to the virtual machine object. Specifically, the function creates a device that holds a borrowed reference to the VM, but it transfers this reference to the caller's file descriptor before converting it into a proper reference. This allows an attacker to close the file descriptor, potentially dropping the borrowed reference and causing the VM's reference count to drop to zero, leading to use-after-free issues. The fix ensures that a proper reference to the VM is obtained with `kvm_get_kvm()` before transferring ownership via `anon_inode_getfd()`, preventing the VM from being prematurely released.","The vulnerability in the original code stems from improper handling of reference counting for the virtual machine object, where a device is created with a borrowed reference that is later transferred to the caller's file descriptor. This sequence allows an attacker to close the file descriptor, potentially dropping the borrowed reference and causing the reference count of the virtual machine to drop to zero, leading to a use-after-free condition. The fix ensures that a proper reference is obtained before transferring ownership, thereby preventing the virtual machine from being released prematurely and maintaining the integrity of the reference counting mechanism.","The vulnerability was fixed by rearranging the sequence of operations related to reference counting. Specifically, the code now ensures that a proper reference to the virtual machine is obtained using `kvm_get_kvm()` before the ownership of the device is transferred through `anon_inode_getfd()`. This change prevents the situation where the file descriptor could be closed by an attacker, which would drop the borrowed reference and potentially lead to the virtual machine's reference count dropping to zero. By securing the reference first, the integrity of the virtual machine's lifecycle is maintained, mitigating the risk of use-after-free vulnerabilities.","The vulnerability was addressed by modifying the order of operations to ensure that a proper reference to the virtual machine is established before transferring ownership of the device to the caller. This adjustment prevents the scenario where an attacker could close the file descriptor and inadvertently drop a borrowed reference, which would lead to a critical drop in the reference count of the virtual machine. By securing the reference prior to the ownership transfer, the fix effectively maintains the integrity of the virtual machine's lifecycle and mitigates the risk of potential use-after-free issues."
40,183036,183036,,Local,Not required,Complete,CVE-2016-10741,https://www.cvedetails.com/cve/CVE-2016-10741/,CWE-362,Medium,,,,2019-02-01,4.7,"In the Linux kernel before 4.9.3, fs/xfs/xfs_aops.c allows local users to cause a denial of service (system crash) because there is a race condition between direct and memory-mapped I/O (associated with a hole) that is handled with BUG_ON instead of an I/O failure.",2019-04-18,DoS ,20,https://github.com/torvalds/linux/commit/04197b341f23b908193308b8d63d17ff23232598,04197b341f23b908193308b8d63d17ff23232598,"xfs: don't BUG() on mixed direct and mapped I/O

We've had reports of generic/095 causing XFS to BUG() in
__xfs_get_blocks() due to the existence of delalloc blocks on a
direct I/O read. generic/095 issues a mix of various types of I/O,
including direct and memory mapped I/O to a single file. This is
clearly not supported behavior and is known to lead to such
problems. E.g., the lack of exclusion between the direct I/O and
write fault paths means that a write fault can allocate delalloc
blocks in a region of a file that was previously a hole after the
direct read has attempted to flush/inval the file range, but before
it actually reads the block mapping. In turn, the direct read
discovers a delalloc extent and cannot proceed.

While the appropriate solution here is to not mix direct and memory
mapped I/O to the same regions of the same file, the current
BUG_ON() behavior is probably overkill as it can crash the entire
system.  Instead, localize the failure to the I/O in question by
returning an error for a direct I/O that cannot be handled safely
due to delalloc blocks. Be careful to allow the case of a direct
write to post-eof delalloc blocks. This can occur due to speculative
preallocation and is safe as post-eof blocks are not accompanied by
dirty pages in pagecache (conversely, preallocation within eof must
have been zeroed, and thus dirtied, before the inode size could have
been increased beyond said blocks).

Finally, provide an additional warning if a direct I/O write occurs
while the file is memory mapped. This may not catch all problematic
scenarios, but provides a hint that some known-to-be-problematic I/O
methods are in use.

Signed-off-by: Brian Foster <bfoster@redhat.com>
Reviewed-by: Dave Chinner <dchinner@redhat.com>
Signed-off-by: Dave Chinner <david@fromorbit.com>",1,fs/xfs/xfs_aops.c,"{""sha"": ""2693ba84ec2541072396d138fbf970bca90a597c"", ""filename"": ""fs/xfs/xfs_aops.c"", ""status"": ""modified"", ""additions"": 20, ""deletions"": 2, ""changes"": 22, ""blob_url"": ""https://github.com/torvalds/linux/blob/04197b341f23b908193308b8d63d17ff23232598/fs/xfs/xfs_aops.c"", ""raw_url"": ""https://github.com/torvalds/linux/raw/04197b341f23b908193308b8d63d17ff23232598/fs/xfs/xfs_aops.c"", ""contents_url"": ""https://api.github.com/repos/torvalds/linux/contents/fs/xfs/xfs_aops.c?ref=04197b341f23b908193308b8d63d17ff23232598"", ""patch"": ""@@ -1361,6 +1361,26 @@ __xfs_get_blocks(\n \tif (error)\n \t\tgoto out_unlock;\n \n+\t/*\n+\t * The only time we can ever safely find delalloc blocks on direct I/O\n+\t * is a dio write to post-eof speculative preallocation. All other\n+\t * scenarios are indicative of a problem or misuse (such as mixing\n+\t * direct and mapped I/O).\n+\t *\n+\t * The file may be unmapped by the time we get here so we cannot\n+\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this\n+\t * is a read or a write within eof. Otherwise, carry on but warn as a\n+\t * precuation if the file happens to be mapped.\n+\t */\n+\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {\n+\t\tif (!create || offset < i_size_read(VFS_I(ip))) {\n+\t\t\tWARN_ON_ONCE(1);\n+\t\t\terror = -EIO;\n+\t\t\tgoto out_unlock;\n+\t\t}\n+\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));\n+\t}\n+\n \t/* for DAX, we convert unwritten extents directly */\n \tif (create &&\n \t    (!nimaps ||\n@@ -1450,8 +1470,6 @@ __xfs_get_blocks(\n \t     (new || ISUNWRITTEN(&imap))))\n \t\tset_buffer_new(bh_result);\n \n-\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);\n-\n \treturn 0;\n \n out_unlock:""}","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
",C,"	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

","	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
",,"@@ -1361,6 +1361,26 @@ __xfs_get_blocks(
 	if (error)
 		goto out_unlock;
 
+	/*
+	 * The only time we can ever safely find delalloc blocks on direct I/O
+	 * is a dio write to post-eof speculative preallocation. All other
+	 * scenarios are indicative of a problem or misuse (such as mixing
+	 * direct and mapped I/O).
+	 *
+	 * The file may be unmapped by the time we get here so we cannot
+	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
+	 * is a read or a write within eof. Otherwise, carry on but warn as a
+	 * precuation if the file happens to be mapped.
+	 */
+	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
+		if (!create || offset < i_size_read(VFS_I(ip))) {
+			WARN_ON_ONCE(1);
+			error = -EIO;
+			goto out_unlock;
+		}
+		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
+	}
+
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
@@ -1450,8 +1470,6 @@ __xfs_get_blocks(
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
-	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
-
 	return 0;
 
 out_unlock:",linux,04197b341f23b908193308b8d63d17ff23232598,399372349a7f9b2d7e56e4fa4467c69822d07024,1,"__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
//fix_flaw_line_below:
//	/*
//fix_flaw_line_below:
//	 * The only time we can ever safely find delalloc blocks on direct I/O
//fix_flaw_line_below:
//	 * is a dio write to post-eof speculative preallocation. All other
//fix_flaw_line_below:
//	 * scenarios are indicative of a problem or misuse (such as mixing
//fix_flaw_line_below:
//	 * direct and mapped I/O).
//fix_flaw_line_below:
//	 *
//fix_flaw_line_below:
//	 * The file may be unmapped by the time we get here so we cannot
//fix_flaw_line_below:
//	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
//fix_flaw_line_below:
//	 * is a read or a write within eof. Otherwise, carry on but warn as a
//fix_flaw_line_below:
//	 * precuation if the file happens to be mapped.
//fix_flaw_line_below:
//	 */
//fix_flaw_line_below:
//	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
//fix_flaw_line_below:
//		if (!create || offset < i_size_read(VFS_I(ip))) {
//fix_flaw_line_below:
//			WARN_ON_ONCE(1);
//fix_flaw_line_below:
//			error = -EIO;
//fix_flaw_line_below:
//			goto out_unlock;
//fix_flaw_line_below:
//		}
//fix_flaw_line_below:
//		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
//fix_flaw_line_below:
//	}
//fix_flaw_line_below:
//
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
//flaw_line_below:
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
//flaw_line_below:

 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
",183036,"__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
	BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","__xfs_get_blocks(
	struct inode		*inode,
	sector_t		iblock,
	struct buffer_head	*bh_result,
	int			create,
	bool			direct,
	bool			dax_fault)
{
	struct xfs_inode	*ip = XFS_I(inode);
	struct xfs_mount	*mp = ip->i_mount;
	xfs_fileoff_t		offset_fsb, end_fsb;
	int			error = 0;
	int			lockmode = 0;
	struct xfs_bmbt_irec	imap;
	int			nimaps = 1;
	xfs_off_t		offset;
	ssize_t			size;
	int			new = 0;
	bool			is_cow = false;
	bool			need_alloc = false;

	BUG_ON(create && !direct);

	if (XFS_FORCED_SHUTDOWN(mp))
		return -EIO;

	offset = (xfs_off_t)iblock << inode->i_blkbits;
	ASSERT(bh_result->b_size >= (1 << inode->i_blkbits));
	size = bh_result->b_size;

	if (!create && offset >= i_size_read(inode))
		return 0;

	/*
	 * Direct I/O is usually done on preallocated files, so try getting
	 * a block mapping without an exclusive lock first.
	 */
	lockmode = xfs_ilock_data_map_shared(ip);

	ASSERT(offset <= mp->m_super->s_maxbytes);
	if (offset + size > mp->m_super->s_maxbytes)
		size = mp->m_super->s_maxbytes - offset;
	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);
	offset_fsb = XFS_B_TO_FSBT(mp, offset);

	if (create && direct && xfs_is_reflink_inode(ip))
		is_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,
					&need_alloc);
	if (!is_cow) {
		error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,
					&imap, &nimaps, XFS_BMAPI_ENTIRE);
		/*
		 * Truncate an overwrite extent if there's a pending CoW
		 * reservation before the end of this extent.  This
		 * forces us to come back to get_blocks to take care of
		 * the CoW.
		 */
		if (create && direct && nimaps &&
		    imap.br_startblock != HOLESTARTBLOCK &&
		    imap.br_startblock != DELAYSTARTBLOCK &&
		    !ISUNWRITTEN(&imap))
			xfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,
					&imap);
	}
	ASSERT(!need_alloc);
 	if (error)
 		goto out_unlock;
 
	/*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
	 */
	if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
		if (!create || offset < i_size_read(VFS_I(ip))) {
			WARN_ON_ONCE(1);
			error = -EIO;
			goto out_unlock;
		}
		WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

 	/* for DAX, we convert unwritten extents directly */
 	if (create &&
 	    (!nimaps ||
	     (imap.br_startblock == HOLESTARTBLOCK ||
	      imap.br_startblock == DELAYSTARTBLOCK) ||
	     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {
		/*
		 * xfs_iomap_write_direct() expects the shared lock. It
		 * is unlocked on return.
		 */
		if (lockmode == XFS_ILOCK_EXCL)
			xfs_ilock_demote(ip, lockmode);

		error = xfs_iomap_write_direct(ip, offset, size,
					       &imap, nimaps);
		if (error)
			return error;
		new = 1;

		trace_xfs_get_blocks_alloc(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_DELALLOC, &imap);
	} else if (nimaps) {
		trace_xfs_get_blocks_found(ip, offset, size,
				ISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN
						   : XFS_IO_OVERWRITE, &imap);
		xfs_iunlock(ip, lockmode);
	} else {
		trace_xfs_get_blocks_notfound(ip, offset, size);
		goto out_unlock;
	}

	if (IS_DAX(inode) && create) {
		ASSERT(!ISUNWRITTEN(&imap));
		/* zeroing is not needed at a higher layer */
		new = 0;
	}

	/* trim mapping down to size requested */
	xfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);

	/*
	 * For unwritten extents do not report a disk address in the buffered
	 * read case (treat as if we're reading into a hole).
	 */
	if (imap.br_startblock != HOLESTARTBLOCK &&
	    imap.br_startblock != DELAYSTARTBLOCK &&
	    (create || !ISUNWRITTEN(&imap))) {
		if (create && direct && !is_cow) {
			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,
					&imap);
			if (error)
				return error;
		}

		xfs_map_buffer(inode, bh_result, &imap, offset);
		if (ISUNWRITTEN(&imap))
			set_buffer_unwritten(bh_result);
		/* direct IO needs special help */
		if (create) {
			if (dax_fault)
				ASSERT(!ISUNWRITTEN(&imap));
			else
				xfs_map_direct(inode, bh_result, &imap, offset,
						is_cow);
		}
	}

	/*
	 * If this is a realtime file, data may be on a different device.
	 * to that pointed to from the buffer_head b_bdev currently.
	 */
	bh_result->b_bdev = xfs_find_bdev_for_inode(inode);

	/*
	 * If we previously allocated a block out beyond eof and we are now
	 * coming back to use it then we will need to flag it as new even if it
	 * has a disk address.
	 *
	 * With sub-block writes into unwritten extents we also need to mark
	 * the buffer as new so that the unwritten parts of the buffer gets
	 * correctly zeroed.
	 */
	if (create &&
	    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||
	     (offset >= i_size_read(inode)) ||
 	     (new || ISUNWRITTEN(&imap))))
 		set_buffer_new(bh_result);
 
 	return 0;
 
 out_unlock:
	xfs_iunlock(ip, lockmode);
	return error;
}
","The `__xfs_get_blocks` function is responsible for managing block allocation and mapping in the XFS filesystem, particularly for reading and writing data to files. It takes parameters such as an inode, block index, a buffer head for results, and flags indicating whether to create new blocks or handle direct I/O. The function first checks for forced shutdowns and calculates the appropriate offsets and sizes. It attempts to read the block mapping, handling copy-on-write scenarios if necessary. Depending on the conditions, it may allocate new blocks, trim mappings, and set buffer states, ensuring proper handling of unwritten extents and direct I/O requirements.","The provided code manages block allocation and mapping within a filesystem, specifically for reading and writing data to files. It checks for system shutdowns and calculates necessary offsets and sizes for data operations. The function attempts to read existing block mappings and handles scenarios where new blocks may need to be created, including special cases for direct input/output operations. It ensures proper management of unwritten data and adjusts buffer states accordingly. The code also accounts for specific file types and conditions, ensuring efficient data handling while maintaining filesystem integrity.","The vulnerability in the original code stemmed from the use of a `BUG_ON` assertion that would crash the system when mixed direct and mapped I/O operations were detected, particularly when delalloc blocks existed during a direct I/O read. This situation could lead to inconsistencies, as a direct read might encounter delalloc extents that were not properly managed, causing the operation to fail. The fix involved replacing the `BUG_ON` with a more graceful error handling approach, allowing the function to return an error instead of crashing the system, while also issuing warnings when direct writes occurred on memory-mapped files, thus improving stability and providing better diagnostics.","The vulnerability in the original code arose from a `BUG_ON` assertion that would crash the system when mixed direct and mapped I/O operations were detected, particularly when delalloc blocks were present during a direct I/O read. This situation could lead to inconsistencies, as the direct read might encounter delalloc extents that were not properly managed, causing the operation to fail. The fix involved replacing the `BUG_ON` with error handling that allows the function to return an error instead of crashing the system, while also issuing warnings for direct writes on memory-mapped files, thereby enhancing stability and providing better diagnostics.","The vulnerability was fixed by replacing the `BUG_ON` assertion with a conditional check that returns an error when a direct I/O operation encounters delalloc blocks, specifically when the `imap.br_startblock` equals `DELAYSTARTBLOCK`. This change allows the function to handle the situation gracefully instead of crashing the system. Additionally, the code now includes warnings using `WARN_ON_ONCE` if a direct write occurs while the file is memory-mapped, ensuring that users are alerted to potentially problematic I/O methods. This approach maintains system stability while still addressing the underlying issue of mixed I/O operations.","The vulnerability was fixed by replacing the assertion that caused system crashes with a more controlled error handling mechanism. Instead of terminating the process when mixed direct and mapped I/O operations were detected, the code now returns an error when encountering conditions that could lead to inconsistencies, specifically regarding the presence of certain block states. Additionally, the implementation includes warnings to alert users when direct writes occur on memory-mapped files, thereby improving diagnostics and encouraging better I/O practices without compromising system stability. This approach allows for safer operation while addressing the underlying issues."
41,184595,184595,,Remote,Not required,Complete,CVE-2011-3961,https://www.cvedetails.com/cve/CVE-2011-3961/,CWE-362,Medium,Complete,Complete,,2012-02-08,9.3,Race condition in Google Chrome before 17.0.963.46 allows remote attackers to execute arbitrary code via vectors that trigger a crash of a utility process.,2017-09-18,Exec Code ,4,https://github.com/chromium/chromium/commit/b712795852f9d6073e062680e280634290c4ba5d,b712795852f9d6073e062680e280634290c4ba5d,"Fix uninitialized variables in HarfBuzzShaperBase
https://bugs.webkit.org/show_bug.cgi?id=79546

Reviewed by Dirk Pranke.

These were introduced in r108733.

* platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp:
(WebCore::HarfBuzzShaperBase::HarfBuzzShaperBase):

git-svn-id: svn://svn.chromium.org/blink/trunk@108871 bbb929c8-8fbe-4397-9dbb-9b2b20218538",0,third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp,"{""sha"": ""47cc92f4eecf9c468b793df4067b1908ccbcfe30"", ""filename"": ""third_party/WebKit/Source/WebCore/ChangeLog"", ""status"": ""modified"", ""additions"": 12, ""deletions"": 0, ""changes"": 12, ""blob_url"": ""https://github.com/chromium/chromium/blob/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/ChangeLog"", ""raw_url"": ""https://github.com/chromium/chromium/raw/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/ChangeLog"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/WebCore/ChangeLog?ref=b712795852f9d6073e062680e280634290c4ba5d"", ""patch"": ""@@ -1,3 +1,15 @@\n+2012-02-24  Adrienne Walker  <enne@google.com>\n+\n+        Fix uninitialized variables in HarfBuzzShaperBase\n+        https://bugs.webkit.org/show_bug.cgi?id=79546\n+\n+        Reviewed by Dirk Pranke.\n+\n+        These were introduced in r108733.\n+\n+        * platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp:\n+        (WebCore::HarfBuzzShaperBase::HarfBuzzShaperBase):\n+\n 2012-02-24  Noel Gordon  <noel.gordon@gmail.com>\n \n         [chromium] JPEG RGB image with Adode Marker fails to turbo swizzle decode""}<_**next**_>{""sha"": ""b8daff9e3a07c5118ba92b6d6a8096b93a2beccc"", ""filename"": ""third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 0, ""changes"": 4, ""blob_url"": ""https://github.com/chromium/chromium/blob/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""raw_url"": ""https://github.com/chromium/chromium/raw/b712795852f9d6073e062680e280634290c4ba5d/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/WebCore/platform/graphics/harfbuzz/HarfBuzzShaperBase.cpp?ref=b712795852f9d6073e062680e280634290c4ba5d"", ""patch"": ""@@ -42,8 +42,12 @@ namespace WebCore {\n \n HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)\n     : m_font(font)\n+    , m_normalizedBufferLength(0)\n     , m_run(run)\n     , m_wordSpacingAdjustment(font->wordSpacing())\n+    , m_padding(0)\n+    , m_padPerWordBreak(0)\n+    , m_padError(0)\n     , m_letterSpacing(font->letterSpacing())\n {\n }""}"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
     , m_letterSpacing(font->letterSpacing())
 {
 }
",C,"    , m_normalizedBufferLength(0)
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
",,,"@@ -42,8 +42,12 @@ namespace WebCore {
 
 HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
+    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
+    , m_padding(0)
+    , m_padPerWordBreak(0)
+    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }",Chrome,b712795852f9d6073e062680e280634290c4ba5d,617f382e9c9abab47e325c5d2e319524a5440faa,1," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
//fix_flaw_line_below:
//    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
//fix_flaw_line_below:
//    , m_padding(0)
//fix_flaw_line_below:
//    , m_padPerWordBreak(0)
//fix_flaw_line_below:
//    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
",184595," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
     , m_letterSpacing(font->letterSpacing())
 {
 }
"," HarfBuzzShaperBase::HarfBuzzShaperBase(const Font* font, const TextRun& run)
     : m_font(font)
    , m_normalizedBufferLength(0)
     , m_run(run)
     , m_wordSpacingAdjustment(font->wordSpacing())
    , m_padding(0)
    , m_padPerWordBreak(0)
    , m_padError(0)
     , m_letterSpacing(font->letterSpacing())
 {
 }
","The `HarfBuzzShaperBase` constructor initializes an instance of the `HarfBuzzShaperBase` class, which is designed to handle text shaping using the HarfBuzz library. It takes two parameters: a pointer to a `Font` object and a `TextRun` object. The constructor initializes member variables with the provided font and text run, while also retrieving and storing the font's word spacing and letter spacing adjustments. This setup is essential for accurately rendering text with the specified typographic properties, ensuring that the text is shaped correctly according to the font's characteristics and the given text run.","The provided code defines a constructor for a class that facilitates text shaping using a specific font and a sequence of text. It initializes the class by accepting a font object and a text sequence, storing these inputs for later use. Additionally, it retrieves and stores adjustments for spacing between words and letters based on the font's properties. This setup is crucial for ensuring that text is rendered accurately, taking into account the typographic features of the font and the characteristics of the text sequence, thereby enhancing the visual presentation of the text.","The vulnerability in the original code stemmed from the use of uninitialized member variables, which could lead to undefined behavior when the class was instantiated. Specifically, variables like `m_normalizedBufferLength`, `m_padding`, `m_padPerWordBreak`, and `m_padError` were not initialized, potentially causing issues during text shaping operations. The fixed code addressed this vulnerability by explicitly initializing these variables to zero in the constructor's member initializer list. This change ensures that all member variables have defined values upon object creation, thereby preventing unpredictable behavior and enhancing the stability of the text shaping functionality.","The vulnerability in the original code was due to the presence of uninitialized member variables, which could lead to unpredictable behavior and potential crashes when the class was used. This issue arose because certain variables were not assigned any initial values, leaving them in an indeterminate state. The fix involved explicitly initializing these variables to zero in the constructor, ensuring that they have defined values upon object creation. This change mitigates the risk of undefined behavior during execution, thereby enhancing the reliability and stability of the text shaping functionality within the class.","The vulnerability was fixed by explicitly initializing previously uninitialized member variables in the constructor of the class. In the updated code, `m_normalizedBufferLength`, `m_padding`, `m_padPerWordBreak`, and `m_padError` were all set to zero during the initialization process. This ensures that these variables have defined values when an instance of the class is created, preventing any undefined behavior that could arise from using them in an uninitialized state. By addressing these uninitialized variables, the code enhances the stability and reliability of the text shaping functionality, reducing the risk of errors during execution.","The vulnerability was fixed by ensuring that all previously uninitialized member variables were explicitly assigned initial values within the constructor. This change guarantees that these variables start in a defined state when an instance of the class is created, thereby eliminating the risk of undefined behavior that could occur if they were accessed before being set. By addressing the initialization of these variables, the code improves its reliability and stability, ensuring that the functionality operates as intended without encountering errors related to uninitialized data during execution."
42,184867,184867,,Remote,Not required,Partial,CVE-2013-2906,https://www.cvedetails.com/cve/CVE-2013-2906/,CWE-362,Medium,Partial,Partial,,2013-10-02,6.8,"Multiple race conditions in the Web Audio implementation in Blink, as used in Google Chrome before 30.0.1599.66, allow remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to threading in core/html/HTMLMediaElement.cpp, core/platform/audio/AudioDSPKernelProcessor.cpp, core/platform/audio/HRTFElevation.cpp, and modules/webaudio/ConvolverNode.cpp.",2017-09-18,DoS ,4,https://github.com/chromium/chromium/commit/c2364e0ce42878a2177c6f4cf7adb3c715b777c1,c2364e0ce42878a2177c6f4cf7adb3c715b777c1,"[OriginChip] Re-enable the chip as necessary when switching tabs.

BUG=369500

Review URL: https://codereview.chromium.org/292493003

git-svn-id: svn://svn.chromium.org/chrome/trunk/src@271161 0039d316-1c4b-4281-b951-d872f2087c98",1,chrome/browser/ui/omnibox/omnibox_edit_model.cc,"{""sha"": ""587dd72386e3a10288ba621f07704dd623f37872"", ""filename"": ""chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""status"": ""modified"", ""additions"": 4, ""deletions"": 1, ""changes"": 5, ""blob_url"": ""https://github.com/chromium/chromium/blob/c2364e0ce42878a2177c6f4cf7adb3c715b777c1/chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/c2364e0ce42878a2177c6f4cf7adb3c715b777c1/chrome/browser/ui/omnibox/omnibox_edit_model.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/chrome/browser/ui/omnibox/omnibox_edit_model.cc?ref=c2364e0ce42878a2177c6f4cf7adb3c715b777c1"", ""patch"": ""@@ -247,8 +247,11 @@ const OmniboxEditModel::State OmniboxEditModel::GetStateForTabSwitch() {\n void OmniboxEditModel::RestoreState(const State* state) {\n   // We need to update the permanent text correctly and revert the view\n   // regardless of whether there is saved state.\n+  bool url_replacement_enabled = !state || state->url_replacement_enabled;\n   controller_->GetToolbarModel()->set_url_replacement_enabled(\n-      !state || state->url_replacement_enabled);\n+      url_replacement_enabled);\n+  controller_->GetToolbarModel()->set_origin_chip_enabled(\n+      url_replacement_enabled);\n   permanent_text_ = controller_->GetToolbarModel()->GetText();\n   // Don't muck with the search term replacement state, as we've just set it\n   // correctly.""}"," void OmniboxEditModel::RestoreState(const State* state) {
  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
"," void OmniboxEditModel::RestoreState(const State* state) {
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      !state || state->url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
",C,"  bool url_replacement_enabled = !state || state->url_replacement_enabled;
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
","      !state || state->url_replacement_enabled);
",,"@@ -247,8 +247,11 @@ const OmniboxEditModel::State OmniboxEditModel::GetStateForTabSwitch() {
 void OmniboxEditModel::RestoreState(const State* state) {
   // We need to update the permanent text correctly and revert the view
   // regardless of whether there is saved state.
+  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
-      !state || state->url_replacement_enabled);
+      url_replacement_enabled);
+  controller_->GetToolbarModel()->set_origin_chip_enabled(
+      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
   // Don't muck with the search term replacement state, as we've just set it
   // correctly.",Chrome,c2364e0ce42878a2177c6f4cf7adb3c715b777c1,9016f5323611c8bb8d24f40cddeb38f75990fdc0,1," void OmniboxEditModel::RestoreState(const State* state) {
   // We need to update the permanent text correctly and revert the view
   // regardless of whether there is saved state.
//fix_flaw_line_below:
//  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
//flaw_line_below:
      !state || state->url_replacement_enabled);
//fix_flaw_line_below:
//      url_replacement_enabled);
//fix_flaw_line_below:
//  controller_->GetToolbarModel()->set_origin_chip_enabled(
//fix_flaw_line_below:
//      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
   // Don't muck with the search term replacement state, as we've just set it
   // correctly.
  view_->RevertWithoutResettingSearchTermReplacement();
  // Restore the autocomplete controller's input, or clear it if this is a new
  // tab.
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  // Restore any user editing.
  if (state->user_input_in_progress) {
    // NOTE: Be sure and set keyword-related state BEFORE invoking
    // DisplayTextFromUserText(), as its result depends upon this state.
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
",184867," void OmniboxEditModel::RestoreState(const State* state) {
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      !state || state->url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
"," void OmniboxEditModel::RestoreState(const State* state) {
  bool url_replacement_enabled = !state || state->url_replacement_enabled;
   controller_->GetToolbarModel()->set_url_replacement_enabled(
      url_replacement_enabled);
  controller_->GetToolbarModel()->set_origin_chip_enabled(
      url_replacement_enabled);
   permanent_text_ = controller_->GetToolbarModel()->GetText();
  view_->RevertWithoutResettingSearchTermReplacement();
  input_ = state ? state->autocomplete_input : AutocompleteInput();
  if (!state)
    return;

  SetFocusState(state->focus_state, OMNIBOX_FOCUS_CHANGE_TAB_SWITCH);
  focus_source_ = state->focus_source;
  if (state->user_input_in_progress) {
    keyword_ = state->keyword;
    is_keyword_hint_ = state->is_keyword_hint;
    view_->SetUserText(state->user_text,
        DisplayTextFromUserText(state->user_text), false);
    view_->SetGrayTextAutocompletion(state->gray_text);
  }
}
","The `RestoreState` function in the `OmniboxEditModel` class is designed to restore the state of the omnibox based on a provided `State` object. It first checks if the state is valid to determine whether to enable URL replacement. The function then retrieves and sets the permanent text from the toolbar model and reverts the view without resetting any search term replacements. If a valid state is present, it updates the focus state and source, and if user input is in progress, it restores the keyword, user text, and gray text autocompletion, ensuring the omnibox reflects the user's previous interactions accurately.","The provided code defines a function that restores the state of a user interface component based on a given state object. It first determines whether to enable a specific feature based on the presence of the state object. The function then retrieves and sets a permanent text value, reverts the view to a previous state without altering certain search terms, and updates the focus and source based on the state. If user input is ongoing, it restores relevant details such as keywords and user text, ensuring that the interface accurately reflects the user's last interactions and preferences.","The vulnerability in the original code stemmed from the lack of enabling the origin chip when the URL replacement feature was toggled, which could lead to inconsistent user interface behavior when switching tabs. Specifically, the code only set the URL replacement feature based on the state but did not address the origin chip's state. The fix introduced a new line that explicitly sets the origin chip's enabled state based on the same condition that determines URL replacement, ensuring that both features are synchronized and function correctly when the user switches tabs, thereby enhancing the overall user experience and interface consistency.","The vulnerability in the original code was that it did not properly manage the state of a user interface component when switching tabs, specifically failing to enable a visual indicator that provides context about the current origin. This oversight could lead to confusion for users, as the interface would not reflect the correct state of the application. The fix addressed this issue by ensuring that the visual indicator is enabled or disabled in accordance with the same conditions that govern another related feature, thereby maintaining consistency in the user interface and improving the overall user experience when navigating between tabs.","The vulnerability was fixed by adding a line of code that explicitly sets the state of the origin chip based on the condition that determines whether URL replacement is enabled. In the updated code, the origin chip is now enabled or disabled using the same boolean value that controls URL replacement, ensuring both features are synchronized. This change was implemented by introducing the line that calls the method to set the origin chip's enabled state, thereby ensuring that when the user switches tabs, the interface accurately reflects the current context and enhances the overall user experience.","The vulnerability was fixed by introducing a mechanism to ensure that a specific user interface component is enabled or disabled in alignment with another related feature. The updated code now checks the same condition that governs the first feature to determine the state of the visual indicator, ensuring consistency across the interface. This adjustment guarantees that when users switch tabs, the visual indicator accurately reflects the current context, thereby preventing confusion and enhancing the overall user experience by maintaining a coherent and responsive interface."
43,184868,184868,,Remote,Not required,Partial,CVE-2013-2906,https://www.cvedetails.com/cve/CVE-2013-2906/,CWE-362,Medium,Partial,Partial,,2013-10-02,6.8,"Multiple race conditions in the Web Audio implementation in Blink, as used in Google Chrome before 30.0.1599.66, allow remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to threading in core/html/HTMLMediaElement.cpp, core/platform/audio/AudioDSPKernelProcessor.cpp, core/platform/audio/HRTFElevation.cpp, and modules/webaudio/ConvolverNode.cpp.",2017-09-18,DoS ,5,https://github.com/chromium/chromium/commit/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,"Suspend shared timers while blockingly closing databases

BUG=388771
R=michaeln@chromium.org

Review URL: https://codereview.chromium.org/409863002

git-svn-id: svn://svn.chromium.org/chrome/trunk/src@284785 0039d316-1c4b-4281-b951-d872f2087c98",0,content/renderer/render_thread_impl.cc,"{""sha"": ""5116fac7442aeafd9f36622a135f8b2669235f42"", ""filename"": ""content/renderer/render_thread_impl.cc"", ""status"": ""modified"", ""additions"": 5, ""deletions"": 0, ""changes"": 5, ""blob_url"": ""https://github.com/chromium/chromium/blob/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2/content/renderer/render_thread_impl.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2/content/renderer/render_thread_impl.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/renderer/render_thread_impl.cc?ref=c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2"", ""patch"": ""@@ -574,8 +574,13 @@ void RenderThreadImpl::Shutdown() {\n \n   // Wait for all databases to be closed.\n   if (webkit_platform_support_) {\n+    // WaitForAllDatabasesToClose might run a nested message loop. To avoid\n+    // processing timer events while we're already in the process of shutting\n+    // down blink, put a ScopePageLoadDeferrer on the stack.\n+    WebView::willEnterModalLoop();\n     webkit_platform_support_->web_database_observer_impl()->\n         WaitForAllDatabasesToClose();\n+    WebView::didExitModalLoop();\n   }\n \n   // Shutdown in reverse of the initialization order.""}","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
    WebView::didExitModalLoop();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
",C,"    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
    WebView::didExitModalLoop();
",,,"@@ -574,8 +574,13 @@ void RenderThreadImpl::Shutdown() {
 
   // Wait for all databases to be closed.
   if (webkit_platform_support_) {
+    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
+    // processing timer events while we're already in the process of shutting
+    // down blink, put a ScopePageLoadDeferrer on the stack.
+    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
+    WebView::didExitModalLoop();
   }
 
   // Shutdown in reverse of the initialization order.",Chrome,c4a4dfb26615b5ef5e9dcc730ef43f70ce9202e2,6e44c30f2dc7371711503b0a942da01180206c47,1,"void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   // Wait for all databases to be closed.
   if (webkit_platform_support_) {
//fix_flaw_line_below:
//    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
//fix_flaw_line_below:
//    // processing timer events while we're already in the process of shutting
//fix_flaw_line_below:
//    // down blink, put a ScopePageLoadDeferrer on the stack.
//fix_flaw_line_below:
//    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
//fix_flaw_line_below:
//    WebView::didExitModalLoop();
   }
 
   // Shutdown in reverse of the initialization order.
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  // Shutdown the file thread if it's running.
  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  // RemoveEmbeddedWorkerRoute may be called while deleting
  // EmbeddedWorkerDispatcher. So it must be deleted before deleting
  // RenderThreadImpl.
  embedded_worker_dispatcher_.reset();

  // Ramp down IDB before we ramp down WebKit (and V8), since IDB classes might
  // hold pointers to V8 objects (e.g., via pending requests).
  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

  // TODO(port)
#if defined(OS_WIN)
  // Clean up plugin channels before this thread goes away.
  NPChannelBase::CleanupChannels();
#endif
}
",184868,"void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","void RenderThreadImpl::Shutdown() {
  FOR_EACH_OBSERVER(
      RenderProcessObserver, observers_, OnRenderProcessShutdown());

  ChildThread::Shutdown();

  if (memory_observer_) {
    message_loop()->RemoveTaskObserver(memory_observer_.get());
    memory_observer_.reset();
  }
 
   if (webkit_platform_support_) {
    // WaitForAllDatabasesToClose might run a nested message loop. To avoid
    // processing timer events while we're already in the process of shutting
    // down blink, put a ScopePageLoadDeferrer on the stack.
    WebView::willEnterModalLoop();
     webkit_platform_support_->web_database_observer_impl()->
         WaitForAllDatabasesToClose();
    WebView::didExitModalLoop();
   }
 
  if (devtools_agent_message_filter_.get()) {
    RemoveFilter(devtools_agent_message_filter_.get());
    devtools_agent_message_filter_ = NULL;
  }

  RemoveFilter(audio_input_message_filter_.get());
  audio_input_message_filter_ = NULL;

  RemoveFilter(audio_message_filter_.get());
  audio_message_filter_ = NULL;

#if defined(ENABLE_WEBRTC)
  RTCPeerConnectionHandler::DestructAllHandlers();

  peer_connection_factory_.reset();
#endif
  RemoveFilter(vc_manager_->video_capture_message_filter());
  vc_manager_.reset();

  RemoveFilter(db_message_filter_.get());
  db_message_filter_ = NULL;

  if (file_thread_)
    file_thread_->Stop();

  if (compositor_output_surface_filter_.get()) {
    RemoveFilter(compositor_output_surface_filter_.get());
    compositor_output_surface_filter_ = NULL;
  }

  media_thread_.reset();
  compositor_thread_.reset();
  input_handler_manager_.reset();
  if (input_event_filter_.get()) {
    RemoveFilter(input_event_filter_.get());
    input_event_filter_ = NULL;
  }

  embedded_worker_dispatcher_.reset();

  main_thread_indexed_db_dispatcher_.reset();

  if (webkit_platform_support_)
    blink::shutdown();

  lazy_tls.Pointer()->Set(NULL);

#if defined(OS_WIN)
  NPChannelBase::CleanupChannels();
#endif
}
","The `Shutdown` function in the `RenderThreadImpl` class is responsible for gracefully terminating various components and observers associated with the rendering process. It first notifies all registered observers of the shutdown event, then proceeds to clean up memory observers and database connections, ensuring all databases are closed. The function removes various message filters related to audio, video capture, and input events, and resets associated resources. Additionally, it stops any active threads, including file and media threads, and performs platform-specific cleanup. Finally, it invokes a shutdown routine for the Blink framework and cleans up any remaining resources, ensuring a complete shutdown.","The provided code implements a shutdown procedure for a rendering process, ensuring that all associated components and observers are properly terminated. It begins by notifying observers of the shutdown event, followed by the removal of memory and database observers to ensure all resources are released. The code systematically removes various filters related to audio, video, and input handling, while also stopping any active threads. Additionally, it performs necessary cleanup specific to the platform and invokes a shutdown routine for the underlying framework, ultimately ensuring that all resources are freed and the rendering process is safely terminated.","The vulnerability in the original code stemmed from the potential for timer events to be processed while the system was in the midst of shutting down, particularly during the database closure process. This could lead to unexpected behavior or race conditions. The fix introduced a mechanism to suspend shared timers by utilizing `WebView::willEnterModalLoop()` and `WebView::didExitModalLoop()` around the call to `WaitForAllDatabasesToClose()`. This ensures that no timer events are processed during the critical shutdown phase, thereby preventing any interference and ensuring a more stable and predictable shutdown process.","The vulnerability in the original code arose from the risk of timer events being processed during the shutdown sequence, particularly while closing databases. This could lead to unexpected behavior or race conditions, compromising the stability of the shutdown process. The fix addressed this issue by introducing a mechanism to suspend shared timers during the critical phase of database closure. By ensuring that no timer events could interfere while the system was shutting down, the updated code provided a more controlled and reliable shutdown process, thereby mitigating the potential for errors and enhancing overall system stability.","The vulnerability was fixed by introducing a mechanism to suspend shared timers during the database closure process. Specifically, the code now uses `WebView::willEnterModalLoop()` before calling `WaitForAllDatabasesToClose()` and `WebView::didExitModalLoop()` afterward. This ensures that while the system is in the process of closing databases, no timer events can be processed, preventing potential race conditions or unexpected behavior. By effectively blocking timer events during this critical operation, the updated code enhances the stability and reliability of the shutdown sequence, ensuring a smoother termination of the rendering process.","The vulnerability was addressed by implementing a mechanism to suspend shared timers during the critical phase of closing databases. This was achieved by introducing a method to signal the system to temporarily halt the processing of timer events while the database closure operation was underway. By ensuring that no timer events could interfere during this process, the fix effectively mitigated the risk of race conditions and unexpected behavior, thereby enhancing the stability and reliability of the shutdown sequence. This change ensures a smoother and more controlled termination of the system's operations."
44,185106,185106,,Remote,Not required,Partial,CVE-2013-0893,https://www.cvedetails.com/cve/CVE-2013-0893/,CWE-362,Medium,Partial,Partial,,2013-02-23,6.8,"Race condition in Google Chrome before 25.0.1364.97 on Windows and Linux, and before 25.0.1364.99 on Mac OS X, allows remote attackers to cause a denial of service or possibly have unspecified other impact via vectors related to media.",2018-10-30,DoS ,6,https://github.com/chromium/chromium/commit/ed6f4545a2a345697e07908c887333f5bdcc97a3,ed6f4545a2a345697e07908c887333f5bdcc97a3,"Apply 'x-content-type-options' check to dynamically inserted script.

BUG=348581

Review URL: https://codereview.chromium.org/185593011

git-svn-id: svn://svn.chromium.org/blink/trunk@168570 bbb929c8-8fbe-4397-9dbb-9b2b20218538",3,third_party/WebKit/Source/core/dom/ScriptLoader.cpp,"{""sha"": ""dafdb9606e69e9b0318994a254c8b73622ff304d"", ""filename"": ""third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""status"": ""added"", ""additions"": 11, ""deletions"": 0, ""changes"": 11, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked-expected.txt?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -0,0 +1,11 @@\n+CONSOLE ERROR: Refused to execute script from 'http://127.0.0.1:8000/security/contentTypeOptions/resources/script-with-header.pl?mime=application/json' because its MIME type ('application/json') is not executable, and strict MIME type checking is enabled.\n+Check that script sent with an 'X-Content-Type-Options: nosniff' header is correctly blocked if the MIME type isn't scripty.\n+\n+On success, you will see a series of \""PASS\"" messages, followed by \""TEST COMPLETE\"".\n+\n+\n+PASS window.scriptsSuccessfullyLoaded is 0\n+PASS successfullyParsed is true\n+\n+TEST COMPLETE\n+""}<_**next**_>{""sha"": ""6d0c62d8e7c1e4cf8369e76e93c00a08b7fa755b"", ""filename"": ""third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""status"": ""added"", ""additions"": 23, ""deletions"": 0, ""changes"": 23, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/LayoutTests/http/tests/security/contentTypeOptions/nosniff-dynamic-script-blocked.html?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -0,0 +1,23 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+    <title>'X-Content-Type-Options: nosniff;' blocks scripts!</title>\n+<body>\n+    <script src=\""/js-test-resources/js-test.js\""></script>\n+    <script>\n+        description('Check that script sent with an \\'X-Content-Type-Options: nosniff\\' header is correctly blocked if the MIME type isn\\'t scripty.');\n+        window.jsTestIsAsync = true;\n+\n+        window.scriptsSuccessfullyLoaded = 0;\n+\n+        var s = document.createElement('script');\n+        s.src = './resources/script-with-header.pl?mime=application/json';\n+        document.querySelector('head').appendChild(s);\n+\n+        window.onload = function () {\n+            shouldBe('window.scriptsSuccessfullyLoaded', '0');\n+            finishJSTest();\n+        };\n+    </script>\n+</body>\n+</html>""}<_**next**_>{""sha"": ""0ee2e25f4d5c4590e7c67948543e1b56ead8b167"", ""filename"": ""third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""status"": ""modified"", ""additions"": 6, ""deletions"": 3, ""changes"": 9, ""blob_url"": ""https://github.com/chromium/chromium/blob/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""raw_url"": ""https://github.com/chromium/chromium/raw/ed6f4545a2a345697e07908c887333f5bdcc97a3/third_party/WebKit/Source/core/dom/ScriptLoader.cpp"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/third_party/WebKit/Source/core/dom/ScriptLoader.cpp?ref=ed6f4545a2a345697e07908c887333f5bdcc97a3"", ""patch"": ""@@ -308,9 +308,12 @@ void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)\n     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))\n         return;\n \n-    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {\n-        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, \""Refused to execute script from '\"" + m_resource->url().elidedString() + \""' because its MIME type ('\"" + m_resource->mimeType() + \""') is not executable, and strict MIME type checking is enabled.\"");\n-        return;\n+    if (m_isExternalScript) {\n+        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();\n+        if (resource && !resource->mimeTypeAllowedByNosniff()) {\n+            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, \""Refused to execute script from '\"" + resource->url().elidedString() + \""' because its MIME type ('\"" + resource->mimeType() + \""') is not executable, and strict MIME type checking is enabled.\"");\n+            return;\n+        }\n     }\n \n     if (frame) {""}","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
",C,"    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
","    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
",,"@@ -308,9 +308,12 @@ void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
-    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
-        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
-        return;
+    if (m_isExternalScript) {
+        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
+        if (resource && !resource->mimeTypeAllowedByNosniff()) {
+            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
+            return;
+        }
     }
 
     if (frame) {",Chrome,ed6f4545a2a345697e07908c887333f5bdcc97a3,2c29c343bc7bff16fb552abf162d17d26a9d9563,1,"void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
//flaw_line_below:
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
//flaw_line_below:
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
//flaw_line_below:
        return;
//fix_flaw_line_below:
//    if (m_isExternalScript) {
//fix_flaw_line_below:
//        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
//fix_flaw_line_below:
//        if (resource && !resource->mimeTypeAllowedByNosniff()) {
//fix_flaw_line_below:
//            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
//fix_flaw_line_below:
//            return;
//fix_flaw_line_below:
//        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        // http://www.whatwg.org/specs/web-apps/current-work/#execute-the-script-block step 2.3
        // with additional support for HTML imports.
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        // Create a script from the script element node, using the script
        // block's source and the script block's type.
        // Note: This is where the script is compiled and actually executed.
        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
",185106,"void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript && m_resource && !m_resource->mimeTypeAllowedByNosniff()) {
        contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + m_resource->url().elidedString() + ""' because its MIME type ('"" + m_resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
        return;
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","void ScriptLoader::executeScript(const ScriptSourceCode& sourceCode)
{
    ASSERT(m_alreadyStarted);

    if (sourceCode.isEmpty())
        return;

    RefPtr<Document> elementDocument(m_element->document());
    RefPtr<Document> contextDocument = elementDocument->contextDocument().get();
    if (!contextDocument)
        return;

    LocalFrame* frame = contextDocument->frame();

    bool shouldBypassMainWorldContentSecurityPolicy = (frame && frame->script().shouldBypassMainWorldContentSecurityPolicy()) || elementDocument->contentSecurityPolicy()->allowScriptNonce(m_element->fastGetAttribute(HTMLNames::nonceAttr)) || elementDocument->contentSecurityPolicy()->allowScriptHash(sourceCode.source());

     if (!m_isExternalScript && (!shouldBypassMainWorldContentSecurityPolicy && !elementDocument->contentSecurityPolicy()->allowInlineScript(elementDocument->url(), m_startLineNumber)))
         return;
 
    if (m_isExternalScript) {
        ScriptResource* resource = m_resource ? m_resource.get() : sourceCode.resource();
        if (resource && !resource->mimeTypeAllowedByNosniff()) {
            contextDocument->addConsoleMessage(SecurityMessageSource, ErrorMessageLevel, ""Refused to execute script from '"" + resource->url().elidedString() + ""' because its MIME type ('"" + resource->mimeType() + ""') is not executable, and strict MIME type checking is enabled."");
            return;
        }
     }
 
     if (frame) {
        const bool isImportedScript = contextDocument != elementDocument;
        IgnoreDestructiveWriteCountIncrementer ignoreDestructiveWriteCountIncrementer(m_isExternalScript || isImportedScript ? contextDocument.get() : 0);

        if (isHTMLScriptLoader(m_element))
            contextDocument->pushCurrentScript(toHTMLScriptElement(m_element));

        AccessControlStatus corsCheck = NotSharableCrossOrigin;
        if (sourceCode.resource() && sourceCode.resource()->passesAccessControlCheck(m_element->document().securityOrigin()))
            corsCheck = SharableCrossOrigin;

        frame->script().executeScriptInMainWorld(sourceCode, corsCheck);

        if (isHTMLScriptLoader(m_element)) {
            ASSERT(contextDocument->currentScript() == m_element);
            contextDocument->popCurrentScript();
        }
    }
}
","The `executeScript` function in the `ScriptLoader` class is responsible for executing a given script source code within a specific document context. It first checks if the script execution has already started and if the source code is not empty. The function then retrieves the associated document and frame, verifying content security policies to determine if the script can be executed. It handles both inline and external scripts, ensuring compliance with MIME type restrictions and cross-origin access controls. If all checks pass, the script is executed in the main world of the frame, while managing the current script context appropriately.","The function is designed to execute a script within a specific document context, ensuring that various security and policy checks are satisfied before proceeding. It first verifies that the script execution has been initiated and that the script source is valid. The function assesses content security policies to determine if the script can bypass restrictions, particularly for inline and external scripts. If the script passes all checks, including MIME type and cross-origin access controls, it is executed in the main context of the document, while managing the current script state to maintain proper execution flow.","The vulnerability in the original code stemmed from insufficient checks on the MIME type of external scripts, which could allow the execution of scripts with unsafe or incorrect MIME types, potentially leading to security issues. Specifically, the code did not adequately verify the MIME type of dynamically inserted scripts, which could be exploited. The fix introduced a check for the MIME type using the `mimeTypeAllowedByNosniff()` function, ensuring that only scripts with valid MIME types are executed. This change enhances security by preventing the execution of scripts that do not meet the required content type standards, thereby mitigating the risk of executing malicious scripts.","The vulnerability in the original code allowed the execution of dynamically inserted scripts without proper validation of their MIME types, which could lead to the execution of potentially harmful scripts. This oversight posed a significant security risk, as it could enable attackers to inject malicious code. The fix addressed this issue by implementing a check for the MIME type of external scripts, ensuring that only scripts with acceptable content types are executed. This enhancement effectively mitigates the risk of executing unsafe scripts, thereby strengthening the overall security of the script execution process.","The vulnerability was fixed by adding a check for the MIME type of external scripts using the `mimeTypeAllowedByNosniff()` function. In the updated code, when an external script is being executed, the code first retrieves the resource associated with the script and verifies its MIME type. If the MIME type is not allowed, a console message is generated, and the script execution is aborted. This ensures that only scripts with valid MIME types are executed, thereby preventing the execution of potentially harmful scripts and enhancing the security of the script loading process.","The vulnerability was fixed by implementing a validation step for the MIME type of external scripts before execution. The updated code checks whether the MIME type of the script resource is acceptable, and if it is not, the execution is halted, and a warning message is logged. This additional layer of security ensures that only scripts with valid and safe content types are allowed to run, effectively preventing the execution of potentially malicious scripts that could compromise the system. This change significantly enhances the overall security of the script execution process."
45,186239,186239,,Remote,Not required,,CVE-2016-1670,https://www.cvedetails.com/cve/CVE-2016-1670/,CWE-362,High,,Partial,,2016-05-14,2.6,Race condition in the ResourceDispatcherHostImpl::BeginRequest function in content/browser/loader/resource_dispatcher_host_impl.cc in Google Chrome before 50.0.2661.102 allows remote attackers to make arbitrary HTTP requests by leveraging access to a renderer process and reusing a request ID.,2018-10-30,,7,https://github.com/chromium/chromium/commit/1af4fada49c4f3890f16daac31d38379a9d782b2,1af4fada49c4f3890f16daac31d38379a9d782b2,"Block a compromised renderer from reusing request ids.

BUG=578882

Review URL: https://codereview.chromium.org/1608573002

Cr-Commit-Position: refs/heads/master@{#372547}",0,content/browser/loader/resource_dispatcher_host_impl.cc,"{""sha"": ""8f0883073c4c5df0d87ebaef4685c987255b7f9a"", ""filename"": ""content/browser/bad_message.h"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/bad_message.h"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/bad_message.h"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/bad_message.h?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -129,6 +129,7 @@ enum BadMessageReason {\n   BDH_DEVICE_NOT_ALLOWED_FOR_ORIGIN = 105,\n   ACI_WRONG_STORAGE_PARTITION = 106,\n   RDHI_WRONG_STORAGE_PARTITION = 107,\n+  RDH_INVALID_REQUEST_ID = 108,\n \n   // Please add new elements here. The naming convention is abbreviated class\n   // name (e.g. RenderFrameHost becomes RFH) plus a unique description of the""}<_**next**_>{""sha"": ""1db3d98e157c29ae3455281fdbd72dbf65d12d4a"", ""filename"": ""content/browser/loader/resource_dispatcher_host_impl.cc"", ""status"": ""modified"", ""additions"": 21, ""deletions"": 0, ""changes"": 21, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/loader/resource_dispatcher_host_impl.cc?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -1187,6 +1187,20 @@ void ResourceDispatcherHostImpl::OnSyncLoad(\n                sync_result->routing_id());\n }\n \n+bool ResourceDispatcherHostImpl::IsRequestIDInUse(\n+    const GlobalRequestID& id) const {\n+  if (pending_loaders_.find(id) != pending_loaders_.end())\n+    return true;\n+  for (const auto& blocked_loaders : blocked_loaders_map_) {\n+    for (const auto& loader : *blocked_loaders.second.get()) {\n+      ResourceRequestInfoImpl* info = loader->GetRequestInfo();\n+      if (info->GetGlobalRequestID() == id)\n+        return true;\n+    }\n+  }\n+  return false;\n+}\n+\n void ResourceDispatcherHostImpl::UpdateRequestForTransfer(\n     int child_id,\n     int route_id,\n@@ -1281,6 +1295,13 @@ void ResourceDispatcherHostImpl::BeginRequest(\n   int process_type = filter_->process_type();\n   int child_id = filter_->child_id();\n \n+  // Reject request id that's currently in use.\n+  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {\n+    bad_message::ReceivedBadMessage(filter_,\n+                                    bad_message::RDH_INVALID_REQUEST_ID);\n+    return;\n+  }\n+\n   // PlzNavigate: reject invalid renderer main resource request.\n   if (IsBrowserSideNavigationEnabled() &&\n       IsResourceTypeFrame(request_data.resource_type) &&""}<_**next**_>{""sha"": ""29a82b248a109c7890fec7e0c2f5b6d2ff717d4c"", ""filename"": ""content/browser/loader/resource_dispatcher_host_impl.h"", ""status"": ""modified"", ""additions"": 2, ""deletions"": 0, ""changes"": 2, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.h"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/loader/resource_dispatcher_host_impl.h"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/loader/resource_dispatcher_host_impl.h?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -422,6 +422,8 @@ class CONTENT_EXPORT ResourceDispatcherHostImpl\n                   const ResourceHostMsg_Request& request_data,\n                   IPC::Message* sync_result);\n \n+  bool IsRequestIDInUse(const GlobalRequestID& id) const;\n+\n   // Update the ResourceRequestInfo and internal maps when a request is\n   // transferred from one process to another.\n   void UpdateRequestForTransfer(int child_id,""}<_**next**_>{""sha"": ""6714e9a086a737607f6947d6fdf8d1608742ee9e"", ""filename"": ""content/browser/security_exploit_browsertest.cc"", ""status"": ""modified"", ""additions"": 75, ""deletions"": 8, ""changes"": 83, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/security_exploit_browsertest.cc"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/content/browser/security_exploit_browsertest.cc"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/content/browser/security_exploit_browsertest.cc?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -23,6 +23,7 @@\n #include \""content/public/browser/content_browser_client.h\""\n #include \""content/public/browser/interstitial_page.h\""\n #include \""content/public/browser/interstitial_page_delegate.h\""\n+#include \""content/public/browser/resource_dispatcher_host.h\""\n #include \""content/public/browser/storage_partition.h\""\n #include \""content/public/common/appcache_info.h\""\n #include \""content/public/common/browser_side_navigation_policy.h\""\n@@ -37,13 +38,19 @@\n #include \""ipc/ipc_security_test_util.h\""\n #include \""net/dns/mock_host_resolver.h\""\n #include \""net/test/embedded_test_server/embedded_test_server.h\""\n+#include \""net/test/url_request/url_request_slow_download_job.h\""\n \n using IPC::IpcSecurityTestUtil;\n \n namespace content {\n \n namespace {\n \n+// This request id is used by tests that craft a\n+// ResourceHostMsg_RequestResource. The id is sufficiently large that it doesn't\n+// collide with ids used by previous navigation requests.\n+const int kRequestIdNotPreviouslyUsed = 10000;\n+\n // This is a helper function for the tests which attempt to create a\n // duplicate RenderViewHost or RenderWidgetHost. It tries to create two objects\n // with the same process and routing ids, which causes a collision.\n@@ -98,13 +105,11 @@ RenderViewHostImpl* PrepareToDuplicateHosts(Shell* shell,\n   return next_rfh->render_view_host();\n }\n \n-ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n+ResourceHostMsg_Request CreateXHRRequest(const char* url) {\n   ResourceHostMsg_Request request;\n   request.method = \""GET\"";\n-  request.url = GURL(\""http://bar.com/simple_page.html\"");\n-  request.first_party_for_cookies = GURL(origin);\n+  request.url = GURL(url);\n   request.referrer_policy = blink::WebReferrerPolicyDefault;\n-  request.headers = base::StringPrintf(\""Origin: %s\\r\\n\"", origin);\n   request.load_flags = 0;\n   request.origin_pid = 0;\n   request.resource_type = RESOURCE_TYPE_XHR;\n@@ -120,6 +125,49 @@ ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n   return request;\n }\n \n+ResourceHostMsg_Request CreateXHRRequestWithOrigin(const char* origin) {\n+  ResourceHostMsg_Request request =\n+      CreateXHRRequest(\""http://bar.com/simple_page.html\"");\n+  request.first_party_for_cookies = GURL(origin);\n+  request.headers = base::StringPrintf(\""Origin: %s\\r\\n\"", origin);\n+  return request;\n+}\n+\n+void TryCreateDuplicateRequestIds(Shell* shell, bool block_loaders) {\n+  NavigateToURL(shell, GURL(\""http://foo.com/simple_page.html\""));\n+  RenderFrameHost* rfh = shell->web_contents()->GetMainFrame();\n+\n+  if (block_loaders) {\n+    // Test the case where loaders are placed into blocked_loaders_map_.\n+    int child_id = rfh->GetProcess()->GetID();\n+    BrowserThread::PostTask(\n+        BrowserThread::IO, FROM_HERE,\n+        base::Bind(&ResourceDispatcherHost::BlockRequestsForRoute,\n+                   base::Unretained(ResourceDispatcherHost::Get()), child_id,\n+                   rfh->GetRoutingID()));\n+  }\n+\n+  // URLRequestSlowDownloadJob waits for another request to kFinishDownloadUrl\n+  // to finish all pending requests. It is never sent, so the following URL\n+  // blocks indefinitely, which is good because the request stays alive and the\n+  // test can try to reuse the request id without a race.\n+  const char* blocking_url = net::URLRequestSlowDownloadJob::kUnknownSizeUrl;\n+  ResourceHostMsg_Request request(CreateXHRRequest(blocking_url));\n+\n+  // Use the same request id twice.\n+  RenderProcessHostWatcher process_killed(\n+      rfh->GetProcess(), RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n+  IPC::IpcSecurityTestUtil::PwnMessageReceived(\n+      rfh->GetProcess()->GetChannel(),\n+      ResourceHostMsg_RequestResource(rfh->GetRoutingID(),\n+                                      kRequestIdNotPreviouslyUsed, request));\n+  IPC::IpcSecurityTestUtil::PwnMessageReceived(\n+      rfh->GetProcess()->GetChannel(),\n+      ResourceHostMsg_RequestResource(rfh->GetRoutingID(),\n+                                      kRequestIdNotPreviouslyUsed, request));\n+  process_killed.Wait();\n+}\n+\n }  // namespace\n \n \n@@ -145,6 +193,12 @@ class SecurityExploitBrowserTest : public ContentBrowserTest {\n             \"",EXCLUDE localhost\"");\n   }\n \n+  void SetUpOnMainThread() override {\n+    BrowserThread::PostTask(\n+        BrowserThread::IO, FROM_HERE,\n+        base::Bind(&net::URLRequestSlowDownloadJob::AddUrlHandler));\n+  }\n+\n  protected:\n   // Tests that a given file path sent in a ViewHostMsg_RunFileChooser will\n   // cause renderer to be killed.\n@@ -384,7 +438,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         chrome_origin_msg));\n     web_process_killed.Wait();\n   }\n@@ -404,7 +459,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         embedder_isolated_origin_msg));\n     web_process_killed.Wait();\n     SetBrowserClientForTesting(old_client);\n@@ -418,7 +474,8 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         invalid_origin_msg));\n     web_process_killed.Wait();\n   }\n@@ -431,10 +488,20 @@ IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidOriginHeaders) {\n         RenderProcessHostWatcher::WATCH_FOR_PROCESS_EXIT);\n     IPC::IpcSecurityTestUtil::PwnMessageReceived(\n         web_rfh->GetProcess()->GetChannel(),\n-        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(), 1,\n+        ResourceHostMsg_RequestResource(web_rfh->GetRoutingID(),\n+                                        kRequestIdNotPreviouslyUsed,\n                                         invalid_scheme_origin_msg));\n     web_process_killed.Wait();\n   }\n }\n \n+// Renderer process should not be able to create multiple requests with the same\n+// id.\n+IN_PROC_BROWSER_TEST_F(SecurityExploitBrowserTest, InvalidRequestId) {\n+  // Existing loader in pending_loaders_.\n+  TryCreateDuplicateRequestIds(shell(), false);\n+  // Existing loader in blocked_loaders_map_.\n+  TryCreateDuplicateRequestIds(shell(), true);\n+}\n+\n }  // namespace content""}<_**next**_>{""sha"": ""9f330134f0b052a4f6a8c6aa85e90c8230edaae6"", ""filename"": ""tools/metrics/histograms/histograms.xml"", ""status"": ""modified"", ""additions"": 1, ""deletions"": 0, ""changes"": 1, ""blob_url"": ""https://github.com/chromium/chromium/blob/1af4fada49c4f3890f16daac31d38379a9d782b2/tools/metrics/histograms/histograms.xml"", ""raw_url"": ""https://github.com/chromium/chromium/raw/1af4fada49c4f3890f16daac31d38379a9d782b2/tools/metrics/histograms/histograms.xml"", ""contents_url"": ""https://api.github.com/repos/chromium/chromium/contents/tools/metrics/histograms/histograms.xml?ref=1af4fada49c4f3890f16daac31d38379a9d782b2"", ""patch"": ""@@ -57944,6 +57944,7 @@ http://cs/file:chrome/histograms.xml - but prefer this file for new entries.\n   <int value=\""105\"" label=\""BDH_DEVICE_NOT_ALLOWED_FOR_ORIGIN\""/>\n   <int value=\""106\"" label=\""ACI_WRONG_STORAGE_PARTITION\""/>\n   <int value=\""107\"" label=\""RDHI_WRONG_STORAGE_PARTITION\""/>\n+  <int value=\""108\"" label=\""RDH_INVALID_REQUEST_ID\""/>\n </enum>\n \n <enum name=\""BadMessageReasonExtensions\"" type=\""int\"">""}","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
",C,"  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

",,,"@@ -1187,6 +1187,20 @@ void ResourceDispatcherHostImpl::OnSyncLoad(
                sync_result->routing_id());
 }
 
+bool ResourceDispatcherHostImpl::IsRequestIDInUse(
+    const GlobalRequestID& id) const {
+  if (pending_loaders_.find(id) != pending_loaders_.end())
+    return true;
+  for (const auto& blocked_loaders : blocked_loaders_map_) {
+    for (const auto& loader : *blocked_loaders.second.get()) {
+      ResourceRequestInfoImpl* info = loader->GetRequestInfo();
+      if (info->GetGlobalRequestID() == id)
+        return true;
+    }
+  }
+  return false;
+}
+
 void ResourceDispatcherHostImpl::UpdateRequestForTransfer(
     int child_id,
     int route_id,
@@ -1281,6 +1295,13 @@ void ResourceDispatcherHostImpl::BeginRequest(
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
+  // Reject request id that's currently in use.
+  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
+    bad_message::ReceivedBadMessage(filter_,
+                                    bad_message::RDH_INVALID_REQUEST_ID);
+    return;
+  }
+
   // PlzNavigate: reject invalid renderer main resource request.
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&",Chrome,1af4fada49c4f3890f16daac31d38379a9d782b2,f5d1a9c599c7cd6833455eacb4ec694a47c1f1b0,1,"void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
//fix_flaw_line_below:
//  // Reject request id that's currently in use.
//fix_flaw_line_below:
//  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
//fix_flaw_line_below:
//    bad_message::ReceivedBadMessage(filter_,
//fix_flaw_line_below:
//                                    bad_message::RDH_INVALID_REQUEST_ID);
//fix_flaw_line_below:
//    return;
//fix_flaw_line_below:
//  }
//fix_flaw_line_below:
//
   // PlzNavigate: reject invalid renderer main resource request.
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  // Reject invalid priority.
  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  // If we crash here, figure out what URL the renderer was requesting.
  // http://crbug.com/91398
  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  // If the request that's coming in is being transferred from another process,
  // we want to reuse and resume the old loader rather than start a new one.
  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    // If the request is transferring to a new process, we can update our
    // state and let it resume with its existing ResourceHandlers.
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  // http://crbug.com/90971
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  // Parse the headers before calling ShouldServiceRequest, so that they are
  // available to be validated.
  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  // Allow the observer to block/handle the request.
  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  // Construct the request.
  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  // If the request is a MAIN_FRAME request, the first-party URL gets updated on
  // redirects.
  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  // Resolve elements from request_body and prepare upload data.
  if (request_data.request_body.get()) {
    // |blob_context| could be null when the request is from the plugins because
    // ResourceMessageFilters created in PluginProcessHost don't have the blob
    // context.
    if (blob_context) {
      // Attaches the BlobDataHandles to request_body not to free the blobs and
      // any attached shareable files until upload completion. These data will
      // be used in UploadDataStream and ServiceWorkerURLRequestJob.
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  // Raw headers are sensitive, as they include Cookie/Set-Cookie, so only
  // allow requesting them if requester has ReadRawCookies permission.
  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    // TODO: crbug.com/523063 can we call bad_message::ReceivedBadMessage here?
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    // Prevent third-party image content from prompting for login, as this
    // is often a scam to extract credentials for another domain from the user.
    // Only block image loads, as the attack applies largely to the ""src""
    // property of the <img> tag. It is common for web properties to allow
    // untrusted values for <img src>; this is considered a fair thing for an
    // HTML sanitizer to do. Conversely, any HTML sanitizer that didn't
    // filter sources for <script>, <link>, <embed>, <object>, <iframe> tags
    // would be considered vulnerable in and of itself.
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  // Sync loads should have maximum priority and should be the only
  // requets that have the ignore limits flag set.
  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  // Make extra info and read footer (contains request ID).
  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  // Request takes ownership.
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    // Hang on to a reference to ensure the blob is not released prior
    // to the job being started.
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  // Initialize the service worker handler for the request. We don't use
  // ServiceWorker for synchronous loads to avoid renderer deadlocks.
  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  // Have the appcache associate its extra info with the request.
  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
",186239,"void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","void ResourceDispatcherHostImpl::BeginRequest(
    int request_id,
    const ResourceHostMsg_Request& request_data,
    IPC::Message* sync_result,  // only valid for sync
    int route_id) {
   int process_type = filter_->process_type();
   int child_id = filter_->child_id();
 
  // Reject request id that's currently in use.
  if (IsRequestIDInUse(GlobalRequestID(child_id, request_id))) {
    bad_message::ReceivedBadMessage(filter_,
                                    bad_message::RDH_INVALID_REQUEST_ID);
    return;
  }

   if (IsBrowserSideNavigationEnabled() &&
       IsResourceTypeFrame(request_data.resource_type) &&
      !request_data.url.SchemeIs(url::kBlobScheme)) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_URL);
    return;
  }

  if (request_data.priority < net::MINIMUM_PRIORITY ||
      request_data.priority > net::MAXIMUM_PRIORITY) {
    bad_message::ReceivedBadMessage(filter_, bad_message::RDH_INVALID_PRIORITY);
    return;
  }

  char url_buf[128];
  base::strlcpy(url_buf, request_data.url.spec().c_str(), arraysize(url_buf));
  base::debug::Alias(url_buf);

  LoaderMap::iterator it = pending_loaders_.find(
      GlobalRequestID(request_data.transferred_request_child_id,
                      request_data.transferred_request_request_id));
  if (it != pending_loaders_.end()) {
    if (it->second->is_transferring()) {
      ResourceLoader* deferred_loader = it->second.get();
      UpdateRequestForTransfer(child_id, route_id, request_id,
                               request_data, it);
      deferred_loader->CompleteTransfer();
    } else {
      bad_message::ReceivedBadMessage(
          filter_, bad_message::RDH_REQUEST_NOT_TRANSFERRING);
    }
    return;
  }

  ResourceContext* resource_context = NULL;
  net::URLRequestContext* request_context = NULL;
  filter_->GetContexts(request_data.resource_type, request_data.origin_pid,
                       &resource_context, &request_context);
  CHECK(ContainsKey(active_resource_contexts_, resource_context));

  net::HttpRequestHeaders headers;
  headers.AddHeadersFromString(request_data.headers);

  if (is_shutdown_ ||
      !ShouldServiceRequest(process_type, child_id, request_data, headers,
                            filter_, resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  if (delegate_ && !delegate_->ShouldBeginRequest(request_data.method,
                                                  request_data.url,
                                                  request_data.resource_type,
                                                  resource_context)) {
    AbortRequestBeforeItStarts(filter_, sync_result, request_id);
    return;
  }

  scoped_ptr<net::URLRequest> new_request = request_context->CreateRequest(
      request_data.url, request_data.priority, NULL);

  new_request->set_method(request_data.method);
  new_request->set_first_party_for_cookies(
      request_data.first_party_for_cookies);
  new_request->set_initiator(request_data.request_initiator);

  if (request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME) {
    new_request->set_first_party_url_policy(
        net::URLRequest::UPDATE_FIRST_PARTY_URL_ON_REDIRECT);
  }

  const Referrer referrer(request_data.referrer, request_data.referrer_policy);
  SetReferrerForRequest(new_request.get(), referrer);

  new_request->SetExtraRequestHeaders(headers);

  storage::BlobStorageContext* blob_context =
      GetBlobStorageContext(filter_->blob_storage_context());
  if (request_data.request_body.get()) {
    if (blob_context) {
      AttachRequestBodyBlobDataHandles(
          request_data.request_body.get(),
          blob_context);
    }
    new_request->set_upload(UploadDataStreamBuilder::Build(
        request_data.request_body.get(),
        blob_context,
        filter_->file_system_context(),
        BrowserThread::GetMessageLoopProxyForThread(BrowserThread::FILE)
            .get()));
  }

  bool allow_download = request_data.allow_download &&
      IsResourceTypeFrame(request_data.resource_type);
  bool do_not_prompt_for_login = request_data.do_not_prompt_for_login;
  bool is_sync_load = sync_result != NULL;

  ChildProcessSecurityPolicyImpl* policy =
      ChildProcessSecurityPolicyImpl::GetInstance();
  bool report_raw_headers = request_data.report_raw_headers;
  if (report_raw_headers && !policy->CanReadRawCookies(child_id)) {
    VLOG(1) << ""Denied unauthorized request for raw headers"";
    report_raw_headers = false;
  }
  int load_flags =
      BuildLoadFlagsForRequest(request_data, child_id, is_sync_load);
  if (request_data.resource_type == RESOURCE_TYPE_PREFETCH ||
      request_data.resource_type == RESOURCE_TYPE_FAVICON) {
    do_not_prompt_for_login = true;
  }
  if (request_data.resource_type == RESOURCE_TYPE_IMAGE &&
      HTTP_AUTH_RELATION_BLOCKED_CROSS ==
          HttpAuthRelationTypeOf(request_data.url,
                                 request_data.first_party_for_cookies)) {
    do_not_prompt_for_login = true;
    load_flags |= net::LOAD_DO_NOT_USE_EMBEDDED_IDENTITY;
  }

  bool support_async_revalidation =
      !is_sync_load && async_revalidation_manager_ &&
      AsyncRevalidationManager::QualifiesForAsyncRevalidation(request_data);

  if (support_async_revalidation)
    load_flags |= net::LOAD_SUPPORT_ASYNC_REVALIDATION;

  if (is_sync_load) {
    DCHECK_EQ(request_data.priority, net::MAXIMUM_PRIORITY);
    DCHECK_NE(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  } else {
    DCHECK_EQ(load_flags & net::LOAD_IGNORE_LIMITS, 0);
  }
  new_request->SetLoadFlags(load_flags);

  ResourceRequestInfoImpl* extra_info = new ResourceRequestInfoImpl(
      process_type, child_id, route_id,
      -1,  // frame_tree_node_id
      request_data.origin_pid, request_id, request_data.render_frame_id,
      request_data.is_main_frame, request_data.parent_is_main_frame,
      request_data.resource_type, request_data.transition_type,
      request_data.should_replace_current_entry,
      false,  // is download
      false,  // is stream
      allow_download, request_data.has_user_gesture,
      request_data.enable_load_timing, request_data.enable_upload_progress,
      do_not_prompt_for_login, request_data.referrer_policy,
      request_data.visiblity_state, resource_context, filter_->GetWeakPtr(),
      report_raw_headers, !is_sync_load,
      IsUsingLoFi(request_data.lofi_state, delegate_, *new_request,
                  resource_context,
                  request_data.resource_type == RESOURCE_TYPE_MAIN_FRAME),
      support_async_revalidation ? request_data.headers : std::string());
  extra_info->AssociateWithRequest(new_request.get());

  if (new_request->url().SchemeIs(url::kBlobScheme)) {
    storage::BlobProtocolHandler::SetRequestedBlobDataHandle(
        new_request.get(),
        filter_->blob_storage_context()->context()->GetBlobDataFromPublicURL(
            new_request->url()));
  }

  const bool should_skip_service_worker =
      request_data.skip_service_worker || is_sync_load;
  ServiceWorkerRequestHandler::InitializeHandler(
      new_request.get(), filter_->service_worker_context(), blob_context,
      child_id, request_data.service_worker_provider_id,
      should_skip_service_worker,
      request_data.fetch_request_mode, request_data.fetch_credentials_mode,
      request_data.fetch_redirect_mode, request_data.resource_type,
      request_data.fetch_request_context_type, request_data.fetch_frame_type,
      request_data.request_body);

  if (base::CommandLine::ForCurrentProcess()->HasSwitch(
          switches::kEnableExperimentalWebPlatformFeatures)) {
    ForeignFetchRequestHandler::InitializeHandler(
        new_request.get(), filter_->service_worker_context(), blob_context,
        child_id, request_data.service_worker_provider_id,
        should_skip_service_worker,
        request_data.fetch_request_mode, request_data.fetch_credentials_mode,
        request_data.fetch_redirect_mode, request_data.resource_type,
        request_data.fetch_request_context_type, request_data.fetch_frame_type,
        request_data.request_body);
  }

  AppCacheInterceptor::SetExtraRequestInfo(
      new_request.get(), filter_->appcache_service(), child_id,
      request_data.appcache_host_id, request_data.resource_type,
      request_data.should_reset_appcache);

  scoped_ptr<ResourceHandler> handler(
       CreateResourceHandler(
           new_request.get(),
           request_data, sync_result, route_id, process_type, child_id,
           resource_context));

  if (handler)
    BeginRequestInternal(std::move(new_request), std::move(handler));
}
","The `BeginRequest` function in the `ResourceDispatcherHostImpl` class initiates a resource request by validating input parameters, managing request priorities, and setting up the necessary contexts and headers. It checks for browser-side navigation and invalid URL schemes, ensuring that the request adheres to specified priority limits. If the request is valid, it creates a new `URLRequest`, configures it with headers, referrer information, and any associated request body, while also handling security policies and potential service worker interactions. Finally, it creates a resource handler and begins the request process, ensuring proper management of asynchronous operations and resource contexts.","The code defines a method that initiates a resource request by validating various parameters, such as URL schemes and priority levels, to ensure they meet specific criteria. It checks for conditions like browser-side navigation and whether the request is transferring. If valid, it creates a new request, setting necessary headers, referrer information, and handling any associated data. The method also manages security policies and interactions with service workers, while preparing to handle asynchronous operations. Finally, it establishes a resource handler to process the request, ensuring proper context and state management throughout the operation.","The vulnerability in the original code allowed a compromised renderer to reuse request IDs, potentially leading to unauthorized access or manipulation of resource requests. This was addressed in the updated code by introducing a check that rejects any request ID currently in use, utilizing the function that verifies if a request ID is active. If a duplicate request ID is detected, the system responds with an error message, effectively preventing the reuse of IDs and enhancing security. This change ensures that each request is uniquely identified, mitigating risks associated with request ID collisions.","The vulnerability in the original code allowed a compromised renderer to reuse request IDs, which could lead to unauthorized access or manipulation of resource requests, potentially compromising the integrity of the system. The fix implemented in the updated code introduces a validation step that checks if a request ID is already in use before processing a new request. If a duplicate ID is detected, the system rejects the request, thereby preventing any unauthorized reuse. This enhancement ensures that each request is uniquely identified, significantly improving the security of the request handling process.","The vulnerability was fixed by adding a check at the beginning of the request handling process to determine if the request ID is already in use. This is accomplished through the function `IsRequestIDInUse`, which verifies the uniqueness of the request ID by checking against existing active requests. If the ID is found to be in use, the code responds with an error message using `bad_message::ReceivedBadMessage`, specifically indicating an invalid request ID. This change effectively blocks any attempts by a compromised renderer to reuse request IDs, thereby enhancing the security of the request handling mechanism.","The vulnerability was fixed by implementing a validation step that checks whether a request ID is already in use before processing any new requests. This additional check ensures that if a request ID is found to be active, the system will reject the new request and respond with an appropriate error message. By enforcing this uniqueness requirement for request IDs, the fix effectively prevents any unauthorized reuse of IDs by compromised components, thereby enhancing the overall security and integrity of the request handling process."
